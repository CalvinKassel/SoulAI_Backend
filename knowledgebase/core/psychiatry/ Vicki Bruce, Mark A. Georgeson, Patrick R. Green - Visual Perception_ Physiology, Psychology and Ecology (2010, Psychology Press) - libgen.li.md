VISUAL PERCEPTION

To the memory of Rodney Green and To Jan and Mimi

VISUAL PERCEPTION PHYSIOLOGY, PSYCHOLOGY, & ECOLOGY FOURTH EDITION

Vicki Bruce College of Humanities and Social Science, University of
Edinburgh, UK

Patrick R. Green School of Mathematical and Computer Sciences,
Heriot-Watt University, UK

Mark A. Georgeson Neurosciences Research Institute, Aston University, UK

First published 2003 by Psychology Press 27 Church Road, Hove, BN3 2FA
Simultaneously published in the USA and Canada by Psychology Press 711
Third Avenue, New York, NY 10017 Psychology Press is a member of the
Taylor & Francis Group, an Informa business Reprinted 2004, 2006 and
2010 © 2003 Psychology Press Cover design by Lisa Dynan Indexes compiled
by Lewis Derrick Typeset in Times by RefineCatch Limited, Bungay,
Suffolk

All rights reserved. No part of this book may be reprinted or reproduced
or utilised in any form or by any electronic, mechanical, or other
means, now known or hereafter invented, including photocopying and
recording, or in any information storage or retrieval system, without
permission in writing from the publishers. British Library Cataloguing
in Publication Data A catalogue record for this book is available from
the British Library Library of Congress Cataloging-in-Publication Data
Bruce, Vicki. Visual perception: physiology, psychology, and
ecology/Vicki Bruce, Patrick R. Green, Mark A. Georgeson---4th ed.
p. cm. Rev. ed of: Visual perception, physiology, psychology and
ecology. c1985. Includes bibliographical references and index. ISBN
1-84169-237-9---ISBN 1-84169-238-7 (pbk.) 1. Visual perception. I.
Green, Patrick R. II. Georgeson, Mark A. III. Bruce, Vicki. Visual
perception, physiology, psychology, and ecology. IV. Title. BF241.B78
2003 152.14---dc21 2002153947 ISBN 978-1-84169--238--8 (pbk)

Contents Preface to the First Edition

Dynamics and feedback in the visual pathway 69 Conclusions 74

vii ix

Preface to the Second Edition Preface to the Third Edition

x

PART II

Preface to the Fourth Edition

xi

Chapter 4: Approaches to the Psychology of Visual Perception 77 Marr's
theory of visual perception 80 Connectionist models of visual perception
82

PART I THE PHYSIOLOGICAL BASIS OF VISUAL PERCEPTION Chapter 1: Light and
Eyes 3 Light and the information it carries 4 The evolution of
light-sensitive structures 7 The adaptive radiation of the vertebrate
eye 14 Conclusions 23

VISION FOR AWARENESS

Chapter 5: Images, Filters, and Features: The Primal Sketch 85 Light,
surfaces, and vision 85 The primal sketch 86 Multiple spatial filters 99
Other routes to the primal sketch 103 Energy models for feature
detection 107 Some unresolved questions in multi-scale vision 111
Illusory contours and orientation coding 113 Summary 117

Chapter 2: The Neurophysiology of the Retina 25 The retina of the
horseshoe crab 25 The vertebrate retina 28 The retina as a filter 37
Conclusions 42

Chapter 6: Perceptual Organisation 119 Ambiguous pictures 120 Gestalt
laws of organisation 123 Concealment and advertisement 129 Perceptual
organisation in other species Why do the Gestalt laws work? 134

Chapter 3: Visual Pathways in the Brain 43 The lateral geniculate
nucleus 45 The striate cortex 47 Beyond the striate cortex 57 The human
brain: Two visual pathways? 62 v

132

vi

CONTENTS

Artificial intelligence approaches to grouping 134 Finding texture
boundaries 140 The neurophysiology of texture segmentation 146
Suppressive surrounds: Psychophysics 151 Beyond filters: Contours and
surfaces 159 Conclusions 167 Chapter 7: Seeing a 3-D World 169 Binocular
stereopsis 171 Pictorial cues to depth 187 Depth from motion 197
Integrating depth cues 201 Conclusions 206 Chapter 8: The Computation of
Image Motion 209 First principles: Motion as orientation in space-time
210 Motion detectors 213 Encoding local velocity 221 A hierarchy of
processing in the motion energy system: From V1 to MT 227 Global motion
234 Second-order and "long-range" motions 240 The integration of motion
measurements 247 Motion from feature tracking 253 Spatial variations in
the velocity field 255 Position, motion, and eye movements 258
Conclusions 264 Chapter 9: Object Recognition 265 Simple mechanisms of
recognition 266 More complex recognition processes 268 Template matching
268 Feature analysis 270 Structural descriptions 272 Marr and
Nishihara's theory of object recognition 276 Beyond generalised cones:
Recognition by components 281 Viewpoint-dependent recognition 287
Discriminating within categories of objects: The case of face
recognition 289 Static vs moving forms 297 Conclusions 298

PART III

VISION FOR ACTION

Chapter 10: Introduction to the Ecological Approach to Visual Perception
301 J.J. Gibson's theory of perception 302 The control of human action
311 Conclusions 314 Chapter 11: Optic Flow and Locomotion 315 Optic flow
and retinal flow 315 The visual control of insect flight 316 Visual
control of posture and locomotion 327 Conclusions 343 Chapter 12: Vision
and the Timing of Actions 345 Scaling an action with distance 345 Timing
actions from optic flow 348 Continuous visual control 358 Predictive
control 363 Conclusions 365 Chapter 13: Perception of the Social World
367 Perceiving other animals' behaviour 367 Human perception of animate
motion 377 Human face perception 388 Conclusions 402 PART IV

CONCLUSIONS

Chapter 14: Contrasting Theories of Visual Perception 405 Cognitive and
ecological theories of perception 405 Active vision 413 Conclusions 416
References

419

Appendix: On-line Resources for Perception and Vision Science 455
Glossary 457 Author Index

465

Subject Index 475

Preface to the First Edition

the role of vision in animal social behaviour and human event
perception. We have assumed that our readers will have some prior
knowledge of basic neurophysiology (for Part I of the book) and
experimental psychology (for Parts II and III), such as might have been
gained from an introductory psychology course. Our choice of topics
differs in some respects from that of most textbooks of visual
perception. Some topics are excluded. In particular, we do not discuss
sensory physiology and psychophysics in as much detail as other texts.
We do include material on animal perception which would usually be found
in an ethology text, and consider in detail research on human and animal
reactions to patterns of optic flow. Our choice of topics is governed by
a consistent theoretical outlook. We have been committed to the value of
an ecological approach in its wider sense; that espoused by David Marr
as much as by James Gibson, who both argued for the need to consider the
structure of the world in which an animal or person lives and perceives.
It is this commitment that has guided the overall plan of the book. We
have attempted to go beyond exposition of theoretical and empirical
developments in each of our three areas, and to discuss promising issues
for further research and to present an analysis

Our primary aim in writing this book has been to present a wide range of
recent evidence and theoretical developments in the field of visual
perception to an advanced undergraduate readership. The material covered
is drawn from three areas: the neurophysiological analysis of vision,
the "computational" accounts of vision that have grown out of
traditional experimental approaches and artificial intelligence, and
work on vision which shares at least some of J. J. Gibson's "ecological"
framework. In the first part of the book we discuss the evolution of
different types of eye, the neurophysiological organisation of visual
pathways, particularly in mammals, and contrasting theoretical
interpretations of single-unit responses in visual systems. We turn in
the second part to psychological and computational models of the
interpretation of information in retinal images, discussing perceptual
organisation, the perception of depth and of movement, and pattern and
object recognition. The contribution of David Marr's work to these
problems is emphasised. In the third part we discuss how extended
patterns of light can provide information for the control of action and
to specify events; topics covered are the visual guidance of animal and
human locomotion, theories of the control of action, and

vii

viii

PREFACE TO THE FIRST EDITION

and critique of the theories we discuss. We have been most speculative
about future possibilities for the ecological approach in Chapters 11
and 12, and address global theoretical issues most fully in Chapter 13.
Our final conclusion is that an ecological perspective offers valuable
insights, but that a "direct" theory of perception is not adequate.
These are exciting times in the study of visual perception, and we
believe that there is much to be gained by cross-fertilisation between
research areas. There are real and important issues dividing different
theoretical camps, which must continue to be debated vigorously, but it
is also worthwhile to mark out some potential common ground. Although
the readers we have in mind are advanced psychology students, we have
designed the book to be useful also to zoology students specialising in
neurophysiology and in animal behaviour, and particularly to students in
the increasingly popular courses that combine zoology and psychology. We
hope that research workers may also find the book useful; although it
will surely be superficial in their areas of primary interest, it may be
helpful in approaching the literature in adjoining areas.

Our manuscript has been improved considerably as a result of critical
comments from a number of people. Our colleague Alan Dodds read the
entire manuscript, and Robin Stevens read parts. A number of anonymous
reviewers furnished further comments. We would like to thank all these
people most sincerely. They spotted blunders, and suggested ways of
making points more clearly, which would not have occurred to us alone;
any remaining errors are, of course, our own responsibility. Mike Burton
helped us with our own, and other people's, mathematical reasoning. He
was also one of several people who introduced us to the mysteries of
word-processing; Chris Blunsdon, Roger Henry, and Anne Lomax are the
others. Roger Somerville (WIDES Advertising and Design) drew all those
figures not otherwise acknowledged, and thus contributed enormously to
the text. Sam Grainger took many photographs, and E. Hildreth, D. H.
Hubel, Fergus Campbell, John Frisby, and Paul Ekman all provided
photographs for us to use. Penny Radcliffe typed endless letters and
helped in many other ways when we ourselves were flagging. Mike Forster
and Rohays Perry encouraged us to see this project through, and gave us
editorial help.

Preface to the Second Edition

throughout the text, and particularly in Chapter 3 where we have brought
our treatment of the physiology of vision up to date. We have also
emphasised the recent convergence between theoretical developments and
physiological evidence at a number of points in Parts II and III. Our
thanks are due again to all those who helped us with the first edition,
and additionally to those who reviewed or otherwise commented on the
text after its publication; they have done much to encourage us to
embark upon this revision. We thank Mark Georgeson and Roger Watt for
their expert scrutiny of our draft second edition; their suggestions
have helped us to improve the accuracy and thoroughness of our new
material. We also thank Mark Georgeson for providing a valuable
illustration. Our thanks are due again to Mike Burton, who gave us
considerable help in interpreting the connectionist literature. Any
errors remaining after help from these colleagues are, of course,
entirely our own reponsibility. Finally, we thank Mike Forster, Rohays
Perry, and Melanie Tarrant for their support and editorial work, which
has made production this time as smooth as last.

As the first edition went to press in 1984, research on visual
perception was progressing rapidly in a number of areas. The second wave
of research inspired by Marr's work was just developing; research into
the computation of optical flow was already accelerating; and the
connectionist revival was gathering momentum. We have tried to mention
all these developments in our new edition by including three (largely)
new chapters. In Chapter 5, we bring our earlier discussion of Marr and
Hildreth's theory of edge detection together with the more recent work
of Watt and Morgan to review approaches to raw primal sketch
computation. Chapter 9 is entirely new, and in it we show how
"connectionist" modelling can be applied to the problems raised in the
second part of the book. Finally, Chapter 13 considers accounts of how
the optic flow field, so central to the third part of the book, could
actually be computed by the visual systems of different species. In
addition to these new chapters, we have reorganised the second half of
Part III somewhat, and have considerably expanded our discussion of
imprinting and face perception in the resulting Chapter 16. We have
updated other material

ix

Preface to the Third Edition

Parts I and II is more technically demanding than before, but we think
more useful as a result. We have tried always to provide verbal
descriptions alongside any equations, so that those for whom
mathematical expressions are a hindrance rather than a help should be
able to skip the formulae but retain the sense. In preparing the final
version we have been helped by careful comments from several reviewers,
including George Mather and Mike Harris, and by suggestions from many
colleagues, including John Wann. We thank them sincerely for their
efforts, but the blame for remaining imperfections lies with us. We are
also grateful to the many colleagues in our three institutions whose
efforts maintain the e-mail links on which we have relied so heavily in
preparing this book. It is a pleasure to thank Mike Forster, Rohays
Perry, Tanya Sagoo, and Caroline Osborne for their editorial support,
which has ensured that production of the book has gone just as smoothly
as before. Finally we thank our long-suffering families, friends, and
dog, who have been deprived of company, food, and walks during long
hours of writing. The new author is grateful to Jan Georgeson for tea
and (some) sympathy over the months of midnight oil, and dedicates his
efforts on the book to Joel (9) and Luke (7), the students of the future
who may one day read it.

Six years have passed since the publication of the second edition of
this book, and the field has continued to flourish. In preparing this
extensively updated edition, the two authors of the earlier editions are
very pleased to include Mark Georgeson as an additional author. Mark was
an enthusiastic reviewer of earlier versions, and has been able to add
considerable expertise in early visual processing mechanisms to this new
edition. In this third edition, the chapter topics and overall structure
are largely unchanged from the second edition. The one exception is that
we have moved the chapter on motion processing from Part III to Part II,
where it sits more easily given its strong computational flavour. In
some individual chapters, however, the treatment of material is
radically different from that in previous editions. The structure of
Chapter 3 has changed considerably to accommodate an updated account of
the primate visual pathway, and in Chapters 5, 6, 7, and 8 recent
advances in psychophysical analysis and computational modelling of early
visual processing have made major revisions necessary. Throughout these
chapters, we have aimed to provide the fuller explanations of basic
principles such as spatial and space-time filtering which are needed to
understand recent advances in the field. As a result, some material in

x

Preface to the Fourth Edition

ence by selecting topics such as the distinction between dorsal and
ventral pathways, and the dynamics of cortical processing. In Part II,
we have covered recent developments in the perception of edges,
surfaces, depth, and motion, and in object recognition. Here we
emphasise the increasingly close integration of psychophysical and
physiological evidence with computational modelling of all these
processes. Part III has been reorganised more extensively. We no longer
attempt to review comprehensively work on vision within the Gibsonian
tradition, but instead concentrate on the roles of vision in controlling
action and in social activity. The chapters in this part deal with optic
flow and locomotion, the timing of actions, and the perception of the
social world, each one drawing on both animal and human evidence. We are
grateful to our many colleagues and students who have commented on the
last edition of the book, or have helped us in other ways in the
development of our thinking. We particularly thank Anne Hillstrom, Mike
Harris, George Mather, and an anonymous reviewer for their comments on
the manuscript, and Tim Meese for discussions about mechanisms for the
perception of global motion, and other topics. Any errors in the book
that remain after their help are of course our own responsibility. We
thank Mike Forster,

A look back over the 18 years that have passed since the first edition
of this book went to press reveals a wealth of advances in vision
science, a subject that did not even have a distinct existence in 1984.
In preparing this new edition, we have become keenly aware of how this
progress has accelerated in the past few years, and of the challenge of
keeping up with new developments. As far as the overall plan of the book
is concerned, we no longer rely on a contrast between David Marr's and
James Gibson's theories of visual perception, although we still provide
a historical introduction to each one. The two approaches have each
developed and diversified, and their boundaries have become increasingly
blurred. We have instead organised the book around the distinction
between two functions of vision---to provide awareness of the
surrounding world, and to control movements of the body. This contrast
is introduced in Part I, in the context of visual neuroscience, and
defines the themes of Parts II and III. In Part IV we return to consider
just how far a distinction between these two functions of vision can be
maintained. Turning to more specific changes, the most extensively
revised and enlarged chapter in Part I is Chapter 3, which deals with
visual processing in the brain. We have tried to capture something of
the recent prodigious growth of visual neurosci-

xi

xii

PREFACE TO THE FOURTH EDITION

Rohays Perry, Mandy Collison, Lucy Farr, Susan Rudkin, and everyone else
at Psychology Press involved with the book for encouraging us to go
ahead with writing a new edition, and for

their exemplary patience and efficiency in overseeing its production.
Finally, we thank Joel and Luke Georgeson for their help in preparing
new figures.

Part I

The Physiological Basis of Visual Perception

Page Intentionally Left Blank

1 Light and Eyes

vided by chemical substances diffusing through air or water. Another is
mechanical energy, whether pressure on the body surface, forces on the
limbs and muscles, or waves of sound pressure in air or water. Further
information sources, to which some animals are sensitive but people
probably are not, are electric and magnetic fields. An animal sensitive
to diffusing chemicals can detect the presence of nearby food or
predators, but often cannot pinpoint their exact location, and cannot
detect the layout of its inanimate surroundings. Pressure on the skin
and mechanical forces on the limbs can provide information about the
environment in immediate contact with an animal, while sound can provide
information about more distant animals but not usually about distant
inanimate structures. Sensitivity to diffusing chemicals and to
mechanical energy gives an animal considerable perceptual abilities, but
leaves it unable to obtain information rapidly about either its
inanimate world or about silent animals at a distance from itself. The
form of energy that can provide these kinds of information is light, and
consequently most animals have some ability to perceive their
surroundings through vision. The only large animals able to move about
rapidly without vision are bats, dolphins and other cetaceans, which use
an echolocation system based on ultrasonic cries

All organisms, whether bacteria, oak trees, or whales, must be adapted
to their environments if they are to survive and reproduce. The
structure and physiology of organisms are not fixed at the start of
life; to some extent, adjustments to changes in the environment can
occur so as to "fine-tune" the organism's adaptation. One way of
achieving this is through the regulation of growth processes, as when
plants grow so that their leaves face the strongest available light.
Another way, which is much more rapid and is only available to animals,
is movement of the body by contraction of muscles. If the movement of an
animal's body is to adapt it to its environment, it must be regulated,
or guided, by the environment. Thus the swimming movements of a fish's
body, tail, and fins are regulated so as to bring it into contact with
food and to avoid obstacles; or the movement of a person's throat,
tongue, and lips in speaking are regulated by the speech of other
people, linguistic rules and so on. In order for its movement to be
regulated by the environment, an animal must be able to detect
structures and events in its surroundings. We call this ability
perception, and it in turn requires that an animal be sensitive to at
least one form of energy that can provide information about the
environment. One source of information is pro-

3

4

VISUAL PERCEPTION

(Griffin, 1958), and some fish species living in murky water, which
detect objects in their surroundings by distortions of their own
electric fields (Heiligenberg, 1973). We will begin our discussion of
visual perception in animals and people by considering first the
physical nature of light and then how the environment structures the
light that reaches an observer.

LIGHT AND THE INFORMATION IT CARRIES Light is one form of
electromagnetic radiation; a mode of propagation of energy through space
which includes radio waves, radiant heat, gamma rays, and X-rays. One
way in which we can picture the nature of electromagnetic radiation is
as a pattern of waves propagated through an imaginary medium with a
velocity of 3 × 108 m/s in a vacuum. Its wavelength ranges from hundreds
of metres in the case of radio waves to 10−12 or 10−13 m in the case of
cosmic rays. Only a very small part of this range is visible; for human
beings,

The spectrum of electromagnetic radiation. Wavelengths are given in
nanometres (1 nm = 10−9 m). The visible part of the spectrum is shown on
the right, with the perceived colours of different wavelengths of light.

radiation with wavelengths between 400 and 700 nanometres (1 nm = 10−9
m) can be seen (Figure 1.1). Light containing only a single wavelength
is called monochromatic. If monochromatic light falls on our eyes, we
perceive a colour that corresponds to its wavelength, in the way shown
in Figure 1.1. For some purposes, however, the model of electromagnetic
radiation as a wave is not appropriate and we must instead treat it as a
stream of tiny wave-like particles called photons travelling in a
straight line at the speed of light. Each photon consists of a quantum
of energy (the shorter the wavelength of the light the larger the energy
quantum), which is given up as it strikes another particle. We need
these two conceptions of the nature of electromagnetic radiation because
nothing in our experience is analogous to the actual nature of it and we
must make do with two imperfect analogies at the same time. These
problems are of no concern in understanding how light is propagated
around the environment. For these purposes, we can think of light as
made up of rays, which vary in both intensity and wavelength. Rays are
emitted from

1. LIGHT AND EYES

light sources and, in a vacuum, travel in a straight line without
attenuation. A vacuum is not a congenial environment for animals,
however, and the fate of light rays travelling through natural habitats
is more complex. First, as light passes through a medium, even a
transparent one such as air or water, it undergoes absorption, as
photons collide with particles of matter, give up their energy and
disappear. Absorption is much stronger in water than in air and even in
the clearest oceans there is no detectable sunlight below about 1000
metres. Longer wavelengths are absorbed more strongly, so that available
light becomes progressively bluer in deeper water. Second, light is
diffracted as it passes through a transparent or translucent medium. Its
energy is not absorbed, but instead rays are scattered on striking small
particles of matter. Diffraction of sunlight by the atmosphere is the
reason why the daytime sky is bright; without an atmosphere, the sky
would be dark, as it is on the moon. The blue colour of the sky arises
because light of shorter wavelengths is scattered more, and so
predominates in the light reaching us from the sky. Third, the velocity
of light is lower when it passes through a transparent medium than when
it passes through a vacuum. The greater the optical density of the
medium, the lower is the velocity of light. When rays of light pass from
a medium of one optical density to a medium of a different density, this
change of velocity causes them to be bent, or refracted (unless they
strike

5

the boundary between the two media perpendicularly). Refraction
therefore occurs at boundaries such as those between air and water, or
air and glass, and we will consider it in more detail when we describe
the structure of eyes. Finally, when light strikes an opaque surface,
some of its energy is absorbed and some of it is reflected. A dark
surface absorbs most of the light falling on it and reflects little,
while a light one does the opposite. The way surfaces reflect light
varies in two important ways. First, the texture of a surface determines
how coherently it reflects light. A perfectly smooth surface such as a
mirror reflects light uniformly, but most natural surfaces have a
rougher texture, made up of a mosaic of tiny reflecting surfaces set at
different angles. Light striking such a surface is therefore reflected
in an incoherent way (see Figure 1.2). Second, a surface may reflect
some wavelengths more strongly than others, so that the spectral
composition of the reflected light (the relative proportions of
wavelengths it contains) differs from that of the incident light. A
leaf, for example, absorbs more (and hence reflects less) red light than
light of other wavelengths. Note that light is never monochromatic in
natural circumstances, and reflection changes the relative proportions
of the different wavelengths that it contains. The relationship between
the spectral composition of reflected light and the perceived colour of
a surface is a very complex one, to which we will return later. Now that
we have described the nature of light

Regular reflection of rays of light from a polished surface such as a
mirror (a) and irregular reflection from a textured surface (b).

6

VISUAL PERCEPTION

and the processes governing its travel through space, we turn to ask how
it carries information for animals about their environments. A useful
concept in understanding this is the ambient optic array, a term coined
by Gibson (1966). Imagine an environment illuminated by sunlight and
therefore filled with rays of light travelling between surfaces. At any
point, light will converge from all directions, and we can imagine the
point surrounded by a sphere divided into tiny solid angles. The
intensity and spectral composition of light will vary from one solid
angle to another, and this spatial pattern of light is the optic array.
Light carries information because the structure of the optic array is
determined by the nature and position of the surfaces from which it has
been reflected. Figure 1.3 illustrates the relationship between
environment and a cross-section through an optic array. The array is
divided into many segments, containing light reflected from different
surfaces, and differing in average intensity and spectral composition.
The boundaries between these segments of the optic array provide
information about the three-dimensional structure of objects in the
world. At a finer level of detail, each segment of the array will be
patterned in a way determined by the texture of the surface from which
its light is reflected. Any movement in the environment will cause
change in the spatial pattern of the optic array, as the boundaries of
some segments move relative to others. This spatiotemporal pattern in
the optic array can carry infor-

Section through the optic array at a point above the ground in an
environment containing objects. The optic array is divided into segments
through which light arrives after reflection from different surfaces.
Each segment has a different fine structure (not shown) corresponding to
the texture of each surface.

mation about the direction, speed, and form of the movement involved. We
have taken as an example an optic array in daylight in an open
terrestrial environment, but the same principles apply in any
illuminated environment. At night, the moon and stars illuminate the
world in the same way as the sun, though with light that is many orders
of magnitude less intense. In water, however, there are some
differences. First, refraction of light at the water surface means that
the segment of the optic array specifying "sky" is of a narrower angle
than on land (Figure 1.4). Second, light is absorbed and scattered much
more by water than by air, so that information about distant objects is
not specified in the pattern of intensities and wavelengths in the optic
array. Third, in deep water, light from below is not reflected from the
substrate but scattered upwards. These examples all illustrate one
important point; the spatial and temporal pattern of light converging on
a point provides information about the structure of the environment and
events occurring in it. The speed of light ensures that, in effect,
events in the environment are represented in the optic array
instantaneously. Only in deep oceans and completely dark caves is no
information at all available in light, although the phenomenon of
bioluminescence---the emission of light by organisms---means that even
in these habitats light may carry information about the biological
surroundings. Up until now, we have considered a point just

1. LIGHT AND EYES

7

Section through the optic array at a point below the water surface.
Because light rays from the sky and sun are refracted at the air-- water
boundary, they are "compressed" into an angle (A) of 98°. Incident light
in angle B has been scattered by water or reflected from underwater
objects.

above the ground, or in open water, and asked what sort of information
is available in the optic array converging on it. For an animal at the
centre of this optic array to detect any information at all, it must
first have some kind of structure sensitive to light energy, and our
next topic is the evolution of such structures among animals. How do
different kinds of light-sensitive structures allow light energy to
influence the activity of animals' nervous systems, and what scope do
these structures have for detecting the fundamental informationcarrying
features of optic arrays; spatial pattern and changes in spatial
patterns?

THE EVOLUTION OF LIGHT-SENSITIVE STRUCTURES Many biological molecules
absorb electromagnetic radiation in the visible part of the spectrum,
changing in chemical structure as they do so. Various biochemical
mechanisms have evolved that couple such changes to other processes. One
such mechanism is photosynthesis, in which absorption of light by
chlorophyll molecules powers the biochemical synthesis of sugars by
plants. Animals, on the other hand, have concentrated on harnessing the
absorption of light by light-sensitive molecules to the mechanisms that
make them move. In single-celled animals, absorption of light

can modulate processes of locomotion directly through biochemical
pathways. Amoeba moves by a streaming motion of the cytoplasm to form
extensions of the cell called pseudopods. If a pseudopod extends into
bright light, streaming stops and is diverted in a different direction,
so that the animal remains in dimly lit areas. Amoeba possesses no known
pigment molecules specialised for light sensitivity, and presumably
light has some direct effect on the enzymes involved in making the
cytoplasm stream. Thus, the animal can avoid bright light despite having
no specialised light-sensitive structures. Other protozoans do have
pigment molecules with the specific function of detecting light. One
example is the ciliate Stentor coeruleus, which responds to an increase
in light intensity by reversing the waves of beating of its cilia that
propel it through the water. Capture of light by a blue pigment causes a
change in the membrane potential of the cell, which in turn affects
movement of the cilia (Wood, 1976). Some protozoans, such as the
flagellate Euglena, have more elaborate light-sensitive structures, in
which pigment is concentrated into an eyespot, but Stentor illustrates
the basic principles of transduction of light energy that operate in
more complex animals. First, when a pigment molecule absorbs light, its
chemical structure changes. This, in turn, is coupled to an alteration
in the structure of the cell membrane, so that the membrane's
permeability to ions is modified,

8

VISUAL PERCEPTION

which in turn leads to a change in the electrical potential across the
membrane. In a single cell, this change in membrane potential needs to
travel only a short distance to influence processes that move the animal
about. In a many-celled animal, however, some cells are specialised for
generating movement and some for detection of light and other external
energy. These are separated by distances too great for passive spread of
a change in membrane potential, and information is instead transmitted
by neurons with long processes, or axons, along which action potentials
are propagated. In many invertebrates, particularly those with
translucent bodies, the motor- and interneurons, which generate patterns
of muscle contraction, contain pigment and are directly sensitive to
light. This is the basis of the diffuse "dermal" light sense of various
molluscs, echinoids, and crustaceans, which do not possess specialised
lightsensitive cells but nevertheless can respond to stimuli such as a
sudden dimming of light caused by an animal passing overhead (Millott,
1968). However, most animals sensitive to light possess photoreceptor
cells, specialised for the transduction of light into a change in the
cell's membrane potential. Photoreceptor cells may be scattered over the
skin, as in earthworms, or may be concentrated into patches called
eyespots, such as those along the mantle edge of some bivalve molluscs.
An animal with single receptor cells or patches of cells in eyespots
cannot detect the spatial pattern of light in the optic array, because a
photoreceptor samples the light reaching it from all directions. It can,
however, detect changes in light intensity over time, and invertebrates
with simple eyespots probably do no more than this. For an aquatic
animal, a sudden reduction in light intensity is likely to mean that a
potential predator is passing overhead, and clams and jellyfish react to
such dimming with defensive responses. If any spatial pattern in the
optic array is to be detected, an animal's photoreceptors must each be
sensitive to light in a narrow segment of the array. In practice, any
photoreceptor cell has some such directional sensitivity. The way
pigment is arranged in the cell makes it more sensi-

tive to light from some directions than from others, and further
directional sensitivity can be achieved in simple eyespots by screening
the receptor cells with a layer of dark pigment. An animal with eyespots
distributed over its body, each screened by pigment, therefore has some
ability to detect spatial pattern in the light reaching it (see Figure
1.5). For example, it would be able to use the pattern of activity in
its eyespots to maintain its swimming orientation by keeping the source
of greatest light intensity above it, or to orient defensive responses
according to the direction of approach of a predator. The evolution of
greater complexity in eyes can be thought of as the invention of various
ways of improving directional sensitivity. One simple way of doing this
is to sink a patch of receptor cells into the skin to make an "eye-cup"
or ocellus (Figure 1.6). Many invertebrates possess eye-cups,
particularly coelenterates, flatworms, molluscs, and annelid worms.
Eye-cups vary in the detail of their structure; some are open to the
water, others are filled with gelatinous material, while many contain a
crystalline lens. The receptor cells in an eye-cup are clearly sensitive
to light from a narrower angle than if they were on the surface of the
skin, and the presence of a refractile lens further helps to reject
light rays at a large angle from the axis. An animal with eyecups
distributed over its body can, because of this greater directional
sensitivity, detect finer spatial pattern in the optic array than can
the animal in Figure 1.5 with its eye-spots. Some molluscs have rows of
regularly spaced eye-cups along the body---examples are the marine
gastropod Corolla and some bivalves with eye-cups along the mantle
edge---and these animals are potentially able to detect nearby moving
objects through successive dimming of light in adjacent eye-cups.
Because the angles through which eye-cups are sensitive to light are
wide, and overlap a good deal, the degree of directional sensitivity
achieved by an animal with many eye-cups is not great. Further
directional sensitivity requires the possession of a true eye. The
eye-cups of molluscs appear to be miniature eyes, but their function
differs from that of eyes in a crucial way. A true eye forms an image on
a layer of photoreceptor

1. LIGHT AND EYES

9

A hypothetical transparent disc-shaped animal with patches of
photoreceptor cells around its edge. The lengths of the bars in the
lower diagram represent the intensities of light falling on each patch.
In (a) each patch is sensitive to light through 360° and so each
receives the same amount of light. In (b) and (c), screening of the
patches by pigment reduces the angle through which they are sensitive to
light. The amounts of light striking each receptor now differ, and the
pattern of differences captures information available in the optic array
specifying the direction of the water surface (b) or an overhead object
(c).

Examples of eye-cups, from the limpet Patella (a) and the snail Murex
(b). Reproduced from Barnes (1974) by permission of the publishers.
Copyright © 1974 by Saunders College Publishing, a division of Holt,
Rinehart and Winston Inc.

cells. When an image is formed, all light rays reaching the eye from one
point in space are brought together at one point in the image, so that
each receptor cell in the eye is struck by light coming from a different
narrow segment of the optic array. As we will see later in this chapter,
a lens provides one means of forming an image. However, the lenses in
most mollusc eye-cups do not form an image on the layer of
photoreceptors (Land, 1968), and so spatial pattern in the optic array
is not mapped onto the array of receptors in a single eye-cup. Instead,
following the principle illustrated in Figure 1.5, it is mapped onto the
array of eye-cups over the body. In a sense, it is the whole animal that
is an eye. To build a true

eye requires both the concentration of photoreceptor cells into one part
of the body and some apparatus for forming an image on them. We next
describe the two basic structural plans of true eyes---the compound and
the single-chambered eye.

The compound eye A compound eye can be constructed by continuing the
process of making eye-cups more directionally sensitive, while at the
same time making them smaller and grouping them all together into a
single structure. Simple eyes of this kind have evolved in some bivalve
molluscs (e.g., Arca) and marine annelids (e.g., Branchiomma), but the
most elaborate eyes based on this principle are

10

VISUAL PERCEPTION

those of crustaceans and insects. A compound eye is made up of a number
of ommatidia; each one is a small, elongated eye-cup with a crystalline
cone at the tip and the light-sensitive rhabdom below it. Transparent
cuticle---the cornea--- covers the whole array of ommatidia (Figure
1.7a). Compound eyes vary in several ways around this basic plan. The
number of ommatidia varies greatly, and the structure of the cone
differs in the eyes of different insect groups, some eyes not having a
cone at all. A particularly important kind of variability in the
compound eye is the degree of optical isolation between adjacent
ommatidia. This is greatest in the apposition type of eye,
characteristic of some crustaceans and of diurnal insects, in which the
rhabdoms and cones touch and there is absorptive screening pigment
between the ommatidia. These two features reduce the amount of light
that can reach a rhabdom from cones other than the one above it and so
keeps the angle of acceptance of light of each ommatidium low. At the
other extreme is the superposition eye, which has less pigment between
ommatidia and a clear space between the layer of cones and the layer of
rhabdoms. As a result, light can reach a

(a) Schematic diagram of the compound eye of an insect. (b) The
    structure of a single ommatidium. (c) Cross-section through an
    ommatidium, showing the rhabdom made up from overlapping folds of
    retinula cell membrane. Adapted from Wigglesworth (1964).

rhabdom from any cone. The difference in structure between apposition
and superposition eyes reflects an important difference in the means by
which they form an image on the layer of rhabdoms. In the apposition
eye, light striking the cone of an ommatidium from outside its narrow
angle of acceptance does not reach the rhabdom, but is either reflected
or absorbed. This mechanism yields an image in bright light, but has the
disadvantage that, in dim light, each ommatidium cannot gather
sufficient light to stimulate the receptors. In a superposition eye, on
the other hand, light rays from one point in space striking many
adjacent cones are brought to a focus on the same rhabdom. Three
different types of superposition eye have been identified, which use
different optical arrangements to focus light rays in this way (for
further details, see Land & Nilsson, 2002, Ch. 8). The result in all
three cases is that each rhabdom gathers light over a wide area of the
eye without losing directional sensitivity. Superposition eyes can
therefore provide vision in dimmer light than can apposition eyes, and
are commonly found in nocturnal insects and in marine crustacea. We have
described only the main optical

1. LIGHT AND EYES

designs of compound eyes, and there is much variation in structure
within each of the major categories. This variation can often be related
to the ecological demands placed on vision in different arthropod
species, and an introduction to this topic is given by Land (1999). Our
next step in describing the general properties of compound eyes is to
ask how rhabdoms transduce the light striking them into electrical
changes in nerve cells which can ultimately modulate behaviour. Figures
1.7b and 1.7c show the structure of a typical rhabdom, made up of
between six and eight retinula cells arranged like the slices of an
orange. The inner membrane of each retinula cell is folded into a
tubular structure called the rhabdomere. The rhabdomeres contain
molecules of rhodopsin pigment. The rhodopsins are a family of
light-sensitive molecules, each made up of two components linked
together; a protein, opsin, and a smaller molecule, retinal. The shape
of retinal changes when it absorbs light, and this change is coupled to
an increase in membrane conductance and consequently a wave of
depolarisation. The size of this receptor potential is proportional to
the logarithm of the intensity of light striking the cell. Most diurnal
insects and crustaceans possess more than one type of retinula cell,
each type having a pigment with a different relationship between
wavelength of light and the probability of absorption, or absorption
spectrum. Commonly, there are three types of pigment, with peak
absorption at different points in the range of wavelengths from yellow
through blue to ultraviolet. As we will see later, possession of two or
more pigments with different absorption spectra makes colour vision
possible. The response of single retinula cells does not depend only on
the wavelength and intensity of light striking them; they are also
sensitive to the plane of polarisation of light. Unpolarised light is
made up of waves vibrating in all planes around the direction of
propagation. If light is absorbed or diffracted in such a way that some
planes of vibration are more strongly represented than others, the light
is said to be polarised. There are many natural situations in which the
pattern of

11

polarisation of light is potentially useful to animals. The plane of
polarisation of light from a small part of the sky can specify the
position of the sun, even when it is completely blocked by cloud, and
bees and ants make use of this information in navigating. Sensitivity to
the plane of polarisation is also useful in detecting a water surface
from the air above and in enhancing the contrast of underwater objects
against a bright background of scattered light (Wehner, 2001).

Single-chambered eyes The second basic structural plan for eyes is that
of the single-chambered eye, which can be derived by enlargement and
modification of a single eyecup rather than by massing eye-cups
together. Figure 1.8 shows three devices---a pinhole camera, a concave
mirror, and a convex lens---which can form an image, and all three
designs can be achieved by modifying an eye-cup in different ways. A
pinhole camera can be made from an eyecup by nearly closing off its
opening, and the cephalopod Nautilus possesses an eye of this kind. This
design of eye is almost unique in the animal kingdom, and suffers the
severe disadvantage that the aperture of a pinhole camera must be very
small to form an image and so it can admit only small amounts of light.
A concave mirror can be made by coating the back of an eye-cup with
reflecting material and moving the photoreceptors forward to the image
plane. The eyes of the scallop Pecten are arranged in this way, with a
silvery layer of guanine crystals at the back of the eye forming an
image at the level of the retina (Land, 1968). Simpler versions of this
type of eye are found in a variety of marine invertebrates, while the
deep-sea ostracod crustacean Gigantocypris has mirror-based eyes that
work on quite different optical principles (for further details see Land
& Nilsson, 2002, Ch. 6). By far the most evolutionarily successful type
of single-chambered eye uses a convex lens, as a camera does. This type
of eye has evolved many times in different animal groups by enlargement
of the eye-cup so that the image formed by the lens falls on the
receptor cells. Examples are

12

VISUAL PERCEPTION

The optics of image formation. An image is formed when rays of light
arriving at one point I in the image plane (I.P.) all come from the same
point O in space. In the pinhole camera (a) this occurs because the
aperture is so small that each point in the image plane is illuminated
by light arriving through a narrow cone. A concave mirror (b) reflects
light in such a way that all rays striking it from one point are brought
to a focus at the same point in the image plane. A convex lens (c)
achieves the same result by refraction of light.

found in various marine invertebrates, such as gastropod molluscs and
alciopid annelids, as well as in spiders, scorpions, and some insect
larvae. The largest and best-known single-chambered eyes, however, are
those of the vertebrates and the cephalopod molluscs (octopus and
squid). For the moment, we will only outline the basic structure of the
vertebrate eye, before looking at it in detail in the next section.
Figure 1.9 shows the important components of this kind of eye--- the
cornea, iris, lens, and retina---and also the remarkable degree of
convergent evolution of the eye in two unrelated groups of animals, the
cephalopods and the vertebrates (a convergence so close that dilation of
the pupil of the eye signals sexual arousal in cuttlefish, just as in
people). The structure of the retina, the mat of photoreceptors at the
back of the eye, does differ in the two groups. Cephalopod
photoreceptors are built on the same rhabdomeric plan as those of the
arthropods, but vertebrate receptors are of the cil-

iary type, in which the layers of membrane containing light-sensitive
pigment are stacked in an outer segment of the cell. There are two types
of ciliary receptor, rods and cones, and they have differently shaped
outer segments (Figure 1.10). Rods and cones are packed into the retina
with their long axes parallel to the direction of incident light, and a
layer of absorptive pigment behind, which reduces internal reflection.
Whereas receptors in the cephalopod eye are arranged to face the light
from the lens, the vertebrate retina is "inverted". The rods and cones
are in the layer of the retina farthest from the lens, with their outer
segments pointing away from it, so that light must pass through a layer
of cells to reach them (for further details, see Chapter 2, p. 28). The
pigments in rods and cones belong to the rhodopsin family, and
transduction begins in the same way as in the retinula cells of insects;
photons absorbed by the retinal part of the rhodopsin molecule cause it
to change in shape and to detach from the opsin part. This change
triggers

1. LIGHT AND EYES

13

Section through (a) an octopus eye and (b) a human eye. (a) Adapted from
Barnes (1968). (b) Adapted from Walls (1942).

ing receptor potential (in contrast to the depolarisation of a
rhabdomeric receptor).

Conclusions

Structure of rods and cones in the vertebrate retina. The outer segments
contain folded layers of membrane packed with light-sensitive pigment.
Adapted from Uttal (1981).

a cascade of enzymatic processes in the outer segment that results in
the breakdown of cyclic guanosine monophosphate (cGMP). This in turn
causes the conductance of a channel in the cell membrane, gated by cGMP,
to fall, and the membrane is hyperpolarised. In darkness, cGMP is
resynthesised and the membrane potential returns to its resting level.
The details of these biochemical processes are described by Pugh and
Cobbs (1986); note that the overall effect of light striking a rod or
cone is to cause a hyperpolaris-

The pattern of evolution of light-sensitive structures in animals, as we
have outlined it, is summarised in Figure 1.11, and the interested
reader will find a wealth of further details in Land and Nilsson (2002).
The central theme in this pattern is increasing directional sensitivity
of photoreceptors and therefore increasing ability to detect spatial
pattern and movement in the optic array. At one extreme, a jellyfish can
detect dimming of the total light reaching it in order to escape from an
overhead predator, while at the other a hawk can spot a mouse running
through grass hundreds of feet below. As we have seen, increased
directional sensitivity has been achieved in a variety of ways, through
the evolution of two quite differently constructed eyes, and the
modification of each type to form an image in more than one way. It
would be mistaken to attempt to rank these different types of eye in
order of merit, or to think of the vertebrate eye as an evolutionary
pinnacle. Although most vertebrate eyes achieve greater directional
sensitivity than any compound eye, this does not imply that the former
is intrinsically a better eye. For optical reasons, a singlechambered
eye small enough to fit an insect's head would not have any greater
directional sensitivity than a compound eye of the same size
(Kirschfeld, 1976). The two optical designs are suited to animals of
different sizes, and neither is more "advanced" than the other.

14

VISUAL PERCEPTION

Schematic diagram of the evolution of different types of eye. (a)
"Pinhole" eye of Nautilus. (b) Reflecting eye of Pecten. (c)
Singlechambered eye. (d) Single ommatidium. (e) Compound eye.

THE ADAPTIVE RADIATION OF THE VERTEBRATE EYE In the remainder of this
chapter we examine in more detail the workings of the vertebrate eye and
particularly the differences between the eyes of different species. In
the course of evolution, many variations on the basic single-chambered
plan have evolved and, to some extent, these variations are related to
the demands of each species' environment and way of life. This kind of
evolutionary modification of a basic structure is called adaptive
radiation.

Focusing the image The fundamental job of a single-chambered eye is to
map the spatial pattern in the optic array onto the retina by forming an
image; all light rays striking the eye from one point in space are
brought to a focus at one point on the retina. What influences how
efficiently vertebrate eyes do this?

The ability of a person or animal to detect fine spatial pattern is
expressed as visual acuity. This can be measured by the use of a
grating; a pattern of parallel vertical dark bars equal in width and
separated by bright bars of the same width. As the dark and bright bars
are made narrower, there comes a point when an observer is no longer
able to resolve the grating; that is, to distinguish it from a uniform
field of the same average brightness. Since the width of the bars at
which this happens will depend on how far the observer is from the
grating, we do not measure width as a distance but as a visual angle;
the angle that a bar subtends at the eye. Figure 1.12 shows how size,
distance, and visual angle are related. Under optimal lighting
conditions, the minimum separation between the centres of adjacent dark
and bright bars that a person can resolve is about 0.5 min of arc.
Visual acuity is limited by several processes. The first is the
efficiency with which the optical apparatus of the eye maps the spatial
pattern of the optic array on to the retina. The second is the

1. LIGHT AND EYES

15

The formula relating visual angle  subtended by an object to its height
h, and its distance from a lens d, and also to the height i of its image
on a projection plane at a distance r from the lens. For small visual
angles, the retinal surface approximates to a projection plane, and r is
the diameter of the eye and i the height of the retinal image. For
larger visual angles, the curvature of the retina is significant, and
the relationship tan  = i/r does not hold.

efficiency with which receptor cells convert that pattern into a pattern
of electrical activity; and the third is the extent to which information
available in the pattern of receptor cell activity is detected by the
neural apparatus of retina and brain. For the moment, our concern is
with the first of these processes, and the important consideration is
how sharply the eye focuses an image on the retina. As an image becomes
more blurred, spatial pattern in the optic array is smoothed out in the
pattern of light at the retina and the detection of fine differences is
compromised. Before considering how the eyes of different species
achieve optimal focusing, we need to explain in more detail the optical
principles governing the formation of an image by a singlechambered eye.
An image is formed because light is bent, or refracted, at the boundary
between two transparent media of different optical densities, such as
air and glass. The degree of bending of light is determined by the
difference in refractive index of the two media. The surfaces of a
convex lens are curved in such a way that rays of light parallel to the
axis are bent through a greater angle the greater their distance from
the axis, and pass through a single point on the other side of the lens
(see Figure 1.8c). If we imagine a bundle of rays arriving at the lens
from one point on an object, these will be virtually parallel if the
object is a large distance from the lens (at "optical infinity"). In
this case, the rays will converge to a point in the focal plane of the
lens. The distance of the focal plane from the centre of the lens is its
focal length, f. The greater the degree to which a lens bends

parallel light rays to converge at the focus, the shorter its focal
length. A convenient measure of a lens' performance is its power,
defined as the reciprocal of its focal length. Power is measured in
dioptres (abbreviated to "D") if focal length is measured in metres. An
image of an object at infinity is therefore formed at a distance f from
the lens, and we say that the image plane lies at this distance. If the
object is brought closer to the lens, the relationship between its
distance u from the lens and the distance v of the image plane is given
by: 1 1 1 = + f u v

As the object comes nearer to the lens, the image plane will therefore
move further back from it, so that the image formed on a surface located
in the focal plane will be blurred. In a camera, the image is kept
focused on the film as object distance varies by changing the distance
between the lens and the film. As we shall see, some vertebrate eyes
solve the problem in a different way. In a vertebrate eye, there are
four refracting surfaces as light passes from external medium to cornea
to aqueous humour to lens to vitreous humour. The refracting power of
each surface is determined both by its curvature (a fatter lens is more
powerful) and by the difference in refractive indices (RIs) of the media
on either side of it. For an animal living in air, there is a large
difference in RI between the external medium (RI = 1) and the cornea (RI
= 1.376 in the human eye), and this

16

VISUAL PERCEPTION

surface has considerable refracting power. Accordingly, the eyes of land
vertebrates have a flattened lens contributing little to the total
refracting power of the eye (in the human eye, the front surface of the
cornea provides 49 out of a total of 59 dioptres). For aquatic
vertebrates, however, none of the refracting surfaces has such a large
difference in RIs, as the RI of water is similar to that of the cornea.
A strongly curved surface is therefore needed somewhere, and it is
provided by a nearspherical lens, not only in aquatic vertebrates but
also in squid and octopus. Also, the lens of a fish eye is of a higher
refractive index than that of the human eye. Now, if the power of the
lens--cornea combination, and its distance from the retina, are both
fixed, then a sharply focused image will only be formed of objects lying
at a certain range of distances from the eye. This range is called the
depth of field, and is the distance over which the object can move to
and from the eye without the image plane falling outside the layer of
retinal receptors. For a human eye focused at infinity, this range is
from about 6 metres to infinity. The reason why we can focus on objects
less than 6 metres from the eye is that its optics can be adjusted by a
process called accommodation. Three basic mechanisms of accommodation
are known in vertebrate eyes. In fish and amphibians, the lens moves
backwards and forwards to keep the image plane on the retina. In
reptiles, birds, and mammals, the power of the lens can be altered by
changing its shape. This is achieved through different mechanisms in
different species; in the human eye, contraction of the ciliary muscles
attached to the lens causes it to thicken, increasing its curvature and
therefore its power, so that nearby objects are brought into focus. When
the ciliary muscles are fully relaxed, the lens takes on a flattened
shape and the eye is focused at infinity. Finally, some bird species can
accommodate by changing the radius of curvature of the cornea (Schaeffel
& Howland, 1987). Accommodation enables the power of the human lens to
be adjusted over a range of up to 15 dioptres, a figure that falls with
age. The ability to accommodate over a wide range is clearly

useful to human beings and to other primates, which characteristically
examine the detail of objects at close range, but may not be so
important to other vertebrates. Because light is scattered and absorbed
by water, there is no need for the eyes of fish and other aquatic
animals to focus on distant objects, and so they can be somewhat myopic
(unable to bring an object at infinity into focus) without missing out
on any information available in the optic array. In contrast, the need
for accommodation is especially great for animals that live both on land
and in water, and that need to achieve acuity in two media differing in
refractive index. Diving birds, seals, turtles, and amphibious fish show
a fascinating variety of adaptations in eye structure and function to
solve this problem (Sivak, 1978). Various mechanisms for increasing the
range of accommodation of the lens are found in otters, turtles, and
diving birds. In diving ducks, the lens is flexible, and contraction of
the ciliary muscles causes it to bulge through the pupil in a rounded
shape. As a result, accommodation of up to 80D is possible, and this is
enough to compensate for the loss of refractive power when the cornea is
immersed in water (Sivak, Hildebrand, & Lebert, 1985). Another solution
to the problem, used in the eyes of seals, penguins, and the flying fish
Cypselurus, is to flatten the cornea so that it has little or no
refractive power in air or water, and to rely largely on the lens for
refraction. Still another tactic is to divide the eye into separate
imageforming systems with different optical properties. Such eyes are
unique to the "four-eyed" fish Anableps, which swims at the surface with
the upper part of the eye, adapted for vision in air, above the surface,
and the lower part, adapted for vision in water, below it. Even when an
eye is optimally focused, there is a certain degree of blur in the image
caused by optical imperfections. For several reasons, lenses do not
bring light rays to a focus in the ideal way we have described so far.
First, parallel rays may be brought to a slightly different focus
depending on how far from the axis they strike the lens (spherical
aberration and coma) or depending on their orientation relative to the
lens

1. LIGHT AND EYES

(astigmatism). The refractive index of a medium varies with the
wavelength of light, so that different wavelengths are brought to a
focus in slightly different planes (chromatic aberration). Finally,
scattering of light in the fluids of the eye and in the retinal layers
overlying the receptors further blurs the image. A boundary between two
segments of the optic array differing in intensity is therefore spread
out on the retina to some extent, even with optimal focus. Given the
figure for the acuity of the human eye mentioned earlier, however, the
impact of these aberrations is not great, at least in optimal
conditions. One way in which some sources of aberration, particularly
spherical aberration, can be reduced is by constricting the pupil so
that light only enters the lens through a narrow aperture. This means of
reducing aberration, however, is not available in dim light.

Vision in bright and dim light If an eye is optimally focused, the
spatial pattern in the optic array is transformed into a pattern of
light intensity on the retina with a minimum degree of blur. The second
constraint on an animal's or a person's visual acuity now comes into
play; the efficiency with which this pattern of light on the retina is
transformed into a pattern of electrical activity in receptor cells. One
factor that will influence this efficiency will clearly be the density
of packing of receptor cells in the retina. The more densely receptors
are packed, the finer the details of a pattern of light intensities that
can be transformed into differences in electrical activity. If acuity
were limited by receptor spacing and not by optical factors, the minimum
distance between adjacent dark and bright bars for a grating to be
resolved would be equal to the average distance between adjacent
receptors (assuming receptors are packed in a rectangular array). The
difference in acuity between people and falcons is the result of a
difference in receptor packing. The photoreceptors in the falcon's eye
are packed three times more densely than in the human eye, and the
falcon can resolve a grating with a spacing of 0.2 min of arc, as
compared with the figure for a human observer

17

of 0.5 min (Fox, Lehmkuhle, & Westendorff, 1976). A second factor, which
we need to dwell on at more length, is the intensity of light striking
the retina. It makes a difference to the detectability of a spatial
difference in light intensity on the retina whether two neighbouring
cells are being struck by (for example) 5 and 10 photons per second or
by 5000 and 10,000. The reason is that, even under constant
illumination, the rate at which photons strike a receptor fluctuates
around an average value, and this variability increases as the square
root of the average value. If light intensity is high, so that the
average rates of photon flux striking two adjacent receptors are also
high, then the difference between the two rates will be large relative
to the fluctuation in each. As the light reaching the eye becomes less
intense, the difference in photon flux at adjacent receptors eventually
becomes comparable to the extent of fluctuation, and so is detectable
only if the two rates of flux are averaged over a period of time. Now,
if this difference is caused by a moving boundary in the optic array,
the difference in photon flux may not be present in any part of the
retina long enough to be detected. As light becomes dimmer, the maximum
speed of movement in the optic array that can be detected will fall. One
solution to this problem would be to increase the cross-sectional area
of receptor cells so that each sampled a larger segment of the optic
array and received a larger flux of photons. Alternatively, the outputs
of neighbouring receptors could be "pooled" by connection to one
interneuron, so that they effectively acted as a single receptor. Either
solution would increase the sensitivity of the eye, but, as we discussed
above, they would both decrease its acuity. The design of an eye is
therefore subject to a trade-off between sensitivity and acuity. What
are the implications of this constraint for the evolution of vertebrate
eyes? Many vertebrate species use only part of the daily cycle for their
activities, being either nocturnal or diurnal. Nocturnal animals need
eyes with high sensitivity in dim moon- and starlight, and this is also
true of deep-sea fish. Diurnal

18

VISUAL PERCEPTION

animals, on the other hand, can have eyes with high acuity. Even so, it
is unusual for a species' vision to operate in a very narrow band of
light intensities. The intensities encountered by either a diurnal or a
nocturnal animal vary over several orders of magnitude (see Figure
1.13), and so in either case vision must be adapted to operate in a
range of light intensities. The most striking way in which the
vertebrate eye is adapted for vision in a range of light intensities is
in the structure of the retina. We noted earlier the two kinds of
vertebrate photoreceptor---rods and cones---and saw that a rod has a
deeper stack of pigment-filled layers of folded membrane in its outer
segment than has a cone. A photon passing through a rod therefore stands
a lower chance of coming out at the other end than one passing through a
cone, and so the membrane potential of a rod will be influenced by
levels of light too low to affect a cone. It is therefore not surprising
that there is a correlation between an animal's ecology and the ratio of
rods to cones in its retina. Diurnal animals have a higher proportion of
cones than do

Typical values of illumination (log amount of visible light energy
striking a unit area of a surface per unit time) at different times of
day and under different conditions. Note that intensity varies over
several log units in both daytime and night-time conditions. Adapted
from Martin (1985).

nocturnal animals, though pure-cone retinas are rare, and found mostly
in lizards and snakes. Pure-rod retinas are also rare, and found only in
animals such as deep-sea fish and bats that never leave dark habitats. A
further adaptation of the retina in animals active in dim light is the
presence of a silvery tapetum behind the retina, which reflects light
back through it and so gives the rods a second bite at the stream of
photons, though at the cost of increasing blur through imperfect
reflection. The glow of a cat's eyes in the dark is caused by reflection
of light by a tapetum behind its retina. A possibility exploited by a
few nocturnal animals is to use infra-red (IR) radiation, which can
provide information about the environment in the absence of visible
light, subject to some limitations. First, IR is rapidly absorbed by
water and therefore is potentially useful only on land. Second, the IR
radiation emitted by an endothermic (warm-blooded) animal would mask any
radiation it detected, and so we would only expect sensitivity to IR in
exothermic (cold-blooded) animals. Third, it could only be used to
detect objects differing in temperature from the rest of the
environment. The one group of vertebrates that does detect information
carried by IR radiation is a group of snakes, the pit vipers, which hunt
for small birds and mammals at night. The cornea and lens of the
vertebrate eye are opaque to IR, and so a different kind of sensory
organ is required. These snakes have specialised organs alongside the
eyes, acting as pinhole cameras to form an IR image of their
surroundings, which enable them to detect the location of nearby
exothermic prey, such as mice. So, a retina containing only rods or only
cones adapts an animal for vision in a narrow range of light
intensities. How is retinal structure adapted for vision over a wider
range? Imagine that we start with an animal that is basically diurnal
but needs to be equipped with some night vision. We add rods to the
pure-cone retina, scattering them about evenly among the cones. We
discover, though, that to capture much light at night, we need a great
many rods. In the human eye, for

1. LIGHT AND EYES

example, there are 120 million rods as opposed to only 7 million cones.
Now, if we have added the rods evenly over the retina, we would find the
distances between the cones are now much larger than when we started and
that acuity in daytime vision is severely reduced. The way out of this
problem in many vertebrate eyes is to divide the retina into two
regions. One small area is rich in cones, with little or no pooling of
the outputs of adjacent receptors, while the other, larger, area is rich
in rods, with considerable pooling of outputs. The conerich region
provides high acuity vision in bright light, whilst the rod-rich area
provides high sensitivity vision in dim light. This pattern is found
especially in birds and in primates, where the cone-rich area is usually
circular, and sometimes contains a pit-like depression in the retina,
called a fovea. Some bird species possess two such areas in each eye,
one corresponding to the frontal and one to the lateral visual field
(Nalbach, Wolf-Oberhollenzer, & Remy, 1993). The cone-rich area of the
human eye, the macula lutea, contains a fovea. There are only cones in
the centre of the fovea, and their proportion and density of packing
decrease further out into the retina. At more than about 10° from the
centre, outside the macula, there are few cones, and the peripheral part
of the retina contains almost all rods. There are other ways in which
vertebrate eyes are adapted to operate over a range of light
intensities. One is movement of cells containing pigment, and sometimes
also the receptor cells, in the retina. These retinomotor responses are
found in fish, and in some reptiles and birds, and act to screen the
rods with pigment in bright light and to expose them in dim light. A
second mechanism, rare in fish but more common in birds and especially
mammals, is dilation and constriction of the pupil by the muscular
tissue of the iris. In diurnal mammals, the pupil is usually round, and
its function is to reduce the aperture of the lens in bright light and
so reduce blur of the retinal image due to spherical and other optical
aberrations. In dim light, when sensitivity and not acuity is at a
premium, the pupil opens to admit more light. Mammals active in

19

both day and night often have a slit pupil, which, for mechanical
reasons, can close more completely than a round one. A cat's eye has a
retina adapted for nocturnal vision and a slit pupil which allows it to
operate in the daytime. A third process, known as adaptation, occurs in
the retinas of many animals to adjust the sensitivity of photoreceptors
and interneurons to varying intensities of light. In the human eye, the
iris does not play a major role in regulating the intensity of light
reaching the retina, and so adaptation is the main means by which
sensitivity is adjusted. We can see over a range of approximately 7 log
units of light intensity (a 107-fold range), but at any one time, our
vision is effective over a range of only one or two log units, and this
range can shift upwards (light adaptation) or downwards (dark
adaptation). We return to adaptation in Chapter 2. To conclude,
vertebrate eyes are subject to the basic physical constraint that
greater sensitivity in dim light can only be won at the cost of reduced
acuity. Different species strike different bargains between these two
factors, depending upon their ecology, but many manage to operate over a
range of light intensities. Human beings are an example of a basically
diurnal species with acute photopic (bright light) vision and some
degree of low acuity scotopic (dim light) vision. We are fairly
adaptable, though not as much as cats!

Sampling the optic array An animal with simple eye-cups distributed over
its body can detect light reaching it from any direction and so, at any
instant, can sample the entire optic array. Once an animal's
photoreceptors are concentrated into a pair of eyes at the front of the
body, this is no longer possible, although it is nearly achieved by some
insects, such as dragonflies, which have large compound eyes wrapped
almost completely around the head. The angle through which light
striking a single vertebrate eye is focused onto the retina can be as
great as 200°, and so nearly panoramic vision can be achieved by placing
the eyes laterally, on either side of the head, with their axes
approximately perpendicular to the body. This arrangement is common
among vertebrates, and can enable an

20

VISUAL PERCEPTION

animal to detect predators approaching from any direction. In mallard
ducks, and some other birds with laterally placed eyes, the visual
fields of the two eyes cover the entire celestial hemisphere and there
is no blind area above or behind the head that a predator could exploit
(Martin, 1994a). Laterally placed eyes give the maximum ability to
detect predators, but other, competing demands on vision have resulted
in the eyes of some species being swung forwards. If the axes of the two
eyes are parallel, or nearly so, they are said to be frontally placed.
In owls and other birds of prey, the eyes are in an intermediate
position between frontal and lateral placement. These birds need high
acuity in order to detect small, distant prey, and this is achieved in
part through the magnification of the retinal image caused by the large
size of their eyes. Just as in a telephoto lens, however, this
magnification is won at the cost of narrowing the angle through which
the eye accepts light, and more frontal placement of the eyes is
therefore necessary to prevent a blind area forward of the head. Another
advantage of a frontal eye position is that it increases the size of the
binocular field, the segment of the optic array sampled by both eyes
simultaneously. The binocular field is narrow in animals with laterally
placed eyes, and largest in those such as cats and primates where the
axes of the eyes are approximately parallel. A human eye accepts light
through an angle of about 150°, but the large degree of binocular
overlap means that the angle for two eyes is not much greater. When the
same information is obtained from the optic array by two eyes, greater
accuracy can be achieved in discriminating spatial and temporal pattern
from noise in photon flux and in photoreceptor responses (Jones & Lee,
1981). Also, for reasons that are explained in Chapter 7, information
about the distances of objects can be obtained. More frontal eye
placement therefore offers a number of advantages, but at the price of
losing the ability to obtain information from a large segment of the
optic array at one time. This constraint can be overcome by movement of
the head, eyes, or both, to change rapidly the segment from which light
is accepted. Head and eye

movements are also important for animals that possess a fovea, as they
can only sample a segment of the optic array a few degrees wide with
high acuity, and this segment must be moved frequently relative to the
retina. Most vertebrates can move their eyes to some extent, although
few can move them through large angles. The chameleon is a striking
exception; its angle of vision is small and, as it searches for prey,
its two laterally placed eyes swivel about quite independently, giving a
distinctly creepy impression! A similar pattern of eye movements has
recently been discovered in a fish, the sandlance, which also shows a
number of other similarities to chameleons in the structure of its eyes
(Pettigrew, Collin, & Ott, 1999). The chameleon and sandlance apart, the
eyes of primates make the largest, most rapid and most precisely
controlled movements. The human eye is held in position by a dynamic
balance between three pairs of antagonistic muscles, and instability in
this balance causes a continuous small-amplitude tremor. As a result,
the image on the retina is in constant motion, any point on it moving by
about the distance between two adjacent foveal cones in 0.1 second.
Sampling of the optic array is achieved by three kinds of eye movement.
First, rapid and intermittent jumps of eye position called saccades are
made in order to fixate an object with foveal vision. As a person looks
at a scene, the eyes make several saccades each second to scan it. Once
an object is fixated, pursuit movements keep it in foveal vision as it
moves, or as the observer moves. If the distance of an object from the
observer changes, convergence movements keep it fixated by the foveas of
both eyes. As an object comes closer, convergence movements turn the
directions of gaze of both eyes towards the nose. If an object comes
within a few inches of the face, further convergence is impossible and
"double vision" occurs (we say more about convergence and depth vision
in Chapter 7). Whereas saccades are sudden, intermittent changes of eye
position, both pursuit and convergence are smooth and continuous. To
summarise, at any instant the human eye samples a relatively large
segment of the optic

1. LIGHT AND EYES

array (the peripheral field) with low acuity, and a much smaller segment
(the central, or foveal field) with high acuity. Smooth and saccadic eye
movements shift this high-acuity segment about rapidly, so that acute
vision over a wide angle is achieved. Further details of human eye
movements, their anatomical and neural mechanisms, and models of their
control may be found in Carpenter (1988). The highly specialised way in
which we sample the optic array makes it difficult for us to appreciate
the different ways in which other animals do so. We are used to
detecting what another person is looking at from the direction in which
their eyes are pointing, but this direction may mean something quite
different in other species. A horse, sheep, or rabbit does not need to
look straight at something in order to see it, while the angle of a
bird's head may be related to what it is looking at in quite a different
way depending on whether it is fixating monocularly or binocularly. It
is only when watching animals such as cats and apes looking that we are
on familiar ground. We discuss the perception of other individuals' gaze
further in Chapter 13.

Detecting colour A vertebrate eye maps not only the pattern of light
intensities in the optic array onto the retina but also the pattern of
different spectral compositions of light. We have already seen that the
spectral composition of light in a segment of the optic array can carry
information about the kind of surface from which the light was
reflected. Imagine a bird or a monkey searching for insect prey, or for
fruit, among dense foliage. The pattern of intensity of light reflected
from the scene will be a patchwork of bright and dark regions, depending
on the orientation and reflectance of leaves and stems, and on the
effects of shadow. Against such a background, it will often be difficult
to detect the patch of light reflected from the target on the basis of
its intensity alone. The task will be easier, however, if the animal is
also sensitive to differences in the spectral composition of light
reflected from different surfaces. This is the basis of the ability to
perceive the colours of surfaces, and it is particularly important

21

to species that need to find food or other targets against a dappled
background such as foliage (Mollon, 1989). In order to detect
differences in the spectral composition of light, an animal must possess
at least two sets of receptor cells, containing pigments with different
absorption spectra (see p. 11). A single receptor type is not enough; a
difference in the spectral composition of light falling on two
neighbouring receptors containing the same pigment will cause a
difference in their electrical response, but a difference in just the
intensity of light can have exactly the same effect. In a retina
containing only one type of receptor cell, the pattern of receptor
potentials therefore cannot provide separate information about the
patterns of intensity and of spectral composition in the optic array,
and an animal with such a retina would be incapable of colour vision. We
have already seen that insects usually possess three types of retinula
cell, each with a different pigment. In any one vertebrate species, all
rods in the retina contain the same pigment type, and so animals with
pure-rod retinas cannot detect spectral and intensity differences
independently. This need not be a handicap; for deep-sea fish, the
spectrum of available light is narrow, and such fish usually have a
pure-rod retina in which the peak of the absorption spectrum of the
pigment matches the blue light available. Species capable of colour
vision may have cones with a pigment different from that in the rods, or
they may have two or more types of cone, each with different pigments.
The cones of birds and some reptiles contain coloured oil droplets
through which light must pass to reach the outer segment, and these
coloured filters will further differentiate the absorption spectra of
cones. Most mammals have two cone types, and their colour vision is said
to be dichromatic (Jacobs, 1993). We belong to a group that is an
exception; together with apes, Old World monkeys, and a few New World
monkeys, humans are normally trichromatic. We possess three cone types
containing pigments with peak absorption at 419, 531, and 558 nm, known
as S, M, and L cones respectively (Figure 1.14). We have colour vision
only in photopic conditions, and none in scotopic

22

VISUAL PERCEPTION

Absorption spectra of the four types of human photoreceptor. From left
to right, curves are for S cones, rods, M cones, and L cones. Reproduced
from Dartnall, Bowmaker, and Mollon (1983) with permission of The Royal
Society.

conditions when only the rods in our retinas are stimulated. We are all
colour-blind at night. Trichromacy imposes a fundamental constraint on
the way that we perceive colour. Light reaching our eyes from different
parts of our environment can have a potentially infinite number of
different spectral compositions, and yet we can discriminate "only"
something of the order of thousands of different colours. The reason is
that any number of different spectral compositions must give rise to the
same perception of colour if they cause the same receptor potentials in
each one of the three cone types. This deduction from the structure of
the retina corresponds to a long-established property of human colour
vision; any colour can be matched by a particular combination of
intensities of just three monochromatic lights drawn from across the
visible spectrum. The trichromacy of colour vision is the basis for all
forms of colour reproduction such as painting, printing, photography,
and television. The evolutionary advantage of trichromacy over
dichromacy is thought to lie in the greater ability that it gives to
detect ripe fruit coloured red, orange, or yellow against a dappled
background of green leaves. This ability contributes to the feeding
success of trichromatic primates. By modelling the output signals of
cones from knowledge of their absorption spectra, Osorio and

Vorobyev (1996) found that the degree to which these outputs
discriminated the natural reflectance spectra of fruits from those of
leaves was greater in a retina with S, M, and L cone types than in one
with only S and L types. Similar evidence has been obtained from
experiments with marmosets, a new-world monkey species in which a
genetic polymorphism results in a mixture of dichromatic and
trichromatic individuals in a population. Caine and Mundy (2000) found
that trichromatic marmosets performed better than dichromats in
searching for small orange food items against natural grass and earth
backgrounds. Although the genetic basis is different, there is a similar
polymorphism in human populations, with about 8% of people (almost all
of whom are males) having some form of colour deficiency. One of these
is dichromacy, which is experienced by people who lack either the M or
the L cone type. The everyday term "colour blindness" is a misleading
description of such a condition, as dichromats can discriminate many
colours just as well as trichromats, but confuse some colours (with red,
orange, yellow, and green hues) that trichromats can distinguish.
Trichromacy is just one arrangement that permits a degree of colour
vision. It is possible to have more than three cone types, and many
birds have four and possibly more (Maier & Bowmaker,

1. LIGHT AND EYES

1993). This implies that two surfaces reflecting light of different
spectral compositions could appear the same colour to humans (if they
excite each of the three cone types equally), but could be discriminated
by a bird. Many bird species possess a type of cone sensitive to
ultraviolet light and so, like bees and other insects (see p. 11), are
able to discriminate surfaces such as flowers and plumage on the basis
of differences in ultraviolet reflectance that are invisible to us
(Bennett & Cuthill, 1994). Many vertebrates (but no mammals) are also
sensitive to the plane of polarisation of light (see p. 11), although
the photoreceptor mechanism involved is different from that in insects
(Cameron & Pugh, 1991). We have explained how the possession of
photoreceptors with two or more pigments differing in their absorption
spectra is a necessary precondition for colour vision, and we deal
further with this topic in Chapter 3 (p. 60). A fuller introduction to
human colour vision and its basis in the properties of cone pigments is
given by Mollon (1982).

CONCLUSIONS In this chapter, our emphasis has been on the eyes of
vertebrates, with the aim of putting human vision into its evolutionary
context. From a wider perspective, however, the adaptive radiation of
the vertebrate eye involves relatively small variations in a structure
that works on the same optical principles in all species. We have only
touched on the much greater range of eyes that are found among
invertebrates, which encompasses several fundamentally different optical
designs. These are explained fully by Land and Nilsson (2002), and their
book is strongly recommended to readers wishing to follow up this topic,
or any other covered in this chapter, in more depth. A more detailed
account of the adaptive radiation of the vertebrate eye is given in
Walls' (1942) classic work, and a modern treatment of the topic,
concentrating on the eyes of birds, can be found in Martin (1994b). A
thorough introduction to the anatomy, physiology

23

and optics of the human eye is given by Barlow and Mollon (1982). In
order to understand the workings of eyes, it is necessary to understand
the physical properties of light, and the biochemistry of its absorption
by photoreceptors. This does not mean, however, that visual perception
is about seeing light; instead, it is about seeing objects, surfaces,
and events in the world around us. Light provides the means of achieving
this, because its spatial and temporal pattern is determined by the
layout of the surrounding world. The formation of an image on the
retina, and the transduction of light in photoreceptors, ensure that at
least some of this pattern is captured in a pattern of electrical
activity in the retina. A camera works in a similar way to an eye,
capturing the spatial pattern of light at a point in space in a pattern
of chemical change in the lightsensitive grains of a film. The camera is
therefore a useful analogy for understanding the optics of the eye, but
it is important not to stretch the analogy too far. There are a number
of design features of the eye that would be highly undesirable in a
camera. The image has a yellowish cast, particularly in the macular
region, and it contains a shadow of the dense network of blood vessels
overlying the layer of receptor cells in the retina. Also, the movements
of the eye consist not just of a series of fixations, during which the
image is static, but of smooth movements and tremor, causing the image
to move continually. A camera that moved in this way would produce
blurred photographs. In principle, as these factors cause predictable
distortions of the retinal image, it would be possible to correct for
them and to recover an image comparable to a photograph. To think in
terms of "cleaning up" the retinal image in this way implies, however,
that the role of the eye is to take a snapshot of the world at each
fixation and to send a stream of pictures to the brain to be examined
there. This conception of the eye's role betrays a second, more serious,
limitation to the analogy between camera and eye. The purpose of a
camera is to produce a picture to be viewed by people, but the purpose
of the eye and brain is to extract the information from the changing
optic

24

VISUAL PERCEPTION

array needed to guide an animal's or a person's actions, or to specify
objects or events of importance. Although this could be achieved by
first converting the retinal image into a neural "image" of
photograph-like quality and then extracting information from this second
image, such a process seems implausible on grounds of economy; it would
be wasteful. Apart from blur caused by optical factors, the
imperfections of the retinal image do not result in any loss of
information about spatial patterns

and their changes. It therefore seems likely that the visual system
extracts important variables embedded in the spatiotemporal pattern of
electrical activity in rods and cones, without first correcting
distortions in it. As we describe in Chapter 2, the extraction of
information about pattern begins in the retina itself, and the optic
nerve does not transmit a stream of pictures to the brain, as a
television camera does to a television set, but instead transmits
information about the pattern of light reaching the eyes.

2 The Neurophysiology of the Retina

Compound and single-chambered eyes map the pattern of light in the optic
array onto a sheet of light-sensitive receptor cells, so transforming it
into a pattern of electrical activity in these cells. This pattern of
receptor cell activity must in turn be transformed so that information
needed to guide the animal's actions is made available. These further
transformations take place in the central nervous system, and one way of
studying them is to record the electrical activity of single nerve cells
in the retina, optic nerve, and brain in response to stimulation by
light. The ultimate aim of this approach is to understand how
information important to an animal is detected by networks of nerve
cells and represented in patterns of neural activity. For all but the
simplest animals, this is a distant goal indeed, and our knowledge does
not yet extend beyond the early stages of neural transformation of
patterns of light. In this chapter we describe the first of these
stages, the transformation that the pattern of receptor cell activity
undergoes in the retina. We consider first a relatively simple example,
and then go on to the more complex retina of vertebrates.

THE RETINA OF THE HORSESHOE CRAB The horseshoe crab Limulus has two
compound eyes placed laterally on its shell, each made up of several
hundred ommatidia (see Figure 1.7, p. 10). Each ommatidium contains 10
or more retinula cells, and their axons form a bundle, the optic nerve,
which runs to the brain. What information passes down these axons when
light falls on the eye? The first step in answering this question was
taken by Hartline and Graham (1932), who recorded the activity of single
axons in the optic nerve while shining a spot of light onto the
corresponding ommatidia. They established that action potentials
(impulses) pass down an axon at a rate roughly proportional to the
logarithm of the intensity of light falling on its ommatidium. As we saw
in Chapter 1, transduction of light by a rhabdomeric receptor causes a
depolarisation of the membrane, which is proportional to the logarithm
of light intensity, and so this result simply implies that the impulse
rate follows depolarisation in a linear way. The resulting logarithmic
coding of stimulus intensity is a common feature of 25

26

VISUAL PERCEPTION

many sensory systems, and is necessary in order to compress a wide range
of physical stimulus intensities (such as natural light) into a narrower
range of impulse rates. So far, it seems that the pattern of light
intensity over the eye of Limulus is reproduced faithfully in the
pattern of activity of optic nerve axons, each one reporting light
intensity in one part of the optic array. In fact, things are by no
means so simple, and both the temporal and the spatial pattern of light
undergo transformation in the retina.

Transformation of temporal pattern Impulse frequency in a receptor cell
axon does not follow changes in light intensity in a simple way, but
shows the phenomenon called adaptation that we mentioned in Chapter 1.
At the onset of a spot of light, impulse rate rises rapidly to a peak
and then falls, in less than a second, to a steady level maintained
while the light is on (Figure 2.1a). Both the peak rate and the steady
level are related logarithmically to the intensity of light (Figure
2.1b). This process is called lightadaptation, and it means that a high
impulse rate signals a sudden increase in light intensity and not a
steady bright light. Second, if a receptor is adapted to light and then
left in darkness, its sensitivity to light gradually rises. The impulse
rate generated in response to a test flash of light increases rapidly
over the first few minutes in darkness, and then more gradually, to
reach a maximum after about an hour. This process of dark-adaptation is
the much

(a) The response of a single ommatidium to light. The frequency of
    discharge of impulses rises rapidly to a peak and then falls to a
    steady level within 0.5 s. (b) The peak response (A) and the steady
    response (B) of a single ommatidium to a flash of light at different
    light intensities. Note the logarithmic relationship between
    intensity and response. Adapted from Hartline and Graham (1932).

slower converse of light-adaptation. Together, they mean that the
activity of an axon does not signal absolute light intensity, but
intensity relative to the level to which the receptor has recently been
exposed. Adaptation is necessary for efficient transduction by a
photoreceptor, because the intensity of light illuminating natural
environments varies over an enormous range, of about 9 log units (see
Chapter 1, p. 18). At any particular level of illumination, the
intensity of light reflected from surfaces will depend on their
reflectances, and these typically vary over an approximately 20-fold
range (de Valois & de Valois, 1990, p. 160). Adaptation allows the
output of a receptor to code this relatively small range of intensities
without being swamped by the much larger variation caused by changes in
illumination with time of day and with weather conditions.

Transformation of spatial pattern In their first experiments, Hartline
and Graham used spots of light small enough to illuminate only one
ommatidium at a time. What happens when, as in real life, light falls on
all the ommatidia of the eye? Is the pattern of activity in the optic
nerve simply the sum of the responses of individual photoreceptors to
light, or do the signals from ommatidia interact with one another? In a
classic experiment, summarised in Figure 2.2a, Hartline, Wagner, and
Ratliff (1956) demonstrated that the outputs of ommatidia do indeed
interact with one another, through a

2. THE NEUROPHYSIOLOGY OF THE RETINA

27

Experiments demonstrating lateral inhibition in the eye of Limulus. (a)
Light falling on ommatidium B inhibits the response of ommatidium A to
light. (b) Ommatidium C is too far from A for it to inhibit A's
response. (c) Even so, light falling on C causes an inhibition of B's
response and therefore lifts the inhibition imposed by B on A.

process of lateral inhibition between neighbouring photoreceptors. Each
cell inhibits the firing rate of those in a roughly circular area around
it. The strength of the inhibition rises with increasing intensity of
light falling on the inhibiting ommatidium, and falls with increasing
distance between the ommatidia. Lateral inhibition is mutual, each
photoreceptor being inhibited by its neighbours, which it in turn
inhibits. Also, each photoreceptor inhibits its own activity, a process
in part responsible for light-adaptation. This model successfully
predicts the effects of more complex patterns of light falling on the
eye. For example, the inhibition imposed on

ommatidium A by illumination of another, B, can be reduced by
illumination of a third ommatidium C on the far side of B from A
(Figures 2.2b,c). This is disinhibition; cell C inhibits B and in so
doing reduces B's inhibition on A. The neuroanatomical basis of lateral
inhibition is in collateral branches spreading sideways from each
receptor cell axon in a layer just below the ommatidia, making
inhibitory synaptic contacts with other nearby cells (Purple & Dodge,
1965). Just as adaptation in receptors causes a transformation of the
temporal pattern of light at the eye, so lateral inhibition causes a
transformation

28

VISUAL PERCEPTION

of its spatial pattern. If the whole eye is evenly and diffusely
illuminated, excitation of receptor cells by light will be largely
cancelled by inhibition from neighbouring cells. The activity of optic
nerve axons will therefore be low and will vary little with changes in
light level. Consider next what happens if there is a sharp boundary
between a brightly and a dimly lit area of the retina. The output of
those ommatidia lying just inside the bright area will be less
inhibited, as their neighbours to the dim side are less active, while
the output of those just across the boundary will be more inhibited, as
their neighbours on the bright side are more active. The result is shown
in Figure 2.3; there will be a marked peak and trough in the firing
pattern at the location of the edge. Lateral inhibition therefore gives
prominence to rapid spatial changes in light intensity, in the same way
as adaptation does for rapid changes over time. We will see in the next
section that the retina of the vertebrate eye works in a similar way to
filter out slow changes in light intensity over time and space, and will
discuss later in the chapter why this should be a general property of
visual systems.

The responses of a row of ommatidia to a dark--light boundary falling on
the eye. Note the sharpening of the response at the boundary caused by
lateral inhibition.

THE VERTEBRATE RETINA In Chapter 1, we described the vertebrate retina
simply as a carpet of rods and cones covering the back of the eye.
However, there are also several layers of nerve cells between the
photoreceptors and the vitreous humour filling the eye, and these
contain cells of four kinds; horizontal, bipolar, amacrine, and ganglion
cells. Dowling (1968) established by electron microscopy that there is a
common pattern of synaptic connections between these neurons in most
vertebrate species (see Figure 2.4). Receptors synapse in the outer
plexiform layer with both horizontal cells and bipolar cells, and
bipolars synapse in the inner plexiform layer with both amacrine and
ganglion cells. Some ganglion cells receive input directly from
bipolars, while others are driven only by amacrines. The axons of
ganglion cells run over the surface of the retina to the blindspot,
where they are bundled together into the optic nerve, which runs to the
brain. The vertebrate retina is clearly more complex than that of
Limulus, and we will see later that it is even more complex than
Dowling's (1968)

2. THE NEUROPHYSIOLOGY OF THE RETINA

29

The structure of the vertebrate retina. RT -- receptor terminals H --
horizontal cells B -- bipolar cells A -- amacrine cells G -- ganglion
cells Reproduced from Dowling (1968) with permission of The Royal
Society.

results suggest. In both kinds of retina, however, there are nerve cell
pathways running in two directions at right angles to one another; a
receptor-- brain pathway and a lateral pathway. The first pathway is
represented in the Limulus retina by the axons of receptor cells, but in
the vertebrate retina it consists of a series of cells linking the
receptors through the bipolars and amacrines to the ganglion cells.
Whereas in Limulus each receptor cell axon runs from one photoreceptor
unit, the outputs of vertebrate photoreceptors are pooled together, so
that a group of receptors acts as a functional unit. This occurs in two
ways; through direct excitatory synaptic contact between neighbouring
rods and cones, and through convergence of a number of receptor outputs
to a single bipolar cell. The number of receptors pooled reflects the
trade-off between acuity and sensitivity discussed in Chapter 1. In a
primate eye, each bipolar cell in the fovea receives input from one or
two cones,

whereas in the periphery many rods may connect to a bipolar cell. The
ganglion cell axons leading from the fovea therefore have the potential
to carry information about fine detail in the pattern of receptor
excitation, whereas those leading from the periphery sacrifice this
potential in order to achieve greater sensitivity to dim light. The
lateral pathway is also simpler in the retina of Limulus, and consists
of the collaterals of receptor cell axons. In vertebrates, it is made up
of two systems, the processes of horizontal cells ramifying in the outer
plexiform layer and the processes of amacrine cells in the inner
plexiform layer. The anatomy of the vertebrate retina therefore suggests
that transformations of spatial pattern similar to those in Limulus are
carried out, but also hints that these patterns are likely to be more
complex. The first step in analysing the transformations of pattern
taking place in the vertebrate retina is to establish the relationship
between its input and

30

VISUAL PERCEPTION

its output; the pattern of light falling on it and the rate at which
ganglion cells fire impulses. The first experiments of this kind were
just like Hartline's on the Limulus retina, using small spots of light
as stimuli. These demonstrated that each ganglion cell has a receptive
field; a roughly circular region of the retina in which stimulation
affects the cell's firing rate. Many different kinds of receptive field
have been identified, but one type---the concentric field---is probably
common to all vertebrates.

Concentric receptive ﬁelds Concentric receptive fields were first
discovered by Kuffler (1953) in a study of the responses of cat ganglion
cells. Kuffler found that the effects of a spot of light depend on
whether the light falls in a small circular area in the centre of the
field, or in the ring-shaped area surrounding the centre. Some cells
respond with a burst of impulses to either the onset of a spot of light
in the centre of the field, or to the offset of a spot of light in the
surround; this is called a centre-on response. Other cells show the
converse, centre-off response; offset of a spot of light in the centre
of the field or onset in the surround causes a burst of impulses

The responses of cat retinal ganglion cells to spots of light. A
centre-on cell (a) responds with a burst of impulses to the onset of a
spot of light in the centre of its field or to the offset of a spot of
light in the surround area. A centre-off cell (b) responds in the
opposite fashion.

(Figure 2.5). Kuffler's experiments therefore demonstrated that light
falling in the two regions of the receptive field has opposite effects,
and so the centre and surround are said to be antagonistic to one
another. When the intensity of light falling in the "on" region
increases or decreases, the strength of the cell's response changes in
the same direction. An increase or decrease in light intensity in the
"off" region causes the response to change in the opposite direction.
Further progress in understanding concentric receptive fields was made
by Enroth-Cugell and Robson (1966), in experiments using sinusoidal
gratings as stimuli. We will describe their methods in detail, because
they have subsequently been widely used in research on the responses of
single cells to light. Like the gratings described in Chapter 1 used to
measure visual acuity, a sinusoidal grating is made up of parallel
bright and dark bars. Their edges have a blurred appearance, however, as
the brightness of the pattern varies sinusoidally with distance rather
than changing sharply at the boundaries of the bars (see Figure 2.6).
The intensity of light reflected from the grating therefore follows a
sine wave, in just the same way as the sound pressure near a vibrating
tuning

2. THE NEUROPHYSIOLOGY OF THE RETINA

31

Sinusoidal gratings varying in orientation and spatial frequency, from a
fairly low frequency (a) through medium frequency (b) to a higher
frequency (c). The luminance profile (d) is a graph of intensity against
position, and for these gratings it is a sine wave. In general, image
intensity varies in two dimensions (x,y) and can be visualized as a
surface (e). The grating shown in (f) has the same frequency as (a) but
lower contrast. The luminance (or intensity) (L) of a sinusoidal grating
as a function of position (x) is: L(x) = LO LO.c.sin(2fx − ) where LO is
the mean luminance and f is the frequency of the grating. Contrast (c),
ranging from −1 to 1, controls the amount of luminance variation around
LO. Changes in phase () shift the position of the grating without
altering anything else.

fork varies sinusoidally with time. One cycle of the grating is the
distance spanned by a light bar and the adjacent dark bar. A grating is
described by the parameters of: spatial frequency, expressed as the
number of cycles per degree of visual angle; contrast, expressed as the
difference between maximum and minimum intensities in the pattern,
divided by their sum; and phase (position) of the pattern relative to a
fixed point, expressed in degrees or radians. Enroth-Cugell and Robson
exposed an area of retina to a diffuse field of light alternating at
regu-

lar intervals with a sinusoidal grating of the same average light
intensity. What responses would be expected at onset and offset of the
grating by cells with concentric receptive fields? Figure 2.7 shows the
distribution of light intensity over the receptive field of an on-centre
cell when it is illuminated by a grating whose bar width matches the
diameter of the receptive field centre. In Figure 2.7a, where the peak
of the grating falls on the centre of the field, a burst of impulses
will occur at the onset of the grating, because the centre becomes
brighter than the surround. If the phase

32

VISUAL PERCEPTION

of the grating relative to the centre of the field is shifted by 180°
(Figure 2.7b), there will be a decrease in response at grating onset,
when the centre becomes dimmer than the surround. If the phase is
shifted by 90° in either direction (Figures 2.7c,d), however, there is
no net change of light intensity over either the centre or the surround
at grating onset or offset. If the response of a ganglion cell is
determined by the difference between light intensity in centre and
surround, we would therefore expect to find two null positions as the
phase of the grating relative to the field centre is changed through a
full 360°. At these positions, in both on- and offcentre cells, there
will be no response to onset or offset of a grating, because an increase
of light in one half of the field is balanced by a decrease in the
other. Enroth-Cugell and Robson found that some of the cat's ganglion
cells, which they called X cells, behave in exactly this way. They are
said to have a linear response since, in order to balance

at the null-point, the summation of light-evoked signals across the
centre and surround regions must occur in a linear (additive) fashion.
Others, called Y cells, behave differently, however. With these, no null
position of a grating can be found, and the cell responds with a burst
of impulses to onset and offset of the grating whatever its phase. A
related distinction between X and Y cells is in their response to moving
gratings. As a sinusoidal pattern of light moves over an X cell's field,
the cell's impulse rate rises and falls with the peaks and troughs of
the pattern. The response of Y cells, on the other hand, shows a
constant elevation to a drifting grating, upon which is superimposed a
modulation in phase with the grating. This test also shows that the
response of a Y cell to the pattern of light in its receptive field is
nonlinear; it cannot be predicted by algebraic summation of excitatory
and inhibitory influences from centre and surround. These early
experiments on the retinal

Schematic view of Enroth-Cugell and Robson's (1966) experiments in which
cat retinal ganglion cells were tested with sine-wave gratings presented
at different spatial phases (a--d), relative to the centre of the cell's
receptive field ( ).

2. THE NEUROPHYSIOLOGY OF THE RETINA

ganglion cells of cats serve to illustrate the basic properties of
concentric receptive fields, and the concept of linear spatial
summation. X and Y cells work in a broadly similar way to those in the
retina of Limulus, as they respond best to contrast in light intensity
between a small region of the retina and the area around it. More recent
research on the primate retina has found these same basic features, but
with important differences in detail that we will describe next.

Retinal ganglion cells of monkeys Most retinal ganglion cells of monkeys
have concentric on- or off-centre receptive fields. These fall into two
distinct categories, and we begin our description of them by considering
differences in their responses to colour patterns (de Monasterio &
Gouras, 1975). One of the types shows a property called
colour-opponency; by using spots of monochromatic light (light of a
single wavelength) as test stimuli, it can be shown that the receptive
fields of these cells have antagonistic centre and surround regions, and
that the peak sensitivities of the two regions are at different
wavelengths. If such a cell is tested with spots of

33

white light, which excite centre and surround about equally, it will
show the same centre-- surround antagonism as a cat X cell. On the other
hand, if a patch of coloured light filling the whole receptive field is
used, spatial antagonism will not occur, since centre and surround are
differently excited, and the cell's response will be a function of the
wavelength of the light. Its firing will be excited at some wavelengths
and inhibited at others (see Figure 2.8). The response of a
colouropponent cell therefore carries information about both the
wavelength of light in its field and differences in light intensity (or
luminance) between centre and surround. The second type of cell
identified by de Monasterio and Gouras (1975) lacks colouropponency, as
the peak sensitivities of centre and surround are at the same
wavelength. In consequence, such a cell responds little to monochromatic
light filling the receptive field, but strongly to a difference in
luminance between centre and surround across a broad band of
wavelengths. Because of these different responses to colour stimuli, the
two cell types have been called

Top: schematic representation of the response of a colouropponent
retinal ganglion cell to the onset of small spots of light of different
wavelengths. The peak excitation by light in the centre of the field,
and the peak inhibition by light in the surround, occur at different
wavelengths. Bottom: response of the same cell to light filling the
receptive field. Excitation occurs at some wavelengths and inhibition at
others.

34

VISUAL PERCEPTION

"colour-opponent" and "broad-band", but they are now more commonly
called P and M cells respectively, because they make connections with
"parvocellular" and "magnocellular" layers of the lateral geniculate
nucleus (LGN) in the brain (described in Chapter 3, p. 45). The
differences between P and M cells in their responses to colour arise
from differences in their synaptic connections in the retina. Recall
that primates have three types of cone, with different absorption
spectra (p. 21). In an M (broad-band) ganglion cell, the receptive field
centre and surround receive input (via bipolar, horizontal, and amacrine
cells) from the same type (or types) of cones, whereas for a P
(colour-opponent) cell, the type (or types) of cone driving the two
parts of the receptive field are different. We will return to colour
opponency and the specific combinations of cones driving P cells in the
next chapter (p. 45). De Monasterio and Gouras (1975) demonstrated a
number of other differences between M and P cells. First, P cells give a
tonic (or sustained) response to a stimulus, continuing to fire impulses
while it is present in the receptive field, whereas M cells show a
phasic (or transient) response which fades quickly if the stimulus does
not change. Second, M cells have thicker axons than P cells and so
conduct impulses more quickly. Third, the receptive field centres of M
cells are larger than those of P cells. The fields of both types
increase steadily in size with eccentricity (the distance from the
fovea, measured in degrees of visual angle), but at all eccentricities
the population of M cells has distinctly larger field centres than their
P cell neighbours (see also Croner & Kaplan, 1995). Leventhal, Rodieck,
and Dreher (1981) matched M and P cells to two types of ganglion cell
with differently shaped dendritic trees, and found that the diameters of
these trees vary in just the way that would be expected; the dendritic
spread of both types increases with eccentricity, and that of M cells is
always greater than that of P cells at the same eccentricity. Because P
cells have smaller receptive fields, it takes more of them to cover the
retinal surface and ensure that there are no gaps where information
would be lost. As a result there are about

eight times as many P cells as M cells (Perry, Oehler, & Cowey, 1984). A
final important difference between M and P cells is in their sensitivity
to patterns of light of low contrast. The contrast sensitivity of a cell
can be measured by projecting a grating onto its receptive field and
then varying the contrast of the grating to determine the threshold
contrast required to evoke a response. When M and P cells are compared
in this way, the contrast threshold of M cells is about 10 times lower
than that of P cells (Kaplan & Shapley, 1986). In other words, M cells
have much higher contrast sensitivity than P cells, but there are fewer
of them. It is worth noting that M and P retinal ganglion cells in
monkeys do not correspond straightforwardly to the X and Y cells of
cats. When Enroth-Cugell and Robson's (1966) tests are applied to monkey
ganglion cells, virtually all P cells and the majority of M cells show
linear spatial summation (like X cells), while a minority of M cells
behave in the same nonlinear way as Y cells (Shapley & Perry, 1986).
There are several possible patterns of evolutionary homology between
ganglion cell types in cats and monkeys. One, favoured by Shapley and
Perry, is that the two types of M cell correspond to cat X and Y cells,
while P cells are a primate specialisation with no equivalent in cats.
Although M and P cells are the best understood types of retinal ganglion
cell, not all fall into these two groups. Some do not have concentric
receptive fields, but respond to light onset, offset, or both, in any
part of the field, or may respond only to moving spots of light (de
Monasterio & Gouras, 1975). Others have colouropponent responses but
without a concentric field (de Monasterio, 1978). These various types of
retinal ganglion cell are now known to form a third distinct pathway to
the brain, known as the koniocellular (or K) pathway (Hendry & Reid,
2000). This is thought to be homologous to a similar group of retinal
ganglion cells in cats, known as W cells (Cleland & Levick, 1974; Stone
& Fukuda, 1974). We will return to consider the functions of the M, P,
and K pathways in Chapter 3 (p. 46).

2. THE NEUROPHYSIOLOGY OF THE RETINA

Retinal mechanisms By recording the responses of ganglion cells to
different patterns of light, we can determine the relationships between
the input and the output of the retina, and so build models of its
internal workings. These in turn can be used to interpret the pattern of
connections between retinal cells revealed by physiological and
pharmacological methods. A classic example of this approach is provided
by Enroth-Cugell and Robson's (1966) model of the processes driving the
responses of on- and off-centre retinal ganglion cells of cats. In their
model, the response of a cell is determined by the algebraic sum of a
centre and a surround component. Each component sums the total light
falling in a circular area, and the contribution of each part of the
area to the sum is weighted according to its distance from the centre of
the receptive field. The relationship between weighting and distance
follows a bell-shaped Gaussian curve (see Figure 2.9), and the effect of
light on the weighted sum therefore falls rapidly with distance from the
field centre. The curve obtained by taking the difference between the
centre and the

Enroth-Cugell and Robson's (1966) model of the organisation of X cell
fields. The strength of the centre component depends on the light
falling in the centre of the field and the strength of the surround
component on light falling throughout the field. In both cases, the
contribution of light intensity is weighted by a Gaussian function. The
response of the cell is determined by the difference between the two
components.

35

surround Gaussians describes the antagonistic organisation of the field.
At one time, it seemed that these two components could be identified in
a straightforward way with two sets of neural connections. Werblin and
Dowling (1969) made intracellular recordings from the retinal neurons of
the amphibian Necturus (see Figure 2.4), and found that the bipolar
cells have centre-on or centre-off concentric fields. This implies that
the linear spatial summation driving centre--surround responses occurs
in the outer plexiform layer. The centre component is generated by the
summed input of a group of neighbouring receptors to a bipolar cell, and
the surround component by the input from a wider circle of receptors,
mediated by horizontal cells. The two inputs act in an opposing way to
generate a sustained slow potential in the bipolar cell. In Necturus,
some ganglion cells are driven directly by bipolars, and therefore have
concentric fields comparable to those of cat X cells. In this classical
model of the retina, nonlinearities in ganglion cell output arise from
neural interactions in the inner plexiform layer. Victor and Shapley
(1979) proposed that these involve a second, indirect input to Y cells
from bipolar cells, by way of amacrine cells. At the synapses between
bipolar and amacrine cells, the bipolar signal is rectified
(rectification is a nonlinear operation in which only the positive or
only the negative part of a signal is passed). This rectified signal is
in turn passed to Y ganglion cells and accounts for the nonlinear
component in their responses to changes in light intensity. In Victor
and Shapley's (1979) model, nonlinear inputs to Y cells arise from
within a cell's receptive field. However, retinal ganglion cells can
also receive inputs of this kind from regions of the retina beyond the
boundaries of the receptive field. By definition, light falling outside
the receptive field cannot, on its own, affect the response of a
ganglion cell, but it may influence the cell's response to light falling
within the receptive field. This "periphery effect" was first found by
McIlwain (1964), and can extend up to 90° or more from the centre of a
cell's field. It implies that there must be long-range connections
across the

36

VISUAL PERCEPTION

retina, which provide nonlinear input to ganglion cells. In recent
years, it has become apparent that the structure of the vertebrate
retina is far more complex than Werblin and Dowling's (1969) pioneering
work suggested. Each class of retinal cell falls into a number of
subclasses that differ in their structure and their synaptic chemistry.
Each subclass has a specific pattern of connections with other cells,
and the result is a set of distinct, parallel pathways between
photoreceptors and ganglion cells in which signals from rods and cones
are kept partly separate. A review of these recent discoveries can be
found in Masland and Raviola (2000), but their importance for present
purposes is their implication that the classical model of retinal
function is also too simple. Recent physiological evidence confirms
this, and we will give two examples to illustrate. First, Cook and
McReynolds (1998) showed that, in the salamander retina, summation of
signals from the centre and surround regions of a receptive field occurs
at more than one stage, and not just in the outer plexiform layer as the
classical model proposed. Summation mediated by horizontal cells in the
outer layer operates over relatively large areas of the retina. The
resulting bipolar cell output drives some ganglion cells directly, and
so these have large receptive fields. Smaller receptive fields are
generated by a further stage of spatial summation in the inner layer,
mediated by amacrine cells. The spatial scale of centre--surround
summation in the retina is therefore controlled by at least two separate
processing stages. Second, retinal ganglion cells can show nonlinear
behaviour not explained by the classical model. As the contrast of a
pattern of light falling in the receptive field increases, the response
of the cell becomes more transient (i.e., falls off more quickly after
the onset of the pattern). In monkeys, this nonlinear behaviour is found
in M cells but not in P cells (Benardete & Kaplan, 1999). It can be
accounted for by adding a feedback loop that inhibits the cell's
response immediately following a large change in contrast (Victor,
1987), and this "contrast gain control" mechanism is thought to operate
in the inner plexiform layer

(Sakai, Wang, & Naka, 1995). This mechanism acts to stabilise the
response as contrast changes (whether or not there is any change in
average illumination) and so to keep maximum sensitivity to small
changes in contrast over a wide operating range. Contrast gain control
is one example of the dynamic properties of retinal circuitry, which
were not captured in the classical model. A dynamic process changes its
characteristics in response to changes in the pattern of input, rather
than carrying out a fixed operation on it. We have already seen one
example in the adaptation of Limulus photoreceptors to changes in
illumination; the relationship between light intensity and receptor
potential (or gain) is not fixed, but changes as the input intensity
changes. Light and dark adaptation also occur in the vertebrate retina,
but not at a single site. Adaptation within individual photoreceptors
can be demonstrated directly by intracellular recordings in large cells
such as the cones of Necturus. These respond to light with a
hyperpolarisation proportional to the logarithm of light intensity over
an intensity range of 3.5 log units. The centre of this range
continually shifts, however, to match the current background
illumination. Figure 2.10 shows the intensity--response curves for cones
adapted to three different background intensities. Note that the cell's
response does not signal absolute light intensity, but intensity
relative to the current level of adaptation. Identification of the
mechanisms of adaptation in the human retina requires more indirect
methods. He and MacLeod (1998) obtained evidence suggesting that
adaptation occurs in individual human cones by using laser interference
fringes. This method effectively bypasses the blurring normally imposed
by the eye's optics (see Chapter 1, p. 16) and so allows a grating to be
projected onto the retina with a higher spatial frequency than would
normally be possible. If the bar width of the grating is about the same
as the diameter of a cone, then modulating its contrast over time will
cause the intensity of light falling on a cone to fluctuate. Under these
conditions, observers do not see the grating, because cone outputs are
smoothed by spatial integration later

2. THE NEUROPHYSIOLOGY OF THE RETINA

37

The relationship between log light intensity and receptor potential of a
cone. The circles mark the light level around which each curve was
obtained and to which the receptors were adapted. Adapted from Normann
and Werblin (1974).

in the retinal circuitry. However, they do see a flicker in the
apparently uniform visual field, which implies that cone signals do not
follow light input in a linear way. Instead, fast changes in sensitivity
to light (with about 20 ms delay) must occur at a stage where the
grating is still resolved, either within the cones themselves or at
their synapses with bipolar cells. In scotopic conditions, when vision
is dependent on rod signals alone, modulating the contrast of an
invisible high spatial frequency pattern does not cause visible flicker
(He & MacLeod, 2000). This finding supports the earlier conclusion of
Rushton (1965) that adaptation does not occur in single rods, but at a
later stage in the retina where rod outputs are pooled. The process that
trades sensitivity to low light intensity against acuity is flexible,
and rod outputs are pooled over a wider area of the retina the lower the
intensity of light. This mechanism of adaptation is slower than that
operating in cones, but noisy fluctuation in rod outputs under low
illumination (see Chapter 1, p. 17) will in any case prevent changes in
average illumination being followed as quickly as in brighter light.
There is also evidence that adaptation in scotopic conditions is
achieved in part through the presence of two or more distinct retinal
pathways for processing rod outputs, each specialised to operate over a
particular range of light intensity (Sharpe & Stockman, 1999). In the
vertebrate retina, it therefore appears that adaptation comes about

through the parallel operation of several different mechanisms.

THE RETINA AS A FILTER At several points in this chapter, we have
described the retina as a filter, and have introduced the idea that
adaptation, lateral inhibition, and centre--surround antagonism filter
out slow changes in light intensity over time and space. In this
section, we will introduce some concepts that allow filtering operations
of this kind to be described in more precise ways, before looking at
their application to retinal processing. Imagine some physical quantity
that varies over time; for example, the pressure propagated through the
air from a sound source. If the source is a tuning fork, which produces
a pure tone, then pressure will vary sinusoidally with time (see Figure
2.11a). If the sound is a note played on a musical instrument, the
variation of air pressure with time will be more complex, because
multiples of the fundamental frequency, or harmonics, are also produced
and add to the fundamental (see Figures 2.11b,c). It is easy to see that
such a waveform can be described in two ways, either as a pattern of
variation of intensity with time, or as a set of component sinusoidal
waveforms added together. In order to convert the second description
back into the first, we would need to know

38

VISUAL PERCEPTION

(a) A sinusoidal waveform and (b) its third and fifth harmonics. The
    relative amplitudes of the fundamental frequency (a) and
    harmonics (b) are similar to those that might be produced by a
    musical instrument. (c) The waveform produced by adding the three
    components together. (d) When even higher harmonic frequencies are
    added appropriately, the resulting waveform (in this example)
    resembles a square-wave more and more closely.

the frequency and amplitude of each component, and also their phase
relationships---how they are to be "lined up" before being added
together. This way of converting a sound pressure waveform into a
description of its component frequencies is a simple example of a
mathematical operation called Fourier analysis. Both descriptions
contain exactly the same information about the pattern of change of
pressure over time, but represented in two different ways that can be
changed back and forth from one to the other. The next step in
understanding the general importance of Fourier analysis is to see that
any waveform can be analysed in the same way; it need not be made up of
a fundamental frequency and harmonics, and need not even be periodic. In
general, the Fourier description of a waveform will not be a small set
of frequencies (as in the simple case of a musical tone), but a
continuous function of amplitude and phase against frequency. So far we
have used patterns of sound pressure varying over time to illustrate
Fourier analysis, but there is nothing special about either sound
pressure or time. The same principles can be

applied to the variation of any quantity over time, such as the
fluctuating light intensity falling on a photoreceptor, or to the
variation of a quantity over space. The light intensity reflected from
the grating shown in Figure 2.6 varies sinusoidally along one spatial
dimension, and Fourier analysis would yield a single component at a
particular spatial frequency. More complex gratings could be made by
adding harmonics to the fundamental spatial frequency. As higher and
higher harmonics were added, the grating would come to resemble a
square-wave grating (for example) more and more closely (Figure 2.11d).
Finally, Fourier analysis can be applied not only to patterns in which
light intensity varies along one spatial dimension, but also to images
in which it varies along two dimensions. The only difference is that
amplitude and phase are now functions not only of spatial frequency but
also of orientation. An example of 2-D Fourier analysis of an image is
given in Figure 2.12. Describing patterns of light in frequency terms
makes it possible to describe filtering operations in a precise,
quantitative way. The input and

2. THE NEUROPHYSIOLOGY OF THE RETINA

39

Some images (above) and their Fourier spectra (below). Each sinusoidal
grating (Fourier component) of the image gives rise to a pair of points
in the spectrum. Thus a simple plaid composed of just two gratings (A)
yields two pairs of points in the spectrum (D). Distance of the points
from the origin (centre) represents the spatial frequency of that
component, while the orientation of the pair of points corresponds to
its orientation in the image. In (D) the pair of points lying on a
horizontal axis in the spectrum corresponds to the vertical component in
(A); the oblique pair of points corresponds to the lower-frequency,
oblique component. (B) An image with complex spatial structure (the
standard test image "Lena") has a complex spectrum with many components
(E). The low-frequency components are strong. This tends to obscure the
fact that much of the image structure is carried by the higher
frequencies, further from the origin. The high frequencies are shown
more clearly in (F) where the low frequencies have been suppressed. The
image (C) corresponding to this modified spectrum retains its spatial
structure, but loses shallow gradients and broad-scale contrasts between
light and dark. Note the oblique band of frequencies in the spectrum,
arising mainly from the oblique structure of the hat.

output of a filter are related by a transfer function, which specifies
how effectively different frequencies pass through the filter. For
example, in highpass and low-pass filters, only the temporal or spatial
frequencies above or below some value are transmitted, while in a
band-pass filter only those frequencies within a particular band of
values are transmitted. An illustration of the results of passing an
image through low- and high-pass spatial filters is shown in Figure
2.13. A more detailed

introduction to the concepts of Fourier analysis and frequency
filtering, which requires only a modest background in mathematics, can
be found in de Valois and de Valois (1990). We will make extensive use
of the concepts outlined above in later chapters, but now turn to
consider their application to retinal processing. The retina is just one
of a series of filters that operate on the optic array. Before
photoreceptor signals pass to the retinal circuitry, the processes

40

VISUAL PERCEPTION

Spatial filtering of an image. (A) A high-pass filtered version of the
original (B), with low frequencies suppressed, as in Figure 2.12C. (C)
The complementary, low-pass filtered image, with high frequencies
suppressed. In fact, (C) was produced by blurring the image directly, to
average or smooth out the higher frequencies, and (A) was formed by
subtracting the low frequencies (C) from the original (B); i.e., B = A +
C. Graphs show the intensity profile of a horizontal slice through the
centre of each image. Note the smoothness of (C) and the lack of
largescale differences in (A).

of image formation and transduction of light described in the last
chapter have already filtered out high spatial and temporal frequencies.
Optical formation of a retinal image and the pooling of light across
each receptor aperture both act as low-pass spatial filters (see p. 16),
and the probabilistic nature of photon capture implies a low-pass
temporal filter, which can be significant at low light intensities (see
p. 17). These early filters are then followed by the neural circuits of
the retina, which in general act as high-pass temporal and spatial
filters, through the processes of adaptation and lateral inhibition
(centre-- surround antagonism) respectively, although the spatial
summation of light responses within

receptive field centres is another low-pass operation, akin to the
blurring imposed by the optical filtering in the eye. It is important to
note that this broad generalisation about the function of retinal
filtering omits many differences in detail between different animal
species. For example, there is evidence that primate ganglion cells with
concentric receptive fields do not carry out as important a spatial
filtering operation as this general account suggests. The response of
monkey ganglion cells does not fall off as steeply as would be expected
when the spatial frequency of a grating is reduced, because the
contribution of the surround component to the cell's response is only
about half of the centre

2. THE NEUROPHYSIOLOGY OF THE RETINA

response even at low frequencies (Croner & Kaplan, 1995). The receptive
field profile and spatial frequency response of a "typical" ganglion
cell in the monkey's central retina are shown in Figure 2.14. Note in
panel A that the inhibitory surround is relatively weak compared to the
receptive field centre, and so there is correspondingly modest
attenuation of the cell's responsiveness at low frequencies. This means
that the ganglion cell's response does not accentuate sharp luminance
changes as much as is often supposed. It may therefore be that in
primates and some other mammals (such as cats) only mild spatial
filtering occurs at the level of the retina (Robson, 1983), while more
diverse and selective filtering occurs in the striate cortex (see
Chapters 3 and 5). Retinal processing serves functions other than

41

linear spatial and temporal filtering. In particular, evidence shows
that it involves multiple gain control mechanisms that maintain
sensitivity to small changes in light intensity over time and space
across a wide range of light conditions. We saw in the previous section
how adaptation operates at various stages of retinal processing to
stabilise nerve cell responses against changes in mean illumination of
the retina. There is also evidence for adaptation to changes in more
complex properties of the retinal image. Smirnakis, Berry, Warland,
Bialek, and Meister (1997) exposed retinal ganglion cells of rabbits and
salamanders to uniform illumination of their receptive fields,
flickering irregularly around a constant mean value. At long intervals,
the range (and therefore the mean contrast) of this flicker changed.
Following a change from low to high contrast, the

"Typical" ganglion cell receptive field in central vision of the monkey.
(A) Line response profile represents the responsiveness of the cell to a
long, thin, bright line presented at different positions across the
receptive field. The curve was generated from median values of
experimental data on P cells in central vision (Croner & Kaplan, 1995),
using Gaussian profiles for centre and surround (thin lines). (B)
Spatial frequency response to gratings of different spatial frequencies.
Note that both the centre and surround act as low-pass filters, but
their net effect (centre minus surround) is a broadly tuned, band-pass
filter. Attenuation at low frequencies is mild, because the surround is
weaker than the centre.

42

VISUAL PERCEPTION

response of the cells increased but then declined slowly, over a period
of tens of seconds; the opposite happened after a change from high to
low contrast. The ganglion cells therefore showed adaptation to a mean
level of contrast maintained over a relatively long period of time.
Smirnakis et al. also obtained evidence suggesting that this adaptation
may occur independently for flickering patterns of different spatial
frequency. What might the functional significance of slow adaptation to
changes in mean contrast be? As an animal moves from one area to
another, or lighting conditions change, it will encounter changes not
only in mean illumination (for example, between open and shaded areas)
but also in mean contrast (for example, between a diffusely shaded
forest floor and a dappled one). The signals transmitted from retina to
brain must carry the maximum information about small spatial and
temporal changes in light over these large changes in light conditions.
Adjusting sensitivity to contrast according to the mean contrast
recently encountered would contribute to the efficiency with which this
could be achieved.

CONCLUSIONS

Beginning with the pioneering work of Hartline and of Kuffler, research
on the properties of retinal cells has reached a point where some
general principles of retinal function are clear. We understand how the
retina acts as a temporal and spatial filter of patterns of light
intensity and spectral composition. More recently, the dynamic
properties of retinal processing have become better understood, and
these imply that one of its functions is to normalise its output to the
brain in relation to the statistics of the retinal image, including mean
illumination and contrast, and perhaps further higher-order properties.
These dynamic processes pose exciting challenges for understanding both
the neural circuitry within the retina and its role in the whole scheme
of the vertebrate visual pathway. In Chapter 3, we will continue our
survey of the physiology of vision by following the optic nerves into
the vertebrate brain.

3 Visual Pathways in the Brain

moving stimuli, arranged in a number of layered retinotopic maps of the
contralateral visual field (Wurtz & Albano, 1980). One major function of
the superior colliculi is to control the eye and head movements that
determine gaze direction (Klier, Wang, & Crawford, 2001). The majority
of mammalian ganglion cells project to the dorsal part of the two
lateral geniculate nuclei (LGN) of the thalamus (see Figure 3.1). Their
axons terminate at synapses with LGN cells, which are arranged in
layers, or laminae. Each layer contains a retinotopic map of half of the
visual field, those in the right LGN having maps of the left side of the
visual field and those in the left LGN maps of the right side. In
animals with laterally placed eyes, this is because the optic nerves
cross over completely at the optic chiasm to run to opposite sides of
the brain. In animals with binocular overlap, there is a partial
crossing over, or decussation, of the optic nerves at the chiasm. The
axons of ganglion cells in the left halves of each retina (carrying
information about the right half of the visual field) run to the left
LGN, and conversely for axons from the right halves of each retina, as
illustrated in Figure 3.2. This pattern of connections means that images
of the same object formed on the right and the left retinas can be
processed together in the same part of the brain. The axons of LGN cells
in turn form

The neural circuitry of the retina transforms a fluctuating pattern of
light into a pattern of neural activity in retinal ganglion cells, which
is then transmitted along the optic nerve to the brain. There, many
further operations take place on it, with the eventual result that the
actions of the animal are controlled appropriately. In this chapter we
will outline the advances that have been made in understanding the
initial operations that occur on visual input to the brain. We will
begin with a short general description of the pathways from eye to brain
in vertebrates, and then go on to consider the visual pathway of
primates in more detail. In exothermic vertebrates, the axons of retinal
ganglion cells making up the optic nerves project to the optic tectum, a
structure in the midbrain. Their projection is orderly, with axons
maintaining the same topographic relationship to each other as that of
their receptive fields on the retina. This order is maintained in the
tectum, where cells are arranged in layers called retinotopic maps, in
which the positions of cells relative to one another correspond to the
relative positions of their fields on the retinal surface. In mammals,
some retinal ganglion cells project in a similar way to the paired
superior colliculi in the midbrain (see Figure 3.1). These structures
are homologous to the optic tectum and contain cells sensitive to

43

44

VISUAL PERCEPTION

Schematic diagram of the main visual pathways of a primate. Adapted from
Gluhbegovic and Williams (1980).

An illustration of the decussation of optic nerve fibres at the optic
chiasm, showing how information about objects in one side of the visual
field is transmitted to the opposite side of the brain.

3. VISUAL PATHWAYS IN THE BRAIN

the optic radiations (see Figure 3.1) and project to the visual cortex,
a part of the highly folded sheet of nerve cells that forms the outer
layer of the cerebral hemispheres. Other ganglion cell axons run to
various structures such as the hypothalamus, tegmentum, pulvinar
nucleus, and ventral LGN, but little is known of these pathways. The two
main routes for visual information to the mammalian brain are therefore
the projections from retina via LGN to the visual cortex
(geniculocortical pathway), and from retina to superior colliculi (the
retinotectal pathway). For the moment, we will concentrate on the first
of these, but later in this chapter we will see something of the role
played by the second.

THE LATERAL GENICULATE NUCLEUS In Old World monkeys, the LGN contains
six major layers of cells, three receiving input from one eye and three
from the other. As in other mammals, each layer maps the contralateral
visual field, and all six maps are aligned in precise register. There is
a striking difference in cell size between the lower two and the upper
four layers. Because cells in the former are larger, they are called the
magnocellular layers, and the latter are called the parvocellular
layers. Most M retinal ganglion cell axons project to the magnocellular
layers, and the rest to the superior colliculi, while all P ganglion
cells project to the parvocellular layers (Leventhal et al., 1981); it
is this pattern of connections that gave M and P ganglion cells their
names. In addition, a population of "K" cells projects to six
koniocellular layers, each one lying just below one of the magno- or
parvocellular layers and containing cells smaller again than the P cells
(Hendry & Reid, 2000). The receptive fields of LGN cells are generally
similar to those of the retinal ganglion cells that drive them, and the
differences between M and P ganglion cells in colour opponency, contrast
sensitivity, and linearity described in Chapter 2 (p. 34) also apply to
M and P cells in the LGN (Derrington & Lennie, 1984). The colour-

45

opponent responses of P cells fall into just four categories
(Derrington, Krauskopf, & Lennie, 1984). The two most common categories
of cell are driven by antagonistic inputs from L and M cones (see p. 21)
and are either excited by red light and inhibited by green (+R−G cells)
or vice-versa (+G−R cells). In the other two cell types, one part of the
field is driven by both L and M cones, in varying combinations, while
the other is driven by S cones. These +B−Y and +Y−B cells show blue--
yellow opponency and are less common than the red--green opponent types.
Figure 3.3 illustrates the responses of all four types, and shows how
the transition between excitation and inhibition of their response is
independent of light intensity. Where a patch of light fills its
receptive field, P cells therefore carry information about its
wavelength independently of its intensity, a property that is necessary
for colour vision (see p. 21). The receptive fields of some K cells are
similar to the centre--surround fields of M and P cells (Xu, Ichida,
Allison, Boyd, Bonds, & Casagrande, 2001), but the population includes a
number of other types. Cells in the two most dorsal koniocellular layers
have fields that are larger than those of either M or P cells at the
same eccentricity, and are driven by a category of retinal ganglion cell
with widely branching dendrites. Those in the central two layers are
excited by short-wavelength light, and continue a pathway in the retina
specialised for the transmission of S cone signals. K cells in the two
most ventral layers have similar properties to those retinal ganglion
cells that project to the superior colliculus and ventral tegmentum
(Hendry & Reid, 2000). The close similarity between the receptive fields
of retinal ganglion cells and the LGN cells that they drive suggests
that little further filtering takes place in the LGN, and that it simply
acts as a relay between retina and visual cortex. However, it is
important to note that axons also run from the visual cortex back to the
LGN, and the existence of this feedback pathway hints at a more complex
role for the LGN. We will return to this problem when we discuss
feedback pathways in the visual system generally (p. 71).

46

VISUAL PERCEPTION

The relationship between wavelength of light and response for the four
classes of P cell in the monkey LGN, each at three different levels of
light intensity. The dashed lines show the cells' spontaneous firing
rate in the absence of light stimulation. Reproduced from De Valois,
Abramov, and Jacobs (1966), Journal of the Optical Society of America,
56, pp. 966--977.

The remarkably strict segregation between M, P, and K pathways,
maintained from the retina straight through the LGN to the visual
cortex, suggests that they have different functions. One technique that
has been used to try to identify the functions of the M and P pathways
is to lesion the appropriate layers of the LGN and then test monkeys on
a range of different visual tasks to determine which ones are impaired.
Schiller, Logothetis, and Charles (1990) found that monkeys with a
lesion of the P layers had marked deficits in a number of tasks when
stimuli were presented in the affected part of the visual field; these
tasks included discriminations of colour,

texture, fine shape, and pattern. Magnocellular lesions had less obvious
effects on vision, affecting only the abilities to detect motion in a
complex display and to discriminate a rapidly flickering stimulus from a
steady one. This deficit in flicker detection was later found to occur
only with a relatively low-contrast stimulus (Merigan & Maunsell, 1993).
Another important finding from these lesion experiments is that, in most
circumstances, monkeys' thresholds for detecting contrast were affected
by lesions of the P but not of the M layers (Merigan, Katz, & Maunsell,
1991; Schiller et al., 1990). This seems surprising, because we

3. VISUAL PATHWAYS IN THE BRAIN

have seen that individual M cells have higher contrast sensitivity than
P cells (see Chapter 2, p. 34). However, the results are consistent, and
illustrate the risks involved in jumping from knowledge of single cell
responses to conclusions about the functions of those cells in the
visual system as a whole. Since there are more P cells than M cells per
unit retinal area (p. 34), the P cell population could yield greater
sensitivity than the M population, despite the lower sensitivity of
individual cells. Overall, the results of lesion experiments imply that
most visual tasks can be performed with little deficit when the P layers
alone are intact, and that M cells have a specialised role in
transmitting information about fast motion. This accounts for the
transient responses and high contrast-sensitivity of M cells, their
importance for the detection of low-contrast flicker, and the finding
that lesions in the M layers only affect behavioural contrast
sensitivity for fast-moving stimulus patterns (Merigan, Byrne, &
Maunsell, 1991). These functions of M and P pathways can be described in
terms of the frequency characteristics of the temporal and spatial
patterns that they transmit. The P pathway carries information about
patterns of light at all spatial frequencies, but only at low to medium
temporal frequencies, while the M pathway extends the range of
information carried by transmitting in a region of high temporal and low
spatial frequencies (Derrington et al., 1984; Merigan & Maunsell, 1993).
In addition, the nature of colour-opponency implies that the P pathway
acts as a channel for transmitting information about luminance contrast
at high spatial frequencies, and about chromatic contrast (differences
in the spectral composition of light) at low spatial frequencies.
Finally, it should be noted that recent knowledge of the diverse
properties of K cells implies that there are rather more than two
parallel channels for transmitting information from the retina to the
LGN. The differences between the M and P pathways must therefore be part
of a larger picture that we do not yet understand (see Xu et al., 2001).

47

THE STRIATE CORTEX

Like other regions of the cerebral cortex, the visual cortex is a folded
sheet of neurons about 2 mm thick. It has a distinctive striped
appearance in cross-section, caused by the arrangement of cells in
layers of different densities (see Figure 3.4), and for this reason it
is also known as the striate cortex. Below these layers is the white
matter, made up of the axons that connect the striate cortex to other
areas of the cortex, and to the LGN and other subcortical structures.
The axons of LGN cells terminate at synapses with cortical cells in most
layers, although their most abundant connections are in layer 4C, where
M and P cells terminate in separate sublayers, and in layers 1 and 3,
where K cells terminate. Cortical cells form a complex network in which
fibres run both vertically between layers and horizontally within them,
and both the number of different cell types, and the complexity of the
connections between them, are much greater than in the retina (Callaway,
1998). The first recordings from single cells in the striate cortex were
made in cats (Hubel & Wiesel, 1959, 1962), and the first results from
monkeys were obtained by Hubel and Wiesel (1968). Some cells, including
all those in layer 4C, have concentric fields similar to those in the
LGN, but others have quite a different kind of receptive field
organisation, in which elongated excitatory and inhibitory areas lie
adjacent and parallel to each other. In some cells, there is just one
excitatory and one inhibitory area, while in others there are three or
more parallel antagonistic areas (see Figure 3.5). Hubel and Wiesel
(1968) classified all these as simple cells. Like the X retinal ganglion
cells of cats (see p. 32), they perform a linear spatial summation of
light intensity in their fields, and so are sensitive to the contrast
and position (or phase) of a grating. Because of the elongated shape of
their receptive fields, they also have a distinctive property absent in
the retina or LGN; they respond most strongly to a particular
orientation of a bar, edge, or grating (see Figure 3.6). This
orientation selectivity (or tuning) is quite

48

VISUAL PERCEPTION

Section of monkey visual cortex stained to show cell bodies. The outer
surface of the cortex is at the top, and the layers are numbered I to VI
down to the white matter (W). The lowest layers of a fold of cortex can
be seen below the white matter. Reproduced from Hubel and Wiesel (1977)
with permission of The Royal Society.

narrow, and turning the stimulus through more than about 20° from the
preferred orientation greatly reduces the cell's firing rate. Hubel and
Wiesel proposed that orientation selectivity arises in the visual cortex
because each simple cell is driven by excitatory and inhibitory input
from alternating, parallel rows of LGN cells aligned in a particular
direction. Research on neural circuitry in the cortex has confirmed that
inputs from LGN to simple cells are organised in this way, and provide
the basis of orientation selectivity (Ferster & Miller, 2000). Another
group of cells in the striate cortex was classified by Hubel and Wiesel
as complex cells. Although these have many properties in common with
simple cells, including orientation selectivity, their defining feature
is that a suitably

oriented stimulus will elicit a response no matter where it lies in the
receptive field. This property is known as "phase invariance" and can be
identified by testing the cell with a "drifting" grating that moves
through the receptive field. The response of a simple cell will rise and
fall as the peaks and troughs of the grating pass the boundaries between
regions of the field, whereas the response of a complex cell remains at
a steady high level (De Valois, Albrecht, & Thorell, 1982). Phase
invariance implies that the receptive field cannot be divided into
distinct excitatory and inhibitory regions. Although complex cell
responses are clearly nonlinear, this is not because they are driven by
a subset of M cells in the LGN that have nonlinear responses. The large
majority of complex cells are

3. VISUAL PATHWAYS IN THE BRAIN

49

Schematic representation of the variety of receptive field structures
discovered for simple cells in the striate cortex. The number of
excitatory (light) and inhibitory (dark) regions may be 2, 3, 4, or
more, and the orientation of the field might be at any angle, not
necessarily vertical. The receptive field may be symmetrical (even) or
anti-symmetrical (odd), or may be asymmetrical (not shown). Receptive
field size also varies widely from cell to cell, and increases greatly
in peripheral vision. The somewhat idealised fields shown here are Gabor
functions (discussed in the text, p. 56; see also Glossary).

Orientation selectivity in a simple cell. A light--dark edge falling on
the vertical boundary between excitatory and inhibitory areas evokes a
strong response. As the edge is rotated, less of the excitatory and more
of the inhibitory area is illuminated, and the response is reduced.

50

VISUAL PERCEPTION

driven by both M and P input, or by P input alone (Malpeli, Schiller, &
Colby, 1981). The nonlinear behaviour of complex cells must therefore be
generated by interactions within the striate cortex. Hubel and Wiesel
proposed that each complex cell is driven by a group of simple cells
sharing the same orientation preference and having closely overlapping
receptive fields. This spatial pooling of simple cell signals would
result in a complex cell response that was invariant with the position
of a stimulus in its receptive field. Recent anatomical and
physiological evidence has provided support for this model (Alonso &
Martinez, 1998). Simple and complex cells are selective for properties
of the pattern of light in their receptive fields other than
orientation. Many respond weakly to stationary stimuli, and often have
direction-selective responses to moving patterns. Some show "end
inhibition", responding more strongly to a short bar or edge that ends
within the receptive field than to a long one that extends across the
whole field. Hubel and Wiesel believed that cells with end inhibition
are a subset of complex cells, and called them hypercomplex cells.
However, it is now known that both simple and complex cells have varying
degrees of end inhibition, as well as "side inhibition", or both
(DeAngelis, Freeman, & Ohzawa, 1994). Such influences from outside the
classic receptive field are discussed later in this chapter, and in
Chapter 6. The chromatic selectivity of simple and complex cells has
been a matter of debate, and in particular it has not been clear whether
they differ from P cells in the LGN in this respect. A recent analysis
of a population of cells in striate cortex suggests that there is indeed
a reorganisation of the colour-opponent responses in LGN (Johnson,
Hawken, & Shapley, 2001). Some cortical cells resemble M or P LGN cells
in their responses to luminance and chromatic patterns within their
receptive fields, but a third class of cell has "double-opponent"
properties. Unlike a P cell, such a cell has different colour opponent
properties in different parts of its field (compare Figure 2.8) and so
will respond to an oriented edge defined by a chromatic difference
alone.

Functional architecture in the striate cortex The striate cortex is
organised not only into layers, parallel to its surface, in which cells
have different properties, but also into columns that run at right
angles to the surface. Hubel and Wiesel (1962) first discovered this
organisation by relating the responses of single cells to their
positions in the cortex, as reconstructed from electrode tracks, and
called it the functional architecture of the cortex. This technique was
later supplemented by a range of methods for staining cells chemically
according to their activity, or according to their connections with
different parts of the LGN. More recently, optical methods have been
developed which allow rapid changes in the activity of neurons in
cortical tissue to be directly imaged. The most basic feature of
functional architecture is an orderly retinotopic mapping of the visual
world onto the surface of the cortex, just like that in the laminae of
the LGN, with the left and right halves of the visual field mapped onto
the right and left cortices respectively. The map is not metrically
accurate, since the cortical area devoted to the central part of the
visual field is proportionally much larger than that devoted to the
periphery. This is mainly a consequence of the greater density of
retinal ganglion cells in the central retina. As in the retina,
receptive fields in the centre of the visual field are smaller than
peripheral fields. Until recently, it was only possible to describe the
functional architecture of the striate cortex in animals, but the
introduction of non-invasive methods for imaging metabolic activity in
the brain has made it possible to obtain comparable data from humans. In
particular, functional magnetic resonance imaging (fMRI) techniques now
make it possible to measure changes in the oxygen level of the blood
passing through cortical tissue with a spatial resolution on the scale
of a millimetre and a temporal resolution of several seconds. There is a
complex relationship between the metabolic activity of a block of brain
tissue, as measured by fMRI signals, and the firing rates of the cells
that it contains, as measured by the single-unit recording methods we
have described so far. This relationship is not yet fully understood
(for an introduction to this topic, see Wandell,

3. VISUAL PATHWAYS IN THE BRAIN

1999), but for present purposes we can assume that the two are roughly
correlated (Rees, Friston, & Koch, 2000). The functional architecture of
the human visual cortex can be studied by exposing observers to displays
in which a region of twinkling dots moves repeatedly through the visual
field while they hold their direction of gaze steady. At the same time,
fMRI signals are recorded from the cortex. These show waves of change in
metabolic activity passing over the cortex in synchrony with the
movement of the visual stimulus, and the relationship between the two
can be used to map out the human visual cortex. This is a region
approximately 4 cm by 8 cm in size that is located, as in other
primates, in the occipital lobe at the back of the brain and is
retinotopically organised (Wandell, 1999; see Figure 3.7). At present,
fMRI does not allow the functional architecture of the human visual
cortex to be analysed in any more detail. In monkeys, the other methods
that are available have revealed a strikingly complex and precise
structure. First, within each small region of the retinotopic map,

Left: The area of the human cerebral cortex occupied by visual area V1.
The medial (inside) surface of the left occipital cortex is shown.
Right: Diagram of the right visual field showing stimulus positions
corresponding to activated regions of V1. As a stimulus moves from the
centre to the periphery of the right visual field it activates regions
further forward in V1. At any given eccentricity (distance from the
centre) a stimulus activates a region higher in V1 the lower it falls in
the visual field. Adapted from Wandell (1999).

51

cells are segregated according to the eye from which they receive input.
In layer 4C of the cortex, where most axons from the LGN terminate,
cells respond to a stimulus presented in one eye only, just as cells in
the LGN do. In other layers, cells have binocular fields, responding to
a stimulus if it is presented to either eye. However, these cells often
respond more strongly to stimuli in one eye than in the other, and are
said to show ocular dominance. Cells sharing the same ocular dominance
are grouped together into bands, and these form an alternating pattern
of right- and left-eye bands running across the cortex. As Figure 3.8
illustrates, these can be made visible with staining techniques (LeVay,
Hubel, & Wiesel, 1975). Note that there is a good deal of overlap in the
receptive fields of nearby cortical cells, and so the pattern of ocular
dominance columns does not imply that alternating stripes of visual
space are represented in the cortex by input from only one eye. A third
feature of functional architecture was discovered by staining the
striate cortex so as to reveal levels of the enzyme cytochrome oxidase
(CO), which are greater in cells that are more metabolically active.
This technique revealed a regular array of dark blobs of tissue rich in
CO (Wong-Riley, 1979), visible most clearly in layers 2 and 3, but
absent in layer 4. There are many other biochemical differences between
cells in these blobs and those in the "inter-blob" regions (see Edwards,
Purpura, & Kaplan, 1995), and Horton and Hubel (1981) showed that the
blobs are aligned along ocular dominance stripes (see Figure 3.9). P LGN
cells provide input to both blobs and inter-blob regions, whereas M cell
and most K cell input is concentrated on the blobs (Edwards et al.,
1995). In the next section, we will see that there may be functional
differences between the two kinds of region in their processing of
visual information. A fourth aspect of functional architecture emerged
when the orientation preferences of cells were related to their
position. Hubel and Wiesel (1974, 1977) had found from single-cell
recordings that the orientation preferences of cells changed
systematically, and linearly, with position across the cortex for
distances of 0.5--1 mm. After some distance, say 1 mm, where cells had

52

VISUAL PERCEPTION

The pattern of ocular dominance columns in the visual cortex revealed as
alternating light and dark bands by autoradiographic methods. Reproduced
from Hubel and Wiesel (1977) with permission of The Royal Society.

Schematic diagram of a small region of the surface of the visual cortex,
showing alternating bands of right- and left-eye dominance, and CO blobs
(dark circles) lying along them.

shown a systematic clockwise stepping of their orientation preferences,
the sequence would reverse to anti-clockwise. There were also
discontinuities in this "sequence regularity", and

regions of no orientation preference. Hubel and Wiesel therefore
suggested that orientationselective cells are organised in columns or
"slabs", in which all cells have the same preferred orientation, and
that adjacent slabs represent adjacent orientations. They also concluded
that the orientation slabs tended to be at right angles to ocular
dominance bands. However, it is very difficult to derive a complete 2-D
map of orientation preference from 1-D probes with a micro-electrode.
Much more powerful, direct evidence comes from recent studies by Blasdel
and colleagues, who used an optical imaging method to photograph
directly the patterns of activation across a region of monkey cortex in
response to gratings of different orientations. By presenting stimuli to
the left and right eyes separately they could relate the patterns of
orientation preference to the structure of ocular dominance columns.
Sophisticated imageprocessing methods were used to draw the underlying
structures out of related sets of response images. In Figure 3.10a we
show a sample of results from Obermayer and Blasdel (1993). The heavy
lines are the boundaries between left and right ocular dominance bands,
while the lighter lines are iso-orientation contours. Along each of
these contours the orientation preferences of cells should be constant
(though recall that

3. VISUAL PATHWAYS IN THE BRAIN

53

(a) Sample of results from Obermayer and Blasdel (1993) showing the
    relationship between ocular dominance bands and the organisation of
    orientation selectivity in the monkey's visual cortex. Heavy lines
    mark the boundary between bands of cortical tissue dominated by
    input from the left eye (L) and the right eye (R). These bands are
    about 0.5 mm wide. Lighter lines are "isoorientation contours".
    Along each contour, cells would have the same "preferred" or optimal
    orientation. Adjacent contours represent orientations 11.25° apart.
    Note how the contours tend to radiate from "singularities" lying
    along the centres of the L and R bands, but cross the L--R border at
    right angles. Adapted from Obermayer & Blasdel (1993), with
    permission of the Society for Neuroscience.
(b) Our schematic summary of Blasdel's findings (see a). Orientation
    rotates either clockwise (−) or anti-clockwise (+) around each
    singularity, but the iso-orientation contours bend around to cross
    the L--R boundary in parallel bands. Orientation changes more slowly
    in the regions between two singularities of the same sign (saddle
    points), but changes very abruptly between singularities of opposite
    sign (fractures).

54

VISUAL PERCEPTION

single cells are not recorded individually here). The iso-orientation
contours radiate out from "singularities" that tend to lie along the
centres of left and right eye bands; the contours then tend to run
parallel to each other, cross the ocular dominance boundaries at right
angles and converge to another singularity. A complete circuit around
each singularity represents a rotation from 0 to 180° (not 0--360°),
either clockwise or anticlockwise. In Figure 3.10b we offer a schematic
view which we think captures the major aspects of Blasdel's very
extensive findings and suggests some functional interpretations. The
basic, repeating unit of structure is the set of isoorientation contours
radiating from each singularity. The radii bend around to meet their
partners from the adjacent left (L) or right (R) eye band. At the ocular
dominance border, then, we find the parallel "slabs" and sequence
regularity described by Hubel and Wiesel, and the intersection of slabs
and bands at right angles. Near the singularities (which are probably
the centres of CO blobs) we find instead a "pinwheel" structure
(described by Bartfeld & Grinvald, 1992). In order for orientations to
match across the L/R border, the singularities must have opposite
rotational signs (+ and −). Cells in the L/R border region tend to be
driven by both eyes, and this pattern of organisation ensures that the
two monocular inputs are well matched in orientation, a feature that is
presumably important for stereoscopic vision (see Chapter 7). Sequence
regularity requires at least two adjacent singularities of the same sign
within L and R bands. Since singularities are about 350 µm apart
(Obermayer & Blasdel, 1993), we can see why a complete rotation of
orientation preference (0--180°) takes about 700 µm in the "linear
zone". Halfway between singularities of the same sign we meet a "saddle
point" where orientation changes much more slowly. Halfway between
singularities of opposite sign within an L or R band we meet a
"fracture"---a discontinuity in sequence regularity. This whole pattern
(Figure 3.10b) suggests that both a radial arrangement and a linear
arrangement of orientation preferences are

important, possibly for different visual functions (see Blasdel, 1992,
for further ideas).

Spatial ﬁltering in the striate cortex It is clear that a complex
transformation of the input from the LGN takes place in the striate
cortex, but what part does this transformation play in achieving the
eventual goal of using information from light to control behaviour? An
early view of the function of the visual cortex (e.g., Barlow, 1972) was
that it creates a representation of the visual world in terms of
features, such as the edges and corners of objects. Simple and complex
cells were regarded as feature detectors, each signalling the presence
of a specific geometric feature by firing impulses. One problem with
this theory, which we will discuss further in Chapter 5, is that
explaining how the edges and other features of natural objects in an
image are detected is a great deal more difficult than was first
thought. Another problem, more relevant here, is that the firing rate of
a simple or complex cell is determined by many parameters of the pattern
of light in its field, including contrast, phase, orientation, colour,
and motion. The firing rate is therefore not a useful signal for the
presence of a particular feature, such as an oriented edge or a corner.
Instead, each cell is selective for multiple dimensions of the image
falling in its receptive field and gives its peak response to a
particular combination of image properties (Lennie, 1998; van Essen,
Anderson, & Felleman, 1992). In this section, we will deal in more
detail with just one aspect of the function of striate cells--- their
selectivity for spatial patterns of luminance---and will follow the
approach that we introduced in Chapter 2 of treating single cells as
spatial filters. We saw that retinal ganglion cells respond to
sinusoidal gratings over a wide range of spatial frequencies (p. 41).
Their response falls steeply above a high-frequency cut-off determined
by the size of the receptive field centre, but is maintained at low
frequencies because of the relative weakness of the receptive field
surround. Ganglion cells therefore act as "low-pass" filters, responding
to a grating across a broad range of spatial frequencies. The breadth of
this range is

3. VISUAL PATHWAYS IN THE BRAIN

55

Measurement of the selectivity of a cell for spatial frequency. Cell (a)
has peak sensitivity at 2.3 cycles/ degree and a bandwidth of log2 (4/1)
= 2 octaves. Cell (b) has peak sensitivity at 3.2 cycles/degree and a
bandwidth of log2 (5/2) = 1.3 octaves.

measured as the bandwidth of the cell, the ratio of the spatial
frequencies at which half the maximum contrast sensitivity (see Chapter
2, p. 34) is obtained, measured in octaves (Figure 3.11). An octave is a
doubling of frequency, e.g., from 2 to 4 or 4 to 8 cycles/degree
(c/deg); 2 octaves is a quadrupling of frequency, and so on. The
bandwidths of LGN cells are typically 5 octaves or more. Cells in the
striate cortex have much narrower bandwidths, averaging 1.5 octaves, and
so can be regarded as "bandpass" filters, sharply tuned to particular
spatial frequencies. Furthermore, striate cells vary greatly in their
spatial frequency tuning, the optimum frequency ranging from 2 to 8
c/deg in the region corresponding to the foveal visual field (De Valois
et al., 1982; see Figure 3.12). The spatial frequency tuning of a cell
is determined by both the size and number of receptive field regions or
"lobes" (see Figure 3.5). Smaller lobes yield a higher optimum
frequency, while a greater number of lobes (of alternating sign) give a
narrower spatial frequency bandwidth. These findings suggest that the
visual cortex may be organised to analyse the spatial pattern of light
in each of many small regions, or "patches", of the retinal image into
its spatial frequency components. If so, then the firing rate of a cell
provides a measure of the amplitude of the frequency component to which
the cell is tuned, at its preferred orientation. The result would be a

Responses of cells in the same region of visual cortex to sinusoidal
gratings of different frequencies. Responses are expressed as contrast
sensitivity, which is the reciprocal of the grating contrast required to
elicit a criterion response from the cell. Reprinted from De Valois et
al., Copyright (1982), with permission from Elsevier Science.

56

VISUAL PERCEPTION

"patch-wise" Fourier analysis of the retinal image, each patch being
analysed by a small block of cortical tissue containing cells with
overlapping receptive fields and a full range of frequency and
orientation tunings (De Valois & De Valois, 1990; Robson, 1983). Since
Johnson et al. (2001) found that double-opponent simple and complex
cells are just as sharply tuned for spatial frequency and orientation as
those that are selective only for luminance, this patch-wise analysis
could operate on both chromatic and luminance patterns in the image.
According to this theory, simple cells might represent a subsystem in
which the phases of spatial frequency components are encoded, while
complex cells form a parallel subsystem in which absolute phase
information is discarded (recall that only simple cells have receptive
fields with excitatory and inhibitory subregions, and so are selective
for the phase of a grating). The information provided by simple cells
could be used in tasks such as the precise location of objects, while
the outputs of complex cells could be used for purposes such as the
discrimination of different texture patterns, or creating stable
representations of moving objects. There is some evidence that the
blocks of cortex that transform each patch of the image may be centred
on CO blobs. Blob cells are tuned to low spatial frequencies, while
inter-blob regions contain cells tuned to a wider range of frequencies
(Born & Tootell, 1991; Edwards et al., 1995; Tootell, Silverman,
Hamilton, Switkes, & De Valois, 1988). A closer study of the relation
between CO density and spatial frequency preference revealed some very
close correlations (Silverman, Grosof, De Valois, & Elfar, 1989),
suggesting that in moving from blob to inter-blob regions the preferred
spatial frequency of cells increases systematically. We have already
seen that on any single (curved) radius (Figure 3.10b) preferred
orientation is constant. There is therefore a strong possibility that
blobs are the centres of blocks of cortex (see Figure 3.10b) that serve
to represent a Fourier-like transform of small patches of the retinal
image (De Valois & De Valois, 1990). Jones and Palmer (1987) showed that
the

receptive fields of simple cells can be well described as Gabor
functions (see Figure 3.5)---the product of a sine-wave multiplied by a
smooth bell-shaped (Gaussian) envelope (Figure A.1, p. 464), although
other mathematical descriptions can also be used. Making the Gaussian
wider introduces more lobes into the receptive field, and makes it
correlate better with a grating of a particular spatial frequency; in
other words, the receptive field is more specifically tuned to that
frequency. Thus, for a fixed frequency, the wider the spatial receptive
field the narrower is its frequency bandwidth and vice-versa. There is
an inevitable trade-off here between specificity for spatial location
and for spatial frequency. To be perfectly location-specific, the
receptive field should be the size of a single receptor, and to be
perfectly frequency-specific it should be an extended sine-wave. The
Gabor function is a localised wave-packet that partly satisfies both
these demands. Simple cell receptive fields thus approximate an optimum
balance between the opposing demands of coding spatial frequency and
spatial location. Further theoretical analysis by Field (1987) suggests
that the 1.5 octave bandwidth of striate cells may represent an
efficient strategy for coding natural images, and optimises the way in
which image information is shared out across a range of filters tuned to
different frequencies. The key idea in this approach is that natural
images of scenes in our environment have specific statistical
properties, which in turn arise from the properties of natural surfaces
that reflect light. In such an image, the intensity and spectral
composition of light falling on one receptor is strongly correlated with
(or predictable from) the light falling on its neighbouring receptors.
This can be appreciated by considering what happens when neighbouring
points are not correlated. Imagine a large number of TV images created
by assigning random brightness and colour to each element of the image,
so that light at each point was statistically independent of light at
any other point. Virtually all the resulting images would be like the
"snow" on a TV screen when there is no signal, without any structure
resembling a natural scene. Natural images are therefore extremely rare
in

3. VISUAL PATHWAYS IN THE BRAIN

relation to the number of images that potentially could be coded in the
pattern of photoreceptor outputs. This suggests that the visual system
may, at an early stage such as the visual cortex, exploit this
"redundancy" present in natural images to code them more economically.
Barlow (1961) put forward an important early statement of this
theoretical argument, and recently it has been developed in considerably
greater detail. The approach has been to analyse collections of natural
images (for example, of landscapes or vegetation) in order to identify a
finite set of local patterns of light intensity into which they can be
decomposed, and from which they can be recovered. Notice that this is
not the same process as a Fourier transformation, which decomposes an
image into global spatial frequency components (see Chapter 2, p. 38).
Here, the components are spatial patterns of light intensity in small
patches of the image. Bell and Sejnowski (1997) used the mathematical
technique of independent components analysis to generate a set of
statistically independent local filters from a set of natural images,
and these turned out to be oriented edges and bars, resembling the
receptive fields of cortical simple cells. Results such as Bell and
Sejnowski's demonstrate that the pattern of light intensity in natural
images can be economically represented using an "alphabet" of small
patches containing light-- dark edges of different orientation and blur.
They also imply that the receptive fields of cortical cells are "tuned"
to these components, so that the cortical representation of the image is
greatly compressed with respect to that achieved at the level of the
photoreceptors. Further work has extended this type of analysis to
moving images (van Hateren & Ruderman, 1998) and to colour images
(Tailor, Finkel, & Buchsbaum, 2000). The results yield filters that
correspond to some properties of the receptive fields of cortical cells,
but not to all. Schwartz and Simoncelli (2001) extended this approach
further by showing that many of the nonlinear response properties of
cortical cells can also be understood and predicted in terms of the
efficient coding of natural signals (see Chapter 6, p. 151). In broad
terms, there is good evidence that

57

neural processing in the visual cortex has been shaped by evolution to
exploit the redundancy present in natural images and so to achieve more
economical coding of visual input. However, this approach has not yet
explained all the details of the organisation of the visual cortex, or
given a full account of its function. To understand the role of the
visual cortex, we have to consider it in relation to the whole visual
pathway, including the operations that take place on its output. This is
the topic to which we turn next.

BEYOND THE STRIATE CORTEX Many regions of the cerebral cortex
surrounding the striate cortex also contain neurons that respond to
visual input. Some of these "visual areas" in the extrastriate (or
prestriate) cortex can be marked out straightforwardly, as they contain
retinotopic maps of the visual field similar to that in striate cortex.
In others, the map is partial, disorderly, or both, and so the
identification of some areas and their boundaries is difficult (Van
Essen, 1985), and a definitive list cannot be made. In the macaque
monkey, there are at least 30 visual areas, covering a large region of
the occipital, temporal, and parietal cortex (Figure 3.13). The deep
folding of the cortex means that some areas, lying within folds (or
sulci), are not visible from the exterior. The pattern of connections
between extrastriate visual areas is by no means a simple chain from one
area to the next. Instead, each one sends output to several others, and
most if not all connections are matched by reciprocal connections
running in the opposite direction. In all, there are more than 300
neural projections linking extrastriate visual areas, and almost all can
be classified according to the cortical layers in which they arise and
terminate as either ascending (leading away from the striate cortex) or
descending (leading towards the striate cortex). When pathways are
classified in this way, it is possible to organise visual areas into a
hierarchical scheme, in which they are placed at different levels
according to their distance from the striate cortex,

58

VISUAL PERCEPTION

View of the right side of the cerebral cortex of a macaque monkey. The
four sulci (folds in the cortex) marked with arrows have been opened out
to reveal parts of the cortical surface that are normally hidden.
Numbers denote visual areas V1, V2, V3, V3A, and V4. Redrawn from an
original drawing by Professor A. Cowey, from Martin, Copyright (1988),
with permission from Professor Cowey and Elsevier Science.

or V1 (Maunsell & Newsome, 1987; see Figure 3.14). We will see later
that this roughly hierarchical pattern of anatomical connections between
visual areas in the cortex is not necessarily matched by a clearly
hierarchical mode of operation. Another complication not shown in Figure
3.14 is that visual input can enter the network of extrastriate areas
through several routes other than V1. These include inputs from the
koniocellular layer of the LGN and from the pulvinar nucleus (which
receives input both directly from the retina and indirectly by way of
the superior colliculus). These inputs all bypass V1 and we will see
later how damage to V1 can reveal their importance for vision. There is
evidence that the connections between extrastriate areas segregate into
two main pathways after area V2. A "dorsal" pathway runs via V3 and V3A
to the middle temporal area (MT), then to the medial superior temporal
area (MST) and finally to area 7A in the parietal lobe. A "ventral"
pathway runs to V4, then to the posterior and anterior inferotemporal
areas (PIT and AIT) in the temporal lobe. As Figure 3.14 shows, the
segregation between these pathways is by no means clear-cut; there is a
degree of overlap and interconnection between them, and some areas do

not fit clearly into the scheme. Despite these complications, however,
there are several sources of evidence for an underlying separation into
two pathways. Baizer, Ungerleider, and Desimone (1991) traced neural
connections back from the posterior parietal and the inferior temporal
cortex (the supposed destinations of the dorsal and ventral pathways)
and found a striking segregation of the two sets of connections into
different parts of each extrastriate area. Using a different approach,
Young (1992) analysed mathematically the connectivity of extrastriate
areas, and demonstrated that they segregate into two systems. Although a
map of connections between areas, such as Figure 3.14, suggests that the
two pathways divide after V2, the internal structure of V1 and V2
reveals that the division can be traced further back. When V2 is stained
for cytochrome oxidase activity, a pattern of stripes appears, quite
different from the blobs in V1. Dark, COrich stripes alternate with pale
interstripes, and the dark stripes themselves are alternately thick and
thin. The outputs of V1 blob and inter-blob cells to V2 are segregated,
connecting to thin stripes and to interstripes respectively (Livingstone
& Hubel, 1983), while thick stripes receive input from layer 4B in V1.
In turn, thin stripes and interstripes send output to V4, while the
thick

3. VISUAL PATHWAYS IN THE BRAIN

59

Extrastriate visual areas in the macaque monkey. Only the best-known
areas, and the most prominent connections between them, are shown. From
Maunsell and Newsome (1987). Reproduced, with permission, from the
Annual Review of Neuroscience, Volume 10 © 1987 by Annual Reviews,
www.annualreviews.org.

stripes connect to MT (De Yoe & Van Essen, 1985). The division between
dorsal and ventral pathways therefore begins in V1 itself, albeit with
some cross-connection between the pathways within V1 and V2, just as
there are crossconnections between areas at later stages. Going back a
stage further, there appears to be a specific link between the
magnocellular input to V1 and the dorsal pathway. The cells in V1 that
connect directly to area MT are driven predominantly by magnocellular
input from the LGN (Yabuta, Sawatari, & Callaway, 2001). Apart from this
specific case, however, V1 seems to combine and reorganise M and P
inputs into a new two-way division between dorsal and ventral pathways
(see Callaway, 1998). Our next question is whether the anatomically
distinct dorsal and ventral pathways also have separate functions. Do
they extract different kinds of information from the pattern of activity
in V1? In the next section we will outline the physiological evidence
relevant to this issue.

Functions of extrastriate visual areas One means of investigating
whether extrastriate areas have specific functions is to examine the
effects of lesions in particular areas on visually guided tasks.
Experiments of this kind have provided strong evidence for
specialisation of dorsal areas in motion processing. Lesions in area MT
cause specific deficits in motion perception while leaving pattern
discrimination unaffected (Newsome & Paré, 1988; see Chapter 8, p. 228).
In contrast, the effects of lesions in ventral areas are not so
clear-cut. V4 lesions affect the ability to learn colour and pattern
discriminations (e.g., Heywood & Cowey, 1987), but other findings
suggest a more complex role for this area. Schiller and Lee (1991) found
that monkeys trained to shift their gaze towards the "odd" element in a
display performed poorly after V4 lesions, but only if the odd element
was less conspicuous than the others. This was so not only for size and
brightness discrimination tasks, but also for a motion task. The
implication is that V4 has a critical role in controlling attentional
strategies, and is not only

60

VISUAL PERCEPTION

involved in the processing of colour and shape. Supporting this
interpretation, Baizer et al. (1991) found that V4 was one of only two
extrastriate areas where connections from temporal and parietal cortex
mix appreciably, suggesting that its function does not fit easily into
the theory of two pathways. Overall, the lack of experiments directly
comparing V4- and MTlesioned animals on the same visual task makes it
hard to draw firm conclusions, but the lesion evidence for a specialised
visual processing role of area MT is much stronger than that for area
V4. In Chapter 8 (p. 228), we will consider in more detail the role of
area MT in the processing of image motion. The responses of single cells
in different extrastriate areas provide another source of information
about functional differences. As we have seen, cells in V1 act as
multidimensional filters, selective for many different stimulus
parameters. If the dorsal and ventral extrastriate pathways have
separate functions, then we would expect that single cells in them would
be selective for fewer stimulus dimensions, and that cells in different
areas would be selective for different dimensions. There is a little
evidence for this kind of differentiation of single cell properties
within V2. Wavelength-selective cells are most common in thin stripes
and in interstripes, while orientation selectivity is most common in
thick stripes, although these differences are only in relative numbers,
and all cell types are found in all regions (De Yoe & Van Essen, 1985).
At later stages, the evidence is that cells continue to be
multidimensional in their selectivity. Cells in both MT (Malonek,
Tootell, & Grinvald, 1994) and V4 (Desimone & Schein, 1987) are
selective for orientation and most other parameters for which V1 cells
are selective. There are differences in the relative numbers of
different cell responses; direction selectivity is rare in V4 but common
in MT, while the opposite is true for wavelength selectivity (Zeki,
1978). Even so, it is not clear that these differences in the
proportions of different responses have any significance for
understanding the functions of the areas concerned (for further
discussion, see Lennie, 1998).

Hierarchical processing in extrastriate pathways Evidence for functional
specialisation of extrastriate areas has also been sought by looking for
increases in the complexity of the dimensions for which cells are
selective. Imagine, for example, that we found an extrastriate area
where cells were selective for wavelength, direction of motion,
contrast, and so on, but also for complex properties of geometric
patterns, such as the number of corners possessed by a shape. If we
could establish that V1 cells were not selective for such properties,
then we would have evidence that the area concerned was part of a
pathway responsible for hierarchical processing of information about
shape. There is strong evidence that cells in MT and MST are selective
for more complex properties of motion than just its direction,
supporting the view that the dorsal pathway is specialised for motion
processing. We will discuss the details of this evidence in Chapter 8
when we deal with the computation of motion in general, and for now will
consider only the ventral pathway. Two properties of cells in V4 and the
inferotemporal (IT) area have been studied extensively, and provide
evidence for hierarchical processing of colour and shape information.
First, Zeki (1980) made the important discovery that some V4 cells show
colour constancy. This term refers to the fact that we perceive a
surface as having a constant colour despite changes in the spectral
composition of light reflected from it. An everyday example of colour
constancy occurs when we move from daylight into an artificially lit
room. The composition of light from an electric bulb is markedly
different from that of sunlight, and is relatively richer in long
wavelengths. The same surface will therefore reflect more longwavelength
light under a lightbulb than in sunlight, and, if our perception of
colour depended on wavelength alone, it would appear redder. In fact,
the colour we perceive remains largely constant; a white sheet of paper,
for example, does not appear orange indoors or bluish outdoors (note
that colour constancy is not perfect, as those with a good eye for
colour know when choosing clothes under artificial light). A powerful
demonstration of colour con-

3. VISUAL PATHWAYS IN THE BRAIN

stancy is provided by experiments in which two surfaces reflect light of
identical spectral composition but are seen as having different colours
(Land, 1977). In a typical experiment, a patchwork display of
differently coloured surfaces is illuminated by mixed light from
projectors with red, blue, and green filters. The intensities of light
from the three projectors are first adjusted to some arbitrary
values---for example, so that the spectral composition of their mixed
light falling on the display is equal to that of daylight---and the
composition of light reflected from one surface A is measured. Now, by
adjusting the intensities of the three projectors, it is possible to
make the spectral composition of light reflected from another surface B
equal to that reflected from A in the first stage. When this is done, B
is not seen as having the same colour as A did in the first stage, as
would be expected if perceived colour is determined by the spectral
composition of reflected light alone. Further discussion of how the
visual system might achieve this compensation for variation in ambient
light, and more detailed descriptions of experiments, can be found in
Land (1977) and Mollon (1982). For the moment, the important point is
that the perception of colour depends on some kind of comparison between
the spectral composition of the light at one point in an image and that
of light in the surrounding area. Returning to area V4, Zeki (1980)
reported a number of cells that responded selectively to a surface of a
particular colour, and which maintained this response as the spectral
composition of light illuminating the surface changed. Under these
circumstances, the composition of light falling in the cell's field
changed, but its response, just like an observer's perception of colour,
remained constant. Zeki (1983) compared the responses of cells in V1,
and found no evidence of colour constancy; instead, all the cells
studied were selective for wavelength alone. These results imply that
some process takes place either in V1, or between V1 and V4, which
converts wavelength-selective responses into ones selective for the more
abstract property of perceived surface colour. Another body of evidence
for hierarchical organisation of the ventral pathway is provided

61

by the finding that some cells in the IT area are selective for quite
elaborate geometrical shapes. By systematically simplifying shape
stimuli until a cell stopped responding, Tanaka (1993) was able to
identify the "minimum" shape for a cell's response, and to show that
cells sensitive to similar shapes are grouped together in columns in IT.
Further experiments have shown that these complex properties are most
common in anterior IT, while V4 and posterior IT appear to represent an
intermediate stage in which these responses are constructed from the
outputs of simple spatial filters in V1 and V2 (Kobatake & Tanaka,
1994). In particular, many cells in V4 are selective for the position
and curvature of contours within complex shapes (Pasupathy & Connor,
2001). In Chapter 9 (p. 285) we return to consider one theoretical
interpretation of the selectivity of V4 and IT neurons for shape.
Another approach to studying shape selectivity in IT has been to use
complex, natural images as stimuli and to determine what variations in
them affect a cell's response. For example, Perrett, Rolls, and Caan
(1982) found that 10% of a sample of IT cells showed a preference for
faces, of either people or monkeys. These were defined as cells that
responded more than twice as vigorously to faces than to any of a wide
variety of other simple or complex stimuli. Perrett et al. (1982) found
that the responses of these cells were unaffected by transformations of
faces that do not affect their recognition by people, such as changes in
distance or colour, although their responses were reduced if faces were
turned from a front to a profile view. The cells were also selective for
the spatial configuration of features making up a face, giving weaker
responses to scrambled photographs of facial features. Further work
(Perrett et al., 1986), has shown that some cells are selective for the
identity of faces, responding most strongly to the faces of specific
individual monkeys or people, or for the view from which a face is seen
(see Chapter 13, p. 377). There is no doubt that these IT cells are
selective for more abstract properties than are V1 cells, and that
hierarchical processing is taking place in the ventral extrastriate
pathway. However, do

62

VISUAL PERCEPTION

these cells act as "face detectors" of the kind that Barlow (1972)
argued might be found at higher levels in visual pathways? Does a monkey
possess a single IT cell for each monkey or person it knows, and does
activity of that cell uniquely signal the presence of that monkey or
person? There are two reasons why we cannot draw these conclusions.
First, the same face excites many cells, meaning that its presence is
not signalled by a unique cell. Second, the same cell is excited by more
than one face, and so its activity does not uniquely specify a
particular face. Although some cells discriminate more sharply between
faces than do others (Baylis, Rolls, & Leonard, 1985; see Figure 3.15),
none has been observed to respond only to a single face. This evidence
implies that recognition of a face is coded not by a single cell but by
a specific pattern of activity in a population of cells. Further
analysis of face-selective cells in IT suggests that a fairly small
population of cells may be sufficient to generate a pattern of activity
coding uniquely for a particular face (Young & Yamane, 1992). Rolls and
Deco (2002) provide an extensive dis-

The responses of four different face-selective cells (A--D) in
inferotemporal cortex to five different face stimuli (1--5). Cell A is
strongly selective for face 2. Cells B and C are selective for faces 4
and 1, respectively, but less strongly. Cell D shows weak selectivity
between faces. Adapted from Baylis et al. (1985).

cussion of population coding in IT cortex, and of theoretical models in
which the recognition of complex objects is based on the distributed
activity of many simple units.

THE HUMAN BRAIN: TWO VISUAL PATHWAYS? The research that we outlined in
the last section has built up a picture of visual processing in which
information flows from V1 along two largely independent extrastriate
pathways. In this section, we will widen our scope to look at evidence
from human vision that is relevant to the functions of these two
pathways. We will then introduce a theory that attempts to draw together
both animal and human evidence into a general account of the
organisation of the visual system of the primate brain. As we saw in the
case of the primary visual cortex, neuroimaging methods provide a source
of evidence about the human visual pathway.

3. VISUAL PATHWAYS IN THE BRAIN

These have demonstrated a number of retinotopically organised areas
outside human V1, and several of these correspond closely to areas
identified in monkey extrastriate cortex, including V2, V3, and V3A
(Tootell, Hadjikhani, Mendola, Marrett, & Dale, 1998; Wandell, 1999).
Other, more anterior areas are less well defined, or do not clearly
match an area known in monkeys, or both. Using functional imaging
methods, it is possible to test whether these areas have specific
functions by giving an observer a task that requires a particular kind
of information to be obtained from visual input and determining whether
the task selectively activates a particular area. Early studies of this
kind used the positron emission tomography (PET) technique, which gives
lower resolution than fMRI. Zeki, Watson, Lueck, Friston, Kennard, and
Frackowiak (1991) compared cortical activity in people watching either a
complex motion display or a changing patchwork of colour. The motion
display activated a dorsal area, near the boundary of the parietal and
occipital lobes, whereas the colour display activated a region of
ventral occipital cortex. The area sensitive to complex visual motion
has subsequently been identified with fMRI methods (Tootell et al.,
1995) and is usually called MT+, as it corresponds to area MT and some
surrounding regions in monkeys (the name V5 is sometimes also used).
Recent experiments using colour patterns have not established such a
clear localisation of activity. Hadjikhani, Liu, Dale, Cavanagh, and
Tootell (1998) identified an area activated by passive observation of
colour patterns in the same region as that found by Zeki et al. (1991),
but showed that it lies outside human V4. They labelled this area V8,
but could not establish a match between it and any area known in
monkeys. Hadjikhani et al. also found that regions of V1, V2, and V3
representing foveal space were selectively activated by colour patterns.
Attempts to identify a single "colour area" have been further
complicated by the finding that a more active task requiring decisions
about colour activates a number of additional regions of extrastriate
cortex outside V8 (Beauchamp, Haxby, Jennings, & DeYoe, 1999).

63

Another source of evidence about visual processing in the human brain is
provided by the effects of naturally occurring lesions to cortical
tissue, caused by strokes, blows to the head, or other accidents. It is
very unusual for the area of damage in the cortex to be sufficiently
small and sharply defined to be comparable with the visual areas we have
been discussing so far, and there are also technical problems involved
in locating cortical damage accurately in living patients. Even so,
cases are known where people who have suffered limited damage in
extrastriate cortex experience the loss of remarkably specific aspects
of their vision, while other aspects remain quite normal. We will
briefly describe two conditions of this kind to show just how specific
these effects can be. In "motion blindness", people lose part or all of
their ability to perceive the motion of objects and people around them.
Zihl, Von Cramon, and Mai (1983) describe one such case, of a patient
who was unable to deal with a wide range of everyday situations that
required motion perception, such as judging the speed of a car, pouring
liquid into a glass, or following another person's facial movements
during conversation. She described water pouring into a glass as
appearing to be frozen, like a glacier, and became distressed when
people around her moved, describing them as disappearing and appearing
again without seeming to have moved in between. In other respects her
vision was normal, both in everyday life and in formal tests. Motion
blindness might be expected if a motion-selective area such as MT+ were
damaged. Schenk and Zihl (1997) found that three patients suffering from
the condition were unable to detect coherent motion of a group of dots
when they were mixed with randomly moving dots. Monkeys with MT lesions
show the same deficit in perceiving "global" motion (Newsome & Paré,
1988; see p. 59), suggesting that motion blindness may arise from damage
to an area with the same functions as MT. A second example of a specific
visual loss caused by brain damage is achromatopsia, or "cerebral colour
blindness". A detailed description of a case can be found in Pearlman,
Birch,

64

VISUAL PERCEPTION

and Meadows (1979). Here, a patient reports that he or she has lost
colour vision and sees the world in "black and white" or "shades of
grey", but this loss cannot be attributed to any abnormality in cone
function in the retina. Again, other visual abilities may be completely
normal. Achromatopsia has been linked to damage in the ventral
occipitotemporal cortex, which includes the region identified by Zeki et
al. (1991) and Hadjikhani et al. (1998) that is activated by viewing
colour patterns. Although conditions such as motion blindness and
achromatopsia are extremely rare, the fact that they exist at all is of
considerable importance. The ability of small brain lesions to "pick
off" particular aspects of a person's visual awareness of the world
clearly supports the view that there are parallel pathways through
extrastriate cortex that can operate independently to analyse particular
properties of visual input. Ungerleider and Mishkin (1982) drew
neuropsychological evidence of this kind together with physiological and
behavioural evidence from animals to propose a general theory of the
processing of visual information in the primate brain. The theory is
based on the distinction between dorsal and ventral pathways that we saw
in the last section. It proposes that the dorsal pathway is responsible
for extracting information about the spatial layout of the environment
and about motion, while the ventral pathway extracts information about
the form, colour, and identity of objects. Loosely speaking, the two
systems are responsible for determining where objects are, and what they
are, respectively. An underlying assumption of Ungerleider and Mishkin's
theory is that the overall function of visual processing is to create a
representation of the surroundings that corresponds to a person's
conscious awareness of what they see. Although the two pathways operate
independently and in different ways, both contribute to this eventual
goal. In an influential alternative theory, Milner and Goodale (1995)
challenge this assumption. They argue that the dorsal and ventral
pathways differ in a more basic way, in the ultimate functions of the
analyses that they carry out. According to Milner and Goodale, it is the

ventral pathway that builds up a representation of the surrounding world
which specifies not only the identities and properties of objects and
other creatures in it, but also their spatial layout. The dorsal
pathway, in contrast, is responsible for visual analyses that provide
information for controlling the movements of an animal's or person's
limbs from one moment to the next, in relation to changing visual input.
Furthermore, in a bold proposal, Milner and Goodale argue that it is the
representation created by the ventral pathway that underlies our
conscious awareness of the visual world. The parallel operation of the
dorsal pathway, they claim, steers our limbs and body rapidly and
efficiently through the world without giving rise to any conscious
perception. Milner and Goodale's (1995) approach represents a
significant break with traditional theories of visual perception.
Earlier approaches had, like Ungerleider and Mishkin, taken the
functions of vision to be those that Milner and Goodale ascribe to the
ventral pathway. They had assumed that the problem of controlling
movement could be left on one side, to purely "motor" processes in the
brain that tapped the single representation of the surroundings created
by visual areas. In contrast, Milner and Goodale say that at a very
early stage in visual processing---at V1---a stream of information is
channelled away from the construction of a visual representation and
into a system---the dorsal pathway---dedicated to the moment by moment
control of movement. These ideas seem to be at odds with our intuitions
about vision, and the earlier approach seems to fit more comfortably our
everyday impression that we have conscious access to a single
representation of the world, which we use to make decisions of all
kinds. The key evidence prompting Milner and Goodale's (1995) theory
comes from demonstrations that brain damage can have separate effects on
conscious awareness and on the visual control of movement. These
functions of vision normally run together so smoothly that the idea of
them being separate is hard to grasp. However, this can happen, and we
will review some of the main findings before considering other kinds of
evidence relevant to Milner and Goodale's theory.

3. VISUAL PATHWAYS IN THE BRAIN

The possibility of a person using visual information to control their
movement without conscious visual awareness was first demonstrated by
Weiskrantz, Warrington, Sanders, and Marshall (1974) in a patient (DB)
who had undergone surgical removal of the right occipital cortex,
including most of V1. As would be expected, DB appeared on all
conventional tests to be blind in a large part of his left visual field.
However, he was able to point accurately towards stimuli projected in
this "blind" area, despite insisting that he could not see any stimulus
and explaining his successful pointing as the result of guessing. In
terms of the testing procedures involved, there was a difference between
the patient's subjective report of seeing no stimulus and the results of
the "forced-choice" pointing test. The fact that pointing accuracy was
well above chance expectation demonstrated that he was not guessing but
was using optical information to control his arm and hand positions. DB
might have achieved this by detecting light scattered in the eye and
falling on retinal areas still connected to intact visual cortex, but a
number of control procedures subsequently ruled out this possibility.
For example, he was able to report at better than chance levels whether
a flash of light had occurred in the "blind" part of his visual field
but could not do so if the flash was projected on to his optic disc (the
small region of the retina without any receptor cells), although
scattered light would have provided equally good information about the
flash in both cases. A full description of this phenomenon, known as
"blindsight", is given by Weiskrantz (1986), and evidence for its
existence in monkeys is presented by Cowey and Stoerig (1995). The
existence of blindsight implies that signals from the retina can still
control movements such as pointing or reaching when they are no longer
connected to functioning primary visual cortex. According to Milner and
Goodale (1995), this is possible because the retina sends outputs to
other structures besides the dorsal LGN (see p. 45). These include the
superior colliculus and pulvinar nucleus, which both connect to areas in
parietal cortex that form part of the dorsal pathway. Damage to V1
therefore prevents visual input from reaching ventral areas but not
dorsal ones,

65

supporting the theory that processing in the ventral pathway is
necessary for conscious visual awareness. Some characteristics of
blindsight suggest that subcortical input to dorsal visual areas can
support more complex uses of visual information than just the control of
limb movements. For example, DB was able to discriminate between large
geometric patterns such as Os and Xs projected into the "blind" part of
his visual field, again attributing this ability to guessing. Milner and
Goodale (1995) suggest that this could happen through "self-cueing"; the
dorsal pathway could generate eye movements to follow the contours of
such patterns, and the patient might have sufficient awareness of these
movements to use them as cues in recognising grossly different shapes.
Further evidence in support of Milner and Goodale's (1995) theory comes
from more subtle dissociations between different uses of visual
information caused by cortical damage. Goodale, Milner, Jakobson, and
Carey (1991) report the case of a patient (DF) who had suffered cortical
damage as a result of carbon monoxide poisoning. This damage was fairly
diffuse, affecting ventral and lateral areas of the occipital cortex,
while leaving V1 largely intact. DF was clearly not blind, and was able
to use vision to move about quite normally. However, she suffered from
visual form agnosia, having severe difficulties in perceiving the
shapes, sizes, and orientations of objects. For example, she was
completely unable to say which of two objects placed in front of her was
larger, or to judge the angle of a slot in a vertical surface. Yet, if
she was asked to reach out and pick up one of the objects, or to "post"
a card through the slot, her arm, wrist, and hand movements were
entirely normal. As she reached for an object, the gap between her
finger and thumb was scaled to its size just as accurately as in
unimpaired people. As she posted the card, she was just as accurate in
rotating her wrist to the correct angle. Goodale et al.'s (1991) results
imply that precise information about the dimensions and orientations of
objects is available to DF's dorsal pathway and controls the muscles of
her arms,

66

VISUAL PERCEPTION

hands, and fingers in an entirely normal way. At the same time, this
information cannot reach the stage of processing in the ventral pathway
necessary to create conscious awareness of these properties of objects.
Patla and Goodale (1996) extended these findings to tasks involving the
control of leg movements during walking, showing that DF could negotiate
an obstacle on the ground normally, but was impaired in reporting
verbally the height of the same object. Ungerleider and Mishkin (1982)
gave the dorsal pathway the function of processing the spatial layout of
surrounding objects, and this provides a specific means of testing their
theory against Milner and Goodale's. Murphy, Carey, and Goodale (1998)
gave DF a number of different tests of her ability to perceive the
positions of differently coloured objects that she was able to tell
apart normally. As expected, she could use information about their
positions relative to her own body by reaching out to touch them as
accurately as controls did. However, when she was given tasks such as
copying an arrangement of the objects she performed less accurately than
controls. Her attempted copies could be recognised as similar to the
originals, but contained large errors in the distances and angles
between objects. Ventral damage therefore did not abolish DF's ability
to perceive spatial relations between objects, but certainly affected it
in a way that Ungerleider and Mishkin would not have predicted. Murphy
et al. (1998) argue that "spatial perception" is not a unitary ability,
but should be broken down into at least two forms, according to whether
spatial locations of objects are perceived relative to the person or
relative to one another. The perception of these "egocentric" and
"allocentric" spatial relationships is mediated, they claim, by the
dorsal and ventral pathways respectively, consistent with Milner and
Goodale's theory. Milner and Goodale (1995) predict the existence of an
opposite dissociation to visual form agnosia. Damage to the dorsal
pathway alone would be expected to leave a patient aware of the
identities and allocentric spatial relationships of surrounding objects,
but unable to move accurately with respect to them. The evidence
relevant to this prediction comes from a condition called

optic ataxia, in which patients with damage to the posterior parietal
cortex (in the dorsal pathway) are unable to make accurate eye movements
towards targets, or to reach towards them. Optic ataxia is not a general
problem of motor control, as someone affected is able to carry out
kinesthetically guided movements (such as touching the tips of their two
index fingers together with their eyes shut) quite normally. Ungerleider
and Mishkin (1982) explained optic ataxia as a general loss of spatial
awareness, but Milner and Goodale (1995) argue that it is caused by a
more specific effect of damage to the dorsal pathway. It can no longer
provide the information about the egocentric locations of objects (i.e.,
in body-centred co-ordinates) that is needed to control movements such
as a saccade or a reach. This interpretation is supported by a
paradoxical finding in patients with optic ataxia; when asked to reach
for a target immediately after it is presented, they show larger errors
than if asked to do so after a delay of 5 seconds (Milner, Paulignan,
Dijkerman, Michel, & Jeannerod, 1999). In contrast, unimpaired people
show a decrease in accuracy over the same period. The implication is
that if the dorsal pathway is damaged, a delay allows processing in the
ventral pathway to generate an allocentric spatial representation that
can be used to control reaching with some degree of accuracy.
Dissociations between visual awareness and control of movement are not
confined to neurological conditions such as blindsight and agnosia. They
can also be demonstrated, in more subtle forms, in people without any
neurological impairment. One such experiment was carried out by Goodale,
Pélisson, and Prablanc (1986). Observers fixated a point in the visual
field, and reached out to touch a light that appeared repeatedly at
unpredictable locations on either side of the fixation point. When the
target appeared, a saccade towards it began at approximately the same
time as the reaching movement but, because eye movements are fast, ended
before the reach. On some trials, unknown to the observer, the target
was moved part way through the saccade. Because of the phenomenon of
saccadic suppression, discussed further in Chapter 8 (p. 261),

3. VISUAL PATHWAYS IN THE BRAIN

observers did not see these displacements. However, the trajectory of
the arm compensated smoothly for the target movement and it was touched
accurately. The inability of observers to see the target move was
confirmed by using a forced-choice procedure; they were unable to report
whether or not the target moved on each trial at better than chance
level. The conclusion is therefore that they were able to use optical
information about target movement to control their arm trajectory
smoothly and efficiently, without any awareness that they were doing so.
Another demonstration of this kind of effect has been made by Burr,
Morrone, and Ross (2001), who showed that a distortion of spatial
perception that occurs as a saccade is prepared does not affect the
accuracy of reaching towards a target. Results such as these are
consistent with Milner and Goodale's (1995) proposal that separate
neural systems are responsible for visuomotor control and for conscious
visual awareness, but obviously they cannot tell us where these systems
are located in the brain. However, more specific

67

tests of the theory might be possible if neurological studies provided
distinctive characteristics, or "signatures" of processing in each of
the pathways that could be detected in behavioural experiments. We saw
earlier Milner and Goodale's argument that the dorsal system codes the
egocentric locations of objects, whereas the ventral system codes
locations in relation to surrounding objects. A number of experiments
have tried to use this difference as a signature of the operation of one
or the other pathway, by comparing the effects of certain visual
illusions on perception and on visually guided movement. In some visual
illusions, people's judgements of the size of one element of a pattern
are influenced by the other elements around it. One such example is the
Titchener illusion (see Figure 3.16). Milner and Goodale's theory
suggests that the conscious impression of the size of an element such as
the central circle in a Titchener pattern is generated in the ventral
pathway, which codes sizes and other properties of objects relative to
their context. It would therefore predict that the

The Titchener circles illusion. The central circles in the two arrays
are the same size, but the one on the left, surrounded by small circles,
appears to be larger than the one on the right, surrounded by large
circles.

68

VISUAL PERCEPTION

dorsal pathway would not generate the same illusion. This prediction has
been tested in numerous experiments that have measured the aperture of
people's grip as they reach to pick up an element in an array of solid
objects, such as flat discs forming a Titchener array. Some results have
supported the prediction, indicating that grip aperture is less
influenced by illusions arising from the context than are verbal
judgements of size, but a number of other factors can influence grip
size in these experiments and the literature is inconclusive (for a
review, see Carey, 2001). Recent evidence suggests that the processes
underlying visuomotor control and visual awareness differ in their
ability to adjust when visual input is distorted. This comes from an
extension of classic experiments by Stratton (1897), who studied the
effects of wearing prisms that invert the retinal image (relative to its
normal orientation). At first, a person wearing such prisms sees the
world as upside-down, and experiences great difficulty in carrying out
even the simplest visually guided movements. The usual account of
Stratton's results is that, after wearing inverting prisms continuously
for several days, people regain their perception of an upright world.
However, the impressions reported by participants in the original
experiment were not so clear-cut, and further experiments by Linden,
Kallenbach, Heinecke, Singer, and Goebel (1999) suggest that the results
depend critically on how the uprightness or otherwise of visual
perception is tested. Linden et al. found that visuomotor skills
recovered almost completely within 5 days, when the four people that
they tested had completely regained their abilities to walk around among
crowds or to ride a bicycle while wearing the prisms. In contrast, tests
of participants' subjective impressions did not show this recovery. One
particularly striking result was obtained with a test of perceived depth
from shading (described in Chapter 7, p. 190). Here, after 7 to 10 days
wearing the prisms, participants made the same depth judgements that
would normally be made with pictures presented the other way up,
implying that the mechanisms processing gradients of shading had not
adjusted to distorted input.

These results provide further evidence that visuomotor control and
visual awareness can be dissociated, and are consistent with a
hypothesis that processing in the dorsal pathway can be more readily
recalibrated in response to gross distortions of visual input than can
ventral processing.

Two visual pathways: Conclusions The evidence assembled by Milner and
Goodale (1995) in support of their theory certainly makes it difficult
to maintain the view that visual processing creates a single neural
representation of the surroundings that can be identified with conscious
awareness and which is used for the control of all body movements. The
theory has been influential in clarifying the problems with this view,
and in proposing an alternative model of brain organisation in which
visual input influences action through two distinct routes. Through the
dorsal pathway, it controls fast, "on-line" movements of the eyes, head,
and limbs; through the ventral pathway, it interacts with memory to
build up a consciously accessible representation of the world that is
used "off-line" to create plans for more complex, extended actions. To
give an example, the ability to identify and appreciate the contents of
a cup of coffee relies on the ventral pathway, while a quick and
accurate reach to grasp its handle can be achieved through dorsal
processing alone. If these general implications of Milner and Goodale's
(1995) theory are accepted, there are still questions that can be asked
about its details. Just how independent are the two proposed pathways?
Anatomical and physiological evidence shows that there are multiple
interconnections between them, through areas such as V4 (see Figure 3.14
and p. 59). Theoretical considerations also suggest that their two
functions cannot be clearly separated. The movements of arm, hand, and
fingers to pick up a coffee cup are determined not only by the position
of the cup and the shape of its handle, but also by the drinker's
perception of how full the cup is or how hot the coffee is. In Milner
and Goodale's terms, we would expect involvement of both pathways in the
control of such a movement, and we have seen evidence that when a
movement is delayed, ventral processing

3. VISUAL PATHWAYS IN THE BRAIN

becomes involved in its control (Milner et al., 1999). Second, are there
only two processing pathways through visual cortical areas? Much
progress has been made in understanding the organisation of areas in
monkey parietal cortex that lie beyond area MT and that send output to
motor areas in the frontal cortex. When single-unit recordings were
first made in alert, behaving monkeys it was apparent that the activity
of cells in these areas is sensitive not only to visual stimuli but also
to the positions of the eyes, head, and limbs (e.g., Hyvärinen &
Poranen, 1974). Further evidence suggested strongly that a
transformation takes place in these areas to represent the location of
stimuli relative to the head or body of an animal, rather than relative
to retinal co-ordinates as at earlier stages in the visual pathway
(e.g., Andersen, Essick, & Siegel, 1985). Recent neuroimaging evidence
indicates that the same process occurs in parietal areas of human cortex
(DeSouza, Dukelow, Gati, Menon, Andersen, & Vilis, 2000). A system that
uses visual information to control movements would have to transform a
retinacentred representation of object locations to a body-centred one.
The evidence that this occurs in areas of parietal cortex associated
with the dorsal pathway, but not in more ventral areas, supports Milner
and Goodale's (1995) theory. However, recent results suggest that there
is not a single transformation of this kind in parietal cortex, but a
set of parallel operations that represent the positions of objects in
multiple frames of reference, centred on parts of the body such as the
head, mouth, or hands. There is disagreement about whether these
parietal areas are involved in the planning of movements, or whether
they just provide spatial representations used by motor control systems
in more frontal areas. Reviews of the evidence from different viewpoints
can be found in Colby and Goldberg (1999) and in Snyder, Batista, and
Andersen (2000). In either case, however, the implication is that the
"dorsal pathway" consists of a network of multiple processing routes for
visual control of eye, head, and limb movement. The progress made in
analysing ventral visual

69

areas has not been as great, and one problem is that functions ascribed
to it such as object recognition or (still more) conscious awareness
cannot be defined as precisely as the visuomotor transformations carried
out in dorsal areas. It may be that the stream of visual processing in
the cortex that begins in V1 is tapped at many points by systems
responsible for controlling movements or actions over varying
timescales. Our understanding of a subset of these, grouped together in
Milner and Goodale's (1995) theory as the dorsal pathway, is growing
rapidly. Perhaps we will come to see the ventral pathway as a larger
subset of processes that contribute to the control of action over
potentially much longer timescales.

DYNAMICS AND FEEDBACK IN THE VISUAL PATHWAY In earlier sections of this
chapter, we described visual processing in the brain at the level of
single nerve cells. In the last section, evidence of this kind from
extrastriate areas led us to consider the larger-scale organisation of
visual pathways, but we now return to look at some further aspects of
the behaviour of single cells and their interconnections. We will see
that these have interesting implications for the overall organisation of
visual processing in the brain.

Effects of context on single-cell responses The receptive field of a
neuron is defined as the retinal region in which some light stimulus
causes a change in its rate of firing action potentials. In Chapter 2
(p. 35), we saw that the responses of a retinal ganglion cell to light
within its receptive field may be influenced by light falling outside
it, which would (by definition) not have any effect on its own. This
"periphery effect" indicates some nonlinear process in which inputs to a
cell from outside the receptive field alter its sensitivity, or gain.
Context effects of this kind are also well known in V1 and in
extrastriate areas. Light falling in a large region surrounding the
"classical" receptive field may either facilitate or suppress the cell's
responsiveness.

70

VISUAL PERCEPTION

An example of facilitation by a surrounding stimulus is shown in Figure
3.17. Here, a V1 cell responds more strongly to a bar within its
receptive field if there is a single bar of the same orientation outside
it (Kapadia, Ito, Gilbert, & Westheimer, 1995). The facilitation is
greatest when the two bars are collinear, and can be reduced by placing
a third, differently oriented bar between them. Note again that this is
not a linear effect; the "flanking" bar outside the receptive field has
no effect on its own, but instead increases the sensitivity of the cell
to its preferred stimulus. If the area surrounding the receptive field
contains numerous bars of the same orientation, resembling a simple
visual texture, a different result is obtained. Now, the surrounding
pattern suppresses the response to the stimulus in the receptive field.
We discuss the significance of such suppressive effects in Chapter 6
(p. 149). Facilitation by stimuli outside the receptive field is thought
to be caused by long-range horizontal connections within the visual
cortex, extending much further than the width of a receptive field and
specifically linking cells with a common orientation preference (Das &
Gilbert, 1999; Gilbert, 1995). Interactions between cells through these
connections may cause large-scale "ensembles" or "assemblies" of
cortical cells to emerge and re-form in continually changing patterns,
as the pattern of input from the LGN changes. Using optical methods for
visualising neural activity, Grinvald, Lieke, Frostig, and

Left: Response of a cell in V1 to a bar with its preferred orientation
falling in its receptive field. Centre: The cell does not respond to the
same bar flanking its receptive field. Right: The cell's response to a
bar within its field is facilitated by a flanking bar. Adapted from
Kapadia et al. (1995).

Hildesheim (1994) have shown that stimulation with a moving grating of a
small site in V1, 1° square, is followed within 200 ms by spreading of
neural activity over distances much greater than predicted from
receptive field sizes. The neural interactions that cause this large
region to be activated appear to be "invisible" to conventional
single-cell recording techniques By blocking LGN input to a small region
of visual cortex, Gilbert and Wiesel (1992) obtained further evidence
for the dynamic formation of cell assemblies. They found that the
receptive fields of cells surrounding the affected area immediately
increased in size. Since these changes occur within minutes, they cannot
be the result of new growth of neural connections, and the implication
is that cortical receptive fields are influenced by dynamic interactions
between cells over wide areas. DeAngelis, Anzai, Ohzawa, and Freeman
(1995) confirmed this contextual influence, but showed that the apparent
increase in receptive field size reported by Gilbert and Wiesel was
better described as an increase in the gain or responsiveness of the
cell, rather than a change in the cell's spatial filtering properties.
This result exemplifies a more general trend of recent findings, that
individual cortical cell responses are influenced by wider factors, such
as image context, eye position, or attention, via dynamic increases or
decreases in the cell's gain. There is evidence that the formation of
cell assemblies in visual cortex is accompanied by

3. VISUAL PATHWAYS IN THE BRAIN

increased synchronisation of firing of action potentials. The firing
rate of cortical cells oscillates with a rhythm in the range 40--60 Hz,
and Gray, König, Engel, and Singer (1989) found that the firing rhythms
of cells with nearby receptive fields became more synchronised when
stimuli of the same orientation and direction of motion were presented
to them. These effects have not been found consistently in all
subsequent experiments, and reviews of this research from a range of
viewpoints can be found in Engel and Singer (2001), Shadlen and Movshon
(1999), and Watt and Phillips (2000). Whatever the significance of
neural synchronisation proves to be for the dynamics of the visual
cortex, it is clear that the responses of cells to stimuli falling in
their "classical" receptive fields are modulated by the structure of the
surrounding image. In Chapter 6 (p. 146) we return to consider these
effects in more detail and to see how both facilitation and suppression
of cell responses by contextual stimuli can provide physiological
explanations for important features of early visual processing.

Feedback between visual areas So far, our discussion of visual pathways
in the brain has assumed that, although there may be many pathways
operating in parallel, each one processes visual information in one
direction only, from V1 to "higher" areas. We have already seen (p. 57)
that almost all connections between visual areas are accompanied by
reciprocal connections running in the opposite direction, and we will
next discuss the possible functional significance of these potential
pathways for feedback between areas. Feedback from one area to another
may operate in a positive fashion to increase the gain of cell
responses, and to "lock the system on" to particular spatial patterns of
light. Sillito, Jones, Gerstein, and West (1994) investigated the
feedback loop between V1 and the LGN. They found that the responses of
pairs of LGN cells were more strongly correlated when they were
stimulated by a drifting grating that extended over both receptive
fields, than by separate stimuli, even though the patterns of light in
both cells' fields were identical. This effect disappeared if feedback
from V1

71

was blocked, implying that feedback boosts the activity of those LGN
cells that drive a common set of V1 cells, and so in turn strengthens
the response in V1. Essentially, the suggestion is that a dynamic cell
assembly emerges which is extended over two "stages" in the pathway
rather than being confined to a single one. The feedback connections
from V1 cells are organised so that they make synaptic contact with rows
of LGN cells lying in the same orientation as their preferred stimuli
(Murphy, Duckett, & Sillito, 1999). These connections match the
feedforward ones proposed in Hubel and Wiesel's (1962) model of
orientation selectivity (p. 48) and suggest that feedback plays some
role in sharpening or amplifying orientation-selective responses in V1.
Also, they extend more widely than the receptive fields of the cortical
cells concerned would predict, suggesting that they are involved in
mediating the long-range surround effects that we described earlier
(e.g., Kapadia et al., 1995). If the only function of feedback
connections were to promote long-range interactions between cells, then
the concept of one-way processing channels through the visual system
would not be seriously affected. It could still be argued that visual
areas are arranged as a hierarchy in which new, more complex properties
of the image are analysed at each successive level, and that feedback
plays secondary roles such as sharpening selectivity, providing gain
control or mediating contextual effects. A more radical proposal has
been made by Lamme and Roelfsema (2000), who argue that the visual
pathway operates in two different modes. Immediately an image is
projected on to the retina (or, in more natural situations, at the start
of each fixation) a wave of neural signals passes from retina to LGN to
V1 and along the extrastriate pathways. In this "feedforward sweep", the
responses of cells in each area are determined by their classical
receptive field properties. In V1, for example, cells act as local
spatiotemporal filters. This phase is complete within about 100 ms, by
which time signals have reached areas farthest from the retina (in terms
of numbers of synapses). It is followed by a phase of recurrent
activity, in which "higher" visual areas send feedback signals to
"lower" ones. The

72

VISUAL PERCEPTION

responses of cells in lower areas may change during this phase (despite
the retinal image remaining constant) in ways that reflect processing at
higher levels or the demands of a particular behavioural task in which
the animal is engaged. If Lamme and Roelfsema (2000) are correct, the
implication is that the processing functions of particular visual areas
are not fixed, but change rapidly during the few hundred milliseconds
following the onset of an image, as a result of feedback influences. An
apparently "low-level" area such as V1 can therefore participate in an
analysis of complex properties of the image that is distributed across
many levels in the pathway. Dynamic changes in receptive field
properties are not confined to local interactions within visual areas,
but occur across the whole visual pathway to configure the processing
properties of each area according to the current retinal image and
behavioural context. This is a novel view of the visual pathway, but it
rests mainly on evidence that the receptive field properties of cells
change over the hundreds of milliseconds following presentation of a
retinal image. Lee, Mumford, Romero, and Lamme (1998) provide evidence
of this kind from V1. Here, complex cells respond to the onset of an
image with a latency of about 40 ms, when the "feedforward sweep"
arrives. For the next 20 ms, their responses show the expected
selectivity for orientation of small line segments falling in the
receptive field. After 60 ms, this orientation-selective response dies
away, and for the next 20 ms the cell behaves differently. It now
responds strongly to a boundary between two regions containing line
segments of different orientations, provided that the boundary is
aligned with its preferred orientation. According to Lee et al. (1998)
the cell's function changes over this period from a "low-level"
filtering of the orientation of local elements of the image to a
"higher-level" filtering of boundaries between differently textured
regions. The significance of changes like these for image processing are
discussed in Chapter 6 (p. 146). Evidence from area IT suggests that the
initial phase of activity following a fixation is sufficient for coarse
coding of stimulus features, while finer coding requires more prolonged
activity. The

responses of some face-sensitive cells are initially selective only for
whether a stimulus is a monkey or a human face. In a second phase,
following about 50 ms later, the response becomes selective for finer
features such as the expression or individual identity of the face
(Sugase, Yamane, Ueno, & Kawano, 1999). If the "feedforward sweep"
proposed by Lamme and Roelfsema (2000) can support coarse coding of
object categories, this would explain the speed with which people are
able to recognise large-scale structures within an image; for example,
to decide whether it contains an animal or not. Recordings of
eventrelated potentials from the scalp demonstrate an electrical change
occurring in the brain within 150 ms of an image appearing that predicts
the observer's decision a few hundred milliseconds later (Thorpe, Fize,
& Marlot, 1996). These recent discoveries about the evolution of single
cell responses over timescales of hundreds of milliseconds do indicate
that earlier conceptions of the processing functions of visual areas
were over-simple. Even so, more evidence is needed to support the
details of the specific theoretical interpretation developed by Lamme
and Roelfsema (2000). Although they attribute the dynamic properties of
single cell responses to feedback between visual areas, it is possible
that relatively slow horizontal interactions within areas could be
responsible for some or all of them. Another problem is that inputs from
the LGN and other subcortical areas can reach extrastriate cortex
directly, bypassing V1 (see p. 65). As a result, the latency of MT cells
to respond to image onset is about the same as that in V1. As Lamme and
Roelfsema point out, these connections complicate the interpretation of
the timing of changes in responses of cells in terms of feedback
connections from higher areas.

Effects of attention If visual pathways worked only (or primarily) in a
feedforward manner, then we would expect cells in lower-level areas to
respond in exactly the same way to the same pattern of light, whatever
the animal's current behaviour. We have already seen one exception;
cells in parietal areas, at relatively high levels in the dorsal
pathway, are influenced

3. VISUAL PATHWAYS IN THE BRAIN

by the positions of eyes, head, and limbs (see p. 69). However, cells in
extrastriate areas can be influenced by a much wider variety of
behavioural states, including "internal" ones such as the location of an
object to which an animal has learned to pay attention. Moran and
Desimone (1985) demonstrated that the responses of about half of a
sample of V4 cells were affected by changes in the location of a
monkey's attention. For each cell, they first determined the boundaries
of its receptive field, and the stimuli to which it responded. Both an
effective and an ineffective stimulus were then presented together at
different locations within the receptive field. If the monkey had
previously been trained to attend to the location of the effective
stimulus in order to detect a signal of some reward, the cell responded
strongly. If, however, it had been trained to attend to the location of
the ineffective stimulus, the response was much weaker, even though the
pattern of light falling in the receptive field was identical in the two
conditions. As Moran and Desimone (1985, p. 783) put it, it was "almost
as if the receptive field had contracted around the attended stimulus."
Motter (1994) demonstrated an effect of attention on V4 responses in a
different way, by using a visual search task in which monkeys had to
locate a "target" bar with a particular combination of orientation and
colour from amongst a number of different bars. When a stimulus fell
within the receptive field of a cell, it evoked a response that was
about twice as large when it was the target compared to when it was not
the target of a search. These results show that responses in V4 can be
influenced by either the spatial location (Moran & Desimone, 1985) or
the particular visual features (Motter, 1994) to which an animal has
learned to attend in order to successfully carry out a task. These
influences of learned patterns of attention must arise from cortical
areas outside the visual pathway as we have described it so far, and are
presumably propagated back down the pathway through feedback connections
to alter the receptive fields of cells at lower levels. Consistent with
this interpretation, and with Lamme and Roelfsema's (2000) model, Motter
(1994) found that a

73

monkey's attentional state begins to affect the activity of a V4 cell
about 75 ms after its initial response to a bar or other stimulus.
Within Milner and Goodale's (1995) theoretical framework, effects of
attention on ventral areas such as V4 are consistent with the idea that
the significance or "meaning" of a stimulus is extracted in this
pathway. In the case of a monkey, this would be the part that a stimulus
plays in a pattern of behaviour that will lead to food reward. Further
support for the same idea comes from evidence that ventral areas are
involved in memory processes. Miller, Li, and Desimone (1993) gave
monkeys a task in which a "sample" picture was flashed in the field of
an AIT cell (receptive fields in this area are large). After a delay,
the same picture was presented, accompanied by another, and the correct
response was to shift gaze towards the picture matching the sample.
During the delay period, Miller et al. (1993) tested the responses of
cells to various pictures. In about half the cells tested, the response
that would normally be observed to a picture was suppressed if that
picture was the sample. In other words, the response was not simply to a
particular visual pattern, but to that pattern if it was unexpected in
the context of the task. Effects of attention on single cell responses
are not confined to the ventral pathway, but have also been demonstrated
in dorsal areas such as MT (see Chapter 8, p. 228 for further details).
These results imply that learned expectations about the location and
movement of stimuli are able to influence processing in the dorsal
pathway, and this may be important for achieving maximum speed and
accuracy in controlling visually guided actions. Can effects of
attention extend further back in the visual pathway, to levels lower
than V4 or MT? In particular, can they influence processing in V1? This
is an important question, bearing on the issue of whether cells in V1
are more or less fixed filters that yield a full, general-purpose
representation of the retinal image, or whether they participate in
specialised, higher-level processes (cf. Lamme & Roelfsema, 2000; see
p. 71). At first, evidence for attentional effects in V1 similar to
those in V4 was inconsistent (Luck, Chelazzi,

74

VISUAL PERCEPTION

Hillyard, & Desimone, 1997; Motter, 1993). However, recent research has
shown that they occur when a target of attention is surrounded by a
number of potential distractors. Ito and Gilbert (1999) trained monkeys
on tasks that required attention either towards a target in the
receptive field of a cell, or away from it, or towards a number of
targets distributed around the field. With distributed attention (the
third condition), a collinear flanking line strongly facilitated the
response to a line in the receptive field in the way that we described
earlier (see p. 70, Figure 3.17). With attention directed towards a
target in the field, however, this facilitation disappeared. Ito and
Gilbert's (1999) results suggest that the effect of attention in V1 is
not directly to modulate responses to single stimuli (as in V4 or MT),
but to control how much influence the surrounding context has on a
cell's response. When attention is directed towards the location of the
cell's receptive field, the area over which stimuli can influence it
appears to shrink. Feedback connections to V1 from higher areas
therefore appear to control the way that long-range interactions between
cells become organised over periods of hundreds of milliseconds (see
Gilbert, Ito, Kapadia, & Westheimer, 2000, for further evidence and
discussion). Although effects of attention to spatial locations and the
features of objects are more subtle in V1 than in "higher" areas such as
V4, their existence does suggest that no cortical area functions in a
completely passive way, driven by visual input alone. An animal's
learned knowledge of the pattern of events occurring in the world around
it apparently can "reach down" into V1 to influence the way that
information is processed there. The same is also true of humans. Several
recent fMRI studies have demonstrated that signals from V1 (as well as
most other visual areas) are modulated by attentional changes, in the
absence of any changes in visual input. These effects are specific to
the particular spatial location that is attended. For example, as
attention is directed to elements in a display further and further from
a fixation point, a wave of activation passes from posterior to anterior
V1, where the fovea and periphery are represented, respectively

(Brefczynksi & DeYoe, 1999; see Figure 3.7). Similar evidence was
obtained by Martinez et al. (1999), who also showed that modulation by
attention occurred after the initial response to a stimulus, suggesting,
as the evidence from monkeys does, that it is driven by feedback from
other cortical areas.

CONCLUSIONS The description of the primate visual pathway that we have
given in this chapter does no more than identify some important themes
in the prodigious growth of research in visual neuroscience over the
decades since the field was founded by Kuffler, Barlow, Hubel, and
Wiesel. At one time, it seemed possible to organise the results from
single-unit physiology into a fairly simple model of a single,
hierarchical processing pathway. A wealth of new techniques and data has
implied that such a framework is far too simple, and it is now
impossible to encompass all that we know about the visual pathway in any
single theory. We have presented a variety of more limited theories in
this chapter and in Chapter 2. Some of these are highly local in their
scope, attempting to explain the neural "wiring" and the synaptic
interactions that underlie the responses of cells in particular parts of
the visual pathway. Others are more general, seeking to model V1 as a
bank of filters tuned to the statistical regularities in natural images,
or to explain how different functions are divided between processing
areas. However, we have only touched on a few of the many theoretical
models that draw on evidence from the physiology of the visual pathway.
One reason is that many such theories attempt to explain the functions
of parts of the visual pathway in terms of more abstract models of the
perception of such things as edges, surfaces, depth, or motion. Models
of this kind, and the psychological evidence on which they are based,
will be the subject of the second section of this book and, throughout
it, we will return frequently to physiological evidence to consider how
such models can interpret it.

Part II

Vision for Awareness

Page Intentionally Left Blank

4 Approaches to the Psychology of Visual Perception

this approach often relies on the assumption that the results of visual
perception are available to our conscious awareness and can be described
to others. As we saw in Chapter 3, this is not always so, and vision can
influence our actions without us being aware of it happening. However,
in Part II of this book we will explore the progress that has been made
in understanding visual perception as it contributes to awareness,
beginning in this chapter with a brief historical introduction to this
approach. In Part III we will return to consider the role of visual
perception in the control of animal and human action. The psychological
study of visual perception grew out of the philosophy of mind, a
tradition in which it was natural to consider perception as a means of
gaining awareness and knowledge of the world. The foundations of modern
theories of vision were laid in the 17th century, when Descartes and
others established the principles of optics, making it possible for the
first time to distinguish between the physical properties of light and
images, and the psychological properties of visual experience. The
discovery that the eye functions like a camera to form an image had a
particularly strong influence on later thinking,

In Part I of this book we were concerned with the workings of the eyes
and those parts of the brain that handle information received from them.
At several points, we saw how this physiological knowledge could explain
quite specific aspects of our conscious awareness of the world. We can
explain in terms of the physiology of the retina why we cannot read a
book in dim light or see colours at night, or why it takes only three
primaries to reproduce any colour. At the same time, however, we saw
that the relationship between most of our visual experience and the
functioning of the brain remains largely unknown. Another approach to
understanding visual perception is to develop models of how the
fluctuating patterns of light reaching the eyes are processed to yield
information about the surrounding world, without necessarily referring
to any physiological mechanisms. Theoretical models of this kind can be
tested against our visual experience as long as they make clear
predictions about the effects that changes in visual input have on what
we see. Psychophysical methods enable us to obtain, under controlled
conditions, the systematic data about our visual experience that are
needed to make such tests. Notice that 77

78

VISUAL PERCEPTION

casting visual perception as the process that converts a retinal image
into a percept of the world beyond the eyes. The images produced by
cameras or eyes lack many of the qualities that we perceive in the
world, such as solidity and meaning. It therefore seemed that visual
perception must involve adding information to that present in the
retinal image, in order to reveal a solid and meaningful world. The
empiricists such as Locke (1690) and Berkeley (1709) argued that
perception was constructed from more primitive sensations through a
process of learning by association. Although nativist philosophies were
also voiced, in which knowledge of the entities of "space" or "time"
were considered inborn, or divinely given, it is probably not too much
of an overgeneralisation to say that it is the empiricist tradition that
has dominated modern thinking in psychology. We will not delve into the
controversy between the "nativists" and the "empiricists" here, but
simply agree with Boring (1942, p. 233) that: No simple exposition of
this great and largely fruitless controversy can, however, be adequate
to its complexities. For one thing, almost every protagonist turns out,
whatever he was called, to have been both nativist and empiricist.
Everyone believed that the organism brought something congenitally to
the solution of the problem of space; everyone believed that the
organisation of space may be altered or developed in experience. The
dominant empiricist position in the 19th century led to the attempted
analysis (often using introspective methods) of perceptions into their
component sensations by the structuralists, and considerable debate
about which elements or attributes should be considered as fundamental
(see Boring, 1942). By analysing elementary sensations, it was hoped
that ultimately the complexities of human thought could be unravelled,
since all complex ideas must ultimately have been derived through
sensory experience. The mechanisms whereby perceptions were constructed
from sensations, through reference to knowledge

previously acquired through learning, were also discussed, most notably
by Helmholtz (1866), whose idea of perception involving unconscious
inference or conclusions is still echoed by more contemporary theorists
(e.g., Gregory, 1973, 1980). Compare their statements: . . . such
objects are always imagined as being present in the field of vision as
would have to be there in order to produce the same impression on the
nervous mechanism . . . The psychic activities that lead us to infer
that there in front of us at a certain place there is a certain object
of a certain character, are generally not conscious activities, but
unconscious ones. In their result they are equivalent to a conclusion .
. . (Helmholtz, 1866, trans. 1925, pp. 2--4). . . . we may think of
sensory stimulation as providing data for hypotheses concerning the
state of the external world. The selected hypotheses, following this
view, are perceptions (Gregory, 1973, pp. 61--63). Thus a view of
perception as indirect and inferential persists today, although the
methods used to study vision have become more sophisticated, and some
rather different ideas about perception have been voiced in the years
between Helmholtz and Gregory. We will mention some of these landmarks
in method and theory here very briefly. Towards the end of the 19th
century, the content of perception was commonly studied using the
methods of analytic introspection---although Fechner's psychophysical
methods (1860) saw the beginning of a more "objective" way to study the
senses. However, introspectionist methods were largely abandoned in the
United States, following J. B. Watson's lead in 1912. Watson put forward
his case for behaviourism (Watson 1913, 1924), in which mentalistic
notions such as "sensations" and "perceptions" were replaced by
objectively observable "discriminative responses". The behaviourists
argued that we can never know how animals, or other people, experience
the world. Therefore, we should only observe their

4. APPROACHES TO PERCEPTION

behaviour, to examine how their responses are related to variations in
the stimuli presented. Ironically, while classical behaviourism provided
the methodological tools for the comparative study of perception, it
considered it illegitimate to explain any observed differences in the
perceptual capabilities of different species in terms of internal
processes. The methods of contemporary psychology are still influenced
by the behaviourist tradition, although most students of perception now
regard participants' verbal reports of their perceptual experience as
legitimate "responses" to be recorded and analysed along with the
nonverbal discriminative responses. At much the same time as Watson was
developing behaviourism, the European Gestalt psychologists reacted
against the structuralist assumptions that perception could be reduced
to sensations. They retained an introspective, although
phenomenological, approach. They were nativist in philosophy,
maintaining that perceptual experience was the result of certain dynamic
field forces within the brain. We discuss Gestalt ideas further in
Chapter 6. Gestaltists apart, most other movements in the psychology of
perception have been empiricist in flavour, and most have implicitly or
explicitly assumed that perception should be regarded as some process of
interpretation or construction from the incomplete information provided
by the retinal image. Two closely related movements that emphasised such
complexities of human perception flourished transiently during the 1940s
and 1950s. The first of these, "transactional functionalism"
(Kilpatrick, 1952), rested on the demonstrations of Ames (Ittelson,
1952). These included a trapezoidal window that looked rectangular, a
collection of sticks that could be seen as a chair, and, perhaps best
known, the "Ames room". This is an enclosed room of normal size, but
strongly distorted in shape. In particular, the front and back walls are
not parallel, so that one back corner is considerably further from the
front wall than the other. When an observer looks into the room with one
eye, through a small peephole in the front wall, they see a room of
normal shape with all its corners right-angled. More strikingly, if a
person walks across the room parallel to the

79

back wall, the observer usually sees him or her change in size,
appearing to be much taller when standing in one back corner than in the
other. Notice that these illusions only occur because the restricted
method of viewing the room removes many of the cues to its real shape
that would normally be available (see Chapter 7). Such demonstrations
were used to illustrate the apparently probabilistic and inferential
nature of seeing. In the Ames room, any number of arrangements of the
walls and the walking person could give rise to the same retinal image
and, it was argued, the actual arrangement that is seen depends on
unconscious inferences that draw on a lifetime's perceptual experience.
Knowledge that rooms are almost always cuboidal in shape appears to
dominate this process of inference, and to lead to the conclusion that
the walking person is changing in height. While transactional
functionalism stressed the individual's history as important in
determining his or her perception, the "new look" (e.g., Bruner &
Goodman, 1947) stressed the importance of individual differences in
motivation, emotion, and personality in influencing what they might see.
Cantril (in Wittreich, 1959) for example claimed that one observer,
whose husband walked across the Ames room, persisted in seeing him
remain constant in size, while the stranger accompanying him shrank or
grew. Wittreich confirmed this observation with some of the married
couples that he tested. During the 1960s, associationist explanations of
perceptual learning and discriminative responding gave way to a new
"cognitive psychology" of perception, attention, and memory. Attempts
were made to describe the stages that intervened between stimulus and
response. The revolution in information technology provided a new
metaphor for psychology, in which information from the senses is
processed in ways not unlike the processing of information in a
computer. Processes of sensory coding, storage, and retrieval of
information were all discussed, and the development of computer models
of some of these processes was seen to be a legitimate goal of
psychological theorising. If a machine could be designed which could
"see", its computer

80

VISUAL PERCEPTION

program could constitute the implementation of a theory of how seeing is
achieved by humans. (See Boden, 1987, or Garnham, 1987, for
introductions to the field of artificial intelligence.) The assumptions
of the cognitive approach to understanding perception were strongly
challenged by James Gibson (1950a, 1966, 1979), who denied that
perception involves construction, interpretation, or representation. His
theory therefore stands apart from those that we have mentioned here
although, paradoxically, one can draw parallels between some of Gibson's
ideas and those of both the Gestaltists and the behaviourists---two
schools that were diametrically opposed. We will give a fuller account
of Gibson's theory in Chapter 10, and will devote the remainder of this
chapter to one particular theoretical approach to visual perception that
grew out of the artificial intelligence tradition. This was developed by
David Marr and was set out in a strongly influential book (Marr, 1982).
While many of the details of this theory have been challenged during the
last 20 years, the general style of explanation that Marr developed has
played a major part in guiding the research on visual perception that we
will describe in the following chapters of Part II. We will return to
specific aspects of Marr's theory at a number of points in those
chapters, and for now will give only an outline of its main
characteristics.

MARR'S THEORY OF VISUAL PERCEPTION Marr's theory lies within the
mainstream that we have been describing, taking the problem of visual
perception to be the transformation of a pattern of light on the retina
into awareness of the visible world. He expresses this problem within
the cognitive and artificial intelligence traditions by describing both
image and awareness as representations of the world. The first is a
"grey-level" representation, or spatial array of values of light
intensity and hue, while the second is a symbolic specification of the
positions, motions, and identities of surrounding objects. Conceived in
this way, the problem is then to understand how the

first representation can be processed to obtain the second. The approach
that Marr introduced to solving this problem was novel in a number of
important ways. First, Marr argued that visual processing is modular; it
can be decomposed into a number of separate subprocesses, each of which
takes one representation and converts it into another. The overall task
of vision is accomplished by a number of modules operating in a series.
The first creates the primal sketch, which represents the changes in
light intensity occurring over space in the image. The primal sketch
also organises these local descriptions of intensity change into a 2-D
representation of image regions and the boundaries between them. Next,
the layout of visible object surfaces---their distances and orientations
relative to the perceiver---are specified in the 2½D sketch. Finally,
this information is used to create 3-D model representations, which
specify the solid shapes of objects and are used to identify them with
representations of objects held in memory. Marr's arguments for a
modular theory of visual processing were based in part on theoretical
considerations about the design of efficient and robust systems in
artificial intelligence, and also on evidence from neuropsychology. In
particular, forms of agnosia caused by brain damage may leave people
unable to identify objects while their other visual abilities are
unaffected. Such evidence, like that discussed in Chapter 3 (p. 63),
strongly suggests an organisation in which one module can fail while
leaving the operation of others unaffected. A second feature of Marr's
theory will be apparent from this brief description. It assumes that
each processing stage draws on the representation created at the one
before, so that there is a one-way flow of information from a grey-level
representation of the image onwards. At first sight, this seems
completely at odds with the empiricist tradition, which holds that
perception requires the use of knowledge of the world to interpret the
retinal image. We saw earlier one modern expression of this in Gregory's
(1973) view of perception as a hypothesis-testing process, in which
guesses based on stored knowledge of the world are tested against
partial, frag-

4. APPROACHES TO PERCEPTION

mentary evidence from the image. In fact, the contrast is not as strong
as it appears. Marr was not opposed to a role for knowledge or
hypotheses in visual processing, and accepted a role for "top-down" or
"conceptually driven" components of this kind. However, his aim was to
establish the limits of what could be achieved through a purely
"bottom-up" or "data-driven" analysis of visual input. In this way, he
argued, it would be possible to specify exactly the circumstances in
which knowledge of the world is needed to resolve ambiguities in visual
processing, and so to avoid falling back on it as a generalpurpose,
poorly specified solution to problems in vision. The concern with
specifying precisely how visual processing is achieved reflects another
general principle of Marr's approach. He argued that a theory of how
some attribute of the visual world is obtained from the retinal image is
only satisfactory if it can be expressed as an algorithm---a specified
logical or mathematical procedure operating on an input to yield an
output. Expressing a theory in the form of a computer program is a means
of testing whether it can be specified in this way, and whether it will
achieve the desired outcome when tested with actual images as input. A
comparison of Marr's theory with those such as Gregory's that involve
top-down processing leads us to a more fundamental feature of his
approach. Although Marr sought to establish how much information could
be obtained from a retinal image without falling back on knowledge of
specific objects or events, he gave great importance to knowledge of the
general properties of the natural visible world. An approach such as
Gregory's assumes, for example, that we can only see a dog in a
cluttered scene because we use knowledge of the shapes, sizes, colours,
and movements of dogs to interpret a complex retinal image of the scene.
As well as being highly specific, such knowledge is also of course
consciously accessible. We know that we know things about dogs, and we
can express that knowledge to other people. Examples of the general
properties of the world that Marr considered are that surfaces of
objects have regular textures, or that the distance

81

of the surface of an object from a perceiver varies smoothly, or that
parts of living things have shapes with regular topological properties.
These are things that, logically, do not have to be true of objects and
scenes, but almost always are true in natural and human-built
environments. They constrain the possible structures that could give
rise to a retinal image, and so can be used as "clues" in processing
them. Marr argued that "knowledge" of this kind is embodied in the
organisation of our visual systems without us being aware of possessing
it or being able to tell anyone else about it. In this respect, his
theory returns to Helmholtz's earlier concept of "unconscious inference"
and departs from those that claim a role for explicit, consciously
accessible knowledge in visual perception. Marr used the term
"computational theory" to describe this aspect of his approach to visual
perception. The term emphatically does not mean a theory that is just
"something to do with computers". Instead, it expresses the specific and
very powerful idea that the first stage in understanding perception is
to identify the information that a perceiver needs from the world, and
the regular properties of the world that can be incorporated into
processes for obtaining that information. In other words, we need to
know what computations a visual system needs to perform, before
attempting to understand how it carries them out. In later chapters, we
will see examples of Marr's application of computational theory to
problems such as detecting the edges of surfaces, perceiving depth, or
recognising objects. The approach has been widely influential; we saw an
example of the same way of thinking in Chapter 3 (p. 57) when we
discussed the possibility that cells in the visual cortex act as filters
tuned to statistical regularities in images of natural scenes. Indeed,
the computational approach even brings some common ground between Marr's
theory and that of Gibson (see Chapter 14, p. 408). Computational
theories of perception can be applied not only to human vision but also
to other species, by considering what information an animal needs from
light in order to guide its activities. Ecological, as well as
physiological, considerations would allow us to tailor a theory in

82

VISUAL PERCEPTION

the spirit of Marr to the beast in question (Marr, 1982, p. 32): Vision,
in short, is used in such a bewildering variety of ways that the visual
systems of different animals must differ significantly from one another.
Can the type of formulation that I have been advocating, in terms of
representations and processes, possibly prove adequate for them all? I
think so. The general point here is that because vision is used by
different animals for such a wide variety of purposes, it is
inconceivable that all seeing animals use the same representations; each
can confidently be expected to use one or more representations that are
nicely tailored to the owner's purpose. We will see in Chapter 11 how
this kind of approach has contributed to our understanding of the visual
control mechanisms involved in insect flight. Finally, Marr's theory is
distinctive in bringing together theory and evidence from artificial
intelligence, psychology, and neurophysiology. In doing so, he made an
important contribution to clarifying the relationships between
psychological and physiological explanations of visual perception, by
distinguishing between three levels of theory. As we have seen,
computational theory specifies what must be computed from images and
sets the scene for algorithmic theories of how such computations are
actually carried out. In turn, physiological and psychophysical evidence
can lead to a theory of how the algorithms involved are actually
implemented in the visual pathways of humans or animals. By proposing
these three levels of explanation (computational theory, algorithm, and
implementation), Marr achieved an important advance over earlier
attempts to explain perception in terms of physiology, which often tried
to make direct links between properties of nerve cells and poorly
specified psychological processes. The lesson of Marr's analysis is that
a sophisticated analysis of the algorithms that underlie aspects of
perception is needed first, before their components can be

identified with properties of nerve cell circuitry. We will see examples
of this approach in the following chapters, not only in Marr's work but
also in the research that has followed it.

CONNECTIONIST MODELS OF VISUAL PERCEPTION In common with other theories
in the cognitive and AI traditions, Marr's theory of vision sees
perception as involving the construction and manipulation of abstract
symbolic descriptions of the viewed scene. In these terms, an algorithm
applied to a representation of the retinal image results in a
description, rather like a set of propositions or sentences, of the
structures that are present in the image and where they are. In the
brain, of course, there can be no such "sentences", but rather there are
neurons, or collections of neurons, that are more or less active
depending on the inputs they receive from other neurons. By
distinguishing between algorithmic and implementation levels of theory,
Marr was able to put on one side the question of how a proposition about
a visual scene might be related to a pattern of activity in nerve cells.
In recent years, some researchers in cognitive science have adopted a
different approach. This involves building "connectionist" models
(sometimes known as "neural networks"), in which representations of the
world are expressed in terms of activities in units that are based on
the properties of neurons. Such a unit has a continuously varying
activation level, analogous to the rate of firing of a neuron, and units
affect one another's activation levels through connections that are
analogous to synapses. These connections change their strengths in
response to changes in the temporal relationship between activities in
the units that they link. Many modelling techniques of this kind have
been devised, using different networks of units and different "learning
rules" governing the changes in the strengths of connections between
them. Examples of such models and their applications can be found in
Ballard, Hinton, and Sejnowski (1983), McClelland and

4. APPROACHES TO PERCEPTION

Rumelhart (1986), Rumelhart and McClelland (1986), or Morris (1989). In
all such models, states of the world are represented not by the
construction and storage of abstract symbols, but by patterns of
activity in units. Connectionist models have several apparent advantages
over conventional "symbolprocessing" models. They appear to be more
biologically plausible, although their properties are only loosely based
on those of nerve cells and networks. They also provide a relatively
easy way to think about parallel computations, which can be particularly
convenient when a number of different constraints need to be satisfied
simultaneously. We will explore an example of this in Chapter 7.
Connectionist models also provide an interesting way of simulating how a
system could learn to recognise certain categories from a collection of
exemplars. This is not unknown in conventional models but becomes an
important central feature of many connectionist models where learning
occurs through adjusting the weights of connections between neuron-like
units. Finally, one class of connectionist models has the additional
property of "distributed" processing. In such models, there is no
one-to-one correspondence between a "unit" and a "concept" as there is
in more "localist" connection schemes (e.g., Feldman, 1985). Rather, a
particular concept is represented in terms of a pattern of activity over
a large set of simple units, and different patterns of activity in those
same units represent different concepts. Such PDP (parallel distributed
processing) versions of connectionist models have some attractive
characteristics when applied to pattern and object classification, as we
will see in Chapter 9.

83

Opinions vary about the relationship between connectionist theories and
those that involve the explicit construction of a symbolic
representation of the visual world. While some claim that connectionist
models pose a fundamental challenge to conventional cognitive science,
others argue that they complement conventional models by addressing
issues at a different level, one that is closer to Marr's notion of the
"implementation" level of theory. For example, Fodor and Pylyshyn (1988)
argue that connectionist models suffer from the same shortcomings as
other purely associative accounts of cognitive phenomena. They cannot
provide an alternative to traditional symbol-processing models since
they cannot embody syntactic constraints to which cognition is
sensitive. Many of these arguments are similar to those levelled by
Chomsky (1959) against the behaviourist account of language acquisition.
Fodor and Pylyshyn's (1988) conclusion is that connectionism can only be
an implementation-level theory, not an algorithmic-level theory.
Connectionist models may show us how, for example, the brain maps from a
retinal to a head-centred coordinate system (Zipser & Andersen, 1988),
but they do not do away with the psychological level of theory at which
the brain must be seen as constructing and manipulating symbolic
representations. Further discussion of these issues can be found in
Broadbent (1985), Rumelhart and McClelland (1985), and Smolensky (1987,
1988). We will return to connectionist modelling in Chapters 7 and 9
when we discuss its application to specific problems in the processing
of visual information.

Page Intentionally Left Blank

5 Images, Filters and Features: The Primal Sketch

composition change, and the places in the surroundings where one surface
or object ends and another begins, but this relationship is by no means
a simple one. We cannot assume that every intensity or spectral change
in an image specifies the edge of an object or surface in the world.
Although the edges of objects do give rise to intensity and spectral
changes in an image of a scene, these changes will also arise for a host
of other reasons, for example where the edge of a shadow is cast on a
surface by another object. When referring to "edges" we must take care
to indicate whether we are referring to features of the scene or the
image of it. These are two separate domains, and the formidable task of
vision is to derive a description of the former from the latter.
Intensity changes also arise from variations in the angle between the
surface and the direction of incident light. The intensity of reflected
light is at a maximum if the surface faces towards the light source, and
falls as it turns away. For an ideal, diffuse Lambertian surface the
luminance (L) of the surface is proportional to the cosine of the angle
(θ) between the direction of illumination and the surface normal, and is
independent

This chapter considers the early stages of visual processing, the nature
of the initial representations built from a retinal image, and the
algorithms that produce them. In Chapter 1, we explained how light
provides information about the visual world, through the relationship
between the spatial structure of the optic array and surrounding
surfaces and objects. A promising starting point for processing the
retinal image, which is a projection of the optic array, is therefore to
create a representation of it that makes its spatial structure explicit,
specifying where the most significant changes occur in the intensity and
spectral composition of light. Notice that we are making a simplifying
assumption here, treating the retinal image as static and ignoring
change in its structure over time caused by movement of the perceiver or
of objects. In Chapter 8, we will return to the question of how motion
in a retinal image can be processed and represented.

LIGHT, SURFACES, AND VISION Clearly, there is a relationship between the
places in an image where light intensity and spectral 85

86

VISUAL PERCEPTION

of the viewing direction or viewing distance. Thus: L = I.R.cos(θ)

where I is the intensity of illumination and R is the reflectance, a
fixed property of the material itself that indicates the proportion of
incident light that is reflected. (See Nayar & Oren, 1995, for an
interesting discussion of the physics of surface reflection, and why the
moon looks flat.) As we saw in Chapter 1 (p. 5), reflectance generally
varies with wavelength, but most of our discussion here centres on
spatial variations in image intensity (luminance), leaving aside
spectral variations. The simple equation for image luminance (above)
tells us a great deal about the problem of vision. Three different
aspects of the visual world (lighting, surface material, surface
orientation) are confounded in one variable, luminance. Mathematics
tells us that it is impossible to solve an equation to recover three
unknowns (I, R, θ) with only one given (L), yet that is the task
apparently faced by the visual system. It is tempting to appeal for help
from additional variables such as colour, motion, or binocular vision,
but since human observers cope very well with single, stationary,
black-and-white photographs it is clear that in some sense vision does
achieve the impossible. It can only do so by applying plausible
assumptions or "constraints" that reduce the ambiguity inherent in the
optical image of a scene. In situations where these assumptions are
incorrect we suffer illusions. It is also important to keep in mind that
vision does not analyse isolated points of light, but very large
collections of points forming an image (or "optic array"). Information
is embedded in the relationships, structures, and patterns contained in
the whole image.

What is an image? We have seen that image luminance varies with
variations in illumination, such as those caused by dappled lighting,
and with variations in surface orientation, which occur over curved
surfaces or at creases, bumps, and folds in a surface.

The luminance profile of an image (Figure 5.1A,B) thus has a complex
waveform across space, not unlike a sound wave. Furthermore, almost all
natural surfaces are textured (see Chapter 1, p. 5), and so give rise to
complex local variations in intensity and spectral composition within
small regions of the image. The complex relationship between natural
surfaces and the intensity of light reflected from them is illustrated
in Figure 5.1A,B, which shows the intensity of light measured along a
line across a natural image. Figure 5.1C,D illustrates the full,
two-dimensional variation, as a "landscape" whose peaks and valleys are
the highs and lows of image intensity. The contour map (E) is also a
useful plot of the intensity variation. Where contour lines bunch
together the intensity is varying most steeply across space, and this
may define the locations of edges, as we shall see later. Notice that
intensity varies in a complex fashion everywhere---within the face, and
within the background. The boundary between head and background is not
particularly special or salient in the luminance profile. In Figure 5.1,
or almost any natural image, the changes in light intensity and spectral
composition associated with the edges of objects are embedded in a mass
of changes caused by surface texture, surface shape, shadows, glossy
highlights, secondary reflections, and the layout of light sources. A
fuller discussion of these factors can be found in Watt (1988, Ch. 1),
but the implication for present purposes is that there is no simple
correspondence between the edges of objects in a natural scene and the
intensity changes in an image.

THE PRIMAL SKETCH Recent theories have recognised the complexity of the
relationship between the structure of natural scenes and the structure
of images, and have proposed that the task of describing the structure
of the image must come before the task of interpreting the image in
terms of objects and events in the world. In Marr's (1982) influential
theory, the first task (image description) is broken down into

5. IMAGES, FILTERS AND FEATURES

87

Images---the raw data for vision. (A) The complex distribution of
intensity across an image is shown by the graph superimposed; it plots
the intensity values along the horizontal line through the eyes. (B) As
A, after a smoothing operation has been applied to the image. In
general, an image (C) has a complicated variation of intensity that can
be visualised as a surface (D) or as a contour map (E). How is spatial
structure found in this elaborate "landscape" of intensity?

two main stages. The first stage is to make local changes in light
intensity explicit, in a representation that Marr termed the primal
sketch. This name was introduced in the context of a particular model
described by Marr and Hildreth (1980), but has come to be used
generically to refer to the early stage of feature representation in
vision. In the second stage of Marr's scheme, larger structures and
regions are identified by other algorithms taking the primal sketch as
their input, and we consider this further in Chapter 6. Before we
consider particular algorithms, it is important to make clear what task
they are required to perform. They must take as their input

a grey-level representation of the retinal image and transform it into
an output that describes local image features---the primal sketch. A
grey-level representation of the image is obtained by measuring light
intensity in each of a large number of small regions of the image,
called pixels (from picture elements). This representation is therefore
simply a two-dimensional array of light intensity values, of the kind
illustrated in Figure 5.1D. Such an input representation corresponds to
that available to the visual pathway, with each pixel and its associated
intensity value corresponding to a photoreceptor and its receptor
potential. The primal sketch must specify where in the

88

VISUAL PERCEPTION

image significant changes in intensity occur, and what those changes are
like. To do this requires a computational theory of what is to be
computed, coupled to a model of how this computation is to be carried
out.

Image computation To get a clearer idea of how visual mechanisms
(receptive fields of cells) may be said to "compute"

things, we start with a simple example. Figure 5.2A shows a
one-dimensional intensity profile L(x), where the luminance (intensity)
at each image point has a numerical value L1, L2, L3, etc. The steepness
of the profile at a given point is the gradient, or 1st derivative
dI/dx. If dI/dx is positive, the luminance at that point is increasing
from left to right; if negative, it is decreasing. The numerical value
of the gradient is closely

How simple arithmetic "operators" can compute useful things from image
intensity values. (A) The gradient of a signal or an image is given by
the local difference (L) in intensity. This is simple, but formally we
can say that we are applying an operator (−1,1) to two points (L1,L2) on
the signal to give the output (L2 − L1). Applying the same operation at
all positions across the signal would yield a whole set of gradient
values, which might vary from one position to another. (B) Smoothing
operator combines adjacent input values in a similar fashion, but the
operator "weights" are all positive. (C) Two operations, such as
smoothing (B) and local differencing (A) can be combined into one
operator. (D) Applying local differencing (A) again to the operator (C)
creates a secondderivative operator. (E) Same operators as D (with a
sign reversal) and C, plotted as bar chart profiles, to show how these
image operators resemble the receptive fields found in the visual
system.

5. IMAGES, FILTERS AND FEATURES

approximated by the difference (δL) in intensity between adjacent
pixels, divided by their separation (δx). That is, δL/δx approximates
the true derivative dI/dx. If we take δx to be a (small) constant, then
finding the value of the local difference δL = (L2 − L1) will give us a
measure that is proportional to the gradient. Now, subtracting one
number from another is a simple enough computation, but it enables us to
introduce the important idea of an "image operator" (widely used in
digital image processing and image enhancement) and this will give us
insight into the functions of visual receptive fields. The operator
(−1,1) shown in Figure 5.2A is applied to two adjacent image points (L1,
L2). The input values and operator values are multiplied point by point,
and then these products are added to give the output ("response") of the
operator: resp = (−1)*L1 + (1)*L2 = (L2 − L1) = δL. Thus, this
local-difference operator computes the image gradient, but we can
construct other operators in the same general fashion to compute other
things. A noisy signal, for example, can be improved to some extent by
smoothing the values. This involves replacing each input value with a
weighted average of that value and its near neighbours---often called a
running average, or a local average. If we set the weights in our
operator to (1,3,1) for example (Figure 5.2B) then each point would be
combined with one-third of its neighbour's values on either side.
Suppose now that we want to apply two operations to an image---
smoothing and gradient measurement. Do we have to apply one operator and
then the other? No, we can increase efficiency by combining the two
operators into one, as shown in Figure 5.2C. The result will be exactly
the same as running the two operators successively. Finally, as we shall
see below, it is often valuable to compute the 2nd derivative of an
image signal. This is written as d2L/dx2; it measures the spatial change
in the gradient and is related to the curvature of the luminance profile
rather than its steepness. Figure 5.2D shows how the simple (−1,1)
operator can be applied to the operator of Figure 5.2C (i.e., twice in
total) to create one that computes the 2nd derivative and smoothes the
signal into the bargain.

89

The link with visual physiology is this: the weight values in each
operator define its receptive field. Notice how the 2nd derivative
operator has an even-symmetrical, three-lobed, centre-- surround
receptive field (Figure 5.2D,E) while the gradient (1st derivative)
operator has an oddsymmetric, two-lobed field (Figure 5.2C,E). Thus we
can now see how the receptive fields mapped out by investigations of the
retina and visual cortex (pp. 35, 47) can also be said to act as "image
operators", carrying out a variety of image computations including
measurement of the spatial derivatives of image intensity. It is
important to note that, although these operators are localised, we
should think of them as being applied many times across the whole image
to create an output array (a "neural image", so to speak) that is a
filtered version of the input image. This is illustrated in Figure 5.3B,
with a larger, smoother operator that is physiologically more realistic.
Armed with these basic concepts about image computation, we can start to
see how they have been applied to the problem of feature detection in
vision. In the last 30 years a large number of algorithms has been
published for edge detection in computer vision; for general
introductions to this topic see Winston (1984), Schalkoff (1989), Jain,
Kasturi, and Schunk (1995), or Mallot (2000). We shall describe and
compare several of the more enduring ones, and those that have received
most attention as models for human visual processing.

Algorithms for edge detection The work of Marr and Hildreth (1980) was
important and influential because it combined a well-motivated
computational theory about what is to be detected with an implementation
that was plausible in terms of known physiology and psychophysics. The
key idea was that edges in an image are those points at which the
luminance is changing most steeply across space. Note that this is a
theory that defines what an edge is, rather than being an empirical fact
about images. Thus, Marr and Hildreth's theory (like others before it)
states that edges are to be found at gradient peaks. They might be found
directly by

90

VISUAL PERCEPTION

How receptive fields operate on an image: standard linear model. (A)
Each cell "looks at" a portion of the image determined by the size and
position of its receptive field on the retina. Each position within the
receptive field of a given cell has a sensitivity value (positive or
negative), as shown. (B) Every input point that falls within the
receptive field makes a (positive or negative) contribution to the
cell's response. That contribution is the product of the input luminance
times the sensitivity value at that point. The net response of the cell
(filled square) is just the sum of the individual contributions.
Importantly, the same process happening in a large set of adjacent cells
creates a spatial array of response values (open squares). The shape of
this "response profile" will depend on both the nature of the input
image and the form of the receptive field.

computing gradient values (dL/dx, as above) and scanning through them to
find those that are larger than their neighbours on either side.
Alternatively, a standard method of elementary calculus is to consider
slope values again. At a peak the slope is zero, and so to find peaks in
dI/ dx we can look for zeroes in its derivative, which is the 2nd
derivative of the image luminance L(x), introduced above. In summary:
"Wherever an intensity change occurs, there will be a corresponding peak
in the first directional derivative, or equivalently, a zero-crossing in
the second derivative of intensity" (Marr & Hildreth, 1980, p. 192).
Both these important points are illustrated in Figure 5.4, where the
gradient operator shows a peak of response at the edge location (at
position x = 0; Figure 5.4A), but the 2nd derivative operator has a
zero-crossing in its response at this point (Figure 5.4B).

Georgeson and Freeman (1997) asked whether the Marr--Hildreth model
would predict human perception of edge location and edge polarity, and
tested it experimentally by getting observers to mark the positions of
edges in a 1-D pattern composed of two sine-wave gratings (Figure
5.5A,B). Their results (Figure 5.5C) showed that people did indeed mark
edges close to all zero crossings of the 2nd derivative. The polarity of
the edges was always correct and the way edge position shifted with 3f
phase was very well predicted (not shown). This gives general support to
the derivative-based approach, but since peaks in the 1st derivative
(gradient) always give ZCs in the 2nd derivative (Figure 5.4), the
results cannot tell us which of these two equivalent implementations may
be used by the human visual system. Effective implementation of these
computational principles has to face several important

91

Measuring image gradients and derivatives. (A) Top trace is the stimulus
luminance profile, representing a thin bright line, a blurred edge, and
a blurred bar. The trace below is the response profile of a 1st
derivative (gradient) filter. Note how there is a peak response
(gradient maximum) at the edge location, but a peak and trough around
the line and bar locations. (B) Similar layout, but the filter is a 2nd
derivative operator, as used by the Marr and Hildreth (1980) and Watt
and Morgan (1985) models of feature representation. Note how the
zero-crossing lines up with the edge location (dotted line). The
response to a thin line (left part of each panel) reveals the profile of
each filter's receptive field---two-lobed (A) and three-lobed (B). These
plots illustrate a key problem for early spatial vision: how is the
information from spatial filters used to derive a description of the
locations and characteristics of features present in the image?

92

Human perception of edges. (A, B) The type of compound (f + 3f) grating
used by Georgeson and Freeman (1997). (C) Sample of results showing a
close match between the perceived location and polarity of edges
(triangles, second row) and location of 2nd derivative zero-crossings
(triangles, top row). Bottom four rows are data from individual
observers. These results (and similar ones for other phases of the 3f
component) are consistent with the Marr--Hildreth (1980) model, and with
a gradient-peak finding model (Canny, 1986), but not with the local
energy model (Morrone & Burr, 1988).

5. IMAGES, FILTERS AND FEATURES

difficulties. First, natural images are twodimensional, not
one-dimensional, so that edges may occur at any orientation. Second,
natural images are noisy, containing unwanted or irrelevant variations
that are exaggerated by derivative operations. Third, edges exist at a
variety of spatial scales, from large (blurred edges) to small (sharp
edges). For blurred, large-scale edges the gradient is shallow and local
differences, ∂L/∂x, become very small and unreliable. The problem of
edge orientation is illustrated in Figure 5.6B which shows the result of
computing the smoothed derivative (∂L/∂x) of an image (A) in a
horizontal direction. Peaks of gradient (light bands in Figure 5.6B)
capture vertical and near-vertical, dark-to-light edges quite nicely,
while the dark bands mark vertical light-to-dark edges. Note, however,
that horizontal edges (e.g., on the collar) are missed by this (∂/∂x)
operator.

93

A 1-D operator is not sufficient for a 2-D image. An obvious, standard
solution is to take derivatives in two or more directions (e.g., ∂/∂x
and ∂/∂y) and combine the results. Since each operator is
orientation-specific (see Figure 5.7B,C), this is consistent with the
variety of orientationselective cells found in visual cortex, but an
efficient simplification was introduced into Marr and Hildreth's (1980)
model by combining these multiple orientations into one circular filter.
Recall that peaks in ∂L/∂x are just those places where the values of the
2nd derivative ∂2L/∂x2 pass through zero, ie., zero-crossings. Marr and
Hildreth's 2nd derivative operator simply summed two oriented operators
at right angles to each other (Figure 5.7E,F), to form what is known as
the Laplacian operator, ∇2 (pronounced "del-squared", defined as
∂2/∂x2 + ∂2/∂y2). It is a non-oriented, 2nd derivative operator.

Edges and gradients in images. (A) Original image. (B) Filtered image
that highlights the places that have a steep gradient in a horizontal
direction. Light points have a steep positive gradient; dark points have
a steep negative gradient. Notice how these points are associated with
vertical or near-vertical edges in the original. The filter's receptive
field (inset) combined two operations---smoothing by a Gaussian filter
(G) and differentiation (∂/∂x).

94

VISUAL PERCEPTION

Gaussian derivative operators. (A) A 2-D Gaussian "blob" G(x,y) that
could be used as a smoothing (local averaging) operator in image
processing. (B) Its 1st derivative in the horizontal (x) direction. (C)
Its 1st derivative in the y-direction. (D) Circular 2nd derivative
operator ("del-squared-G") formed as the sum of the two directional 2nd
derivatives shown in E and F. They in turn are the derivatives of B and
C respectively. Note the negative surround in D but not A.

To combat noise, Marr and Hildreth proposed that the image should also
be smoothed by local averaging (an idea introduced in Figure 5.2). The
bell-shaped Gaussian function (G; Figure 5.7A) acts as an effective and
well-behaved smoothing operator, and it can be combined with the
Laplacian to form a single operator that smoothes and computes the 2nd
derivative. This combined operator can be expressed as: ∇2G
\["del-squaredG", equal to (∂2G/∂x2 + ∂2G/∂y2)\], the
Laplacianof-Gaussian operator. The receptive field of this operator is
shown in Figure 5.7D, and it has a circular, centre--surround form
analogous to the receptive fields of retinal ganglion cells (see Chapter
2, p. 30), LGN cells and some cells in the input layers of V1 (see
Chapter 3, p. 47). (Since it is defined as the sum of two directional
operators \[Figure 5.7E,F\] such a circular filter could also be formed
by summing the outputs of orientationselective cortical cells, and there
is some psychophysical evidence to support this suggestion \[Georgeson,
1992, 1994\].) The response of this type of filter to a natural

image is shown in Figure 5.8B,E, for two different degrees of smoothing.
Zero-crossings (ZCs) are those points at which the filter response
passes from positive to negative, i.e., transitions from light to dark
in Figure 5.8B,E. The spatial pattern of ZC locations in these filter
responses is seen (superimposed on the original image) in panels C,F.
The ZC pattern produced by the ∇2G filter does capture much of the edge
structure of the image, but it is clearly not perfect. Some ZCs seem to
wander off course, terminate too soon, or miss fine structure that is
visible to the eye. Such a ZC pattern is only a starting point for a
more comprehensive theory of edge representation. Perhaps the main
limitation of any one ZC pattern (e.g., Figure 5.8C or E) is that the
filter producing it had a single, fixed size. This means that it will
average-out smaller details that we actually wish to preserve, and will
be unreliable for much larger, blurred features that present the
receptive field with a shallow gradient and hence a very weak signal.
Marr and Hildreth's (1980) proposed solution

95

Illustrating the Marr--Hildreth theory of edge detection. (A) Input
image. (B) Image filtered by a small, circular 2nd derivative operator
(see Figure 5.7D). (C) Zero-crossings (ZCs) in the filter output,
superimposed on the original image to show the association between ZCs
and edges. D--F: As A--C, but the filter is twice as big. Note the
coarser structure in the filter output (E) and the sparser pattern of
ZCs (F).

96

VISUAL PERCEPTION

to these limitations, inspired by the evidence for multiple-sized
filters in human vision ("spatial frequency channels", discussed further
below; see pp. 99--102), was to incorporate a range of coarse through to
fine filter sizes operating on the image

Example of spatial filtering at multiple spatial scales. Top left:
Original image. Below: Four filtered images obtained using circular,
Laplacian-ofGaussian, centre--surround receptive fields. The receptive
fields, shown attached to each image, ranged from large to small. Note
that the filter output is shown only within the black outlines. Top
right: Average of the four filtered images. The similarity between this
and the original image shows that multi-scale filtering can preserve all
the image information, even though any one scale does not.

at the same time. Figure 5.9 shows Einstein's face filtered through
circular, ∇2G filters of four different sizes, each differing from the
next by a factor of 2 in width. The receptive fields used are shown to
scale, attached to each image. Note how the

5. IMAGES, FILTERS AND FEATURES

image information is de-composed, and distributed across the four
scales. The coarsest scale captures the broad contrasts between
different image regions, while smoothing out all fine detail. The
reverse is true for the smallest scale, where only fine, local contrasts
are preserved. Larger regions are reduced to a mean grey value, implying
little or no response from the filter. Information is not lost, however,
but distributed across scales. The image formed by averaging across the
four scales (adding the four image intensities, pixel by pixel, and
dividing by 4) is very similar to the original. In other words, the
spatial frequency content of the original image is split into four
adjacent, overlapping bands, and when these bands are summed together
the original image can be reconstructed. In the Marr--Hildreth model,
zero-crossings (ZCs) are extracted separately for each filter size
(Figure 5.10), and we should think of the ZCs only as candidate edges.
Whether a ZC location is written into the primal sketch as an edge
segment depends on the relation between ZCs at adjacent scales. The
spatial coincidence assumption pro-

97

posed that an edge existed in the image only if ZCs at the same position
and orientation existed in at least two filters of adjacent sizes.
Figures 5.8 and 5.10 reveal that the number and density of ZCs increase
with the fineness of the filter. At each scale there will therefore be
many ZCs that are not supported by partners in the next larger scale,
but may be supported by those at a finer scale. This proposal, however,
was never fully elaborated or tested, and the problem of combining
information across spatial scales remains a difficult one for all
approaches to edge detection. One principled solution to such problems
of multi-scale combination is to use only a single scale of filter, but
to allow that scale to vary according to the local image context.
Intuitively, one needs to use small filters where the image contains
fine detail, but large filters where the information is coarse or
blurred. This is known as space-variant, or adaptive scale filtering
(Morrone, Novańgione, & Burr 1995). In Elder and Zucker's (1998)
computational model the rule for choosing the scale of the 2nd
derivative filter is to

The results of passing an image (upper left) through filters of three
different widths. Upper right, lower left, and lower right:
Zero-crossings obtained with a narrow, an intermediate, and a wide
filter. Reproduced from Marr and Hildreth (1980) with permission of The
Royal Society.

98

VISUAL PERCEPTION

use the smallest reliable scale at each location (as evaluated by
measurements of the local signal: noise ratio), and then to derive an
edge-map from the zero-crossings in the space-variant filter response.
This "scale-selection" approach neatly avoids the filter combination
problem by ensuring that only one ZC map is produced. It has proved to
be a robust method for finding the location, orientation, and blur of
edges, especially in difficult images where there are shadows whose blur
varies across space, or where image blur varies with distance of objects
in the scene. It has not yet been tested as a theory for human vision.
In Marr's theory, the primal sketch is proposed as a symbolic
representation of the image in terms of four different tokens---edge
segments, bars, terminations, and blobs---denoting four different kinds
of intensity change, illustrated in Figure 5.11. These basic elements of
the primal sketch are derived by grouping and combining the ZCs. For
example, edge segments are found by examining the coincidence of ZCs
across filter scales (as described above) and bars are found where there
are nearby, parallel edge segments of opposite polarity. Blobs are
asserted where ZCs

The zero-crossings obtained from a picture of a plant behind a wire mesh
fence, using (a) a narrow and (b) a wide ∇2G filter. These are combined
to give the raw primal sketch; the locations of blobs, edge segments,
and bars are shown in (c), (d), and (e). Reproduced from Marr and
Hildreth (1980) with permission of The Royal Society.

form small, closed contours. As the example in Figure 5.11 shows, this
"raw" primal sketch does not selectively pick out the outlines of
objects, since the tokens also represent texture elements and markings
within surfaces. In Chapter 6, we describe Marr's proposals for further
grouping algorithms that take the raw primal sketch as input and recover
the boundaries of objects. The aim at this first stage is to extract
from a noisy image a symbolic representation of the significant
gradients of light intensity.

Implementation: Derivative operators as spatial ﬁlters Marr and Hildreth
(1980) argued that the first stage of computing the primal sketch is
carried out in the visual pathway from retina to striate cortex, and
that the role of simple cells is to measure the positions, strengths,
and orientations of zero-crossing segments. Simple cells do not detect
edges, as the feature detection theory held (see Chapter 3, p. 54), but
instead measure parameters from which the locations of edges can later
be obtained. According to Marr and Hildreth, the known properties of the
visual cortex

5. IMAGES, FILTERS AND FEATURES

99

Marr and Hildreth's scheme for the detection of zero-crossing segments
by cortical simple cells. In (a), the fields of an on-centre and an
off-centre LGN cell overlap. If both are active, then a zerocrossing ZC
must lie between them, and an AND gate connected to them would detect
it. In (b), an AND gate connected to parallel rows of on- and off-centre
cells (field surrounds not shown) will detect a zero-crossing segment
falling between the dashed lines. Adapted from Marr and Hildreth (1980).

take us only to an early stage in visual perception---the computation of
zero-crossing segments (Figure 5.12). The idea that cortical simple
cells act as 1st, 2nd and higher Gaussian derivative operators has
received close attention, from Koenderink and van Doorn (1987) and Young
(1985, 1987; Young & Lesperance, 2001). Interestingly, Young (1985) has
shown that dG/dx operators (see Figure 5.7) can be approximated rather
well by subtracting one Gaussian from an adjacent one, rather as in the
model of Figure 2.9, but offset from each other by as much as 1--2
standard deviations. These "difference of offset Gaussian" (DOOG)
functions are not only convenient to implement in computer programs, but
would also be a simple and robust way for biological systems to carry
out calculus operations, even with higher-order (2nd, 3rd, 4th, . . .)
derivatives. The next section looks more closely at the evidence for
these cortical filters, and their role in spatial vision.

MULTIPLE SPATIAL FILTERS A central feature of Marr and Hildreth's theory
is its use of 2nd derivative operators at several spatial scales. Figure
5.13 illustrates the important fact that the coarse-scale filters, with
large recep-

tive fields, respond selectively to low spatial frequencies, while
fine-scale filters (with small receptive fields) respond best to high
spatial frequencies. Thus the theory assigns a functional role to cells
in the visual cortex tuned to a range of different spatial frequencies
(De Valois et al., 1982; see Chapter 3, p. 55). Along with this
physiological evidence, psychophysical studies of the ability of
observers to detect low-contrast gratings and other spatial patterns
also imply that there are multiple, spatial frequency-tuned filters in
the human visual system, discussed next.

Subthreshold summation Campbell and Robson (1968) and Graham and
Nachmias (1971) measured human observers' contrast sensitivity for
detecting compound gratings formed by adding together two sinusoidal
components with spatial frequencies f and 3f, e.g., 3 and 9 c/deg,
similar to Figure 5.5A. Surprisingly, sensitivity for these compounds
was about the same as for the components presented individually. How
could this be so? The basic rationale in this type of subthreshold
summation experiment is that if adding two patterns together improves
sensitivity then the two patterns must be summing their effects in a
common channel or filter; if not, they must be passing through separate
channels. On this basis, then, the 3 and 9 c/deg gratings must have been
detected by different

100

Spatial frequency filtering. Same Gaussian derivative filters as Figure
5.4, but now at four different receptive field (RF) sizes or scales, as
shown. Main panels: input (top) is a sine-wave that sweeps from low to
high frequency (left to right). Note how the smaller-scale filters
respond better to high frequencies. Each filter is selective to a band
of frequencies. The bandwidth is broader for the two-lobed, 1st
derivative filters (A) and narrower for the three-lobed 2nd derivative
filters (B).

5. IMAGES, FILTERS AND FEATURES

channels. Hence these studies at very low contrast led to the important
idea that visual input is processed in multiple, independent channels,
each analysing a different band of spatial frequencies. Actually, the
visibility of a two-component pattern may be slightly higher than a
single grating, because when two channels are weakly stimulated the
observer has two chances to detect a channel response, rather than one.
The same would be true if a weak stimulus were presented twice instead
of once. This small improvement is known as probability summation. The
visibility of a compound pattern whose two components differ widely in
spatial frequency or orientation is no better than would be expected by
probability summation alone. Thus, for the combination of gratings of 3
and 9 c/deg, or of 0 and 90° orientations, the data favoured independent
detection by separate filters. More recent evidence suggests that at
higher contrasts the channels interact with each other (e.g., through
mutual inhibition) more than one would suspect from threshold studies.
Thus although spatial filters in human vision are probably not entirely
independent, there is wide agreement that multiple spatial filters
exist. For a very full treatment of this research, consult Graham's
(1989) monograph.

Contrast adaptation and masking Spatial frequency channels have been
analysed in detail by measuring contrast thresholds for detecting
sinusoidal gratings before and after exposure to gratings of
higher-contrast (Blakemore & Campbell, 1969). If an observer views a
high-contrast grating at a spatial frequency of (say) 5 c/deg for 5
minutes, then the ability to see a low-contrast 5 c/deg grating is much
reduced for a few minutes afterwards. The process resulting in temporary
loss of sensitivity is termed contrast adaptation. It is quite distinct
from local adaptation to the mean light level (light adaptation), since
eye movements or pattern movements sufficient to average out local light
adaptation do not eliminate the observed contrast adaptation (Jones &
Tulunay-Keesey, 1980; Kelly & Burbeck, 1980). Testing the observer with
a grating of 10 c/deg

101

or 2 c/deg after adaptation to 5 c/deg produces little or no change in
sensitivity, showing that adaptation is specific, or "tuned", to spatial
frequency. The bandwidth of the tuning curves is about 1.4 octaves
(Georgeson & Harris, 1984), the same as the median bandwidth of cortical
cells (Chapter 3, p. 55), but the bandwidth of both adaptation and
cortical cells decreases systematically with increasing spatial
frequency (Figure 5.14). Adaptation is also tuned to orientation,
producing little effect on test pattern sensitivity when the adapting
and test patterns are more than 30--45° apart in orientation (Movshon &
Blakemore, 1973; Snowden, 1992). Finally, if the adapting stimulus is
given to one eye and then the other eye is tested, loss of sensitivity
still occurs in the same "tuned" fashion but its strength is reduced by
about 30% (Bjorklund & Magnussen, 1981; Blakemore & Campbell, 1969).
This "interocular transfer" of adaptation is therefore occurring at
binocular site(s) in the visual system. This whole cluster of results
compellingly argues that contrast adaptation, and the oriented spatial
frequency filters implied by it, reside in primary visual cortex, V1.
Analogous reductions of contrast sensitivity occur if the test grating
is masked by a superimposed, higher-contrast, masking pattern. Many of
the properties of masking turn out to be similar to those of adaptation
(Foley & Boynton, 1993; Georgeson & Georgeson, 1987). Wilson, McFarlane,
& Phillips (1983; also Wilson, 1983) have interpreted the spatial
frequency tuning of masking in a model with six independent channels
tuned to different spatial frequency bands, with peak sensitivities
ranging from 0.7 to 15 c/deg (Figure 5.15). Note that six is the minimum
number of filters required to account for the masking data. There may be
more and, given the huge number of cells in the visual cortex, it may be
reasonable to think of a continuum of filters spanning the visible range
of spatial frequencies. In the central fovea the spatial scale of the
smallest filters may be very small indeed. A recent psychophysical study
of contrast adaptation and spatial frequency discrimination at high
spatial frequencies (using very fine gratings produced

102

VISUAL PERCEPTION

Estimates of the tuning of spatial frequency filters (expressed as
bandwidth, in octaves) obtained from single-cell recordings in striate
cortex (De Valois et al., 1982; solid squares) and from psychophysical
studies of contrast adaptation and masking (open symbols). The
single-cell data are averages, and the wide spread of values around each
mean value is not shown. Horizontal dashes (left) mark the bandwidth of
Gaussian derivative filters, from 1st (G1) to 7th (G7) derivative.
Illusory grating data (open triangles) are from Georgeson (1980);
masking data (open circles) from Wilson et al. (1983); adaptation data
(open squares) re-calculated from Blakemore and Campbell (1969).

Spatial frequency tuning curves of the six spatial filters derived from
contrast masking data via a mathematical model of contrast detection.
Comparison with Figure 2.14B shows that these filters are much more
sharply tuned than retinal ganglion cells, but are similar to the tuning
of cortical cells (Figure 5.14). Replotted from Wilson et al. (1983).

directly on the retina by laser interferometry) concluded that there
must be visual channels in the fovea whose peak spatial frequency is as
high as 35 c/deg, or even higher (Smallman, MacLeod,

He, & Kentridge, 1996). Their receptive field centres would be only one
or two cone receptors in width, and they presumably contribute to vision
at the finest possible level of detail.

5. IMAGES, FILTERS AND FEATURES

Filter bandwidths and spatial derivatives The bandwidths of spatial
filters derived from contrast adaptation and masking, and from monkey V1
cells, are compared in Figure 5.14. The agreement is remarkably good. It
is evident that filters tuned to high spatial frequencies have narrower
bandwidths and so are more specific to their preferred spatial
frequency. Presumably they have a greater number of alternating positive
and negative lobes within their receptive fields (see Chapter 3, p. 47).
De Valois, Thorell, and Albrecht (1985) obtained direct evidence for
this by measuring both the bandwidth of cat and monkey V1 cells and the
number of cycles of a grating that produced the cells' greatest
response. For simple cells, those with broad bandwidth (1.2 to 2
octaves) were most activated by rather few bars (1--2 cycles of a
grating at the optimal frequency), while narrowband cells (0.5 to 1.1
octaves) preferred to have many more bars in their receptive fields,
ranging from 2 to 5 cycles of the grating pattern. The presence of very
narrow-band simple cells (0.5 to 1 octave) was confirmed in the cat's
striate cortex by Robson, Tolhurst, Freeman, & Ohzawa, (1988), and von
der Heydt Peterhans, & Dursteler (1992) similarly found that the
narrow-band "grating cells" of monkeys tended to be tuned to higher
spatial frequencies (8--16 c/deg), as already implied by Figure 5.13.
The bandwidth of cells thus describes their spatial filtering effect
(cf. Figure 5.13), and has implications for the underlying receptive
field structure and for the idea that visual cells act as Gaussian 1st
and 2nd derivative operators (Canny, 1986; Marr & Hildreth, 1980). No
matter whether they are of large or small scale, derivative operators
have a fixed bandwidth (in octaves), and the bandwidth decreases with
increasing derivative number (1st, 2nd, 3rd . . .). It is worth listing
the first few (from Young, 1985). The first seven derivatives have
bandwidths of 2.6, 1.77, 1.42, 1.22, 1.09, 0.99, 0.92 octaves, as shown
on the left of Figure 5.14. It is clear that bandwidths from physiology
and psychophysics fall well below the 2nd derivative bandwidth (G2, 1.77
octaves) at medium and high spatial frequencies. We must therefore
conclude that while some filters may well serve as 1st and 2nd
derivative oper-

103

ators, many cannot do so, especially at the higher frequencies, because
they are too narrowly tuned. In other words, Marr and Hildreth's (1980)
theory does not provide a role for the spatial filtering properties of
many cortical cells, especially those responsive to fine detail. There
must therefore be other roles played by spatial filters in vision.
Recall that to describe the image usefully, the primal sketch must not
only locate the positions of edges, but also describe their properties,
such as blur, contrast, length, and orientation. Several researchers
have suggested a role for 3rd derivative operators in encoding the blur
of edges. While the locations of edges may indeed be given by
zero-crossings in the 2nd derivative output (see Figure 5.5; Georgeson &
Freeman, 1997), their blur could be recovered from the ratio of 1st to
3rd derivative values taken at the edge location (Georgeson, 1994; see
Kayargadde & Martens, 1994a, 1994b, for more detailed computational
analysis along similar lines). An alternative rule for coding blur is to
measure the spatial separation between the positions of peak and trough
responses flanking the ZC (Figure 5.4B; Watt & Morgan, 1983). This
separation increases with increasing blur and so can be used to estimate
blur at an edge. Elder and Zucker (1998) used this algorithm, and
implemented it by finding the separation between ZCs in the 3rd
derivative (arising from peaks and troughs in the 2nd). Thus we can see
possible roles for 2nd and 3rd derivative filtering, but the role of the
very narrow band filters in vision, perhaps representing even higher
derivatives, remains largely unknown.

OTHER ROUTES TO THE PRIMAL SKETCH Marr's paradigm evidently did not
deliver a complete solution to the problem of analysing features in
images, and others have explored different routes from filters to
features. Pearson and Robinson (1985) were disappointed with the results
obtained when zero-crossings were applied to the problem of automatic
sketching of images of faces. They were trying to solve the applied
problem of compressing moving images of faces

104

VISUAL PERCEPTION

into an economical form for transmission down telephone lines to provide
video-phones, particularly useful in enabling deaf people to communicate
at a distance using sign language. An implementation of Marr and
Hildreth's algorithm led to a cluttered, unusable sketch (although we
should recall that ZC maps are intended as a coded representation of
image information, not as a picture to be looked at). This setback led
Pearson and Robinson to consider in detail the nature of the edges that
it was important for their artificial artist to sketch. Pearson and
Robinson (1985) noted that the face is not a flat pattern (see also
Bruce, 1988, and Chapters 9 and 13) but is a bumpy surface. The places
where an artist draws lines in order to sketch the face often correspond
to places where the surface of the face turns sharply away from the
lighting direction, in addition to places where there is a sharp
contrast such as at the hairline. Subject to certain lighting
constraints, the former surface feature gives rise to a dark line or
valley in an image, while the latter gives rise to a luminance
discontinuity (a step edge). Pearson and Robinson devised a
"valley-detecting" algorithm that captured these kinds of intensity
changes in an image. It first applies a small-scale, centre-- surround
(2nd derivative) filter to the image (Figure 5.16A,B). Rather than
detecting zerocrossings in the output of this filter, Pearson and
Robinson's algorithm then looks for the (negative) peak responses of the
filter (Figure 5.16C) that arise from dark valleys and step edges in the
image. Valleys and edges at different orientations are found by applying
oriented filters to detect horizontal, vertical, and diagonal valleys,
or by using a single circular filter as we have done in Figure 5.16. The
"drawing" produced in this way (Figure 5.16C) captures a reasonable
likeness. Pearson and Robinson showed that an even more satisfying
sketch (Figure 5.16H) is produced when blocks of hair, clothing, etc.,
are filled in by setting to black any area whose mean intensity falls
below some criterion. The result is an automatic sketch that bears a
striking resemblance to that produced by a human artist. This method
clearly captures features that are perceptually relevant, and it is
interesting that the

positive parts of the filter output ("light valleys", Figure 5.16E)
appear to be much poorer at rendering the face image. When the dark
valleys are drawn with light lines (see Figure 5.15D), this too looks
poorer than the dark valley sketch even though the binary information is
(logically) unchanged. These effects may tell us something about the
importance of contrast polarity in higher-level perceptual processes
concerned with the interpretation of 3-D shape from shading, since the
direction of shading is an important cue to whether a 3-D surface is
convex or concave (see Chapter 7, p. 190). Nevertheless, we should be
careful not to confuse the information captured by an algorithm with the
interpretation we put upon it as viewers of the picture. Let us return
to the general problem of how information from multiple spatial filters
is used to represent local features. Starting from the same basis as
Marr and Hildreth (a set of 2nd derivative filters of different sizes),
Watt and Morgan (1985) proposed a rather different set of processes and
rules (the "MIRAGE" model) to derive edges and bars from the image. In
MIRAGE, the outputs of all the filters (Figure 5.17A) are combined very
early, in a fixed way that keeps the positive and negative portions of
the responses separate, inspired by the evidence for separate ON and OFF
pathways in the visual system. Positive and negative parts of the filter
outputs are separately summed to yield two signals, the S+ and S−
signals (Figure 5.17B). The locations of edges, bars, and luminance
plateaux are then determined from measurements made on the S+ and S−
signals, as follows. Each of the S+ and S− signals contain sections (Z)
which are zero and response regions (R+, R−) depart from zero (Figure
5.17B). Thus there are just two kinds of "primitive", Z and R, that
describe the signals. The spatial sequence of Z and R regions can be
used to interpret the intensity changes present. Edges give rise to one
kind of sequence while bars give rise to another. The rules (Watt &
Morgan, 1985) are quite simple: (1) Null rule: a Z region of zero
response is a luminance plateau. (2) Edge rule: an R region with a Z on
only one side (and therefore an R of opposite sign on the other) marks
the boundary of an edge. (3) Bar rule: an R region with a Z on both or

105

Automatic sketching: the main steps in Pearson and Robinson's (1985)
valley detecting scheme. (A) Original greyscale image. (B) Fine-scale
output from a small spatial filter with a circular, centre--surround
receptive field. (C) Binary (black--white) image in which dark points of
panel B (below a certain threshold) are plotted as black, all other
points as white. This tends to capture the main dark lines and edges of
the original, and gives a fairly recognisable sketch. (D) Polarity of
the sketch (C) is important; with polarity reversed (D), the sketch is
less convincing. (E, F) Polarity of the valleys is important; when the
light blobs and ridges of B are picked out, the sketch is even less
effective (E), especially when the light valleys are drawn as dark (F).
(G) Large-scale patterns of shading can be rendered in a binary image
simply by setting a threshold on the original image (A); points below
threshold are black, those above threshold are white. (H) Final sketch
is a simple combination of dark valleys (C) and gross shading (G).

106

Outline of Watt and Morgan's (1985) 1-D MIRAGE model. (A) Response of
four Gaussian 2nd derivative filters to an image (top trace) containing
a linear ramp, a blurred edge, and a bar. (B) Positive parts of each
filter response are added across all scales to form the "S+" signal,
while negative parts form the "S−" signal. These two signals are then
segmented into response regions (R+, R−) and regions of zero response
(Z). Finally, the sequence of Z and R regions is interpreted as "bar" or
"edge" features (arrowed) by a simple set of parsing rules (see text).
In brief: ZRZ or RRR = bar, ZRR or RRZ = edge. Note that illusory bars
(Mach Bands) are indeed seen at the corners of a ramp, even though there
is no luminance peak.

5. IMAGES, FILTERS AND FEATURES

neither side is a light or dark bar, depending on the sign of R.
Additionally, the position (centroid), "mass" (area of the R+ or R−
response lobe), and spread of the response regions are measured. The
masses and their spreads can be used to describe the contrast and blur
of edges and the width of bars (see Watt, 1988; Watt & Morgan, 1985).
This scheme gives a good qualitative account of the appearance of
various luminance profiles, including several well-known illusions of
brightness. Figure 5.17B shows how the illusory light and dark bars
(Mach Bands) seen at the corners of a luminance ramp are predicted by
MIRAGE. Turning to quantitative results, Watt (1988) summarises a
considerable amount of psychophysical evidence to argue that the MIRAGE
algorithm (along with a number of ancillary assumptions) provides a good
account of feature representation in human vision. One example comes
from an experiment by Watt and Morgan (1983), who asked subjects to say
which of two edges looked more blurred. Watt and Morgan measured the
smallest difference in blur that could be detected between the test and
reference edges for different amounts and different types of blur.
Discrimination of different types of blur was consistent with a
mechanism in which peaks and troughs in the 2nd derivative output were
localised and compared. Thus a metric, or internal code, for blur could
be the separation between peaks and troughs, or between centroids, in
the filter response around an edge. MIRAGE was one of the first serious
attempts to use ideas from computational vision theory to explain
detailed, quantitative psychophysical data. It remains an interesting
model, especially in its ideas on how a set of interpretive rules (see
above) is applied to the filter outputs to describe a set of localised
image features (Watt & Morgan, 1985). Morgan and Watt (1997) reviewed
the current status of MIRAGE in relation to other models of feature
detection, and concluded that its three main proposals still stand: the
lack of perceptual access to individual filters (forced by the early,
obligatory filter combination), the importance of half-wave
rectification (the separation of ON and OFF channels), and the role of

107

positional "noise" in limiting the precision of fine visuo-spatial
tasks.

ENERGY MODELS FOR FEATURE DETECTION The models considered so far have
used multiple scales of filter, but restricted themselves to one type of
filter---usually with even symmetric receptive fields (like ∇2G or
∂2G/∂x2, Figure 5.7D,E,F). Since the early days of Hubel and Wiesel
(1962, 1968) it has been recognised that simple cell receptive fields
can also be odd-symmetric (like ∂G/∂x or ∂G/∂y, Figure 5.7B,C). By
definition, in an even-symmetric shape the right half is a simple
reflection of the left half, while in an oddsymmetric shape the right
half is an inverted reflection of the left half. An early suggestion
based on supporting psychophysical studies was that these two subtypes
of cells acted as "bar detectors" and "edge detectors" respectively
(e.g., Kulikowski & King-Smith, 1973). Certainly an odd-symmetric field
will show a peak of activity at the location of a step edge (Figure
5.4A), while an even one will show a peak of activity at the location of
a line or bar (Figure 5.4B). Unfortunately this simple "detector" idea
is inadequate because the even filter also shows a peak (and trough)
adjacent to an edge (Figure 5.4B again), so these response peaks do not
distinguish between bars and edges. This was useful in the automatic
sketching system (Figure 5.16) but is not adequate as a model for visual
coding--- hence the need for rules to parse or interpret the pattern of
responses in models such as MIRAGE and the Marr--Hildreth model.

Local energy from odd and even ﬁlters Another approach that has emerged
from the interplay of studies in biological and machine vision uses both
the even and odd filters together to overcome this problem of ambiguity
in the filter responses. It is based on the concept of local energy or
contrast energy. Suppose first that there are matched pairs of even and
odd filters (such as Gabor functions; see Figure 3.5) that have the

108

VISUAL PERCEPTION

same preference and tuning for orientation and spatial frequency. Pollen
and Ronner (1981) found cell pairs of this kind in the cat's visual
cortex. Given an intensity image, L(x), let the output signal from the
even filter be e(x) and that from the odd filter be o(x). The energy
function E(x) is then defined by: E2(x) = e2(x) + o2(x)

We can see immediately that energy (E) is always positive and that it
will have a high value where either e(x) is high or o(x) is high, or
both. Figure 5.18A shows an example of responses of odd and even filters
to an edge, a bar, and a ramp. The response to the bar reveals the
shapes of the two receptive fields. Both are multi-lobed, and so the
response profiles have several peaks and troughs across space. The
energy response E(x), however, has just a single peak at the location of
the line, and at the edge. Why? The even and odd receptive fields can
each be expressed as a Gabor function (see Figure 3.5) which is simply a
sinewave oscillation at some frequency (f), tapered by a bell-shaped,
Gaussian envelope, G(x) \[see glossary Figure A.1\]. We have seen that
Gabor functions can provide a good general description of most simple
cell profiles (Chapter 3, p. 56). Thus the even field can be written as:
G(x).cos(2πfx)

and the odd field as: G(x).sin(2πfx).

The energy response to a line is therefore: E2(x) =
\[G(x).sin(2πfx)\]2 + \[G(x).cos(2πfx)\]2, = \[G(x)\]2, because sin2() +
cos2() = 1. The amplitude E(x) of this combined response to a bar thus
simplifies to G(x), the smooth single-peaked curve shown at the right of
Figure 5.18A. Much the same is true for the edge response (centre of
Figure 5.18A), although it would be more elaborate to show this
mathematically. A good intuition to hold in mind

is that at any given location (x) the energy response reflects the
amount of image contrast present within the receptive field aperture
centred at x. Thus the energy response peaks at points of high contrast,
but does not represent the fine structure of the image at that point.
More formally, the energy response preserves amplitude, but throws away
phase information. Conversely the ratio o(x)/e(x) represents local phase
but not amplitude. Applying these signal-processing concepts to vision,
Morrone and colleagues pioneered the idea that visual features are
detected specifically at peaks of local energy (Morrone & Burr, 1988;
Morrone & Owens, 1987; Morrone, Ross, Burr, & Owens 1986). Going beyond
simple lines and edges, they showed that energy peaks also coincided
with the location of a variety of other features, including triangle
waves and features whose luminance profile was intermediate between that
of a line and an edge. The energy peak could also account quantitatively
for the well-known illusory features (Mach Bands) seen at the junction
between a luminance ramp and a plateau (Ross, Morrone, & Burr, 1989;
left part of Figure 5.18B). It should be obvious from Figure 5.18B that
the energy peak itself does not identify the type of feature, and it is
suggested that the system could consult the e(x) and o(x) signals at the
energy peak location. If the even signal also has a peak, the feature is
a bar, but if the odd signal has a peak, it is an edge. Thus the
"contrast energy" theory interprets the linear filter output in the same
way as the early feature detector models, with the crucial difference
that energy peaks select the locations to be interpreted. This removes
the need for the more elaborate parsing rules of the Marr and MIRAGE
methods. Malik and Perona (1992, their Section IV) analysed energy
models more generally, showing that they are well-suited for the task of
detecting composite edges. These occur quite frequently in natural
images, and consist of two superimposed luminance profiles, such as an
edge and a line, or an edge and a ramp. For 2-D images the energy
calculation must be done at a number of different orientations, since
any odd-symmetric filter

109

Outline of Morrone and Burr's (1988) 1-D energy model. (A) Response of
the odd- and even-symmetric filters at one spatial scale to an image
(top trace) containing a linear ramp, a blurred edge, and a bar. Energy
profile formed by combining the odd and even filters (see text) is also
shown. (B) Energy profile summed across four spatial scales. Energy
peaks are taken as feature locations, but the type of feature is
interpreted by reference to the odd and even signals at those locations.

110

VISUAL PERCEPTION

cannot be circular and so must be orientationspecific. Malik and
Perona's rule for finding features in 2-D images is, for each pixel
location, to find by interpolation the orientation at which energy is
maximum, then to determine whether that pixel location is also a spatial
peak of energy. The performance of this algorithm was claimed to surpass
that of the Canny (1986) system based on finding points of maximum
gradient, but the comparison between the two approaches was not
extensive.

Experimental evidence: Energy computations in physiology and
psychophysics The energy-peak proposal is simple and attractive in
itself, and also has strong physiological support. The energy
computation \[e2(x) + o2(x)\] has been widely accepted as a model for
the behaviour of cortical complex cells (Adelson & Bergen, 1985;
Emerson, Bergen, & Adelson, 1992 Heeger, 1992a). Complex cells are tuned
for orientation and spatial frequency, as simple cells are, but do not
show distinct lobes of excitation and inhibition within the receptive
field (see Chapter 3, p. 48). Our calculations above show how computing
energy E(x) eliminates the alternating spatial structure of the simple
cell. If a line is drifted across a complex cell's field, the response
rate just rises and then falls, reflecting only the envelope G(x)
(Pollen, Gaska, & Jacobson, 1988). Similarly, when a grating is drifted
across the complex field the most common response is a uniform increase
in firing rate (Movshon, Thompson, & Tolhurst, 1978; Skottun, De Valois,
Grosof, Movshon, Albrecht, & Bonds, 1991). Simple cells' responses, on
the other hand, rise and fall in time with the passage of light and dark
bars across the field. This is diagnostic of a linear filter underlying
the behaviour of the cell, since in a linear system a sine-wave input
gives sine-wave output at the same frequency. Complex cells clearly
don't do this and so they, and the energy computation, are nonlinear
operators, but they do exhibit the spatial and orientation tuning passed
on to them by simple or (simple-like) linear units. How comprehensive is
the energy model as a description of human feature extraction? This is
hard to say, since the number of psychophysical

studies is small, but the intrinsic nature of the energy function
suggests that a full description of spatial structure may need both
energy-peak detection and zero-crossing detection operating in parallel.
Recall that the energy function E(x) follows the envelope of local
contrast variation in the filtered image (Figure 5.18A; see also Figure
8.16). For a sine-wave input, sin(fx), the paired outputs e(x) and o(x)
are of the form k.sin(fx) and k.cos(fx) respectively. Therefore E(x) =
k, a constant, with no peaks at all; the local energy profile is flat.
This would imply that a sine-wave grating is featureless, which seems
implausible to say the least. Georgeson and Freeman (reported by
Georgeson, 1994) found that subjects could accurately adjust the blur of
a single edge (possessing a strong energy peak) to match the blur of
edges in a sine-wave grating (with no energy peak), implying that a
similar edge description is derived in both cases, contrary to the
energy model. Georgeson and Freeman (1997) compared ZC and energy models
directly in their featuremarking experiments. As we saw in Figure 5.5,
observers reported six edges per cycle of the waveform, as predicted by
the ZC model, rather than two as predicted from energy peaks. The ZC
model also correctly predicted the perceived locations of the edges, and
the way they shifted with 3f phase, but the energy model could not.
These perceptual findings support the idea that human observers see
edges at peaks of luminance gradient, and are consistent with
computational models that use 2nd derivative ZCs to make those locations
explicit (Elder & Zucker, 1998; Marr & Hildreth, 1980). The results also
show that human observers do not rely on energy peaks to find luminance
edges. On the other hand, positive evidence for the use of energy peaks
comes from experiments on the perceived location and movement of
"contrast blobs"---i.e., patches of a grating whose luminance profile is
a Gabor function (see glossary; Figure A.1). Subjects can accurately
align the positions of such "blobs" even when the underlying frequencies
or orientations are different. This could not be done on the basis of
local ZC features, and suggests that the peak of the envelope is
available from an energy-type computation

5. IMAGES, FILTERS AND FEATURES

(Hess & Holliday, 1992; McGraw, Levi, & Whitaker, 1999). Similarly, the
perceived movement of such blobs can, under appropriate spatial and
temporal conditions, be seen to follow the movement of the envelope
rather than the internal features of the blobs (Boulton & Baker, 1993).
Under other conditions (at inter-stimulus intervals shorter than 40 ms)
perceived movement follows the internal structure of the blobs instead.
These complementary lines of evidence suggest that parallel analyses are
applied, using both the "1st order" linear filter outputs and the "2nd
order" energy profile. This important distinction is discussed further
in Chapters 6 and 8 in the contexts of texture segmentation and motion
perception.

SOME UNRESOLVED QUESTIONS IN MULTISCALE VISION In this chapter we have
seen clear experimental evidence that early vision analyses and encodes
the input image using arrays of receptive fields of different sizes
("scales") and orientations. It is thus a "multi-scale" system,
presumably organised that way because important information in the image
can exist at any scale from coarse through to fine (Bonnar, Gosselin, &
Schyns, 2002; Schyns & Oliva, 1997). Considerable progress has been made
in understanding multi-scale vision, and we have reviewed several
theoretical accounts of the way this system of filters is used to create
an initial description ("primal sketch") of image structure.
Nevertheless, much remains unsolved or uncertain, and Figure 5.19
summarises some of the remaining issues in pictorial form. Panel B
illustrates a classic effect, that rendering a picture in blocks as
shown makes recognition difficult or impossible (Harmon & Julesz, 1973).
It is a trick often used by TV producers to disguise people in
"sensitive" news footage or interviews. Surprisingly, recognition seems
to be improved by blurring the image (panel C) or viewing from afar (try
it). One might guess that blurring would only make matters worse, but
both blurring and distant viewing filter out

111

the high spatial frequencies that form the block edges (panel D) and
reveal that the lower spatial frequencies (panel C) still contain
useable information that is quite similar to a blurred version of the
original (panel A). The puzzle is: why can we not access that useful
information in the blocked image (B)? One view is that conscious
perception does not have access to the individual filters at different
scales, but only to the feature representations produced from them. In
MIRAGE for example, combination across scales is early, and obligatory.
For Marr and Hildreth, the alignment of ZCs across scales (the spatial
coincidence assumption) might also lead to only the block edges being
"asserted" in the primal sketch. The alignment of phases across scales
might lead the energy model to do the same (Burr & Morrone, 1994). On
the other hand, adaptive scale selection (Elder & Zucker, 1998) does not
combine responses across scales, but is driven to select the "smallest
reliable scale" and so would (probably) also find only the block edges.
We might conclude that, one way or another, vision is biased to find
only the sharpest features in an image. If so, then panels E and F of
Figure 5.19 present quite a challenge. In E, the block edges (D) have
been added back to (C) but with their contrast reversed in sign. Now the
blurred face is seen transparently through the sharp edges (Burr &
Morrone, 1994), as if we now had separate access to coarse and fine
scales. In F, two different faces are represented in a single image at
the coarse and fine scales. Increasing viewing distance (or blurring)
makes the girl's face predominate, but it is possible to select a
viewing distance at which the two faces balance in their salience. It
then seems that one can choose which face to perceive but, as with most
ambiguous figures, it is difficult or impossible to see both at the same
time. In both E and F, then, we appear to have separate access to coarse
and fine scales, but not in panel B. Which early vision model might give
the best account of this (apparent) paradox is presently unclear, and
more experimental work is needed to disentangle the local and more
global factors that contribute to these challenging perceptual effects.

112

Illustrating some unresolved questions in multi-scale vision. See text
for details.

5. IMAGES, FILTERS AND FEATURES

ILLUSORY CONTOURS AND ORIENTATION CODING Most models of feature
detection have aimed to make explicit the location, orientation, and
contrast of lines and edges in the image. But it may also be important
for vision to describe the location of events along a contour---such as
line-ends, corners, or points of high curvature. It has long been
recognised that such local 2-D features are especially informative about
the shape of a boundary and about the occlusion of one surface by
another lying in front of it. Figures 5.20 A,B,C illustrate three
varieties of "illusory contour" that are generated by configurations of
line ends and interrupted edges. Such contours seem to be created
especially when the arrangement of line ends is consistent with the
presence of one surface occluding another. However, the term "illusory"
presumes that the only "real" contour is a luminance boundary. It is
becoming clear that vision extracts accurately the location and
orientation of a wide variety of spatial discontinuities in image
properties, that is, boundaries where there is a relatively sharp change
in colour, texture, or motion (e.g., Regan & Hamstra, 1992; Sary,
Vogels, & Orban, 1994), and so the lack of a luminance edge does not
make a border "illusory". However, such semantic issues matter less than
the questions of how contours in general are detected and represented by
visual processes. To be consistent with earlier usage, we shall continue
to refer to the visible boundaries created by occlusion cues as
"illusory" or "anomalous", while bearing in mind that from a wider
perspective they are no less "real" than other visible contours. Their
"reality" is reinforced by physiological and psychophysical findings
that point to common pathways in early vision processing "real" and
"illusory" contours. Von der Heydt and Peterhans (1989) studied the
responses of V1 and V2 cells in alert monkeys to "anomalous" contours of
different orientations (Figure 5.20 D,E). Almost no V1 cells responded
to these contours, although they responded to the line gratings from
which they

113

were constructed. In contrast, about 40% of the V2 cells sampled were
classified as "contour neurons" because they showed an
orientationspecific response both to "real" lines (Figure 5.20F) and to
anomalous contours (Figure 5.20D). The preferred orientation,
orientation bandwidth, and degree of direction selectivity tended to be
very similar for the two types of contour. The orientation of the
anomalous contour can be varied separately from the orientation of the
lines that create it (compare Figures 5.20D and E) and it was found that
orientation tuning depended mainly on the orientation of the anomalous
contour, but the response rates were typically higher when the grating
lines were at right angles to the contour. This may correspond to the
perceptual finding that illusory contours (as in Figure 5.20A) are seen
more vividly when the inducing lines intersect the illusory contour at
right angles than at other angles (Kennedy, 1978).

A model for "illusory contours" What neural mechanisms might underlie
this apparent ability of "contour neurons" to encode the orientation of
such different types of contour? Nothing in the basic model for simple
and complex cell fields would produce this behaviour, but the
"end-inhibited" hypercomplex cells described in Chapter 3 (p. 50) are
especially sensitive to truncated gratings, lines, or edges. At one or
both ends of the receptive field (which may be simple or complex) there
is an inhibitory zone that suppresses the cell's response when the
stimulus line or edge is longer than the length of the main (central)
receptive field. Heitger, Rossenthaler, von der Heydt, Peterhans, &
Kubler, (1992) pointed out that energy peaks from oriented filters
locate luminance contours, but do not isolate the 2-D key points such as
line-ends and T-junctions. They extended the energy model for complex
cells described in the last section to include the property of end
inhibition (DeAngelis et al., 1994), by applying a further stage of
spatial differentiation along the preferred orientation of the complex
filter. The 1st derivative served to model cells with one end-zone while
the 2nd derivative modelled those with two. To isolate the key points
more precisely further inhibitory

114

(A, B, C) Three varieties of illusory contour created by line ends and
truncated edges acting as occlusion cues. (D, E) "Anomalous" contours
that produced orientation-specific responses from V2 cells in monkey
cortex (von der Heydt & Peterhans, 1989). (F, G) Real and virtual
contours used by Peterhans and von der Heydt (1989). Arrows signify
back-and-forth movement. Some contour cells in V2 responded to both
types of moving contour, but not to the control stimulus (H). In G,
movement strengthens the impression of a single, vertical black object
moving in front of the horizontal black strip. (I) Periodic version of
G---the phantom grating. (J) Control version of I; no phantom is seen
when the black masking strip is parallel to the grating.

5. IMAGES, FILTERS AND FEATURES

interactions were added between cells to suppress responses to elongated
contours. By linking together several line-end-sensitive mechanisms, a
reliable means of detecting anomalous contours can be obtained.
Peterhans and von der Heydt (1989) proposed this as a model for the
detection of both real and anomalous contours by "contour neurons" in V2
(see Figure 5.21). In this model, two distinct mechanisms converge onto
a common path. The simple or complex oriented unit responds selectively
to (say) vertical lines or edges, while the group of end-stopped units
(tuned to horizontal orientation) responds well to a vertical alignment
of lineends such as that in the abutting grating stimulus of Figure
5.20D. The contour neuron thus responds in a similar
orientation-specific way to both kinds of contour and (presumably) to
their combination. Peterhans and von der Heydt (1989) also tested V2
neurons with stimuli derived from the well-known Kanisza (1976) figures
(Figure 5.20B). When patterns such as Figure 5.20G move, human observers
see especially strong illusory contours, as shown by Tynan and Sekuler

115

(1975) who studied a spatially periodic version known as the "phantom
       grating" (Figure 5.20I). The phantom grating is seen to "fill-in"
       across the black strip where there is no luminance contrast, and
       to move along with the real grating. Interestingly, the phantoms
       are not seen if the central black strip is parallel to the bars
       (Figure 5.20J). Note the absence of "key points" (line-ends,
       corners) in this case. Some 30% of V2 cells appeared to respond
       to these contours (Figure 5.20G) as well as to "real" contours
       (Figure 5.20F), while V1 cells appeared not to do so. We say
       "appeared" because the interpretation is more difficult in these
       experiments. If the conventional receptive field overlapped the
       "real" parts of the phantom figure then the responses would be
       unsurprising. The control for this was to break the continuity of
       the phantom figure, as shown in Figure 5.20H. This tends to
       eliminate the illusory contour and so a "contour neuron" should
       reduce its response in this case. Some cells did this, but others
       did so rather little, or even increased their response. Overall,
       though, the evidence in favour of a model like that of Figure
       5.21 is now substantial.

Model proposed by Peterhans and von der Heydt (1989) for detection of
real and anomalous contours by contour neurons in V2. The oriented unit
responds selectively to (say) vertical lines or edges, while the group
of end-stopped units (tuned to horizontal lines) responds well to the
vertical alignment of lineends in the abutting grating stimulus (see
Figure 5.20D). The two types of input mechanism are actually
superimposed in space. Not all V2 neurons show evidence for this dual
input.

116

VISUAL PERCEPTION

Leventhal, Wang, Schmolensky, and Zhou (1998) reported that
orientation-selective, "cueinvariant" cells were readily found in V2,
but not V1, of cats and monkeys. These cells responded in a similar way
to the orientation of contours defined by a change in luminance and
those defined by an iso-luminant change in texture size or texture
orientation. Importantly these cells showed summation of responses when
a boundary was defined by two cues (e.g., texture and luminance, or
texture and motion) rather than one. These V2 neurons appear to be an
important step towards the goal of object recognition, since they
abstract orientation while generalising across very different types of
image discontinuity. Such generalisation (cue invariance) and cue
combination are vital in the task of shape recognition and (ultimately)
object constancy. The nonlinear mechanisms needed to create this
sensitivity to texture boundaries are discussed further in Chapter 6.

Convergence of cues in orientation coding The convergence of mechanisms
that process the orientation of "real" and "illusory" contours is also
implied by studies of the tilt aftereffect (TAE). Inspection of lines or
gratings tilted (say) 20° off vertical makes a subsequent vertical
pattern seem tilted a few degrees in the opposite direction. This
classic aftereffect of adaptation (Gibson & Radner, 1937), like other
aftereffects such as contrast adaptation (p. 101), is widely used as a
tool for probing visual mechanisms. Why does the illusory tilt occur?
Suppose we adapt to a grating tilted 20° anti-clockwise from vertical,
and suppose also that the main effect of this adapting exposure is to
reduce the sensitivity of orientation-specific cells that respond to the
adapting stimulus. When we now present a vertical test grating, cells
sensitive to clockwise tilts will be more responsive than the adapted
cells sensitive to anti-clockwise tilts. Thus the distribution of
activity across cells will be biased or shifted towards the more
sensitive, unadapted cells that normally respond best to clockwise
orientations. This "distribution shift" leads to a bias in perceived
orientation towards clockwise, and this tells us that the visual system
uses some

aspect of the response distribution, perhaps the peak or the centre of
gravity, to encode contour orientation. When that is shifted by
selective adaptation, we see a tilted line or grating, even though its
orientation on the retina remains unchanged. Aftereffects of this kind
underline the general point that "seeing" is an informationcoding
operation, not a simple transfer of images from eye to brain. If one eye
is adapted and the other is tested, the TAE is still observed, showing
that adaptation is at least partly at a binocular site where the two
eyes can interact. As with the motion aftereffect (MAE, see Chapter 8),
this binocularity and orientation specificity are usually taken to imply
a cortical site for the aftereffect, since cells earlier in the visual
pathway are not binocular and not orientation-selective. In a remarkable
recent study, He and MacLeod (2001) found evidence implying that the
visual cortex can even register the orientation of, and produce a tilt
aftereffect from, gratings that are too fine to be visible. Using laser
interference fringes (see Chapter 2, p. 36), they produced very fine
gratings on the retina and then determined the finest grating (highest
spatial frequency) at which the observers could still see the difference
between horizontal and vertical. This limit was about 55 c/deg. They
then found that adapting to even finer gratings whose orientation was
undetectable (around 60--65 c/deg) still produced orientation-specific
adaptation and a clear tilt aftereffect on visible gratings (at 45 c/
deg). Invisible gratings can produce perceptual aftereffects. But how?
He and MacLeod (2001) suggest that oriented mechanisms early in the
visual cortex are (just about) capable of resolving these very fine
gratings, and that the limits on conscious visual resolution must
therefore lie after these early cortical stages. The transfer of an
aftereffect from one condition to another (e.g., between the eyes, see
above) can also be exploited to test for the convergence of cues or
pathways in orientation coding. What would happen if we adapted to a
"real" contour, and then tested with an "illusory" contour, or
vice-versa? A failure of the TAE to transfer from one kind to the other
would support independent processing of the two cues, while complete
trans-

5. IMAGES, FILTERS AND FEATURES

fer would imply common processing. In fact, several studies have
confirmed the transfer of TAE between real and illusory contours, and
viceversa, implying some form of common processing (Berkley, DeBruyn, &
Orban, 1994; Paradiso, Shimojo, & Nakayama, 1989; Smith & Over, 1975,
1977). Moreover, there was almost 100% interocular transfer of the TAE
for an illusory contour (like Figure 5.20D) while there was only partial
transfer for a real contour. This suggests that the illusory contour
arises at a higher cortical site that is more completely driven by
binocular input, and is indicative of V2 rather than V1 (Paradiso et
al., 1989). A similar conclusion was drawn from the effects of binocular
rivalry on the TAE. Rivalry occurs when two very different images are
presented to the two eyes, such that normal binocular fusion (see
Chapter 7) is impossible. Fragmented parts of each image may be seen, or
a strong image in one eye may partly or completely suppress a weaker one
in the other eye. If an adaptation effect is reduced by rivalry
suppression then we may conclude that its site is at or after the site
of rivalry. Van der Zwan and Wenderoth (1994) found that the presence of
binocular rivalry during the period of adaptation suppressed the TAE for
an illusory contour, but not for a real contour. Several earlier studies
had also found that the aftereffects of contrast adaptation (thought to
be in V1) were unaffected by rivalry (e.g., Blake & Overton, 1979).
These findings suggest that illusory contours arise at a higher site
than the first site of adaptation to real contours (V1), and this is
consistent with the physiological evidence (see above) that V2 can
encode illusory contours and texture boundaries while V1 cannot. Most
importantly, the transfer of aftereffect between different types of
contour implies some common processing of different edge cues at the
higher site. In fact, this convergence of information from different
contour cues may be even more extensive, involving luminance contours,
line-ends, colour, and motion. Berkley et al., (1994) found that
adapting to contours defined by real lines or by line-ends or by a
reversal in motion direction could in each case produce a TAE on test
con-

117

tours of all three types. This suggests the sort of convergence of cues
sketched in Figure 5.22. Furthermore, Flanagan, Cavanagh, and Favreau
(1990) examined the TAE using gratings defined by colour contrast (e.g.,
red--green or blue-- yellow) or luminance contrast. Adapting to pairs of
luminance and colour gratings tilted ±15° from vertical in opposite
directions revealed aftereffects specific to luminance or colour
contrast. That is, after such dual adaptation a vertical test grating
seemed tilted one way if it was defined by luminance contrast, but the
opposite way if defined by colour contrast. This suggests that there are
different oriented units specific to colour and to luminance contrast.
Conversely, adapting to single gratings showed considerable transfer of
the TAE between colour and luminance conditions. These two results are
not contradictory, and suggest two stages of orientation-selective
mechanism: at the first stage (Figure 5.22) units are specific to colour
or luminance contrast and mediate the colour- or luminance-specific TAE,
while the second stage receives converging input from colour and
luminance units and explains the generalisation of the TAE between
colour and luminance conditions (Flanagan et al., 1990). In short, the
concept of converging cues (Figure 5.22) unifies quite a wide range of
psychophysical and physiological findings on the encoding of oriented
features in images. We shall meet the idea again when we consider depth
cues in Chapter 7.

SUMMARY We began this chapter by considering the primal sketch theory of
Marr (1982). The core of this theory is that a description of local
intensity changes or features in the image forms the gateway to later
descriptions of image regions, object surfaces, and object shapes. Some
alternative approaches, such as MIRAGE or the energy model, share the
same broad aims but differ at the level of algorithm and implementation.
All these models filter the retinal image through receptive fields that
are sensitive to luminance contrast,

118

VISUAL PERCEPTION

Summary of orientation coding derived from physiological and
psychophysical findings. Both types of evidence suggest that cuespecific
analysis may be more closely associated with area V1, while the
convergence of cues occurs at a higher, more completely binocular site
(V2 or beyond).

and then employ nonlinear operations to derive features from the filter
outputs. All agree on the importance of multiple scales (sizes) of
filter, but differ in their use of this information. Detailed study of
the filters themselves has shown that at small scales (high spatial
frequency) the filters are much more tightly tuned for spatial frequency
(and orientation) than any of these models would require, and this
leaves open many questions about their role. One possibility is that the
whole set of filters, tuned jointly to different orientations and
spatial scales, delivers a rather general but compact coding of local
patches of the image. Such coding schemes are used widely in digital
applications for efficient coding, storage, and transmission of image
data (using the JPEG and

MPEG formats). Perhaps the functional architecture of V1 serves the same
very general purpose, while the encoding of features is done mainly at
later stages. We saw evidence that V2 begins to abstract contour
orientation while generalising across a variety of "cues" or types of
image discontinuity. We also saw that gratings too fine to be visible
may be registered, perhaps in V1, but not transferred to stages that
result in conscious awareness. Parallel processing of luminance
contrast, colour, and motion might be better thought of as occurring
before the feature representation rather than after it as in Marr's
(1982) modular scheme. The complexity of known neural pathways
(cf. Chapter 3) allows for many possibilities.

6 Perceptual Organisation

of human visual perception during the late 19th and early 20th centuries
was dominated by associationism. It was assumed that perception could be
analysed in terms of its component sensations, and that complex ideas
were the result of associating together simpler ones. However, as the
Gestalt psychologists pointed out, an analysis of perception into
discrete sensations overlooks some important aspects of form and
structure. Each of the arrangements shown in Figure 6.1 (top row)
possesses the quality of "squareness" despite being composed of quite
different elements. The two on the bottom row are not generally seen as
squares, despite containing all the elements of the square. A tune is
recognisable despite being played in a different key or at a different
speed. The spatial and temporal relationships between elements are as
important as the absolute size, location, or nature of the elements
themselves, and a sensation-based account of perception fails to capture
this. Even Wundt (1896) recognised that a simple structuralist analysis
failed to capture certain perceptual phenomena: "A compound clang is
more in its ideational and affective attributes than merely a sum of
single tones" (Wundt, 1896, trans. 1907, p. 368). But it was the Gestalt
psychologists, notably Wertheimer (1923), Köhler (1947), and Koffka
(1935), with whom

In Chapter 5 we considered how the "raw" primal sketch---description of
edge and line segments, terminations, and other key features---may be
derived from an array of intensities in the retinal image. In this
chapter we turn to consider how the primal sketch is completed by
organising such low-level descriptions into larger perceptual "chunks".
When we view the world we do not see a collection of edges and
blobs---unless we adopt a very analytical perceptual attitude---but see
instead an organised world of surfaces and objects. How is such
perceptual organisation achieved? How do we know which parts of the
visual information reaching our sensory apparatus belong together? These
are the questions addressed in this chapter. The first part of the
chapter concentrates on human perception, since it was through the study
of this that many of the principles of perceptual organisation became
established. We return to the broader perspective of animal vision when
we consider how such perceptual principles may be exploited in natural
camouflage and advertisement. In the final part of the chapter we
describe some of the principles derived from work on artificial
intelligence and we review recent developments in understanding the
neurobiological basis of perceptual organisation. As we discussed in
Chapter 4, the psychology

119

120

VISUAL PERCEPTION

The three forms on the top row are seen as squares, even though they are
composed of different elements. The two below are not generally seen as
squares, despite containing all the elements of the square. On the left
we tend to see a "diamond" balancing precariously on one point.

the catch-phrase "the whole is greater than the sum of its parts" became
identified. We will first describe the Gestalt ideas about perceptual
organisation, and then consider more recent accounts.

used the face/vase picture (Figure 6.2) to illustrate this. The picture
can be seen either as a pair of black faces in profile, or as a white
vase, but it is

AMBIGUOUS PICTURES The world that we view appears to be composed of
discrete objects of various sizes, which are seen against a background
of textured surfaces. We usually have no difficulty in seeing the
boundaries of objects, unless these are successfully camouflaged (see
later), and there is generally no doubt about which areas are "figures"
and which comprise the "ground". However, it is possible to construct
pictures in which there is ambiguity about which region is "figure" and
which "ground". Edgar Rubin, one of the Gestalt psychologists,

This picture, devised by E. Rubin in 1915, can be seen either as a pair
of black faces in silhouette, or as a white vase.

6. PERCEPTUAL ORGANISATION

impossible to maintain simultaneously the perception of both the faces
and the vase. The contour dividing the black and white regions of the
picture appears to have a one-sided function. It "belongs" to whichever
region is perceived as figure. People viewing this picture usually find
that their perception of it shifts from one interpretation to the other,
sometimes quite spontaneously. The artist M.C. Escher exploited this
principle of perceptual reversibility when he produced etchings in which
there is figure/ground ambiguity (see Figure 6.3). It is also possible
to construct pictures so that the internal organisation of a particular
figure is ambiguous. Jastrow's duck--rabbit picture (Figure 6.4a) may be
seen as a duck (beak at the left), or a rabbit (ears at the left), but
not both simultaneously. Even a figure as simple as a triangle turns out
to be perceptually ambiguous, as Figure

121

6.4b shows (Attneave, 1971). The triangles appear to "point" in any one
of the three possible directions, and when they appear to change
direction they all change together, implying some spatially extended
organising process that is applied to all the individual triangles
simultaneously (Palmer, 1992). Some abstract and "op"-art may be
perplexing to view because no stable organisation is apparent (see
Figure 6.5). The perception of such ambiguous displays is interesting in
its own right, and psychologists have investigated the factors
influencing which organisation of an ambiguous display will be
preferred, and the factors determining perceptual reversals (for example
see Attneave, 1971; Hochberg, 1950; Pheiffer, Eure, & Hamilton, 1956).
In all these examples, the input "data" remain the same, while the
interpretation varies. Higher levels of perceptual interpretation appear
to be

M.C. Escher's "Circle Limit IV" © 2003 Cordon Art B.V., Baarn, Holland.
All rights reserved.

122

VISUAL PERCEPTION

(a) Duck or rabbit? This ambiguous picture was introduced to
    psychologists by J. Jastrow in 1900. (b) Triangles as ambiguous
    figures. Note how the whole group of triangles appears to point in
    one, then another, of the three possible directions. After Attneave
    (1971).

providing alternative descriptions of the output of lower levels of
analysis. The 2-D features may remain the same, but the surfaces and
object parts derived from them change radically. Rock (1973) argued that
the perceptual description was built around the assignment of both a
major axis to the figure and a direction to that axis, thus defining
object parts such as "base" and "top" or "front" and "back". These are
object-level descriptors, not image or surface features. In the
duck--rabbit, a major axis is horizontal in both cases, but its implicit
direction switches: the right-hand side is the front of the rabbit's
head, but the back of the duck's. Likewise, in the triangles, there are
three axes of symmetry that may be assigned as the object axis, and as a
result the parts that are "base" and "top" of the tri-

"Supernovae" 1959--61 by Victor Vasarely. Copyright © ADAGP, Paris and
DACS, London 2003. Used with permission.

angle are also reassigned with every switch of axis. However, these
ambiguous pictures have been cleverly constructed, and our perception of
them is not necessarily typical of normal processing. Ambiguity
generally does not arise in the real world, nor in most pictures. Rather
than having constantly shifting interpretations, we usually see a stable
and organised world. For example, viewing Figure 6.6a in isolation, most
people would report seeing a hexagon, while those viewing Figure 6.6b
report seeing a picture of a three-dimensional cube, even though Figure
6.6a is a legitimate view of a cube, viewed corner on. Figure 6.7 is
seen as a set of overlapping circles, rather than as one circle touching
two adjoining shapes that have "bites" taken out of them. Why, given
these possible alternative perceptions, do we see these pictures in
these ways?

6. PERCEPTUAL ORGANISATION

123

The form at (a) looks like a hexagon, while that at (b) looks like a
cube. Of course (a) is also a legitimate view of a cube.

Most people would see this as a set of overlapping circles, although two
of the shapes might have "bites" taken out of them.

The Gestalt psychologists formulated a number of principles of
perceptual organisation to describe how certain perceptions are more
likely to occur than others. Some of their principles were primarily to
do with the grouping of subregions of figures, and others were more
concerned with the segregation of figure from ground. However, since
subregions of a figure need to be grouped in order for a larger region
to be seen as "belonging together" as figure, we will discuss all these
principles together.

ity of the elements within it. Things that are close together are
grouped together. In Figure 6.8a the perception is of columns, because
the horizontal spacing of the dots is greater than their vertical
spacing. In Figure 6.8b we see rows, because the horizontal spacing of
the dots is the smaller, and Figure 6.8c is ambiguous; the dots are
equally spaced in both directions. Proximity in depth is a powerful
organising factor. The central square in a Julesz random-dot stereogram
(see Chapter 7, p. 177) is not visible until the two halves of the
stereo pair are viewed in a stereoscope. Dots with the same disparity
values are then grouped together and the square is seen as a distinct
figure floating above its background.

Proximity

Similarity

One of the most important factors determining the perceptual
organisation of a scene is proxim-

Things that look "similar" are grouped together. The examples shown at
the top of Figure 6.16

GESTALT LAWS OF ORGANISATION

124

VISUAL PERCEPTION

The dots in (a) form columns because they are nearer vertically than
horizontally. At (b) we see rows, the dots here are nearer horizontally;
(c) is ambiguous, the dots are equally spaced in both directions.

(p. 129) appear to consist of two distinct regions, with a boundary
between them. The elements on one side of this boundary have a different
orientation from those on the other. In Figure 6.9 the perception is of
columns, even though the proximity information suggests rows,
illustrating that similarity may override proximity information. The
question of how similar items must be in order to be grouped together is
an empirical one to which we will return.

Common fate Things that appear to move together are grouped
together---think of a flock of birds or a school of fish. A camouflaged
animal will remain wellhidden only if it remains stationary. As soon as
it moves it is easier to see. Gibson, Gibson, Smith,

and Flock (1959) illustrated grouping by common fate with a simple
demonstration. They sprinkled powder on two sheets of glass, and
projected an image of the powder onto a screen. While the sheets were
held still a single collection of powder was seen. As soon as one sheet
was moved across the other, viewers saw the powder segregated into two
independent collections, by virtue of the movement in the display. A
further example is provided by random-dot kinematograms (Braddick,
1974), in which a central region of texture is revealed through the
apparent motions of the elements it contains. Johansson (1973) has
produced an even more dramatic demonstration of the power of movement to
confer organisation. He attached lights to the joints of a darkly
clothed actor and filmed him as he moved in a dark room, so that only
the lights were visible. When the actor was at rest, observers reported
perceiving a disorganised collection of points. As soon as the actor
walked, their perception was that of a moving human figure, whose
actions, gait, and even gender could be discerned from the pattern of
moving points. Johannson's demonstrations suggest that "common fate"
involves much more than simply grouping together elements that have a
common speed and direction, and we shall return to discuss the
perceptual organisation of such complex displays in Chapter 13.

Good continuation

This picture is seen as columns. Similarity in brightness of the dots
overrides proximity.

In a figure such as Figure 6.10, one tends to perceive two smooth curves
that cross at point X, rather than perceiving two irregular V-shaped
forms touching at X. The Gestaltists argued that perceptual organisation
will tend to preserve

6. PERCEPTUAL ORGANISATION

125

This is seen as two smooth lines that cross at X, rather than as two
V-shapes touching at X.

smooth continuity rather than yielding abrupt changes. Quite dissimilar
objects may be perceived as "belonging together" by virtue of a
combination of proximity and good continuity (see Figure 6.11). Good
continuation may be considered the spatial analogue of common fate.

Closure Of several geometrically possible perceptual organisations, that
one will be seen which produces a "closed" rather than an "open" figure.
Thus the first two patterns in Figure 6.1 (top line) are seen as squares
rather than as crosses, because the former are closed. The Gestaltists
suggested that the stellar constellation "the plough" might be seen as a
plough because of closure and good continuation.

Relative size, surroundedness, orientation, and symmetry Other things
being equal, the smaller of two areas will be seen as figure against a
larger background.

Thus Figure 6.12a will tend to be perceived as a black propellor shape
against a white background since the black area is the smaller. This
effect is enhanced if the white area actually surrounds the black as in
Figure 6.12b, since surrounded areas tend to be seen as figures.
However, if we orient the figure so that the white area is arranged
around the horizontal and vertical axes then it is easier to see this
larger area as a figure (Figure 6.12c). There seems to be a preference
for horizontally or vertically oriented regions to be seen as figures.
Also note that both these sets of patterns are symmetrical. Symmetry is
a powerful perceptual property, and may be more salient perceptually
than nonreflected repetition (Bruce & Morgan, 1975). Examples of
symmetry and repetition are shown in Figure 6.13. Symmetrical areas will
tend to be perceived as figures, against asymmetrical backgrounds.
Figure 6.14 shows how relative size, orientation, symmetry, and
surroundedness can all operate together so that it is difficult if not
impossible to see anything other

Quite dissimilar shapes may be grouped together through a combination of
proximity and good continuation.

126

VISUAL PERCEPTION

The preferred perception of (a) is a black propeller on a white
background. This preference is enhanced if the white area surrounds the
black as at (b). If the orientation of the forms is altered, so that the
white area is oriented around the horizontal and vertical axes, as at
(c), then it is easier to see the larger white area as a figure.

At (a) one form is repeated without reflection around a vertical axis.
This arrangement is not as perceptually salient as the arrangement shown
at (b), where repetition with reflection around the vertical axis
produces bilateral symmetry.

This picture clearly shows black shapes on a white background. The black
shapes are vertically oriented, symmetrical, small (relative to the
background), and surrounded by the background.

6. PERCEPTUAL ORGANISATION

than the black areas as the figures in this picture. The reader will
note the perceptual stability of this picture compared with the
ambiguity of Figure 6.2, where the relative sizes, surroundedness, and
symmetries in the display favour neither the "faces" nor the "vase"
particularly strongly.

The law of Prägnanz For the Gestalt psychologists, many of these laws
were held to be manifestations of the Law of Prägnanz, introduced by
Wertheimer. Koffka (1935, p. 138) describes the law: "Of several
geometrically possible organisations that one will actually occur which
possesses the best, simplest and most stable shape." Thus an
organisation of four dots arranged as though they were at the corners of
a square (Figure 6.1, left) will be seen as a "square" since this is a
"better" arrangement than, say, a cross or a triangle plus an extra dot.
The square is a closed, symmetrical form, which the Gestaltists
maintained was the most stable. While the Gestaltists accepted that
familiarity with objects in the world, and "objective set", might
influence perceptual organisation, they rejected an explanation solely
in these terms. A major determinant of perceptual organisation for them
was couched in terms of certain "field forces", which they thought
operated within the brain. The Gestaltists maintained a Doctrine of
Isomorphism, according to which there is, underlying every sensory
experience, a brain event that is structurally similar to that
experience. Thus when one perceives a circle, a "circular trace" is
established, and so on. Field forces were held to operate to make the
outcome as stable as possible, just as the forces operating on a soap
bubble are such that its most stable state is a sphere. Unfortunately,
there has been no evidence provided for such field forces, and the
physiological theory of the Gestalts has fallen by the wayside, leaving
us with a set of descriptive principles, but without a model of
perceptual processing. Indeed some of their "Laws" of perceptual
organisation today sound vague and inadequate. What is meant by a "good"
or a "simple" shape, for example? Later workers attempted

127

to formalise at least some of the Gestalt perceptual principles, and to
validate them through experiments.

Experimental approaches to perceptual organisation Hochberg and Brooks
(1960) tried to provide a more objective criterion for the notion of
"goodness" of shape by presenting subjects with line drawings (Figure
6.15) and asking them to rate the apparent tridimensionality in these
figures. They argued that as the complexity of the figures as
two-dimensional line drawings increased, so there should be a tendency
for the figures to be perceived as though they were three-dimensional
objects. They made a number of measurements

Examples of the forms used by Hochberg and Brooks (1960). In each of
rows 1--4, the figure at the right is most likely to be seen as
three-dimensional. From Julian E. Hochberg, Perception, 2nd edn.,
p. 142. Reprinted by permission of Pearson Education, Inc., Upper Saddle
River, NJ.

VISUAL PERCEPTION

on the figures and looked for those that correlated well with perceived
tridimensionality. The best measure was the number of angles in the
figure. This measure seems to represent "complexity". The more angles
the figure contains, the more complex it is in two dimensions, and the
more likely it is to be perceived as a representation of a "simpler",
three-dimensional object. A second measure that correlated well was the
number of differently sized angles. This reflects the asymmetry in the
2-D figure, since a figure in which many of the angles are of the same
size is more likely to be symmetrical than one in which many differently
sized angles are present. A final measure was the number of continuous
lines. This reflects the discontinuity present, since the more
continuous lines there are, the more discontinuities must be present
between each. Thus the more complex, asymmetrical and discontinuous the
2-D pattern, the more likely it was to be perceived as the projection of
a 3-D figure. Hochberg and Brooks then applied their measures to a set
of new figures and found they correlated well with the perceived
tridimensionality in these. Thus it is possible to express Gestalt ideas
such as "good shape" more precisely. In similar vein we now consider
attempts to tackle the problem of grouping by similarity. How similar
must items be before they are grouped together? It is unlikely that they
must be identical, since no camouflage can ever perfectly match its
surroundings, yet we know that camouflage can be remarkably successful.
But if identity is not required, what are the important variables that
determine grouping by similarity? This has been investigated by seeing
how easily two different regions of a pattern, or more naturally
textured image, segregate perceptually from each other. The logic of
this is that the more the elements in two different regions cohere with
one another, by virtue of the perceptual similarity that exists between
them, the less visible will be the boundary between these two regions.
Olson and Attneave (1970) required observers to indicate where the "odd"
quadrant lay within a circular display of simple pattern elements (see
Figure 6.16). They found that the quadrant was

most easily spotted if the elements within it differed in slope from
those of the rest of the display (e.g., \< V) and was most difficult to
find if the elements differed in configuration, but not in the slopes of
their component parts (e.g., \> \<). Similar conclusions were reached by
Beck (1972) who asked his subjects to count elements of one type (e.g.,
\<) which were distributed randomly within a display containing elements
of a different type (e.g., \>). Again he reasoned that the more the odd
elements stood out from the background elements, and grouped with each
other rather than with the background, the easier they would be to
isolate and count. Like Olson and Attneave, Beck found that slope
differences led to faster counting than configurational differences.
Such findings are interesting because they demonstrate that the
variables that influence grouping by similarity are not necessarily the
same as those that would influence the judged conceptual similarity of
the same elements viewed individually by humans. Thus L and L might be
considered more similar (both letter L, with one example tilted) when
viewed as a single pair, than would L and . However, when large numbers
of these elements are combined, it is the difference in the common
orientations of two populations that is perceptually more salient than
the difference between the conceptual identity of the individual
members. Julesz (1965) stressed that in similarity grouping we are
looking at spontaneous, preattentive visual processing, which precedes
the identification of patterns and objects, and which is quite different
from deliberate scrutiny. In some textures it is possible to discern
that an odd region is present by carefully examining and comparing
individual pattern elements, just as one can find a camouflaged animal
by careful inspection. Such processes of scrutiny, however, are at a
much higher level than the grouping mechanisms we are discussing here.
These "preattentive" grouping mechanisms appear to be implemented at a
relatively early stage of processing; indeed, De Yoe, Knierem, Sagi,
Julesz, and Van Essen (1986) observed cells in areas V1 and V2 of the
monkey cortex that respond if texture elements differ in orientation
between the centre and surround of their L

128

6. PERCEPTUAL ORGANISATION

129

Some of the displays used by Olson and Attneave (1970) to investigate
grouping by similarity. In the displays marked 1, 2, and 3, the lines in
one region are of a different orientation to the rest, and the odd
region is easy to spot. In display 4, odd elements are curved, and the
odd region is reasonably evident. In displays 5 and 6, the
configurations, but not the slopes, of the elements differ from one
region to the next. Here it is much harder to spot the odd quadrant.
Reprinted from Olson and Attneave (1970). Copyright © 1970 by the Board
of Trustees of the University of Illinois.

receptive fields. We return to this evidence in more detail later in the
chapter. Thus we have seen how work in the area of perceptual grouping
attempted to quantify the Gestalt principles of "good shape" and
"similarity", but only made use of artificial patterns and textures. Can
these laws of perceptual organisation be demonstrated in more natural
settings?

CONCEALMENT AND ADVERTISEMENT Gestalt principles can give some insights
into the ways in which the colouration and shapes of animals can help to
conceal or to reveal them. The study of camouflage in nature also
provides us with a way of exploring how perceptual grouping

130

VISUAL PERCEPTION

in other species may be similar to, or different from, our own. Animals
that remain concealed from predators have a greater chance of surviving
and reproducing, and a predator also stands a better chance of getting
food if it is not easily visible to its own prey. In order to remain
concealed an animal should not stand out as a figure against its
background, but needs instead to blend with it. On the other hand, for
the purposes of breeding or defending territory, animals may need to be
conspicuous to potential mates or competitors. Under such circumstances
the animal may need to stand out as a distinctive figure against its
surroundings. Whether an animal will be camouflaged or conspicuous
depends on its behavioural needs and the habitat in which it lives. Some
animals may need to remain hidden for much of the time but have the
potential of occasionally giving a highly distinctive display or warning
sign. This can be achieved by temporarily revealing distinctive surface
features that are normally hidden beneath wings or tails, or by changing
skin colour, coat, or plumage to meet changing circumstances. Thus to
understand why a creature is coloured in a particular way we need to
consider ecological factors as well as perceptual ones.

Merging and contrasting An animal that needs to be hidden should avoid
standing out as figure against its background. An animal that needs to
be seen should stand out as a figure distinctly from its background. The
way in which this is achieved will depend on the nature of the
background habitat. An animal can blend with a uniform background by
being of similar average colour and brightness. Many species are
coloured fairly simply to match the habitat on which they live---for
example tropical tree snakes are green and polar bears are white. Some
animals show seasonal variation in their colouration to match their
changing habitats. The Arctic fox is white in the winter and brown
during summer. In other species, different animals may be coloured
differently in differing environments. The peppered moth is darker in
urban than in rural areas for example. A more radical way to prevent
standing out as

a figure, and one that is more effective against non-uniform habitats,
is to break up the perceptual cohesiveness of the surface of the body by
disruptive colouration. Dissimilar surface areas are less likely to be
grouped together as a single figure. If some of the patches of surface
colour are in turn similar to elements in the background this leads the
animal's surface to be grouped together with its habitat. Disruptive
colouration along with background picturing is an efficient method of
camouflage where the habitat contains different coloured and shaped
elements and variations in light and shade. The tree frog shown in
Figure 6.17 is a good example of a creature whose surface markings
incorporate these features. An artificial example is given by the green,
brown, and black mottled pattern painted on tanks and combat jackets to
achieve camouflage for human military purposes. In these examples of
camouflage we see exploitation of the principle that grouping by
similarity (with the background habitat) may override grouping by
proximity (of adjacent parts of the animal's surface). Effective
disruptive colouration must match the background in terms of average
brightness, colour, and density of markings. Where the background
texture elements have some intrinsic orientation, as in Figure 6.17,
surface markings must be similarly oriented. Conversely, in order to be
conspicuous, an animal needs to have high brightness or colour contrast
with its background. The all-black crow is a distinctive form against
grassland or stubble fields. In addition, the outline of an animal's
body or of significant signs or structures can be enhanced by outlining.
Many butterflies have contrasting borders around the edges of their
wings, and some fish may have black edges to emphasise their distinctive
fin shapes. Since flat structures are less conspicuous as figures than
solid ones (see below), and solid bodies have no single contour line
(the particular contour that will correspond to the animal's outline in
an image of it will depend on vantage point), it makes sense that only
"flat" structures such as butterfly wings or fish fins should be
outlined in this way (Hailman 1977). More frequently, we find local
outlining to emphasise the shape of a patch that serves as a

6. PERCEPTUAL ORGANISATION

131

This African frog is well camouflaged when seen against the bark of the
tree on which it habitually rests. Its asymmetrical markings show
disruptive colouration along with background picturing. However, the
shadow cast by the frog's head on the bark could reveal it. Photograph
by P. Ward. Copyright © P. Ward/ Bruce Coleman Ltd. Used by permission.

courtship or warning signal. Such patches are often outlined in white or
black in fish and birds.

Symmetry and regularity Symmetrical forms stand out more readily as
figures against their backgrounds than do asymmetrical ones. Since
animals are bilaterally symmetric, those that need to be hidden must
reduce their apparent symmetry. Camouflaged snakes may rest in irregular
coils so that the symmetry of surface markings is not evident. The frog
in Figure 6.17 has asymmetrical markings which make it less visible as
separate figure. Conversely, symmetry and repetition may be employed to
enhance an animal's outline or internal features. Butterflies that have
dramatic markings on their wings become highly salient symmetrical
figures when their wings are spread. Signal patches can also be made
conspicuous by virtue of their shape. Regular geometric forms such as
circles, squares, and triangles are perceptually salient figures, and
they are also rare in the habitats of animals, so that surface markings
that are geometrically regular will be additionally distinctive due to
their dissimilarity to background elements. Hailman (1977) suggests that

the common use of circular signal patches may be because they are
regular, and hence unusual, rather than functioning as eye mimics.
Triangular patterns can be seen on the breeding plumage of some male
birds such as peacocks, and rectangular patches are found on the wings
of some ducks. In many birds and fish we find another kind of regularity
in the form of repeated markings---the series of tail spots on cuckoos
and head stripes on sparrows for example.

Countershading and reverse countershading A creature might be well
camouflaged in terms of matching the colours and contrasts in its
background, but could still be apparent as a distinct figure by virtue
of its solidity. There will be a distinct depth difference between the
upper part of a cylindrical body and the surface upon which it rests,
which may be revealed to an observer by stereopsis or motion parallax
(see Chapters 7 and 8). This will lead to grouping by proximity in depth
and by common fate, just as central regions of texture can be revealed
in random-dot stereograms and random-dot kinematograms (see Chapters 7
and 8). Many birds use sideways movements of their heads to reveal prey
by

132

VISUAL PERCEPTION

motion parallax. A solution to this problem is to be as flat as
possible, either behaviourally by crouching, or structurally by becoming
flat during the course of evolution. Moths and flatfish, by virtue of
their flat shapes, are at minimal depth differences from the surfaces on
which they rest. Crouching also reduces the likelihood that an animal
will be revealed by the shadow it casts. The head of the frog in Figure
6.17 casts such a shadow on the bark. Creatures that inhabit
environments where there is a strong light source have the additional
problem of unequal illumination of their surfaces, leading to
self-shading, which again will tend to reveal. This can be compensated
for by countershading. A countershaded animal has its darkest surface
areas where the most light strikes its body, and is lighter where less
light is incident (Thayer, 1918). The zebra's stripes may in part serve
a countershading function. The black stripes are at their broadest (and
hence the coat on average is at its darkest) where the body receives
most light. The clearest examples of countershading are found among the
fish, who often have dark dorsal and light ventral regions. This means
that they are relatively concealed from airborne predators where their
darker backs will be seen against the murk of the water, and also
concealed when viewed by a predator swimming beneath them, as their
light undersides are now seen against the brighter sky above. The
countershading evident in an animal can usually be explained in terms of
the direction of the habitual light source and the shape of its body.
Caterpillars that live on the undersides of leaves have "reverse"
coutershading, with their undersides, which receive the most light,
darker than their backs. If appropriate countershading can serve to
conceal an animal then reverse countershading could act to reveal it.
There are some animals, who lead their lives in upright posture, who are
lighter dorsally and darker ventrally. In the male bobolink (a kind of
blackbird) reverse countershaded plumage, in which the head and back are
white, is adopted for the breeding season where the bird needs to be
distinctive, but discarded

during winter when the bird's plumage is darker dorsally (Hailman,
1977).

Immobility and camouﬂage However well concealed a stationary animal may
be, grouping by "common fate" would tend to reveal it if it moved. It
therefore benefits a camouflaged animal if it can remain still for a
large proportion of the time. In many species, while the adults may be
brightly coloured for courtship or aggressive purposes, the young may
have quite different plumage or coats, which merge with their
backgrounds. Whether the young of a species are camouflaged or not will
depend on the habitat in which the nest or den is sited, and on the
behaviour of both young and parents. For example, if the nest is on open
ground, and both parents leave it to forage, then camouflage of the
young is more important. However, as Cott (1940) pointed out, while
immobility is advantageous to concealment, it is not essential. A green
tennis ball is harder to follow than a white one on a grass court. Thus
even if an animal is active, it will be harder to spot or track if it
merges with its background. Some creatures have markings that appear to
make it harder to track their movement. Many snakes that flee in defence
(Jackson, Ingram, & Campbell, 1976), and some fish, have longitudinal
stripes which may deceive observers since they appear to remain still as
the animal moves forward. Of course movement is one of the easiest ways
for an animal to reveal itself when it needs to be conspicuous. Some
make use of temporal redundancy by making repetitive or stereotyped
movements of their bodies in their displays, and others repeatedly flash
signal patches beneath their tails or wings.

PERCEPTUAL ORGANISATION IN OTHER SPECIES These examples of animal
colouration thus illustrate how the Gestalt laws may be useful to help
understand camouflage and concealment principles, at least when assessed
by human vision.

6. PERCEPTUAL ORGANISATION

However, a successful camouflage for a particular species is not
necessarily that which prevents its detection by a human visual system.
It is the properties of the predator's, the prey's, and the
conspecific's visual systems that are important. Some species may not
appear well hidden to us, because their colours are different from those
of their habitats. However, provided they need to be hidden from
colour-blind species, only the brightness levels are important.
Conversely, crab spiders, which match the flowers on which they live,
may be well concealed to our eyes and to the eyes of many of their
predators, but they may be detectable to any insect prey, which have
good sensitivity to ultra-violet radiation (Eisner, Silberglied,
Aneshansley, Carrel, & Howland, 1969). How can we find out whether an
animal's colouration is having its apparent (to our eyes) effect of
hiding an animal or making it conspicuous? It is possible to examine the
accuracy of our own perceptual intuitions about the relative degrees of
concealment attained by camouflaged animals by observing the "success"
that different surface markings confer to an animal in terms of its
survival. This can be done through natural observation or through
experiment. For example, a radical change in the predominant colouration
of the peppered moth has been observed in areas where industrial
pollution is present. At one time, darker members of the peppered moth
species were rare. Over the last 200 years or so, in areas where
buildings and trees are polluted with soot and grime, the predominant
colouration in the moths has changed. Darker members are much more
frequent than lighter ones, while in rural areas the lighter moths are
still common. This suggests that the avian predators that feed on such
moths find light moths distinctive on dark backgrounds in the same way
that we do. In industrial areas the gene for darker colouration has
conferred an advantage on those possessing it, whose chances of
surviving and reproducing have therefore been enhanced (Kettlewell,
1973). As well as such "natural" experiments, it is possible to conduct
controlled experiments in which members of a prey species are placed
against different backgrounds and then exposed to predators. The success
of a particular camou-

133

flage can be assessed in terms of the number of prey that survive!
Sumner (1934), for example, reared mosquito fish in differently coloured
tanks. These are fish that, in common with many others, adjust their
colours to tone in with their surroundings. After 7 to 8 weeks those
fish reared in a black tank were very dark, while those raised in a
white tank were a much paler buff or grey. Equal numbers of the "black"
and "white" fish were transferred into experimental tanks that were
painted black or pale grey, and exposed to the Galapagos penguin as
predator. Of the fish that were appropriately colour-adapted 32% were
eaten, as compared with 68% of those that were inappropriately
colour-adapted. Thus the Galapagos penguin seems to find fish of high
contrast to their background more easily than those of low contrast,
again in agreement with our own perceptions. More dramatic experiments
can involve artificially colouring the prey species before exposing them
to predation (e.g., Croze, 1970). Experiments such as these are not
always ethically acceptable (most people would be unhappy if the prey
used in such studies were mammals or birds rather than fish or insects).
A further experimental way of assessing degree of concealment without
the sacrifice of too many animals was illustrated by Pietrewicz and
Kamil (1977). They conducted operant conditioning experiments in which
blue jays were trained to detect moths in colour slides. Interestingly
they showed that the birds were sensitive to the orientation of the
moths (whether their heads were pointing up, down, or horizontally), as
well as to the degree of visual similarity existing between the moth's
markings and those of the bark against which it was photographed. Only
the latter aspect is noticeable to us. Similarly, Dittrich, Gilbert,
Green, McGregor, and Grewcock (1993) trained pigeons to respond to
photographs of wasps and then tested their transfer to photographs of
various hoverfly species with black and yellow striped markings. The
pigeons' ranking of hoverflies in terms of their similarity to wasps was
different from that made by humans. Observing the "success" of various
surface markings is not, of course, the only way to study perceptual
organisation in other species. Hertz

134

VISUAL PERCEPTION

(1928, 1929) for example describes some delightful experiments with jays
and bees in which she investigated aspects of their figural perception
directly as they searched for food from arrangements of objects. She
concluded that for the jay birds (though not for the bees), perceptual
organisation was very similar to our own. To appreciate fully the
adaptive significance of animal colouration requires continued research
along these lines.

WHY DO THE GESTALT LAWS WORK? We have shown that many of the Gestalt
laws are useful descriptive tools for a discussion of perceptual
organisation in the real world, but we are still some way from having an
adequate theory of why the principles work and how perceptual
organisation is achieved. We mentioned earlier how the Gestalt
psychologists themselves attempted to answer both these points with
their model of brain field forces. What alternative answers would
contemporary theorists provide? Marr's (1976, 1982) approach to vision
emphasised that we should always consider what general assumptions about
the world can be brought to bear on visual processing to constrain the
range of possible interpretations for any particular image. The Gestalt
principles of organisation may work because they reflect a set of
sensible assumptions that can be made about the world of physical and
biological objects. Because the same kind of surface reflects and
absorbs light in the same kind of way, the different subregions of a
single object are likely to look similar. Because matter is cohesive,
adjacent regions are likely to belong together, and will retain their
adjacency relations despite movement of the object. The shapes of
natural objects tend to vary smoothly rather than having abrupt
discontinuities, and many natural objects (at least those that grow) are
symmetrical. A solid object stands upon (and hence is at a different
depth from) the surface on which it rests, and objects tend to be small
compared with the ground. A perceptual system that made use of such
assumptions to

interpret natural images would generally achieve correct solutions to
perceptual organisation, unless deceived by a camouflage exploiting
these very same assumptions. It is perhaps not surprising that in our
perception of unnatural displays (such as the patterns used by
experimental psychologists or the authors of textbooks), we employ the
same set of assumptions that serves us well in interpreting natural
images. However, having a set of descriptive principles, even if we know
why they work, is still only a starting point for a full
information-processing theory of grouping processes. We need to know how
such principles can be applied to primitive elements recovered from
images---edges, blobs, and so on---in order to recover the potentially
significant structures present. Research in artificial intelligence (AI)
attempted to provide such a process theory of perceptual organisation,
which is much more powerful than a purely descriptive theory, such as
that of the Gestaltists. Marr's (1976) early visual processing program
implemented such a process theory and made extensive use of Gestalt
principles to achieve perceptual organisation. Before describing Marr's
work, we digress briefly to introduce other research in AI which has
attempted to formalise organisational processes, by making use of a
rather different set of constraints.

ARTIFICIAL INTELLIGENCE APPROACHES TO GROUPING Scene analysis programs
Many researchers in AI during the 1960s and 1970s attempted to solve
what became known as the "segmentation problem". This is the problem of
dividing up a visual scene into a number of distinct objects. Most
researchers avoided the complexities of natural images, and restricted
their programs to a world of matt, white prismatic solids which were
evenly illuminated. Figure 6.18 shows a line drawing of the outlines of
such a collection of objects. Viewed analytically, Figure 6.18 is just a
collection of straight lines in a variety of orientations. However, our
spontaneous perception of such a scene is more likely

6. PERCEPTUAL ORGANISATION

135

We have no difficulty in seeing that regions a, b, and c belong together
(likewise d, e, and f; g, h, and i). Guzman's program SEE interpreted
pictures like this by examining the junctions present. Examples of arrow
junctions are shown at 1 and 2, and T junctions are shown at 3 and 4.
What other kinds of junction are there in this picture?

to be of a collection of distinct objects. Thus this scene is readily
described by our visual apparatus as being made up of two blocks and a
wedge, with one block partially occluding the other two structures. Here
again we have an example of perceptual organisation. Somehow we know
that the regions labelled a, b, and c belong together as one structure,
distinct from d, e, and f, which belong to another. The Gestalt
psychologists might argue that the perception of regions a, b, and c as
belonging to a "cube" provides a closed, simple, and symmetrical
interpretation, but this does not really address the question of how
such a solution is achieved by visual processing. This was the kind of
problem tackled by Guzman (1968), Clowes (1971), and Waltz (1975), among
others, who set out to produce computer programs that could "see"
objects from collections of lines such as these. The common principle in
all their work was a consideration of the junctions present in these
figures. A junction is a point where two or more lines meet. Different
junction types have different implications for the possible arrangement
of surfaces within the picture. Thus Guzman suggested that the presence
of an arrow junction would generally imply that

the edges which formed the fins of the arrow belonged to a single body,
while a T junction generally implied that the shaft and the crossbar of
the T belonged to different bodies. Figure 6.18 shows how these
principles apply in our example. Guzman's program SEE considered only
junctions, and incorporated his own informal intuitions about the
interpretations of different junction types. Clowes tackled the problem
of junction specification more systematically, and employed a more
sophisticated notion of how different junction types in the image relate
to the organisation of objects in the "scene" depicted. This was
achieved by considering the nature of the edges depicted by the junction
lines (Clowes, 1971; Huffman, 1971), as well as the nature of the
intersection of these lines. Edges may be convex, concave, or occluding
(see Figure 6.19). Only certain combinations of edge types are
compatible with a particular configuration of lines at a junction. By
ensuring that edges were consistently labelled along their entire
length, Clowes' program OBSCENE was able to interpret pictures
successfully provided that no more than three lines met at a single
junction. The program was also able to "reject" certain pictures as

136

VISUAL PERCEPTION

Three different kinds of edge are shown; concave (−), convex ( ), and
occluding (\>).

program was able successfully to parse scenes containing shadows. His
work illustrates how adding more information in the form of light and
shading may actually aid the interpretation of a scene by providing
additional local constraints. While such AI programs are intrinsically
interesting, and point out the complicated processing that may underlie
our everyday ability to perceive patterns such as these, they are of
limited importance. The programs work by incorporating the constraints
of their visual worlds, but the particular constraints employed are
specific to the world of white prismatic solids---an artificially
manufactured world that our visual systems did not evolve to perceive.
The principles embodied within these segmentation programs would fail to
recover the significant structures in natural images. Natural objects
may have internal markings, texture, and shading (see Chapter 5, p. 86).
Straight lines and angular junctions are rare. Indeed AI segmentation
programs of the above type either started with a line drawing as input,
or made use of initial programs to find the edges in images of prismatic
solids by using the assumption that edges are straight, along with
higherlevel knowledge about "likely" places to find lines (e.g., Shirai,
1973). More recent work in computer vision has of course become more
sophisticated, but is largely outside the scope of the present book.

Marr's program An impossible object. Reprinted from Clowes, Copyright
(1971), with permission from Elsevier Science.

"impossible" (see Figure 6.20 for example), whereas SEE would simply
accept such examples as objects. The most elegant example of work of
this type was that of Waltz (1975), who introduced a fourth edge type,
the crack, and whose program accepted pictures of scenes containing
shadows. Once shadows are introduced, the possible labellings for a
particular type of junction increase dramatically, since a number of
different types of edge could now be present. Nevertheless, Waltz's

Of more interest to our discussion is a processing model that aims to
recover structures from natural images of everyday objects and surfaces,
despite their noise, texture, and shadow. Marr's (1976, 1982) early
visual processing program finds occluding and internal contours from
images such as those shown in Figure 6.21. We have already considered
its first stages in Chapter 5. Marr proposed that cells in the retina
and visual cortex of mammals function to locate zerocrossings (see
p. 94) in the spatially filtered retinal image, which serve as the first
step towards recovering information about edges in the world. A
comparison of the zero-crossings found by sets of cells with different
receptive field sizes leads to a set of assertions about the "features"
present at

6. PERCEPTUAL ORGANISATION

137

Examples of the images analysed by Marr's (1976) early vision program:
(a) a chair; (b) a rod; (c) a plant; (d) and (e) textures; (f) a teddy
bear. Reproduced from Marr (1976) with permission of The Royal Society.

each location in the image. This set of assertions is the "raw" primal
sketch. The primitives in the raw primal sketch are edges, bars, blobs,
and terminations, which have the associated attributes of orientation,
contrast, length, width, and position. The representation of a straight
line would consist of a termination,

then several segments having the same orientation, then a final
termination. The raw primal sketch is a very complex, messy affair (see
Figure 5.11), from which we need to recover global structures as well as
internal structures and surface texture. Marr proposed that this is
achieved in the next

138

VISUAL PERCEPTION

stage of early visual processing, in which the primal sketch is
completed, by the recursive assignment of place tokens to small
structures, or aggregations of structures, in the raw primal sketch.
These place tokens are in turn aggregated together to form larger units,
in a cyclical manner. Place tokens can be defined by the position of a
blob, or of a short line or edge; by the termination of a longer edge,
line, or elongated blob, or by a small aggregation of tokens.
Aggregation of these place tokens can proceed by clustering nearby place
tokens on the basis of changes in spatial density (see Figure 6.22), by
curvilinear aggregation, which produces contours by joining aligned
items that are near to one another (see Figure 6.23), and finally by
theta aggregation. Theta aggregation involves the grouping of similarly
oriented items in a direction that relies upon, but differs from, their
intrinsic orientation. For example, theta aggregation can be used to
recover the vertical stripes in a herringbone pattern where all the
individual texture elements are oriented obliquely (see Figure 6.24).
The grouping together of place tokens thus relies upon local proximity
(adjacent elements are combined) and similarity (similarly oriented

Place tokens corresponding to small dots can be grouped together by
proximity to yield higher-order place tokens. Here, place tokens at a,
b, and c are grouped to yield a place token at A, and likewise for the
other dots in this figure.

Curvilinear aggregation will group place tokens at a, b, c, d, and so on
to yield a single structure A.

elements are combined), but more global considerations can also
influence the structures detected. For example, in curvilinear
aggregation, a "closure" principle could allow two edge segments to be
joined even though the contrast across the edge segments differed due to
illumination effects (see the image in Figure 6.25). Marr's theory
therefore embodied many of the Gestalt principles that we earlier
discussed at length. The grouping procedures use the construction of
tokens at different scales to locate physically meaningful boundaries in
the image. It is essential that different scales are used in order to
recover different kinds of surface properties (Marr, 1982, p. 91): Thus
if the image was a close-up view of a cat, the raw primal sketch might
yield descriptions mostly at the scale of the cat's hairs. At the next
level the markings on its coat may appear . . . and at a yet higher
level there is the parallel stripe structure of these markings. For a
herringbone pattern we know both that the "bones" are short parallel
segments oriented at 45°, and that these form vertical stripes.

6. PERCEPTUAL ORGANISATION

139

Theta aggregation can recover the vertical orientation of the stripes of
a herringbone pattern.

Curvilinear aggregation along with the application of a closure
principle could reveal the contour a-b-c-d, despite the different
contrasts of the edge segments along this contour. This pattern of
shading might arise if a tube was illuminated in the direction shown.

Boundaries arising from changes in surface material (where two different
objects overlap, for example), or from abrupt changes in surface
orientation or depth, can be revealed in two ways. First, boundaries may
simply be marked by place

tokens. The elliptical boundary perceived in Figure 5.20C, for example,
could be produced by the curvilinear aggregation of the place tokens
assigned to the termination of each line. We saw earlier (Figure 5.21)
how complex cells in V2 may be organised to link such line-ends to
implement the kind of aggregation process that Marr had in mind. Second,
texture boundaries may be revealed by discontinuities in parameters that
describe the spatial organisation of an image. Changes in the local
density of place tokens, their spacing or their overall orientation
structure could all be used to reveal such boundaries. Although the
boundary in Figure 6.27 is not defined by the spacing of place tokens,
it is revealed by discontinuity in the spatial distribution of
orientations of the small elements in the image. The success of these
organising principles in Marr's (1976, 1982) early visual processing
program can be judged by its ability to recover the occluding contours
from realistic images such as the teddy bear (see Figure 6.21), and to
reveal the internal contours of the bear that correspond to eyes, nose,
and muzzle outlines (Figure 6.26). Such structures are recovered without
recourse to high-level knowledge. The program knows nothing of the usual
shape of a teddy bear's head, and does not find the contours that
correspond to its

140

VISUAL PERCEPTION

The image of a teddy bear (Figure 6.21f) is printed at (a), and shown as
an intensity map at (b). The location of all the small edge segments in
the raw primal sketch is shown at (c). The structures that emerge after
grouping operations are shown at (d), (e), and (f). Reproduced from Marr
(1976) with permission of the Royal Society.

eyes because it "expects" to find them. Marr's theory of early visual
processing thus contrasts strongly with some computer models, or more
general theories of visual perception where expectations and
"object-hypotheses" guide every stage of perceptual analysis (e.g.,
Gregory, 1973; Roberts, 1965). The processing of natural images by
Marr's program works because the program embodies grouping principles
which reflect general properties of the world. Things that are oriented
similarly, or lie next to each other, are more likely to "belong
together" than things that are oriented dissimilarly and spaced far
apart.

FINDING TEXTURE BOUNDARIES In Marr's scheme, local oriented features
were found first, then subjected to a variety of grouping operations,
described above. Recent thinking about texture segmentation in the
context of spatial filtering has suggested, on the other hand,

that boundaries between regions differing in texture may be detected
rather more easily and directly than previously suspected. Consider the
image of Figure 6.27 in which the centre square and background regions
are defined only by a difference in element orientation. A spatial
filter tuned to right oblique orientation would respond strongly to the
centre region, but rather little to the background, as illustrated in
Figure 6.28A. At this point, the texture border has been transformed
into a boundary between low and high contrast areas in the filter's
response pattern. How might this boundary be found? A simple, but
important idea is to do further processing that converts this contrast
difference into an intensity difference, and then to find that intensity
edge by standard methods (cf. Chapter 5). To convert the contrast
difference into an intensity difference requires a nonlinear operation,
and there are several, roughly equivalent, possibilities that the
modeller, or evolution, could select. To get the flavour of this, let us
suppose that the filter response values of Figure 6.28A were either +10
or −10 (arbitrary units) in the

6. PERCEPTUAL ORGANISATION

141

with some residual variation at a fine scale. An appropriate degree of
spatial smoothing in the edge-detection operator allows this variation
to be ironed out (Figure 6.28C) before the boundary is located (Figure
6.28D).

Energy models for texture segmentation

Example of texture segmentation, defined by "orientation contrast". The
centre and background regions have the same mean luminance but differ in
element orientation.

centre region and +1 or −1 in the background. The mean response is 0 in
each region, and so local averaging would lose the border altogether.
Full-wave rectification, however, will do what we want. By definition, a
full-wave rectifier sets negative values to positive. All values in the
centre region would then become +10, while the surround values were +1.
Contrast difference has been converted to intensity difference as
required. Half-wave rectification sets negative values to zero, leaving
positive values unchanged. The centre responses would then become 0 or
10 (mean 5) while the background values were 0 or 1 (mean 0.5). Again a
mean intensity difference emerges, but accompanied by greater
variability of local values. A third option is squaring of the filter
responses. Like the full-wave case all values go positive, but in
addition high values are emphasised relative to low ones. Centre
responses would go to +100, and surround values to +1. These nonlinear
rectifying operations are both simple and physiologically plausible.
Figure 6.28B shows an example of the effect of full-wave rectification
on the pattern of filter responses. A mean intensity difference is
clearly achieved, but

In summary then, the foundation for several recent energy models of
texture segmentation (Bergen & Adelson, 1988; Bergen & Landy, 1991;
Bovik, Clark, Geisler, 1990; Malik & Perona, 1990) is a chain of simple
ideas: (i) regions that are differently textured necessarily differ in
their spatial structure; (ii) if we apply a bank of spatial filters to
the image, at different orientations and different scales, then at least
some of those filters will be more strongly activated by one texture
than the other; (iii) the difference in activation can be converted into
a simple intensity difference in response energy across space; (iv) this
energy difference is greatest at the texture boundary and can be found
by procedures already familiar from the detection of luminance edges. In
short, spatial filtering \[step (ii)\] and a suitable nonlinearity
\[step (iii)\] can convert the problem of texture segmentation into a
much simpler problem of edge detection. How well does this approach fit
with human perception of textured regions? Nothdurft (1985) found that
people could recognise the shape of texture regions such as that in
Figure 6.27 especially well when the line elements were densely packed
and relatively long. Short lines, sparsely spaced, yielded no visible
shapes at all. Shape discrimination also improved with the difference in
orientation between lines in the "figure" and lines in the background,
being fairly poor for a 10° difference and much better for differences
of 30° or 90°. All of these factors would be expected to enhance the
differential activation of filters between the two regions, and so the
energy models are broadly consistent with Nothdurft's results. Further,
we can ask whether the edge is vital in segmentation or whether we need
consider only the general difference in activation between the regions.
Nothdurft (1985) offers a demonstration suggesting that the edge is
crucial and

Worked example of the energy approach to finding texture boundaries. The
input image is a texture with centre region defined by orientation
contrast (Figure 6.27). (A) Output image from a small, right-oblique
filter. Note large response to right oblique lines, as expected. In A
and C large positive and negative responses are represented by light and
dark respectively; zero response is midgrey. (B) Filter output (as A)
after full-wave rectification---i.e., negative values in A set to
positive. This converts the difference in contrast to a difference in
mean intensity of response. Zero response is black in this plot. (C)
Rectified image (as B) after a second stage of filtering by a smoothing,
2nd derivative filter (LoG; see Figures 5.8, 5.9). This image should
have zero-crossings at the texture edges, and this is confirmed by the
zero-crossing contour shown in (D). 142

6. PERCEPTUAL ORGANISATION

specifically that orientation contrast at the boundary is the important
factor. When element orientation changed abruptly at the region boundary
segmentation was very clear, but when the same elements were
redistributed within the centre and background regions, segmentation
disappeared. When the variability of orientations in the background
increased, the orientation change at the boundary also had to be
increased to maintain segregation (Nothdurft, 1991). The importance of
feature contrast at boundaries also seems to hold for segmentation based
on changes in motion direction and colour (Nothdurft, 1993). These
principles for finding texture boundaries were applied in a full,
multi-channel model by Malik and Perona (1990). Their model used both
circular and oriented filters at a range of spatial scales, in order to
capture the variety of textural differences that might occur in images.
The model also incorporated some inhibitory interactions between filter
responses to accentuate the larger values, followed by smoothing and
edge detection, as described above. The model performed well when tested
against a variety of texture pairs for which good psychophysical data
exist. The rank order of the model's predictions matched the rank order
of human performance. An important test case is where adjacent regions
are formed from randomly oriented "+" and "L" elements. The two regions
have the same mean luminance, and the line segments have the same length
and the same (random) distribution of orientations. Since segmentation
is very clear for this texture pair, Julesz (1984) argued that
lineintersections (in the "+" elements) acted as one of the primitive
atoms ("textons") of texture vision. The success of filter-based energy
models on this and related tasks clearly challenges the need for texture
vision to make features or textons explicit (Bergen & Adelson, 1988;
Malik & Perona, 1990; see Barth, Zetsche, & Rentschler, 1998, for a new
view of "textons"). Bergen and Landy (1991) developed a model with a
similar flavour to the Malik and Perona model, specifically aimed at
segmentation by orientation differences. We sketch their model in Figure
6.29, because it incorporates several additional mechanisms of wider
interest in early

143

vision. Its first stages---squaring and smoothing of oriented filter
outputs---were discussed above. The next stage is opponency, taking the
difference between energies in channels tuned to orthogonal orientations
(horizontal--vertical, H--V; left--right oblique, L--R). Opponency is a
well-known feature of colour vision (see Chapter 2, p. 33), and motion
analysis too (Adelson & Bergen, 1985; see Chapter 8). Its function in
the model is to improve the orientation coding properties of the system
by enhancing the difference in energies between regions. There is
physiological evidence for opponency between orientations, since V1
cells can be inhibited by stimuli oriented at rightangles to their
preferred one (Morrone, Burr, & Maffei, 1982) and corresponding effects
are observed in the electrical response of the human visual cortex
(Morrone & Burr, 1986). This supports the proposed orientation opponency
in Bergen and Landy's model, although it may not be as complete as the
simple subtraction of H--V might suggest. At the contrast gain control
stage (see Bonds, 1991; Heeger, 1992a), the outputs of the opponent
mechanisms, (H--V) and (L--R), are each divided by the sum (H+V+L+R) of
the response energies from the four oriented channels. This normalises
the response values, making them independent of the overall luminance
contrast of the stimulus pattern. It also suppresses weak responses in
one channel (e.g., H--V) when responses are strong in the other channel
(L--R). The full model was able to give a good account of psychophysical
data on shape discrimination for textured regions defined by orientation
difference (Bergen & Landy, 1991). We shall see many of its features
reappear in our discussion of motion analysis (Chapter 8). In summary,
spatial contrast in the magnitude or "energy" of filter responses may be
adequate to account for much of texture segregation. A few examples of
texture boundaries that the model would handle successfully are shown in
Figure 6.30B,C,E,F. The spatial selectivity of the filters achieves a
grouping by similarity of orientation (panels B,E), spatial scale (panel
C) or contrast (panel F), without having to represent the texture
elements individually. This makes good sense for natural surfaces such
as bark (see Figure 6.17) or

144

Multi-stage energy model for finding texture boundaries defined by a
change in local orientation. Adapted from Bergen and Landy (1991). See
text for details.

145

Perceptual segregation of textured regions can be based on differences
in light--dark polarity (A), element orientation (B,E), spatial scale
(C), or contrast (F), but not contrast polarity (local spatial phase)
(D).

146

VISUAL PERCEPTION

skin, which have readily identifiable texture, but where it is not clear
that individual "elements" even exist. Interestingly, human perception
fails to distinguish any separate regions in Figure 6.30D. Close
scrutiny reveals that there is a central square, as in the other
examples, but it is defined by a contrast reversal (phase reversal) of
the oriented elements. In the model, the squaring operation makes energy
a purely positive quantity that discards the sign or phase of the input,
as complex cells do (see Chapter 5, p. 110), and so this perceptual
failure to extract a central square is just what the energy model
predicts. By the same token we might guess that the model would predict
no segregation in Figure 6.30A, where the elements again differ only in
sign (light vs dark blobs). However, a dark square is easily visible, so
is this a failure of the energy model? Happily no, because here there is
a simple luminance difference between the two regions, and the
light--dark edges would be readily detected by filters at larger scales.
Try blurring your eyes to confirm this. It is a useful reminder that
visual processes make good use of the filtered information that is
distributed across space, orientation, and scale. Another Gestalt
"law"---proximity---can be seen in the same light, since more closely
packed elements increase the intensity and uniformity of the filter
outputs after the nonlinear stage. For example, in the classic "rows and
columns" demonstration (Figure 6.8), closer packing in the columns will
activate vertical filters more strongly than horizontal, and vice-versa
for rows. When vertical and horizontal filters are equally activated
(Figure 6.8c) the organisation is ambiguous. The grouping and linking of
explicitly derived local features---in the Marr and Julesz
traditions---is therefore not the only basis for more global levels of
perceptual organisation.

THE NEUROPHYSIOLOGY OF TEXTURE SEGMENTATION The neural basis for the
first five steps in the energy model for texture segmentation (Figure
6.29) is now fairly well understood. Linear

oriented filtering by simple cells in V1, squaring and summing their
outputs onto complex cells in V1 or V2 (as confirmed recently by Alonso
& Martinez, 1998), cross-orientation inhibition, and divisive gain
control have all been well documented by single-cell studies. We now
discuss the neural basis for the next step: the process that locates the
texture boundary, labelled "edge detection" in Figure 6.29. We have seen
(Figure 6.28C) how a second stage of centre--surround filtering can
localise the spatial changes in texture energy. As discussed in Chapter
5, a centre--surround filter suppresses responses to uniform regions but
gives strong responses in the vicinity of step changes in its input
value. Recent physiological studies have revealed that cells in V1 and
V2 often respond more vigorously in the presence of texture contrast.
The basis for this is suppression of the cell's response to texture in
its receptive field when the same texture falls in the area surrounding
it. This is analogous to the facilitating effect that we described in
Chapter 3 (p. 70), but opposite in sign. A cell with these properties
can effectively act as a centre--surround filter for detecting texture
edges. Figure 6.31 illustrates the work of Zipser, Lamme, and Schiller
(1996), who found that many cells in monkey V1 were more responsive to
texture contrast (panel B) than to a homogeneous texture (panel A), even
though the contrasting surround lay entirely outside the receptive field
(RF) and produced no response when the centre region was blank. The
surrounding texture does not drive the cell's response, but does
influence it---an effect known as "contextual modulation". Differences
in the surrounding orientation, colour, luminance, and depth (Figure
6.31 C,D) were all effective in producing this response enhancement.
Zipser et al. (1996) argue that contextual modulation plays an important
role in perceptual "figure--ground" interpretation. Indeed, several
other contextual differences that did not produce a clear "figure" for
human perception also did not cause contextual modulation of the cells'
responses. The enhancement of V1 cells' responses in "figure" regions
takes some little time to develop after the onset of a stimulus pattern.
It starts first for RFs located close to the texture boundary,

6. PERCEPTUAL ORGANISATION

147

The kind of textured line displays used by Zipser et al. (1996) to study
V1 responses to texture contrast. (A) Homogeneous texture of vertical
line segments. Dotted rectangle indicates the receptive field (RF) area
of a V1 cell or a cluster of V1 cells (multi-unit recording). (B) RF
area contains the same texture as A, but the surrounding region is
horizontal, giving strong orientation contrast and perception of a
disc-shaped figure. Many V1 cells respond more vigorously to B than A.
This implies that they are influenced by information that is well
outside their "classical" receptive field---an effect known as
contextual modulation. This extra response was also found when the
figure--ground contrast was given by other cues such as colour,
luminance, or depth (C,D). Binocular fusion of the stereo pair (C,D)
will reveal a "hidden" disc that is defined only by binocular depth
information (binocular disparity--- see Chapter 7).

reaching a peak there after about 120 ms. This stage corresponds to that
described by Lee et al. (1998) (see Chapter 3, p. 72). Enhanced
responses in the centre of the figure region, away from the boundary,
develop more slowly until,

after a few hundred ms, the enhancement is unform across the figure
region (Lamme, Rodriguez-Rodriguez, & Spekreijse, 1999). Lamme et
al. (1999) thus make a distinction between fairly rapid processes that
enhance the detection of

148

VISUAL PERCEPTION

texture edges, and a rather slower process that fills in a
representation of the surface region between the edges. There is
considerable psychophysical evidence that the perceived brightness and
colour of uniform regions "fill in" between the edges, and that filling
in takes time to complete (cf. Pessoa & Neumann, 1998). Lamme et
al. (1999) suggest that this filling in of a surface representation in
V1 depends on feedback from higher cortical areas, because it was
abolished by lesions of extrastriate cortex but the texture edge
enhancement was not.

Perception as predictive coding The evidence discussed above further
reinforces the important new view of V1's role in perception that we
introduced in Chapter 3. Where previously V1 was seen as an early,
feed-forward stage of fairly localised image processing, we are now
beginning to see it as an image processor whose output is also shaped by
the results of more global perceptual processing which are fed back from
higher areas. Rao and Ballard (1999) have embedded these ideas into a
novel computational framework for how perception works in general, based
on the concept of "predictive coding". We may recall Helmholtz's (1866)
famous suggestion that perception was an "unconscious inference" about
what must be out there in the world to produce the data we currently
have at the eye. Perception, then, can be seen as a "hypothesis" that is
consistent with the current sensory data (Gregory, 1973). To know
whether a hypothesis is indeed consistent with the data, one has to
generate predictions about the expected data, and compare them with the
actual data. When there is a good match, the hypothesis can serve as an
economical description of the data. While the agreement holds, only the
hypothesis is needed, but as soon as a mismatch occurs (the hypothesis
is out of date, or wrong) then the new incoming data need to be used to
revise the hypothesis. Rao and Ballard (1999) envisage this process
occurring at each of the many stages of the visual processing hierarchy.
Feedback from one stage to the previous one carries the predictions
generated by the current hypothesis at that level. The predicted data
are subtracted from the current input

and only the difference or "error signal" is fed forward to the next
stage. That error signal is used to modify the predictions that are then
fed back. And so on. At early stages in the hierarchy of visual areas,
the data and predictions may relate to rather elementary information
about patches of the image, but at later stages receptive fields become
increasingly large and the data and predictions here may represent
surface regions (e.g., in V2), parts of objects (e.g., V4) or whole
objects, faces, etc. (in temporal lobe areas). Rao and Ballard's
simulations of this predictive coding model show how a neural network
(see Chapter 4, p. 82) exposed to natural images can learn to generate
appropriate predictions based on the statistical regularities that
images contain, such as the coherence of object contours, the locally
similar structure of textured regions, or the bilateral symmetry of
natural forms. The approach is allied to those in which mathematical
techniques such as independent components analysis have been applied to
collections of natural images (see Chapter 3, p. 57). In both cases,
regularities in the visual world are identified and used to gain economy
of representation. However, Rao and Ballard's result was obtained from a
model based on physiological and anatomical evidence rather than more
abstract mathematical principles. An important property of Rao and
Ballard's model is that suppressive surrounds emerge as part of the
process by which data and predictions are compared. For example, the
response of a local RF to the central portion of a line is well
predicted from the other parts of the line, and so data about the
central portion do not need to be fed forward. Thus the predictive
coding model gives a natural account of "end-stopping"---the suppression
of responses to long lines that extend beyond the RF. For a short line,
however, there is no such prediction from adjacent regions, and so the
error signal is large and needs to be fed forward. Rao and Ballard show
how physiologically plausible RFs and suppressive surrounds develop in
this model framework. Given this theoretical importance, we now turn to
a closer look at the empirical evidence.

6. PERCEPTUAL ORGANISATION

Suppressive surrounds: Physiology What are the mechanisms underlying
contextual modulation? Little is known about the filling-in process, but
we are beginning to understand the basis of texture edge detection. One
important aspect is the "suppressive surround" to the classical
receptive field, a phenomenon that has been recognised since the 1970s
(Maffei & Fiorentini, 1976; Nelson & Frost, 1978). If a V1 cell is
stimulated with a small patch of grating, and the size of the patch is
systematically varied (Figure 6.32 A,B,C), then a typical finding is
that the response rate increases with patch size, but beyond a certain
size (panel B) further increases in patch size produce smaller and
smaller responses (panel C). If receptive fields were independent local
areas, perhaps defined by oriented Gabor functions (see Chapter 3,
p. 56), we should expect the response to increase until the patch filled
the RF, and then to stay at this level for any larger patches. The

149

progressive decrease in response with larger patches implies an
inhibitory influence from beyond the "classical" RF. Walker, Ohzawa, and
Freeman (2000) found that the degree of suppression measured in this way
varied from cell to cell, but was found about equally for simple and
complex cells in the cat cortex, and was about evenly distributed across
all cortical layers. Suppression is typically greatest when the surround
matches the preferred orientation and spatial frequency of the RF, and
decreases as the surround is made more different (Figure 6.33) (De
Angelis et al., 1994; Knierim & Van Essen, 1992; Levitt & Lund, 1997;
Walker, Ohzawa, Freeman, 1999). Importantly, then, the suppressive
effect seems to be tuned to the same characteristics that drive the RF,
but the surround often responds to a broader range of orientations,
spatial frequencies, and speeds than the RF itself (Li & Li, 1994).
These studies might suggest that RFs are

Suppressive surrounds in visual cortex. (A--C) As grating area increases
(A,B) so response of a V1 cell will increase, but then further increases
in size (C) lead to a reduced response, implying a suppressive surround
to the classical RF. Dotted circle indicates the classical RF size
implied by such a result. (D) Arrangement of central and surround
gratings used by Walker et al. (1999) to study the spatial organisation
of the suppressive surround. Dotted circles indicate possible positions
for the surround patch. (E--H) Models of surround structure---annular
surround (E), end-stopping (F), side-stopping (G), asymmetric (H).

150

VISUAL PERCEPTION

Typical effect of an oriented surround on the response of cells in V1.
This example assumes the cell's preferred orientation is vertical.
Surrounds of similar orientation and spatial frequency tend to give the
most suppression of the response to the central stimulus. Thus the
suppressive surround often (but not always) has a selectivity that is
similar to the classical RF itself.

completely surrounded by a "moat" of suppression arising from
neighbouring cells with similar tunings (Figure 6.32E). We shall see
below that this looks like a good model at the psychophysical level, but
how well does it describe the surround effects for individual cells?
Other possible models are that cells have inhibitory "end-zones" (Figure
6.32F), originally suggested as the defining characteristic of
hypercomplex cells by Hubel and Wiesel (1962, 1968), and/or inhibitory
"sidezones" (Figure 6.32G) as found for some cells by De Angelis et
al. (1994). The most direct answer to this question comes from a study
in which the surrounds of RFs in cat cortex were tested systematically
at eight different locations (Figure 6.32D). Surprisingly, the source of
suppression for most cells was localised to one or two adjacent patches
of the eight surround positions, and the suppression from these
"hot-spots" was as great as with a complete surrounding grating,
implying little or no influence from other locations. Moreover, across
different cells the source could be any position around the RF (Figure
6.32H), not just the side or end-zones (Walker et al., 1999). Although
the suppressive surround appears to be a patchy affair for individual V1
cells, it seems quite likely that the pooling of clusters of cells would
create more homogeneous surrounds at

higher stages (V2, V4, MT, etc.) where suppressive surrounds are also
very evident, but this has not been studied. The localised source and
rapid onset of suppression (only 10 ms delay relative to the main RF
response; Walker et al., 1999) suggest that it arises locally within the
striate cortex, rather than via feedback from higher areas such as V2.
This was confirmed by Hupe, James, Girard, and Bullier (2001) who
inactivated local regions of V2 in the monkey cortex and found that
suppressive surrounds in V1 were unaffected. One note of caution is that
these last two studies were on anaesthetised animals, while those of
Lamme and colleagues (above) tested alert monkeys. Basic RF tuning
properties and surround suppression appear to be largely unaffected by
anaesthesia (Nothdurft, Gallant, & van Essen, 1999), but it is possible
that feedback would be impaired by anaesthesia, and indeed the
figure--ground effects observed in V1 were not found in the
anaesthetised animal (Lamme, Zipser, & Spekreijser, 1998). It is perhaps
not too surprising that being asleep affects perceptual processes.
Overall, then, the evidence favours the view that suppression from the
surround arises locally within V1, while other aspects of figure--ground
enhancement depend on feedback from higher areas.

6. PERCEPTUAL ORGANISATION

An important functional conclusion from this review of suppressive
surrounds is that centre and surround mechanisms respond selectively to
similar patterns and that this enables a role in texture segmentation.
Because they respond to similar spatial structure, the surround tends to
suppress centre responses in homogeneous regions of texture, and
"release" responses at boundaries where centre and surround contain
textures with different spatial structure. By delineating regions that
share common texture properties this process can be seen as a mechanism
for perceptual grouping by similarity. This could also be the mechanism
underlying the rapid, perceptual "pop-out" of single features that
differ from their surrounding elements in orientation, colour, size,
etc. (Figure 6.34; Knierim & van Essen, 1992; Nothdurft et al., 1999).
At the physiological level there is (as always) a lot of variation
between cells in the extent to which they exhibit this kind of
selectivity. This role in texture segmentation may be just one
expression of a more general functional role played by suppressive gain
controls in perception. Schwartz and Simoncelli (2001) have shown how
the cortical gain control effects observed physio-

"Pop-out" of single features that differ from their context.

151

logically, including suppressive surrounds, emerge as a consequence of
making the responses of visual filters statistically independent of each
other. Networks of sensory neurons may strive for efficient coding of
natural signals by reducing the extent to which different neurons
redundantly carry the same information. This "efficient coding
hypothesis" was first proposed over 40 years ago by Barlow (1961), but
Schwartz and Simoncelli have importantly shown what sorts of
interdependencies exist between the responses of neighbouring linear
filters when presented with a large variety of natural images, and how
these can be eliminated by the nonlinear, suppressive gain control.

SUPPRESSIVE SURROUNDS: PSYCHOPHYSICS If the suppressive surround is
functionally important in vision, we should expect to be able to find
psychophysical evidence for it in human observers. A repeated finding of
this kind is that

152

VISUAL PERCEPTION

if a patch of grating or texture is surrounded by a similar,
high-contrast pattern, then the contrast of the centre patch appears to
be less than when it is presented against a blank grey surround (Chubb,
Sperling, & Solomon, 1989). Cannon and Fullenkamp (1991) measured the
degree of suppression produced by inducing gratings of different shapes,
sizes, and positions (illustrated in Figure 6.35). When the inducer was
large and abutting the test patch, the effect was large--- around 50%
reduction in perceived contrast. Total area of the inducer was
important, but end vs side location was not, and the influence of the
inducing grating gradually fell away as it was separated from the test
grating. The suppressive effect decreased when the orientation or
spatial

frequency of the surround was made increasingly different from the
centre patch. Fairly similar results (with some complications) were
reported recently by Yu, Klein, and Levi (2001). These findings on
contrast perception are remarkably consistent with the physiological
data, and suggest the presence of a complete annular suppressive
surround (as sketched in Figure 6.32E) whose selectivity for orientation
and spatial frequency is similar to, but more broadly tuned than, the
centre mechanism. On this basis we should expect, perhaps contrary to
intuition, that gratings of large area should appear to have less
contrast than small patches, just as V1 cell responses are partially
suppressed by large gratings (e.g., Walker et al., 2000). This

Stimulus pattern arrangements used by Cannon and Fullenkamp (1991) to
measure the influence of suppressive surrounds on perception of
contrast. The small patch of grating (top centre) is the control and
should be compared with the centre region of the other images. Fixing
your gaze in the white space between two images seems to enhance the
perceived differences. All the centre patches are physically the same.
In the experiments, large, abutting surrounds (top right) reduced the
centre patch contrast most of all. Spatial separation (bottom right)
reduced the suppressive effect. Gratings restricted to either the end-
or side-zones were equally effective suppressors.

6. PERCEPTUAL ORGANISATION

was predicted and indeed confirmed by Cannon and Fullenkamp (1991).
Several other experimental paradigms have contributed to our
understanding of the suppressive surround, namely contrast adaptation
and contrast discrimination, which we shall examine in turn. If the
spatial area surrounding the RF normally depresses a cell's responses,
then if we could remove that suppression we might find that visual
performance improved in some way. This was shown in an ingenious study
by Ejima and Takahashi (1984, 1985), looking at contrast sensitivity for
a grating patch after adapting to other grating patches (see Figure
6.36A,B). As we saw in Chapter 5, adapting to a high-contrast grating
for some minutes makes it harder to see lowcontrast test gratings
afterwards: the contrast threshold goes up. However, Ejima and Takahashi
found that if one adapted to gratings flanking the test area (rather
than superimposed on it), then contrast sensitivity actually improved
after adaptation---the threshold was lower than normal. Figure 6.36C
shows that this reduction of threshold was similar for adapting patches
in side- or end-locations, provided they were not too far from the test
region. The data clearly resemble cross-sections of a centre--surround
receptive field, as sketched in panels A and B. To understand the
improvement, we must suppose that cells receive some level of
suppression from their surrounds even when the surround area is blank.
This is not implausible given that many cortical cells have a
"spontaneous" firing rate (mainly complex cells rather than simple
cells) and so can generate sustained or "tonic" inhibition on other
cells. When cells in the surround area are adapted, it would follow that
the tonic suppression they exert on the centre is reduced; responses go
up; contrast detection is improved.

Contrast discrimination Alongside studies of contrast perception and
contrast adaptation, the third paradigm that has been used to reveal the
suppressive surround is contrast discrimination. The basic task is to
say which of two otherwise identical gratings has the higher contrast.
This is typically done in a twoalternative forced-choice task. Gratings
with con-

153

trast C and C+δC are presented in turn (in random order), and the
observer has to identify the higher contrast. The experimenter varies
the difference δC over a series of trials to find the contrast
difference that can just be discriminated reliably at (say) 75% correct.
Note that 50% correct is chance, 100% is perfect performance, so 75%
correct is a reasonable definition of "just detectable". Measuring
contrast discrimination at different levels of base contrast (C) has
proved to be an important way of probing various processes in spatial
vision. The fundamental result is that over most of the contrast range
the just-detectable difference δC increases as C increases. If the
increase in δC were directly proportional to C, this would be an example
of "Weber's Law" (δC = k.C, where k is a constant, the "Weber fraction"
that defines what proportional change is just detectable). In practice
most experiments have found a near-miss to Weber's Law, approximately δC
= k.C0.7 (e.g., Legge, 1981). So far, this relation is purely
descriptive, but it starts to get more interesting when we consider why
δC should vary in this way. Nearly all models of contrast discrimination
share the basic assumption that a change δC will be just detectable when
an internal response R changes by some fixed amount, δR. Thus the key
factor controlling discriminability will be the slope of the relation
between input contrast (C) and the contrast response (R) of the
mechanism or filter that is tuned to the orientation and spatial
frequency of the test pattern. This logic is illustrated in Figure 6.37.
At point (a), the slope is steep and so discrimination will be good
because we get a big response δR from a small change δC. At point (b),
where the slope is shallower, discrimination thresholds will be worse
(higher) because a much larger δC is needed to give the same δR. This
means that the measurement of δC gives us an estimate of the slope of
the underlying response function at contrast C. In effect, a set of
these slope values can be put together (integrated) to derive the whole
response function, as shown in Figure 6.37. In practice, this is done by
deriving predictions from a mathematical model of the response function
and then finding the specific function that best predicts the
experimental data

154

Suppressive surround revealed by contrast adaptation. Observers had to
detect the contrast of a test grating (centre patch) after several
minutes of adaptation to the contrast of the flanking gratings. The
separation (S) between the adapting gratings was varied either
lengthwise (A) or laterally (B). When S = 0, the adapters were
superimposed on the test area and contrast sensitivity was worse after
adaptation, as usual. But when the offset of each grating (S/2) equalled
about 2--4 periods (4--8 bar widths), then contrast sensitivity improved
after adaptation (thresholds were reduced, panel C). This may occur
because neurons in the suppressive surround (large dotted circles) are
adapted by the flanking gratings, and so their suppression of cells in
the test region ( ) is reduced, resulting in increased sensitivity---a
decrease of contrast threshold (C). Drawn to scale from the experiments
of Ejima and Takahashi (1984, 1985).

6. PERCEPTUAL ORGANISATION

155

Basic principles underlying contrast discrimination. Most models assume
that the observer can tell the difference between two contrasts when an
internal response R changes by a fixed amount R. The contrast difference
C needed to produce this change is (by definition) the threshold value,
and it will vary with the slope of the response function, as
illustrated. See text for details.

(Legge & Foley, 1980). The equation for a typical contrast response
function is given in Figure 6.37, and it can be interpreted fairly
simply as the ratio of two processes: excitation (E) divided by a gain
control factor (G). Note that both factors depend on stimulus contrast
C. The excitation E rises rapidly with C, but is counteracted by the
accompanying rise in the divisive gain term G. The net result is a
response function that rises rapidly at low contrasts, then decelerates
and has an increasingly shallow slope at higher contrasts, as shown. It
accounts for the reduced discriminability observed at high contrasts,
and was reasonably well confirmed by a recent fMRI study of

contrast responses in the human brain (Boynton, Demb, Glover, & Heeger,
1999). More generally, this type of model is able to give a good,
detailed account of many of the response characteristics of V1 cells
(Heeger, 1992a) and the results of contrast discrimination and contrast
masking experiments in humans (Foley, 1994; Watson & Solomon, 1997). In
this broader context the gain term G is thought to pool a weighted sum
of the excitations of many cells tuned to different spatial frequencies
and orientations at the same location. This means that the response gain
of a cell or psychophysical mechanism is not set simply

156

VISUAL PERCEPTION

by its own response level, but by the aggregate response of cells in its
neighbourhood, even if the stimulus is one that gives little direct
excitation (E) to the cell in question (e.g., a horizontal grating for
an RF that is vertical). The greater the input contrast, the more all
cells' responsiveness will be damped down at that location. With this
dynamic contrast gain control in mind, we can now turn to some
surprising effects of a surrounding pattern on contrast discrimin-

ation in a central region. Yu and Levi (1997) studied how well obervers
could detect a grating test patch of contrast δC, superimposed on a
background masking grating of contrast C (Figure 6.38A). Note that this
is an extension of the standard discrimination experiment because here
the area or length of the "mask" (contrast C) can be different from the
test patch. As the length (L) of the mask was increased, Yu and Levi
found that the discrimination threshold δC at first increased with
length L, but when L was longer

Suppressive surround revealed by contrast masking experiments. (A) The
display used by Yu and Levi (1997). In general, the ability to detect a
weak test pattern is made worse ("masked") by superimposing a masking
pattern of similar orientation and spatial frequency. However, Yu and
Levi found that the degree of masking depended on the mask length, L.
Masking was greatest when L was slightly longer than the test length (as
shown), but became less when the mask was made even longer. (B) This
reduction of masking was still found when the additional parts of the
mask were out of phase with the central section. (C) Display analogous
to (B) used by Chen and Tyler (2001).

6. PERCEPTUAL ORGANISATION

than about 12 min arc (0.2° of visual angle), discrimination began to
improve progressively with further increases in length. Thus, larger
masks produced less masking, which sounds paradoxical. Yu and Levi
interpreted the initial rise in threshold as due to the increasing
effect that the mask would have within a receptive field, and the
improvement thereafter as arising from the intrusion of the mask into
the inhibitory zone of the surround that (somehow) offsets the effect of
the mask in the central region. This release from masking arising from
the end-zones was the same whether the end-zone masks were in-phase or
out-of-phase with the central region (Figure 6.38B). Such lack of
sensitivity to sign indicates a nonlinear mechanism that is distinct
from the central, linear RF itself. Other experiments showed that with
end-masks or with completely surrounding masks, the reduction of masking
increased with the contrast of the mask (up to 40% contrast), and was
greatest when the masks matched the spatial frequency and orientation of
the test region (Yu & Levi, 1998, 2000). Orthogonal masks, at right
angles to the test patch, also produced quite strong improvements in
contrast discrimination, relative to the no-surround baseline. To
summarise so far: contrast discrimination depends on the slope ("gain")
of the contrast response function, and this in turn is determined by a
gain control mechanism whose spatial tuning characteristics we would
like to understand. When contrast discrimination is tested in a central
region, discrimination performance can improve markedly when a
surrounding mask is added, implying that contrast gain goes up in the
presence of a surrounding pattern. How might the suppressive surround
mechanism bring this about? Yu and Levi (2000) suggested that the gain
control is driven by pooled signals from the centre region (as in
standard models; above) but also that stimulation of the surround
mechanism may subtract a value from the divisive gain term G. They
envisaged that this would increase the gain of the mechanism, and so
improve contrast discrimination as observed experimentally. It turns
out, however, that when this suggestion is simulated numerically it pro-

157

duces the opposite of the desired effect: response levels go up but
response gain (slope) goes down. A more promising model is obtained if
we suppose instead that the gain control combines signals additively
from the centre and surround regions. That is, we add a value to the
gain term G when a surround pattern is present. Figure 6.39E(3) gives a
simple version of the equation for the centre-plus-surround gain
control, and the other panels show how response levels and response gain
are affected. We can see (in A) that surrounding contrast drives
response levels down, in accord with the reduction of perceived
contrast, and the reduction of firing rate at the single cell level.
However the slope of the response curve at a given contrast (point "a")
increases when the surround is present (point "b"). This is shown
explicitly in panel B, where response gain (slope, dR/dC) is greater
over most of the contrast range when the surround is present. Note how
in panel C the response suppression increases progressively with
surround contrast, but is present to some degree even when the surround
is lower in contrast than the centre. This matches quite well the
observations of Cannon and Fullenkamp (1991). Finally, in panel D, we
see how contrast discrimination thresholds would vary with surround
contrast. On this model they improve smoothly by about a factor of two
as surround contrast increases from 10 to 80%, similar to Yu and Levi's
(2000) data with orthogonal surrounds. In summary, although this model
of surround suppression is no more than preliminary, it looks promising
and offers a specific view of the surround mechanism. It draws together
a variety of surround effects on perceived contrast and contrast
discrimination that we had previously found difficult to resolve. Its
main suggestion is that the contrast gain control pools squaredcontrast
signals (contrast energy) from both central and surrounding zones of the
receptive field. Why then are the two zones antagonistic? The basic
answer is that the centre region contributes excitation (from the
classical RF) as well as suppression, while the surround contributes
only suppression.

158

Contrast gain control model (E3) explains a range of findings on the
effects of surrounding patterns on perceived contrast and contrast
discirmination. A surround pattern drives the centre response R down
(A,C), leading to reduction in perceived contrast. But the surround also
drives up the response gain (slope, B) leading to improvements in
contrast discrimination (lower thresholds, D). See text for details.

6. PERCEPTUAL ORGANISATION

BEYOND FILTERS: CONTOURS AND SURFACES We have seen that the RFs of
spatial filters have suppressive surrounds that probably arise from
spatial interactions between cells in V1, and may play an important role
in texture segmentation and perceptual "pop-out" of salient features.
However, it would be wrong to conclude from these studies on textures
and spatial interactions that fairly simple post-processing of filter
outputs provides a complete account of perceptual organisation. The
energy models are relevant to the effects of proximity and similarity,
but have little to say about structural factors such as symmetry,
closure or "good form". We round off our discussion with several lines
of recent experimental evidence pointing to further organising
processes: first, that the Gestaltists' "good continuation", or
equivalently Marr's notion of "curvilinear aggregation", is implemented
by facilitatory, cooperative processes across the image; second, that
elongated contours are segmented into parts that correspond to the parts
of objects; and finally that textural grouping and segmentation can
depend on the representation of surfaces in depth, and not just on
image-processing operations of the kind sketched in Figure 6.29.

Contour integration: Perceptual linking along straight and curved paths
Simple demonstrations of "good continuation", such as Figures 6.10 and
6.11, are amenable to a spatial filter approach, since oriented
receptive fields that spanned several elements could automatically pick
up the dominant orientation along the curve, and at the intersection of
two curves. However, the experiments of Field, Hayes, and Hess (1993)
cannot be explained so easily, and point to a more active linking
process for contour integration. Their observers were presented with
arrays of 256 small, randomly scattered striped patches (Gabor patches),
and within this large set of randomly oriented distractors a subset of
just 12 patches was arranged along an undulating, snake-like path. An
example of this type of dis-

159

play is shown (with fewer patches) in Figure 6.40A. The observer's task
was to determine which of two such arrays contained the "snake". The
experimenters varied the "wiggliness" of the snake (path angle; Figure
6.40B), and the degree to which the path elements were aligned or
misaligned along the direction of the path (Figure 6.40C). Straight
paths, with elements aligned along the path, were most easily detected.
Much more surprising was the finding that undulating paths, which
changed direction by up to 40--60° with every step, were still highly
detectable, provided the elements were aligned along the path. Since the
path contains elements in all orientations, it cannot be distinguished
from the distractors by applying any single oriented filter. Instead it
was the alignment of orientations along the curve that was important.
Misalignment by ±15° reduced performance, and misalignment by ±30° made
the path almost undetectable. By contrast, alignment or misalignment of
spatial phase had little effect. Field et al. (1993) interpret these
findings in terms of an "association field", suggesting "a localised
linking process or association between the responses to the elements in
the path according to a specific set of rules" (p. 185). Their scheme
for this linking process is shown in Figure 6.40D, indicating that links
will be made between two adjacent stimulus elements if their orientation
and position are such that they would lie on a simple smooth curve
passing through both elements. This is more specific than the Gestalt
idea that elements are grouped if they are close and/or similarly
oriented. The association field could serve to build a chain of linked
elements representing a continuous contour, and could contribute to the
"flow" of texture seen in Figure 6.41A. Figure 6.41 also illustrates
some other aspects of "global" shape perception in which extended
spatial integration of local features takes place. For a recent review
of the evidence on contour integration, see Hess and Field (1999). In
contour integration the linking appears to be between fairly low-level
neural elements, rather than between abstract "tokens" for orientation,
because when the path elements alternated between two spatial
frequencies (say, 3 and 6 c/deg),

160

Contour integration. (A) Typical experimental display in which observers
have to detect the presence or absence of a smooth path running through
an unknown subset of the display elements. In this display there are 7
elements in a path amongst 34 randomly oriented distractors. Can you see
it? (B) Path angle between successive elements determines how straight
or curved the path is. (C) Misalignment between element orientation and
path direction makes the contour jagged and much harder to find. (D) The
"association field" proposed by Field et al. (1993) as a process that
selectively links local orientation analysers in order to detect the
global structure of the contour. Dashed lines represent a facilitatory
link between cells whose receptive fields have positions and
orientations that lie on smooth curves passing through the central unit.

161

Examples of global structure visible in texture patterns. (A) Texture
"flow"---randomly placed Gabor patterns but with a sine-wave undulation
of orientation. (B) As A, but with a consistent spiral pattern of
orientations. (C) As B, but with circular structure. (D) As C, but with
65% of the elements randomly oriented; the circular structure is just
about visible. (E) "Glass patterns" (Dakin, 1997; Dakin & Bex, 2001;
Glass, 1969); dot-pairs are positioned at random but their orientation
follows a circular structure. (F) This structure is destroyed
perceptually if light and dark dots are paired. All these examples imply
some large-scale analysis of shape and form after local encoding of
orientation. Wilson and Wilkinson (1998) modelled this process in two
stages: local coding in V1, followed by a number of shape-encoding
operators that combine the V1 outputs in brain area V4.

162

VISUAL PERCEPTION

or when the striped patches were defined by contrast modulation rather
than luminance modulation, observers found it much more difficult to
detect the path (Dakin & Hess, 1998; Hess, Ledgeway, & Dakin, 2000),
even though the orientations of the individual elements were equally
well resolved in all cases. Intriguingly, dyslexic observers performed
less well on this task than a control group (Simmers & Bex, 2001),
suggesting that an impairment of low-level perceptual integration across
space is one aspect of this multi-faceted problem.

Collinear facilitation Further evidence for spatially extended linking
of local responses comes from experiments of Polat and Sagi (1993,
1994). Their experiments tested a very basic visual task---the
forced-choice detection of very low-contrast patches of sinusoidal
grating---with and without the presence of two adjacent "masking"
patches of higher contrast (Figure 6.42A). One might guess that
contextual and grouping effects would be minimal in this kind of task,
but it was not so. When the masking patches were superimposed on the
test patch they made it harder to see. This is expected, because the
task is then equivalent to contrast discrimination, and we saw above
that thresholds rise in the presence of a background or "pedestal"
contrast

(see p. 153). However, when the masking patches were adjacent to the
test patch they tended to improve its detection; contrast thresholds
were lower than in the baseline condition without a mask. This is
facilitation, and the forced-choice method ensures that it is a genuine
improvement in visual performance, and not an artefact of guessing or
reporting bias. Facilitation was greatest when the masking patches had
the same spatial frequency and orientation as the test (Polat & Sagi,
1993), and were co-axially aligned with it (Polat & Sagi, 1994). The
spatial range of facilitation was extensive: it remained quite strong
even when the masks were displaced from the test patch by as much as 6
spatial periods of the test grating. Unlike the "snake" detection
experiments discussed above, facilitation did not seem to extend along
smoothly curved paths, but was greatest for collinear alignment (Polat &
Sagi, 1994; see Figure 6.42A--D). In addition to the suppressive
surround, then, there seems to be a second form of contextual
modulation, more specifically structured in space, serving to facilitate
the detection of coherent contours. There may well be a rather delicate
balance between facilitation and suppression (Adini, Sagi, & Tsodyks,
1997), such that one or other predominates under different
circumstances. Solomon and Morgan (2000), for example, confirmed

Collinear facilitation at threshold. Contrast threshold for the central
test patch (at a much lower contrast than shown here) was measured in
the presence of flanking maskers (Polat & Sagi, 1993, 1994). Masks that
were in-line with the test (A, D) improved the visibility of the test
patch, but noncollinear masks (B, C) did not. When additional, lateral
patches were added to the collinear configuration (A), as shown in (E),
the collinear facilitation of test patch detection was abolished
(Solomon & Morgan, 2000).

6. PERCEPTUAL ORGANISATION

the collinear facilitation discovered by Polat and Sagi, but found that
it was abolished by adding more patches in lateral positions around the
test patch (Figure 6.42E). Presumably facilitation and suppression
cancel each other in this case. Chen and Tyler (2001) extended these
studies by testing contrast discrimination, as well as the basic
contrast detection threshold, in the presence of collinear flanking
patches (see Figure 6.38C). They too confirmed that the collinear flanks
facilitated the detection threshold, but found that the flanks made
contrast discrimination worse, not better, over the whole of the visible
contrast range. It sounds odd that a facilitatory process makes contrast
discrimination worse, but it is the converse of the paradox we
encountered earlier, where suppression made discrimination better. If
suppression drives responses down and response gain up (see Figure
6.39), it seems reasonable that facilitation should do the opposite.
Note that when the contrast of a test patch is physically increased then
neural response rates and perceived contrast go up, but contrast
discrimination thresholds are worse (higher). Thus the facilita-

163

tion and suppression effects on a target patch are analogous to the
effects of increasing and decreasing its physical contrast respectively.
The spatial distribution of facilitation and suppression outside the
receptive field has now been mapped out both physiologically and
psychophysically. Kapadia, Westheimer, and Gilbert (2000) used a short
line at the preferred orientation to stimulate the RF of V1 neurons in
alert monkeys, and probed the surround effects with other short lines,
usually in pairs, at many different surround positions (Figure 6.43A).
Facilitation (response enhancement) and suppression were both evoked by
surrounding lines, but in a characteristic spatial pattern (Figure
6.43C) that depended somewhat on the contrasts of the test and
surrounding lines. In a striking confirmation of the "association field"
theory (discussed above; Field et al., 1993), facilitation was found to
be mainly co-axial, while inhibition could be evoked from all around the
RF but especially from lateral positions. In analogous experiments for
human vision, Kapadia et al. (2000) used the tilt illusion to

(A) Arrangement of lines used to map out the contextual interactions
    around the RF (dotted circle) of V1 neurons in alert monkeys. The
    test line lies within the RF while positions of flanking lines were
    varied over a 2-D array of possible locations. Only the lateral and
    collinear cases are shown here. (B) Analogous mapping for human
    vision, assessed through the tilt illusion. The lines were short and
    narrow (8 × 1 min arc) in order to probe the smallest foveal RFs,
    and briefly presented (100 ms) to enhance tilt illusion. (C) Sketch
    of the main regions of facilitation (+) and suppression (−) produced
    by short flanking lines that have the same orientation as the
    classical receptive field (after Kapadia et al., 2000).

164

VISUAL PERCEPTION

assess the contextual interactions around a short vertical test line. It
has long been known that a vertical line will look tilted a few degrees
(say, clockwise) when an adjacent or overlapping line is tilted about 20
degrees the opposite way (Figure 6.43B). This kind of simultaneous
orientation contrast is very like the tilt aftereffect (cf. Chapter 5),
and is the basis for many of the classical geometrical illusions. It can
most plausibly be explained as a result of orientation-specific
inhibition (Blakemore, Carpenter, & Georgeson, 1970), and so Kapadia et
al. used this logic to map out the zones of inhibition around the test
line. The putative inhibitory effect fell away quickly with spatial
separation between test and inducing lines within a range of 30 min arc,
but in addition the tilt effect tended to reverse sign in the co-axial
positions (Figure 6.43B, bottom) when the induction angle was small
(5°). If tilt contrast is explained by inhibition, it seems reasonable
to suppose that the reversed effect ("tilt assimilation") is produced by
facilitation from these coaxial positions. Hence the final map of
contextual interactions from these data looked quite similar in form to
that revealed at the single cell level (Figure 6.43C), even though very
different methods were used for the monkey and human studies. Thus, in
line with our discussion above, Kapadia et al. suggest that this spatial
arrangement plays two key roles in perceptual organisation, by enabling
both contour integration (through facilitation) and texture segmentation
(through suppression) at the single cell level. It is no small matter,
however, to know that suggestions made through verbal reasoning about
neural systems will actually produce the effects envisaged. We are
dealing with a nonlinear, dynamic, interactive set of components, and
intuition alone is a poor guide that needs to be supported by
computational modelling and simulation. Importantly, then, Li (2000) has
shown that a neural network of orientation-specific cells equipped with
an association field of facilitation and inhibition rather like Figure
6.43C can indeed produce the kind of contour integration, feature
pop-out, and texture segmentation that has been proposed. The Gestalt
psychologists speculated 80 years ago that dynamic forces were at work
in

the brain to structure perception according to principles of similarity,
proximity, good continuation, symmetry, good figure, and so on. The
first three in this list appear to be closely related to the properties
of the contextual interactions discussed here. It seems that we are at
last getting some real insight into the mechanisms that yield global
structures from the mass of elementary features that images contain.

Segmenting parts for recognition Marr's (1976) program showed how an
object's occluding contour and internal markings could be assembled from
the collection of more primitive descriptions comprising the raw primal
sketch, on the basis of assumptions that generally hold true in the
world of natural objects. Similar general assumptions can be used to
segment a complex occluding contour into different "part" components---a
problem reminiscent of that originally tackled by Guzman and others with
artificial objects. For example, Hoffman and Richards (1984) provided a
formal computational analysis of the role played by concavities in
contour segmentation. They discussed the transversality regularity:
distinct parts of objects intersect in a contour of concave
discontinuity of their tangent planes. At any point around this
intersection, a tangent to the surface of one part forms a concave cusp
with the tangent to the surface of the other part (concave means it
points into the object rather than into the background; see Figure
6.44). This transversality regularity means that in an image of a
complex shape, "concavities" mark the divisions between the contours of
distinct parts. Concavities can be detected in contours of smooth shapes
by seeking places where there is greatest negative curvature (see Figure
6.46). Hoffman and Richards provided some compelling demonstrations as
evidence for the importance of these concavities in our segmentation of
shapes. They examined a number of classic ambiguous "reversing" figures,
such as the Schröder staircase (Figure 6.45) and the faces-- vase figure
(Figure 6.2), and showed how reversals of these figures are related to
the possible alternative part segmentations. In the Schröder staircase

6. PERCEPTUAL ORGANISATION

165

The "transversality regularity": When two surfaces interpenetrate they
always meet in concave discontinuities. Reprinted from Hoffman and
Richards, Copyright (1984), with permission from Elsevier Science.

The Schröder staircase shows how part boundaries change as the figure
and ground reverse. Reprinted from Hoffman and Richards, Copyright
(1984), with permission from Elsevier Science.

The faces--vase figure. When the vase region is taken as figure, then
the concavities (minima of curvature) divide the vase into a base, stem,
etc. When the faces regions are taken as figure, the concavities reveal
parts corresponding to forehead, nose, etc. Reprinted from Hoffman and
Richards, Copyright (1984), with permission from Elsevier Science.

(Figure 6.45) for example, according to the partitioning scheme, "parts"
of the figure must be "steps", since each of the steps is bounded by two
lines of concave discontinuity. When the staircase is seen in such a way
that the plane marked "x" in Figure 6.45 appears to face upwards, then
the steps that are defined by concave discontinuities pointing into the
staircase are such that planes "x" and "y" are seen to belong together
as faces of the same step. But when the figure reverses, so that the
staircase now lies in the upper right of the picture, and plane "x"
appears to face downwards, the concavities pointing into the body of the
staircase define a different set of steps; planes "x" and "y" now form
faces of different, adjacent steps. In the faces--vase figure (Figure
6.46), when the figure is seen as the vase, then concavities pointing
into it define its parts as the base, stem, and bowl. When the figure is
seen as a pair of faces, then the concavities pointing into them define
parts as forehead, nose, and so forth. This demonstration shows how the
same contour can be "recognised" as two distinct objects: what matters
is the way in which the contour is partitioned prior to recognition, and
this in turn seems to involve a simple search for concavities referred
to the centre of whichever region is seen as figure. Hoffman and
Richards (1984) have shown how the kind of occluding contour that might
result from the application of the Gestalt grouping principles may be
resegmented into its parts. As we will see in Chapter 9, such
segmentation forms an essential stage in Marr's theory of the

166

VISUAL PERCEPTION

analysis and recognition of occluding contours, and at that point we
will return to take up the story of the part structure of objects.

Perceptual grouping and the representation of surfaces Marr saw the goal
of early visual processing as describing the surfaces present in the
scene. The idea of this representation, the "2½D sketch", was to
describe the layout of visible surfaces from the observer's current
viewpoint. Surface elements might be described by their tilt, slant,
curvature, and distance from the viewer (as well as other properties
such as lightness, illumination, surface colour, glossiness, etc.).
Notice how different this is from the image-based descriptions
considered so far, and how it could serve as an intermediate stage
between image-based and object-based descriptions. For example, an
ellipse and a trapezium are 2-D shapes at the image level but might be
re-described as a tilted disc and a tilted rectangle at the surface
level. Similarly, a region containing a luminance edge in the image

might be represented in the 2½D sketch as a surface of uniform colour
with a shadow falling across it. In recent years, Nakayama and
colleagues have marshalled experimental evidence supporting the view
that many early vision operations such as texture segmentation and
motion correspondence can take place at the level of surface
representation, and not necessarily in the 2-D image domain. Of
particular relevance to this chapter are the experiments of He and
Nakayama (1994) on texture segmentation. The task was to discriminate a
target region containing white L-shaped elements from a background
region containing white vertical bars, or vice versa (Figure 6.47A).
Thus target and background regions are distinguished by the presence or
absence of horizontal bars. This relatively simple task could therefore
be done by grouping features in the primal sketch, or by a filter-based
scheme of the kind discussed earlier. Next, by manipulating stereoscopic
cues (see Chapter 7), He and Nakayama (1994) made the "L" and "bar"
elements appear to be an array of

Schematic illustration of the displays used to show that an easy texture
discrimination could be eliminated by creating the appearance of two
homogeneous surfaces, one occluding the other. Segmentation of regions
containing white bars and Ls was initially strong (the experiments used
many more elements than shown here). But when the white elements
appeared to be a uniform field of white squares, partially occluded by
nearer black squares, segmentation was weak. After He and Nakayama
(1994).

6. PERCEPTUAL ORGANISATION

white rectangles occluded by nearer, black rectangles (Figure 6.47B). It
seems to be the surface description ("white rectangles partly occluded
by black rectangles") that was most readily available to the observers,
and at this level there is little to distinguish the target and
background regions. Hence perceived texture segmentation was weak, even
though the same 2-D image features (Ls and bars) were present as in
control conditions. In the control conditions, where segmentation was
strong, occlusion of surfaces was not perceived, and the elements were
seen simply as Ls and bars. These results certainly present a strong
challenge to both the filter-based and feature grouping accounts. They
suggest either that feature grouping can operate at a relatively high
level, at which surfaces and the occlusions between surfaces are
represented, or that feature grouping can be controlled by those higher
levels of representation. It is to the representation of surfaces in
depth that we turn in Chapter 7.

CONCLUSIONS The Gestalt psychologists, through the study of perception
of simple patterns, gave us insights

167

into the organisational principles that may apply to the perception of
the world. The study of natural camouflage and concealment shows that
these principles fare well in describing the utility of various surface
markings of animals. Marr showed how such principles might be
incorporated within a processing model that reveals the structures
hidden in the messy data obtained from natural images. Recent research
has begun to uncover some of the basic mechanisms underlying this
recovery of "global" structure by human and animal visual systems. At a
relatively low level, nonlinear transformations of the outputs of
spatial filters can convert higher-order structures, such as texture
boundaries, into simpler intensity edges whose detection is relatively
well understood. The suppressive surrounds of receptive fields play an
important part in locating those boundaries and making them perceptually
salient. In addition, psychophysical and physiological experiments
confirm the existence of active facilitatory processes that link
elements into coherent contours across visual space. The work of
Nakayama implies that at least some of these processes take place at a
fairly high level where a representation of surfaces in depth has been
established.

Page Intentionally Left Blank

7 Seeing a 3-D World

The problem of how we recover the third dimension was tackled resolutely
by the British empiricist philosophers, notably by Berkeley (1709).
Berkeley's views have come to dominate our thinking on many aspects of
perception. The British empiricists rejected any notion that ideas were
implanted in the mind at birth, saying instead that all complex ideas
had to be built up by the association of simpler ones. Since all
information is received via the organs of sense, ultimately all
knowledge must be achieved by associating simple sensations. It was
assumed that the third dimension must be perceived by associating visual
"cues" with the position of objects felt by touch.

The visual world consists of surfaces extending away into the distance,
and solid objects resting on them at different distances and with their
surfaces inclined differently towards us (Figure 7.1). In addition, the
pattern of light reaching the retina is never static. The eyes, heads,
and bodies of observers move, and objects and animals in the scene being
viewed also move. In this chapter we consider some of the optical
information available to animals which allows them to perceive the
layout of surfaces and the distances of objects, and we consider the
mechanisms in the brain that may serve to analyse and represent that
information.

Perceiving the third dimension The psychology of perception has been
dominated by the apparent paradox of threedimensional vision. As
discussed in Chapter 1, the eye can for some purposes be thought of as a
camera, with the cornea and lens acting to focus light onto a mosaic of
retinal receptors. At any instant of time therefore one can conceive of
the pattern of excitation of retinal receptors as a "picture", curved
around the back of an eyeball. Though curved, the image is essentially
twodimensional, and yet our perception is of a three-dimensional world.
How might depth be recovered?

Convergence and accommodation The primary cues that Berkeley suggested
could become associated with the felt positions of objects were the
different angles of inclination of the eyes (convergence; see Figure
7.2), different degrees of blurring of the image, and different degrees
of strain in the muscles around the lens (accommodation; see Chapter 1,
p. 16). Convergence and accommodation are often listed in introductory
texts as "physiological" cues to depth. Since the degree of convergence
or accommodation is only a single value, these cues could at best
provide information about only one 169

170

A stereo pair of images that illustrate most of the important sources of
3-D information ("depth cues") available in images (except motion, which
is important but cannot be shown). The 3-D structure of the objects and
the scene is clear from each image alone, but extra clarity and realism
emerge when the two images are combined binocularly. \[Note that the
images are arranged for "cross-eyed fusion" (see text): the right eye's
image is on the left, and vice-versa. Pictures courtesy of Joel
Georgeson, rendered using Bryce 5 software.\]

7. SEEING A 3-D WORLD

171

The eyes swing inwards to focus on a near object.

distance at a time. Most research has concluded that in practice they
are minor sources of depth information (e.g., Foley, 1980). Blur, on the
other hand, is a visual cue that varies over the whole image, as a
function of objects' distances from the plane of clearest focus.
Pentland (1987) has shown that blur could in principle be used to
compute an accurate depth map, and he gave evidence that humans do use
it to some extent. There is, however, a much more important and accurate
visual source of information about relative distance for creatures with
binocularly overlapping fields of vision---stereopsis.

BINOCULAR STEREOPSIS Animals with overlapping visual fields have
stereoscopic information available to them from a comparison of the
images obtained at the two eyes. Each eye sees a slightly different view
of the world (Figure 7.1) due to the horizontal separation of the two
eyes. You can confirm this by alternately opening and closing each eye.
Objects at different distances will appear to move together, or apart,
reflecting the horizontal disparity between the two views. This
binocular view must surely be of high value, since it is gained only by
losing the 360° panoramic view enjoyed by animals with side-facing eyes,
such as fish, rabbits, or horses (see Chapter 1, p. 19). Figure 7.3A
shows how the geometry of binocular vision gives rise to slightly
different images in the two eyes. If the two eyes are fixating on a
point P, then the images cast by P fall at the centre

of the fovea in each eye. Now consider a second point Q. If the image of
Q fell (say) 5° away from the fovea in both eyes we should say that Q
stimulated corresponding points in the two eyes, and that Q had zero
disparity. If instead the image was located 6° away from the fovea in
one eye but 5° away in the other, we should say that Q stimulated
disparate, or noncorresponding points and that Q produced a disparity of
1°. In general, if Q's image falls α degrees from the fovea in the left
eye and β degrees from the fovea in the right eye then the binocular
disparity is (β − α), measured in degrees of visual angle. The amount of
disparity depends on the physical depth (d) of Q relative to the current
fixation point P. In fact, disparity is approximately proportional to
this depth difference divided by the square of the distance (v) between
the observer and the point P at which the two lines of sight converge.
Thus disparity increases with the amount of depth, but decreases rapidly
with increasing viewing distance. Disparity has both a magnitude and a
sign. A point Q further than the fixation distance creates uncrossed
disparity. That is, the right eye's view of Q is shifted to the right
relative to the left eye's view (β \> α; see Figure 7.3B). Points nearer
than the fixation distance create crossed disparity (right eye's view is
shifted more to the left; β \< α). Notice how the nearer sphere in
Figure 7.1 is shifted a little to the left in the right eye's view. If
the brain can compute the sign and magnitude of disparity, this will
give precise information about the relative distances of objects in the
world. For most people this is so, but some 2--5% of people show
stereo-anomalous or stereo-blind performance and appear to lack the
mechanisms for

172

VISUAL PERCEPTION

Geometry of binocular vision. The eyes fixate point P, whose image thus
falls on the centre of the fovea in each eye. Point Q, further away than
P, is imaged on noncorresponding or disparate points in the two eyes.
The disparity ( − ) produced by point Q is measured by the discrepancy
between the two image locations, and is measured in degrees of visual
angle. Disparity is approximately proportional to depth, d, but
inversely proportional to the squared viewing distance. (A) shows a plan
view, from above; (B) shows the field of view seen by each eye.

processing either crossed or uncrossed disparities (Richards, 1971).
Because disparity decreases with squared distance, the value of stereo
vision must be greatest in the near space around an animal. For example,
increasing an object's distance from 2m to 20m decreases the disparity
by a factor of 100. Hence far objects will tend to yield disparities
that are too small to be detected. Also, to interpret image disparity in
terms of real depth (in metres, or feet and inches), the distance must
be taken into account, since a given disparity for a far object
represents a much greater depth than for a near object. The convergence
cue (see above) may play

an important supporting role here in scaling disparity information to
recover real depth.

Encoding disparity and depth How might disparity be sensed? It requires
a comparison of information from the two eyes, and so retinal cells
clearly cannot do it. Left and right eye information remains segregated
in different layers of the LGN, and the earliest stage of processing
that shows binocular responsiveness is the primary visual cortex (V1),
where many simple and complex cells are driven by input from both eyes
(see Chapter 3, p. 51). Among these cells, some respond maximally when
their optimal

7. SEEING A 3-D WORLD

stimuli fall on disparate areas of the two retinas (e.g., Barlow,
Blakemore, & Pettigrew, 1967; Blakemore, 1970; Hubel & Wiesel, 1970). A
cell selective for disparity would respond most strongly to a stimulus
lying at a particular distance, or within a range of distances, from the
eyes, and so could help to code depth information. These early accounts
of disparity selectivity suggested that for a binocular cortical cell
the receptive fields mapped out for the left eye and for the right eye
were similar in their size and orientation preference but had slightly
different retinal positions. For example, if the RF for the right eye
were shifted a little to the right of the left eye's RF position, then
maximum stimulation of the cell would be given by a feature that had
uncrossed (far) disparity (cf. Figure 7.3B) which fitted well with the
disparity of the two receptive fields. When stimulus disparity did not
match the RF disparity, the left and right eyes' images would not
stimulate the RFs at the same time, and so the net response would be
less. In this way the cell's response would be "tuned" for disparity by
virtue of the offset between its left and right eye RF positions. It is
now well established that disparityselective cells are found in many
visual areas including the primary visual cortex of both cats and
monkeys (Barlow et al., 1967; Poggio & Poggio, 1984; DeAngelis, Ohzawa,
& Freeman, 1991; Ohzawa, DeAngelis, & Freeman, 1996; see DeAngelis,
2000, for a recent overview). The experiments of Poggio and Fischer
(1977) were especially important in establishing the depth sensitivity
of cortical cells recorded from trained, alert monkeys. The animals were
presented with moving-bar stimuli on a screen placed at different real
distances and four patterns of response were identified. Tuned
excitatory cells responded vigorously to appropriate stimuli at or close
to the fixation distance and were often inhibited by further or closer
stimuli. Tuned inhibitory cells responded to a wide range of depths but
were inhibited around the fixation distance. Near cells were excited by
stimuli nearer than the fixation distance, but inhibited beyond it, and
"Far" cells showed the opposite pattern to "Near" cells.

173

As a code for disparity, the firing rate of a single V1 or V2 cell is
ambiguous because it varies with factors such as contrast and speed as
well as disparity. However, a comparison of the activity within a
population of such disparity-sensitive cells could serve to encode
disparity unambiguously. To see how the different factors in a cell's
response might be disentangled, let us take a simplified example.
Suppose a Far cell's response RFar was proportional to (1 + disparity),
but scaled by various other factors (contrast, speed etc), while a Near
cell's complementary response was proportional to (1 − disparity) and
other factors. Thus: RFar = (1 + disparity) × (other factors) RNear = (1
− disparity) × (other factors).

If we take the ratio of responses, the other factors cancel out, and
disparity can be recovered. Simple algebra then shows us that: disparity
= (RFar -- RNear)/(RFar + RNear).

In this schematic example, disparity is recovered as the contrast
between the two cell's responses. In general, the required computation
will depend on the particular relation between disparity and response,
but the point is that appropriate comparisons between cells can recover
unambiguous information, even though individual cells have many pieces
of information contributing to their response level. This is an
important point that applies to any domain of sensory coding.

Stereopsis and single vision Turning now to psychological evidence, the
importance of disparity information for the perception of depth can
easily be demonstrated. It is possible to create strong depth
impressions from pictures by sending to each eye separately the view
that the eye would see if an actual object in depth were presented
(Figure 7.1). Wheatstone (1838) is usually attributed with the invention
of the first stereoscope, shown in Figure 7.4A. He drew the view of an
object as it appeared to each eye (Figure 7.4B), and then with an
arrangement of mirrors sent the left eye view to the left eye of

174

VISUAL PERCEPTION

an observer and the right eye view to the right eye. The result was that
the observer saw a "solid" object, in depth. It is possible to arrange
such stereo demonstrations in a number of ways, all of which depend on
separating out the left and right eye views, and then sending these
separately to each eye. A common technique is to use anaglyphs where one
view is drawn in red and one in green, and the two superimposed. The
viewer looks through glasses containing a red filter to one eye and a
green filter to the other, so that only one of the images is passed to
each eye. The resulting perceptions of solid objects in depth are of
course illusory. No actual solid object exists, but the disparities that
would exist if an object were present have been captured in the
anaglyphs. The brain thus receives the binocular information that it
would receive if an actual 3-D object were presented, and the phenomenal
impression reflects this. Frisby (1979)

(A) Wheatstone's stereoscope (1838). A picture of an object is drawn as
    it would appear to the left eye (I1) and to the right eye (I2). If
    these two images are sent to each eye separately, via mirrors M1 and
    M2, an observer sees a single object in depth. Adapted from Boring
    (1942). (B) Stereogram of the type used by Wheatstone. With
    crosseyed fusion (see text) it should appear in depth as a truncated
    pyramid.

provides numerous examples of anaglyphs and his book is well worth
consulting. In a similar way, stereoscopic movies can be very effective
when cross-polarised filters are used to project the left and right
views onto an aluminised screen, and the viewer wears cross-polarised
glasses. Modern computer graphics systems can display excellent stereo
images by alternating the left and right views frame-by-frame while the
viewer wears synchronised LCD shutter glasses that allow each eye to see
only the appropriate images. In this book, we present various
stereograms for "free fusion". A stereo pair of images is presented
side-by-side, as in Figure 7.4B, and by converging the eyes in front of
the page, the right eye can fixate the left image while the left eye
fixates the right. Some practice is needed to acquire this trick of
cross-eyed fusion, but for students of vision it is a skill worth
acquiring. In the early stages, fix your gaze on a pencil held in front
of

7. SEEING A 3-D WORLD

the page, and vary its distance from the page until fusion is achieved.
One sign of this is seeing three images: a central binocularly fused
one, flanked by the two monocular views. Ignore the monocular views.
While it seems relatively easy to appreciate how disparity in the images
to the two eyes might be computed, and there is evidence that cells in
the cortex may indeed measure disparities, it is less easy to appreciate
why our phenomenal impressions of objects in the real world, or of
illusory forms in a stereoscope, are of single objects. If you focus on
a pen held near your eyes, you will experience double images
("diplopia") of more distant objects, but within a certain range of
distance a single view is seen, known as "fusion" or single vision. If
the eyes fixate a given point, the region of space within which single
vision is possible is known as Panum's fusional space, and the
corresponding range of disparities over which single vision holds is
Panum's limit. The classical value for Panum's limit is a disparity of
about 0.1°. This means that if a point or line is at the fovea in one
eye, and we shift its partner more than 0.1° off the fovea in the other
eye, single vision will give way to diplopia. This limit is all the more
surprising when translated into real depth. If we view an object at 57
cm (about arm's length), the fusional space has a depth of only ±9 mm!
Taken at face value, this would imply that almost all views of a 3-D
scene should appear as double. Why does the visual world not fall apart
in this way? Several factors play a part in our current understanding. •
Attention. We fixate on, and attend to, the object of interest. Perhaps
we simply don't notice the diplopia present elsewhere? This might be so,
but there are also more specific structural and dynamic effects at work.
• Retinal eccentricity. Panum's limit is larger in peripheral vision
than in central vision (Ogle 1964), probably reflecting the larger size
of receptive fields in the periphery, and so the fusional space is also
larger away from the fovea. • Image size. The classical experiments on
Panum's limit were done with very small targets---

175

dots or thin wires---for which the limit is indeed small. However, more
recent research has revealed an important size--disparity correlation.
The disparity limit for fusion is much larger for broad, blurred targets
(low spatial frequencies) than it is for narrow targets containing much
higher spatial frequencies. Indeed, for blurred bars wider than about 10
min arc the disparity limit for fusion increases in direct proportion to
the width of the target (Schor, Wood, & Ogawa, 1984). The minimum
disparity that will just produce a sense of depth also increases with
width in the same way (Schor & Wood, 1983). The size--disparity
correlation was confirmed by experiments evaluating contrast sensitivity
for stereopsis. Smallman and MacLeod (1994) measured the lowest contrast
at which disparity (and presumably depth) could just be discriminated,
for a series of spatially filtered textures. They found that for large
disparities (10--20 min arc) sensitivity was greatest for coarse, low
spatial frequency textures (1 c/deg) while for small disparities (1--5
min arc) sensitivity peaked at increasingly high spatial frequencies
(5--10 c/deg). These relations between size (spatial frequency) and
disparity can be understood in the context of a multi-scale or
multiple-filter model, with receptive fields of different sizes at the
same retinal location (Freeman & Ohzawa, 1990). If receptive field
disparities (discussed above) are roughly proportional to receptive
field size then the system will be able to handle much larger
disparities for large, fuzzy targets because they are processed by
larger receptive fields. Ferster (1981) found evidence of this kind in
the cat's visual cortex. This multi-scale aspect plays an important part
in current models of stereo vision, discussed later. • Dynamic factors?
The classical Panum's limit was measured with targets that were not only
narrow, but also stationary. Until fairly recently, it was thought that
the range of disparities for which single vision holds can be much
larger if disparity is slowly increased from a small, fused value. That
is, Panum's limit stretches over time (Fender & Julesz,

176

VISUAL PERCEPTION

1967). The effect appeared to be important for natural vision where we
look from one object to another at a different distance. If the first
object is seen as fused (single vision), and we change our convergence
to fixate a second object, then the disparity of the first object
increases but fusion of it may be maintained as Panum's limit supposedly
stretches. This dynamic effect would serve to extend the range of fusion
in normal viewing conditions. Unfortunately for this neat theory,
careful work by Erkelens (1988) implies that the stretching effect does
not exist. He confirmed that the fusional range was indeed larger for
increasing disparity than for decreasing disparity, but he added a vital
control condition. The fusional range for increasing disparity was no
bigger than for stereograms simply switched on at a fixed disparity.
This leaves no room for a dynamic stretching effect. The greater
difficulty in the decreasing condition appeared to be caused by the
period of binocular rivalry that preceded re-fusion. Nevertheless,
fusion and stereopsis do have important temporal characteristics: human
vision is poor at picking up rapidly changing disparity. Schor and Tyler
(1981) used line stimuli whose disparity (and perceived depth)
oscillated back and forth over time. They found that Panum's limit for
single vision was much smaller for disparities that changed rapidly
(above about 0.5 cycles/sec) than for slower changes. In the language of
filtering, the fusion process behaves as a low-pass temporal filter.
Perceived stereoscopic motion in depth is restricted to slow changes in
rather the same way.

How is binocular matching achieved? Most theories of stereo vision have
argued that in analysing binocular disparity the visual system must
determine which parts of one eye's image correspond to particular parts
in the other eye's image. After all, the disparity between two randomly
chosen features is completely arbitrary; it is only the disparity
between corresponding features that is meaningfully related to depth in
the world. Measuring the sign and magnitude of dis-

parity therefore seems a relatively easy task once the correspondence
problem has been solved---but we shall see later that
"correspondenceless" algorithms have also been developed. It used to be
thought that the spatial forms presented to each eye were recognised
independently and then the images were matched and fused. For example,
Sherrington (1906, p. 380) suggested: During binocular regard of an
objective image each uniocular mechanism develops independently a
sensual image of considerable completeness. The singleness of binocular
perception results from the union of these elaborated uniocular
sensations. The singleness is therefore the product of a synthesis that
works with already elaborated sensations contemporaneously proceeding.
However not all agreed with this---particularly given the strange
results that could be obtained using stereoscopes. For example, Darwin
received a communication from A.L. Austin in New Zealand (Galton, 1907,
p.227): Although a perfect stranger to you, and living on the reverse
side of the globe, I have taken the liberty of writing to you on a small
discovery I have made in binocular vision in the stereoscope. I find by
taking two ordinary carte-de-visite photos of two different persons'
faces, the portraits being about the same sizes, and looking about the
same direction, and placing them in a stereoscope, the faces blend into
one in a most remarkable manner, producing in the case of some ladies'
portraits, in every instance, a decided improvement in beauty. Darwin
passed this information on to Galton, who confirmed these observations.
Ross (1976) suggests that Galton disagreed with the monocular
combination explanation because the binocular perception of two
different faces was so unlike an "optical" combination of the faces. A
more serious challenge to contemporary versions of the recognition and
fusion theory

7. SEEING A 3-D WORLD

(e.g., Ogle, 1964) came with the important work of Bela Julesz in the
1960s, summarised in his 1971 book. Julesz developed random-dot
stereograms as a tool to explore the processes of stereopsis. A
random-dot stereogram is shown in Figure 7.5. Both members of the stereo
pair consist of a uniform, randomly generated texture of black and white
dots. There is no recognisable form present in either member of the
pair. However if the stereogram shown in Figure 7.5 is viewed in a
stereoscope, or with free fusion (described earlier), the viewer sees a
central square of texture floating above the background. This is because

A random-dot stereogram of the kind devised by Julesz. If this pair were
viewed in a stereoscope, a square would be seen floating above the
background. Photograph courtesy John Frisby.

177

the stereogram in fact contains disparate "forms" camouflaged by the
background texture. Each half of the stereogram contains identical
background elements. The central regions, corresponding to the perceived
square, also match, but are displaced sideways as shown in Figure 7.6.
The gaps that remain after this lateral shifting are then filled in with
more texture. Thus the eyes are presented with the disparities that
would be present if an inner square of texture were actually held above
the background texture. This important demonstration makes it difficult
to maintain any simple theory of stereopsis that depends on the
recognition and fusion of monocular contours. It could be argued that
local patterns or clusters of black and white dots are detected and
matched, but Julesz (1971) argued against this. For example, fusion can
be achieved from stereograms in which one member of the pair is blurred,
reduced in size, or has "noise" added to it such that local patterns are
disrupted. Julesz proposed instead that stereopsis proceeds on a
point-by-point comparison of dots of the same brightness polarity (white
or black). He argued that polarities must be the same since fusion
cannot be achieved if one member of a random-dot stereo pair is reversed
in contrast, i.e., if black dots in one image correspond to white dots
in the other. The idea that individual dots are the basis of binocular
matching leads to an enormous

(a) and (b) are the two halves of a random-dot stereogram shown in
    simplified form. Both have the same surrounding texture (S). A
    central region (A) has been shifted to the right in (a) and to the
    left in (b). The gaps left have been filled in with more texture (X
    and Y). If such a stereogram were constructed, and viewed in a
    stereoscope, the central square would be seen floating above the
    background, as shown at (c). Adapted from Julesz (1965).

178

VISUAL PERCEPTION

correspondence problem. Any given dot in one eye might be matched with
tens or hundreds of dots in the other eye. To see just how big a problem
this creates, consider a small array of 4 dots in each eye. As Figure
7.7 shows, each dot in one eye might be matched with any of the 4 dots
in the other eye, yielding 16 possible locations in space from which the
images could have arisen. Four are "true", the other 12 are "ghosts" or
"false fusions". The correspondence problem is in principle much worse
than this, however, since we should also consider the number of
different arrangements of dots that might be chosen as a solution. For
each dot in the left eye there are 4 choices in the right eye. Thus, in
the absence of any constraints, the number of different matching
arrangements is 4 × 4 × 4 × 4 = 256 solutions to the correspondence
problem for only 4 dots. For a modest 10 × 10 array of dots there would
be 1010 solutions for each row, implying no less than 10100 different
possible solutions. In practice observers rapidly reach the same, simple
solutions to such stereoscopic puzzles, and most theorists suggest that
the problem is rendered less severe by adopt-

Both eyes look at four dots, but the correspondence between the two
retinal projections is ambiguous. Each of the dots L1 to L4 could match
any of the four projections of dots R1 to R4. "False" matches are shown
as open circles in the projection field. Correct matches are shown as
solid circles. Adapted from Marr and Poggio (1976).

ing plausible constraints on what can be matched. For example, things
can't be in two places at once, so the uniqueness constraint allows any
dot to enter into only one pairing (see below). Julesz (1971) proposed
that the correspondence problem is solved by a global stereopsis
mechanism, which selects a match based on the most uniform set of
disparity measurements (p. 150): Thus, to obtain local stereopsis of a
few edges or dots, one can visualize how the binocular disparity units
will maximally fire for similar receptive fields in the two retinae of
the same shapes, orientations and retinal co-ordinates. On the other
hand, for complex textured surfaces, another level of neural processing
has to be evoked that evaluates the possible local solutions and selects
the densest firing units of the same disparity. This processing I will
call global stereopsis, and it is on another level of complexity from
the commonly quoted local stereopsis of the textbooks.

7. SEEING A 3-D WORLD

Julesz (1971) outlined a mechanism for global stereopsis, couched in
mechanical terms, where neural units for the left and right eyes were
represented by two arrays of magnets, coupled laterally by springs. The
model was ingenious, but qualitative and untested. However, the idea
that arrays of units co-operate to achieve global binocular matching was
influential, and was taken up by Marr and Poggio (1976), who developed a
stereo algorithm based on co-operativity between units. In their theory,
they took the crucial further step of considering what constraints could
be provided by physical properties of the world and used in a theory of
stereopsis (Marr, 1982, p. 112): 1) A given point on a physical surface
has a unique position in space at any one time and 2) matter is
cohesive, it is separated into objects, and the surfaces of objects are
generally smooth in the sense that the surface variation due to
roughness, cracks, or other sharp differences that can be attributed to
changes in distance from the viewer, are small compared with the overall
distance from the viewer. These physical considerations give rise to
three constraints in the matching of stereo images. First, a pair of
candidate image elements to be matched must be physically similar if
they are to have originated from the same point on an object's surface
(the compatibility constraint). Second, any item in one image should
match only one item in the other image (the uniqueness constraint).
Finally, the disparity should vary smoothly almost everywhere in an
image (the continuity constraint). These matching constraints can be
successfully embodied within an algorithm that can "solve" random-dot
stereograms (Marr & Poggio 1976). The solution gradually drops out of a
network of excitatory and inhibitory connections that take account of
these three constraints simultaneously. Each unit in the network
represents a surface feature or patch at a particular depth. Each
excited unit can in turn excite or inhibit the activity of other units,
and have its own activity increased or decreased in turn by the
excitation

179

and inhibition it receives from other units. Marr and Poggio (1976)
showed how the matching constraints could be embodied in the patterns of
excitation and inhibition in such a network. First, the compatibility
constraint means that a unit will only be active initially if it is
excited by similar features from both eye's inputs (e.g., both are black
dots). Second, the uniqueness constraint is embodied by inhibition
passing between units that represent different disparities at the same
position. Finally, the continuity constraint is embodied by having
excitation pass laterally between units that represent adjacent features
at the same disparity. Thus unique matches that preserve areas of
similar disparity are encouraged, while false fusions are suppressed.
Marr and Poggio showed that a simulation of such a network "solved"
random-dot stereograms. For the stereogram of Figure 7.5, the pattern of
excitation of the network gradually settled down to a state where
adjacent units in a square-shaped region were active at a disparity
distinct from that associated with units activated by the background
dots. Marr and Poggio's (1976) algorithm provided an early example of
how considering natural constraints can lead to a workable solution, and
it pioneered the application of neural networks to vision. The algorithm
was co-operative and based on the matching of dots---both strongly
emphasised by Julesz---but it soon became clear that stereo vision can
operate at multiple spatial scales, not just at the level of small dots.
For example, Julesz and Miller (1975) showed that high spatial frequency
"noise" added to stereograms did not disrupt stereopsis obtained from
lower spatial frequency components, provided there was no overlap in the
spatial frequencies of the noise and the stereo image. This was one of a
number of demonstrations that stereo matching may proceed within
independent spatialfrequency tuned channels. Marr and Poggio (1979)
therefore devised a second stereo algorithm in which the image is
analysed by a set of channels of successively finer resolution. The
inputs to this stereo matching algorithm are edges---or more precisely,
the zero-crossings obtained using spatial filters of different widths

180

VISUAL PERCEPTION

(see Chapter 5). To satisfy the compatibility constraint, candidate
matches must be zero-crossings of the same sign and orientation. The
broadest width filters produce relatively sparse zerocrossings because
finer structure is blurred out (see Figures 5.8, 5.9). Provided that
matches are tolerated only up to a disparity limit equal to about half
the width of the filter, then because the zero-crossings are sparse, the
probability of false matches is small. The scheme thus exploits the
size--disparity correlation (see earlier) to minimise the ambiguity of
matching. The algorithm rather neatly sidesteps, rather than solves, the
correspondence problem. Matching in this scheme proceeds from coarse to
fine. Once a match has been achieved within a low spatial frequency
channel, this controls vergence movements of the eyes, reducing the
disparity and causing finer channels to come into correspondence.
Interim results are held in a temporary memory, termed the 2½D sketch.
The model was implemented and tested extensively by Grimson (1981), who
also extended the model by adding processes that would interpolate a
smooth surface description from the sparse ZC points at which disparity
was measured. Even without surface interpolation the model performed
well on many natural and random-dot images. Importantly, it succeeded on
those stereograms where one image was blurred or partly restructured
(discussed above) which had led Julesz to propose global, co-operative
stereopsis. In a multi-scale, coarse-to-fine model then, co-operativity
between disparity-sensing units may not be essential to stereo matching.
Additional constraints on the stereo-matching process were incorporated
in models by Mayhew and Frisby (1981) and by Pollard, Mayhew, and Frisby
(1985). The latter aimed to accommodate the results of Burt and Julesz
(1980) who found that human binocular fusion is limited not simply by an
upper limit on the disparity of individual points, but by an upper limit
on the gradient of disparity across space. The disparity gradient is
closely related to the slant of a surface in space, relative to the
observer's line of sight. If the slant in depth is too steep, fusion is
not possible. Burt and Julesz found that observers could not achieve

fused, single vision of two points in space if the disparity gradient
between them (defined as the difference in disparity divided by the mean
angular separation between the points) was greater than about 1. Pollard
et al. (1985) argued that a binocular combination rule that tolerates
matches up to a disparity gradient of 1 would preclude false matches and
implicitly satisfy a constraint of surface continuity or smoothness.

Disparity from spatial phase Almost all models of stereo vision share
the broad view that some set of "things" (dots, edges, or in the 19th
century complete objects) is located in the left and right images, a
correspondence is established between the two sets, and then the
distance (disparity) between corresponding things is measured. An
alternative concept has emerged in recent years, based on the idea
already touched upon several times in this book, that cells in early
vision act as spatial filters. The approach is quite radical, since the
locations of "things" are not made explicit, and distances as such are
not measured. The heart of the idea is that even- and oddsymmetric
receptive fields (described earlier; see Figure 3.5) can be used
together to sense changes in the position (or phase) of an input signal.
To see how, consider Figure 7.8 in which even and odd fields centred on
a fixed position (P) are shown superimposed on a grating stimulus. In
panel A the grating bar is centred at position P, and the even field is
stimulated strongly, while the odd field's response is zero (panel D,
left). If we shift the grating by a quarter or half a bar width to the
right (panels B,C), the even field's response drops while the odd
field's response increases. Thus the balance between the even and odd
responses (panel D) varies directly with the displacement of the
grating. Moreover, a particular ratio of responses specifies the
position (phase) of the grating relative to point P. Although the even
and odd responses would both increase with the stimulus contrast, this
conveniently cancels out in the ratio, making it depend on position but
not contrast. Binocular disparity is a shift in image position between
the eyes, and so a simple algorithm for

7. SEEING A 3-D WORLD

181

How binocular disparity can be recovered from phase information, using
even and odd receptive fields. (A) The even field is stimulated best
when a grating bar is centred on it. (B,C) The even response drops, and
the odd response rises, when the grating is shifted to the right by up
to half a bar width (a 90° phase shift; cf. Figure 2.7). The odd field
is stimulated best when an edge is centred on it. (D) The balance (and
ratio) of the two responses varies smoothly with the position (phase) of
the grating. If such a position code was extracted separately for the
left and right eyes then the binocular disparity could be given by the
difference between the codes for each eye at corresponding points P.

recovering disparity at any point P is to extract the position codes
(response ratios) for each eye separately, and then compare them by
subtraction (Sanger, 1988). In symbols, if the even and odd filter
responses at any position x are eL(x), oL(x) for the left eye and eR(x),
oR(x) for the right eye, then the monocular phases ΦL(x), ΦR(x) are
given by tan\[ΦL(x)\] = oL(x)/eL(x), tan\[ΦR(x)\] = oR(x)/eR(x),

and phase disparity = ΦR(x) -- ΦL(x). The differ-

ence in phase between the two eyes is proportional to the spatial
disparity; for example, a 90° phase difference represents a spatial
disparity of half a bar width. The idea of stereo from phase disparity
has two main advantages. First, matching features or items do not have
to be identified, and so there is no correspondence problem. Second, the
computation can be carried out at all locations, not just at
zero-crossings or peaks of filter output, so that a dense, rather than
sparse, set of disparity values tends to be produced, leading more
easily to a surface description. One may wonder whether the logic of
comput-

182

VISUAL PERCEPTION

ing stereo from phase disparity holds for natural vision where the input
is a complex image rather than a periodic grating. Computational
experiments and analysis suggest that, although there are complications
(e.g., Fleet, Jepson, & Jenkin, 1991; Langley, Atherton, Wilson, &
Larcombe, 1990), the approach can still be applied. Since the spatial
filters in the visual cortex respond best to a particular orientation
and spatial frequency, the output of a given filter in response to a
complex image is actually much more periodic and "grating-like" than its
input (cf. Figure 5.2), so that the local phase of the output is usually
well defined and can be recovered from the ratio of odd and even filter
responses, as required. To test this approach, Sanger (1988) implemented
a multi-scale algorithm for phase disparity using Gabor filters of three
or four different sizes. Each filter delivered its own disparity
estimates, but the weight or confidence attached to these estimates
varied across space and across filters. For example in a blurred region
of the image, the larger filters would be given greater weight. Each
filter has a disparity limit proportional to its receptive field
size---i.e., the size--disparity correlation (p. 175) appears again. The
final output was a confidence-weighted average of the estimates taken
across all filter sizes. This worked well on a variety of random-dot
stereograms and natural images, but like other models it had difficulty
where changes in depth were very steep. This is because at steep depth
edges, parts of a rear surface are visible to one eye but not the other,
and so disparity cannot be defined in these regions. The role of such
occlusions in stereo vision is discussed later (p. 185). We saw earlier
that psychophysical work strongly supports a multi-scale model of
stereopsis, and some of that evidence bears on the idea of phase
disparity. Several studies have found that the smallest disparity for
which depth can be discriminated with sinusoidal gratings represents a
constant phase shift of about 4--6° (1 to 1.5% of the grating period),
at spatial frequencies below about 3 c/deg (Legge & Gu, 1989; Schor &
Wood, 1983). Likewise the largest disparity for which single vision held
good was about a 90° phase shift for frequencies below 3 c/deg (Schor et
al.,

1984). On the other hand, for higher spatial frequencies these disparity
thresholds were approximately a constant spatial (not phase)
displacement. Thus the little evidence so far is rather mixed, and more
crucial tests have yet to be conducted. Nevertheless, the role of phase
disparity must be taken seriously, because physiological evidence for
this type of coding is now strong. First, we know that cortical simple
cells can be well described as Gabor filters (see Chapter 5) and that
filters with a 90° phase difference tend to co-occur at a given cortical
location (Pollen & Ronner, 1981). Second, R.D. Freeman and colleagues
(DeAngelis et al., 1991; Freeman & Ohzawa, 1990) have plotted very
accurately the spatial receptive field structure of binocular simple
cells in the cat. While some cells have nearly identical fields for both
eyes (Figure 7.9, Cell 1), other cells have fields of similar size,
orientation, and periodicity, but with clearly different phases in the
two eyes (Figure 7.9, Cell 2). For cells tuned near to vertical the
interocular phase disparities ranged from 0 to almost 180°, while cells
with nearhorizontal receptive fields had disparities close to zero. This
suggests that vertically tuned cells play the greatest part in encoding
horizontal disparities for stereo vision. These disparity-sensitive
simple cells probably feed on to complex cells, possibly in several
stages that serve to reduce the cells' dependence on other factors such
as position and contrast, and make responses more specific to disparity
per se (Ohzawa, et al., 1990, 1996). Freeman and Ohzawa (1990) argue
that the Near, Far, and Tuned cells of Poggio and colleagues (see
p. 173) can be better interpreted as cells with different interocular
phase disparities and different spatial scales. This way of describing
binocular cells copes better with the existence of cells whose
properties are intermediate between the Near, Far, and Tuned "types"
(Ferster, 1981; LeVay & Voight, 1988). Are these physiological findings
consistent with the computational models of phase disparity coding? At
first glance it might seem not, since the models combine even and odd
filters within each eye to make monocular phase explicit, and then
compare phase between the eyes, while binocular

7. SEEING A 3-D WORLD

simple cells appear to combine even and odd filters between the eyes
first. Thus it does not seem likely that the hierarchy of cortical cells
follows the same steps as the model. However, this is a nice example of
the need to distinguish between an algorithm and its implementation
(Marr, 1982). The computational goal is to recover phase difference, and
in fact it is straightforward to show that from a common starting
point---the assumption of even and odd filters for the left and right
eyes---the same result can be calculated in several different, but
equivalent, ways. The physiological results of Ohzawa et al. (1990,
1996)

183

strongly suggest that a chain of calculations is carried out by the
simple-to-complex cell sequence that does not make monocular phase
explicit but could readily compute phase difference. In short, the
computational theory of stereo vision from phase difference is
supported, and a specific implementation is suggested by the
physiological findings. The task of computing positional change is
common to stereopsis and motion analysis, and there are many parallels
to be drawn between the two processes. In Chapter 8 we consider in more
detail how the sequence of simple and complex

Phase disparity. Receptive fields of binocular simple cells plotted
separately with stimuli given to the left and right eyes. (Top) A cell
for which the receptive fields were well-matched in size, orientation,
and phase of the receptive field's on and off subregions. Middle:
Receptive fields were wellmatched in size and orientation but differed
in phase. Thus this cell exhibited a phase disparity. Data from Freeman
and Ohzawa, 1990. (Bottom) Shows how the phase and phase disparity can
be quantified. Taking a Gabor function to model the receptive field (see
Figure 3.5), the best-fitting phase of the underlying sine-wave can be
found. (Inset) the profile of the Gabor function. On the left the Gabor
model has been quantised into large pixels to match the sampling of the
real data above.

184

VISUAL PERCEPTION

cells may use even and odd filters to compute direction and speed of
motion in a way that is closely allied to the computation of disparity
discussed here.

Stereo depth and occlusion cues Having surveyed a variety of
well-developed models for stereopsis, we now turn to the relation
between stereo and other cues for depth. Many of the computational
accounts we have discussed assume that stereopsis is achieved prior to,
or in parallel with, the elaboration of the features present in the
image, and is therefore a relatively lowlevel process. The clarity of
depth in random-dot stereograms, where familiarity and all other depth
cues are eliminated, encouraged the view that stereo vision operates as
an independent module, uninfluenced by other processes. However, several
demonstrations show that other factors can override stereo information.
In "pseudoscopic" viewing the left and right eyes' views are
interchanged by optical means. This reverses all the disparities, and so
one might expect perceived depth always to reverse. This is true for
random-dot stereograms, and for simple depictions of wire-frame objects,
but if a photographed scene is shown pseudoscopically, one rarely sees
trees, cars, and buildings inside-out. Stereo vision is being overridden
here, either by other depth cues such as occlusion, perspective, and
shading (see later) or by the knowledge we have of familiar objects.
Gregory's (1973) "hollow face" illusion (see Figure 7.10) is similar. If
a hollow mask of a face is viewed from a distance of a few feet, the
impression is of a normal face, with the nose nearer to the observer
than the forehead. Only at very close range indeed does the stereoscopic
information dominate, to make the mask look hollow. Hill and Bruce
(1993, 1994) have shown experimentally that both the familiarity of
faces and a general preference for convexity tend to favour the
illusory, face-like interpretation of the hollow mask. There is also
increasing evidence that other processes can not only veto stereopsis
but also cooperate with it. Harris and Gregory (1973) and Ramachandran
and Cavanagh (1985) produced stereograms in which each eye was
stimulated

This is a picture of a hollow mask, illuminated from behind. In real
life, as in this photograph, we see a normal face, with the tip of the
nose nearer to us than the eyelids. Photograph by Sam Grainger.

with disparate illusory contours (Figure 7.11; cf. Chapter 5, Figure
5.20). When the inducing edges had crossed disparity (hence stood
forward) illusory contours were observed completing the gaps between the
sectors. Subjects could see the illusory square floating above its
background but did not see it as readily if the disparity suggested it
should be behind the background. This supports the view that these
subjective contours occur specifically as representations of occluding
contours. They can be evoked by monocular occlusion cues such as the
terminations of lines and edges, and they appear to be reinforced by
disparity cues consistent with the occlusion interpretation, but to be
suppressed by the opposite disparity cues.

7. SEEING A 3-D WORLD

185

(a) Disparity between left and right illusory contours can produce a
    stereoscopic image of a square floating above its background. When
    the illusory contours are superimposed on a repeating "wallpaper"
    pattern, the floating square "captures" the pattern. (b) The same
    "capture" occurs even when the background is of continuous lines.
    The stereoscopic fusion of the disparate illusory contours overrides
    both the continuity and zero disparity of the background lines. From
    Ramachandran and Cavanagh (1985). Reprinted with permission from
    Nature (Vol. 317, pp. 527--531) © Macmillan Magazines Limited.

The representation of stereo depth and of occlusion appear to be
intimately linked. We saw earlier that all models of disparity
processing have difficulty with unpaired regions of an image. These
occur where one eye sees part of a surface, but the other eye's view is
blocked by another surface in front---an occluding surface. Although
their disparity is indeterminate, unpaired regions are perceived as
lying in the same plane as the more distant surface. Shimojo and
Nakayama (1990) realised that the geometry of occlusion imposes a
particular relation between the monocular (unpaired) and binocular
regions. At a step edge in depth (Figure 7.12A), if the nearer surface
lies to the right of the rear surface then an unpaired region is seen by
the left eye, while if the nearer surface lies to the left, the unpaired
region is seen by the right eye. Both these situations are valid in
terms of ecological optics, as they arise from viewing a 3-D world. On
the other hand, if an unpaired region is seen by the right eye in the
first situation (nearer surface to the right), then this is invalid and
cannot arise from viewing an arrangement of surfaces in depth. Shimojo
and Nakayama (1990) found that valid unpaired regions were seen at the
depth of the rear surface

and suffered little binocular rivalry or suppression, while similar
invalid regions appeared in front of the rear surface and were
frequently suppressed by rivalry. A demonstration of these stereo
effects is given in Figure 7.12B. The grey crescents are unpaired
regions, and for cross-eyed fusion the upper left and lower right
crescents are the valid ones. They should appear stable and not subject
to rivalry, unlike the other two crescents. The finding that rivalry
takes account of occlusion relations implies that there is more to
stereopsis than the process of binocular matching and disparity
analysis. It has only recently been realised that the analysis of
half-occlusions (unpaired regions) is an important component of
binocular stereopsis. Anderson (1994, p. 365) argues that disparity and
occlusion are complementary sources of information: "While disparity
provides relative depth information about surface features visible to
both eyes, half-occlusions provide information to segment the visual
world into coherent objects at object boundaries." Further discussion of
this is given by Anderson and Nakayama (1994), and Figure 7.12C,D gives
even more striking demonstrations of occlusion relations playing a part
in stereopsis itself. In C, the

186

VISUAL PERCEPTION

(A) When one surface is viewed behind another, parts of the rear surface
    are seen only by the left eye, or only by the right eye. These are
    "half-occlusions". A "valid" unpaired region seen only by the right
    eye lies to the right of a step edge in depth where the nearer
    surface forms the left-hand side of the edge, and vice versa for the
    left eye.
(B) Stereo pair of images arranged for cross-eyed fusion. The grey
    crescents are unpaired regions, but only two of them are "valid"
    (see text). These regions appear stable, lying in the rear plane.
    The depth of "invalid" crescents is less clear and they suffer
    binocular rivalry. After Shimojo and Nakayama (1990). (C, D)
    Stereopsis from occlusion cues, without matching features. Crosseyed
    fusion of the centre and left images creates a white rectangle
    floating in front of a dark one. Fusing the centre and right images
    reverses the (implicit) disparity and makes the white rectangle
    float behind a dark aperture. After Liu et al. (1994).

geometry of the binocular stimulus is entirely consistent with a white
rectangle floating in front of a larger black one. This is perceived,
even though conventional disparity analysis of matching edges is absent
(Liu, Sevewron, & Schor, 1994). The centre and right pair in C and D,
take

a little more thought, since when cross-fused a white rectangle appears
behind, not in front. Where is the occlusion here? Closer inspection
reveals the answer. The white rectangle appears as if behind an aperture
cut in the white page, but in front of a dark space that lies beyond the
aperture.

7. SEEING A 3-D WORLD

The binocular geometry is consistent with this perception.
Occlusion-based stereo vision is at work in both cases, but at present
little is known of the mechanisms through which occlusion cues and
disparity interact in stereopsis. This account of stereopsis has
necessarily been selective. The interested reader is advised to consult
more detailed treatments of stereopsis given by Julesz (1971), Gulick
and Lawson (1976), Poggio and Poggio (1984), and especially the
comprehensive tome by Howard and Rogers (1995) as well as the relevant
sections of Kaufman (1974) and Marr (1982).

PICTORIAL CUES TO DEPTH One-eyed humans can be accurate at gauging
distance (as you will see if you close one eye and try reaching for
objects), and creatures with panoramic vision and little binocular
overlap include birds, which can take flight, navigate, and land.
Convergence, accommodation, and stereopsis can work only over relatively
short distances. There must therefore be sources of information other
than disparity to tell animals about the distances of objects in their
world. Given the "flat" retinal image, what can these sources be? As
well as the physiological and binocular cues, there are the "pictorial"
cues to depth, socalled because artists since the Renaissance have
employed them to convey an impression of depth in their work. If certain
features can give depth information on a canvas then perhaps those same
features may be used by the brain in its interpretation of the "flat"
retinal image.

Perspective Many of these pictorial cues are varieties of perspective.
They arise from the way in which the 3-D world is projected onto a 2-D
retina, from a particular viewpoint. The size of an image cast by an
object is smaller if the object is far away, as at (a) in Figure 7.13,
and becomes larger as the object approaches the eye, as at (b). Thus the
size of an image depends on the object's size and distance. Let us
consider a textured surface consisting

187

of large numbers of similar texture elements, slanting away from us
(Figure 7.14A). The image cast by such a receding plane contains a
gradient of image sizes, because the further elements are projected as
progressively smaller images. This texture gradient, or texture
perspective, is a "cue" for the perception of surface slant and relative
distance. Linear perspective is perhaps the best-known pictorial cue to
depth. The horizontal separation of images cast by a pair of railway
lines is larger for the nearer portions of the tracks and smaller for
the more distant portions. The images projected by parallel lines
converge as the lines recede from the observer (Figure 7.14E). In
addition to convergence, however, there is foreshortening. Notice how in
Figure 7.14A the texture elements become more "squashed" or elliptical
with increasing distance. The projected height of the elements evidently
decreases faster than their projected width. Why? The answer lies in the
fact that in this case we are looking down onto a horizontal plane and
so more distant elements are increasingly tilted away from our line of
sight. This produces a further compression of projected size, compounded
with that produced by distance per se. Thus if we consider a surface
"painted" with horizontal and vertical lines, slanting away (say) to the
right (Figure 7.14B), we find that the horizontal and vertical
components undergo different transformations. The lines running in the
gradient direction (horizontal in this example) change in both
orientation and spatial frequency (Figure 7.14E), while the lines
orthogonal to the gradient (vertical in this example) change only in
spatial frequency (Figure 7.14D). Curvature of this surface introduces
curvature into the projected horizontal lines, but not the vertical
(Figure 7.14F). Li and Zaidi (2000, 2001) tested which aspects of these
perspective effects are important in producing reliable and accurate
depth perception--- with rather surprising results. Some examples of
their type of display are shown in Figure 7.15. All the images were
perspective views of corrugated surfaces, with three cycles of vertical
corrugation visible. The textures "painted" on the surface were
synthesised from different numbers of

188

VISUAL PERCEPTION

Object size and image size. (A) Size of retinal image projected from an
object is measured by the visual angle (a) subtended by the object at
the eye. Image size gets smaller as object distance increases. This
scaling-down of image size with distance is the basis of all perspective
effects in images. The same object further away (a) clearly gives a
smaller image and smaller visual angle than when it is nearer (b). Image
size is proportional to object size (S) but inversely proportional to
object distance (D). In symbols: α = S/D. (B) A small near object and a
large far one can project to the same image size. Hence image size alone
is not a guide to object size. A convenient way to visualise the effects
of perspective is to consider where the lines of sight intersect a
frontal plane. This method was widely used by artists, and so the
frontal plane is also known as "Leonardo's window".

gratings at different orientations. Figure 7.15 shows textures that had
one, two, or four grating components. The top row (A,B,C) shows examples
of displays that produced stable, accurate perception of the surface in
depth, while those on the bottom row (D,F) did not (except the middle
one, E). Thus it was not the complexity or regularity or contrast of the
texture that mattered since these differed widely between equally
effective surfaces (top row) and were often similar for textures that
were and were not reliable (A, D).

From these and many other cases, Li and Zaidi concluded that the key
factor in producing depth from texture gradients was the visible
presence of image contours that project from lines of maximum gradient
or maximum curvature on the surface. In these examples, that means the
projections of horizontal texture lines. Looking again at the top row of
Figure 7.15, we see that all these effective images (A,B,C) contain the
zig-zag contours that arise from horizontal texture lines, while those
on the bottom line (D,F) do not. The

7. SEEING A 3-D WORLD

189

Perspective projection. (A) Horizontal plane receding from the viewer.
Note the convergence, size reduction, and foreshortening of the elements
with distance. (B) Flat textured surface composed of horizontal and
vertical sine-wave gratings, slanted in depth, with perspective
projection into the image plane. (C) As B, but with parallel projection
into the image. (D, E) The vertical and horizontal components of B,
illustrating texture compression (D) and perspective convergence (E).
(F) Perspective projection of a smoothly curved surface gives compelling
depth and surface shape.

(A,D) comparison is particularly instructive since the two textures are
essentially the same, and differ only by a 22.5° rotation. Gradients of
element size or spatial frequency do not seem to support the reliable
perception of surface depth in the absence of the linear perspective
effects that are carried by the variations of orientation along lines of
maximum gradient and curvature. However, both cues together (Figure
7.15B) seem to be more effective than either cue alone (C,F). The
surface of Figure 7.15E seems to be contrary to Li and Zaidi's rule. Its
texture is a plaid consisting only of two oblique grating components.
Why does it give reliable depth perception when there is no horizontal
Fourier component? The likely answer is that plaids of this kind have
vertical and horizontal zero-

crossings (discussed at length in Chapter 5). We know that people
perceive these as vertical and horizontal edges (Georgeson, 1992), and
so the perspective projection of these visible edges does satisfy Li and
Zaidi's rule: reliable depth from texture requires image contours that
arise from lines of maximum gradient on the surface. Finally, there are
also differences in the height in the visual field of images cast by
objects at different distances. For objects that are below eye level,
more distant ones are imaged higher in the visual field. For objects
above eye level, the reverse is true. The "cues" of relative size,
perspective, and relative height are all direct consequences of the
geometry of the retinal image, and all three operate together whenever
objects are viewed by the human eye (e.g., Figure 7.1). It

190

VISUAL PERCEPTION

Perspective projections of textured surfaces, corrugated in depth,
studied by Li and Zaidi (2000, 2001). A,B,C,E gave reliable and accurate
depth perception; D, F did not. H compt: horizontal component. See text
for details.

may be a general goal of perception to find the interpretation that is
most consistent with the set of available cues, even when individual
cues are quite ambiguous.

Shading Shading is an important aspect of pictures (or images) that
conveys an impression of solidity and depth, and in the last chapter we
described how animals may be counter-shaded to counteract this. A most
basic fact about the visual world is that lighting comes from above
(sun, sky), and not from the ground. Because most surfaces reflect light
diffusely, the intensity of light (luminance) reflected from a surface
is greatest when the surface faces the light source, and decreases as
the surface slants or curves away from the lighting direction (see
Chapter 5, p. 85). Thus the pattern of intensity variation (shading)
across a surface carries information about the 3-D surface shape

of the object. At its simplest, the direction of shading can distinguish
between convex "bumps" and concave "dents". In Figure 7.16, we see
"bumps" on the left but "dents" on the right. This is surprising when we
realise that those on the right are identical to the left ones, but
turned upside down. Turn the book upside down, and the dents will become
bumps and vice-versa. This surprising and robust effect means that the
human visual system is strongly biased towards assuming that lighting
comes from above. With this ecologically valid constraint, direction of
shading can be interpreted. Shading from light-to-dark downwards implies
a convex surface (its upper part catches the light more) while the
reverse shading implies a concave "dent" (its lower part catches the
light more). This accounts for the perceived depth and depth reversal in
Figure 7.16 and many similar demonstrations. Without some constraint of
this kind the shading is

7. SEEING A 3-D WORLD

191

Direction of shading can determine perceived convexity ("bumps", left)
or concavity ("dents", right). Turning the book upside will turn the
bumps into dents and vice-versa. This means that, to interpret depth
from the direction of shading, vision has a strong bias to assume that
lighting comes from above. After Ramachandran (1988). What happens if
you turn your head upside-down? Or turn the book through 90°?

ambiguous, but such ambiguity is rarely perceived. Indeed this lighting
constraint is deeply and unconsciously embedded into early visual
processing, since it is tied to retinal (or head) orientation, not
external or gravitational orientation. Try viewing Figure 7.16 with
various combinations of tilted book and tilted head to confirm this. A
number of machine vision algorithms have been developed to recover
shape-from-shading (cf. Horn & Brooks, 1989), with the aim of describing
the 3-D surface shape given only the pattern of reflected light
intensities. Because the problem is inherently ambiguous or
"under-constrained", all algorithms have to adopt strong assumptions or
constraints to arrive at a solution. These may include assumptions about
the nature of the lighting, or about the reflective nature of the
surfaces, or about the smoothness of the surface shape. For example, one
might assume matt surfaces of uniform reflectance, like a plaster
sculpture. Needless to say, such constraints are powerful if correct
(e.g., when applied to aerial photographs of a desert) but may go
hopelessly wrong. Unlike a

desert the human face, for example, is neither matt nor of uniform
reflectance. Pentland (1989) identified an intriguing simplification
that could in principle be implemented by human vision. Provided that
the lighting direction is not too close to the line of sight, it turns
out that the depth profile of a surface (of uniform reflectance) is
approximately a linear transformation of the intensity image received by
the camera (or retina). Any linear transformation can be expressed as a
filtering operation, involving changes in Fourier amplitude and phase,
and so if the visual system implemented the inverse filtering operation
(which is fairly straightforward, given our current knowledge of visual
filtering at multiple scales, orientations, and phases) it could recover
the surface shape. Pentland's algorithm worked well on pictures of
low-relief surfaces, and tolerably well on a photograph of a face and a
cartoon line drawing. Although human vision clearly uses
shapefrom-shading rules, it is less clear that it uses shading in a
quantitative manner. It may be used qualitatively in conjunction with
other cues such

192

VISUAL PERCEPTION

Cue combination. Shading and texture variation together create reliable
perception of a 3-D surface (A,C,D). This is so even in cases (D) where
either cue alone is ineffective (E, F). Pattern B is seen reliably in
depth (Li & Zaidi, 2001) but appears flattened in the centre; the
introduction of shading (A, C) makes the surface seem uniformly
corrugated. Lighting is from the left (A,D,F), from the front (C), or is
nondirectional (diffuse; B, E). H compt: horizontal component.

as texture variation, surface contours, and the shape of occluding
(boundary) contours (Ramachandran, 1988). Knill (1992) gives some very
nice examples in which the same shading pattern is interpreted as two
quite different corrugated surfaces according to the shape of contours
"painted" on the surface. Figure 7.17 gives further examples where
shading either enhances the depth created by surface contours (A,C,
compared with B) or creates a strong 3-D surface (D) where neither
texture (E) nor shading (F) alone is an effective cue.

Cast shadows There is a subtle but important distinction between shading
and shadow. Shading arises from the variation of lighting across a
surface as the surface orientation changes relative to the

light source. Shadows are cast by one object onto another or onto the
ground when the first object blocks the path of light onto the second.
Shadows are not objects and have no substance. They are literally
immaterial, but not unimportant even though they rarely capture our
attention. The direction and length of the shadow are cues to the
direction and elevation of the light source, and even (in natural
scenes) a potential cue to the time of day. The darkness or "depth" of
shadow gives information about the nature of the lighting. Diffuse
lighting gives weak, blurred shadows while strong, direct sunlight gives
sharp, high-contrast shadows. Such differences in shadow and shading may
explain how it is possible to tell that a photograph was shot in sunny
or cloudy conditions, even when the sky is not in the picture. The
Impressionist painters were masters of this cue.

7. SEEING A 3-D WORLD

There is not a great body of research on the role of shadows in
perception, but some striking demonstrations by Kersten et al. (Kersten,
Knill, Mamassian, & Bülthoff, 1996; Kersten, Mamassian, & Knill, 1997;
http://gandalf. psych.umn.edu/∼kersten/kersten-lab/demos.html)

193

have revealed that shadows do play an important role in the perception
of depth, and motion in depth. When an object sits on the ground
surface, its shadow is connected to the base of the object (Figure
7.18A), but if the object is suspended or floating above a surface then
object and shadow

Shadows, depth, and motion: the ball-in-a-box experiment. The grey ball
and the dark ellipse move together on a display screen, as shown by the
dashed arrows. The ball's movement in the image is identical in both
cases, but observers see a dramatic difference in the ball's movement
through 3-D space. In A, the ball is seen to slide across the floor of
the box from the front to back, while in B it is seen to rise and fall
obliquely at the front of the box. This complete switch in 3-D
perception is caused by the different path of the shadow. After Kersten
et al. (1997).

194

VISUAL PERCEPTION

become disconnected (Figure 7.18B). Kersten's experiments have shown
that this cue is indeed used to interpret the 3-D motion of an object,
either along the ground (when the shadow remains connected) or as rising
above the surface (when the distance between object and shadow varies).
Observers appear to be quite unaware of using the shadow information,
and are surprised to find that the ball's motion in the image (Figure
7.18) is really not different in the two cases. In much the same way,
the movement of a shadow can make an object appear to rise and fall in
depth, even when there are no other depth cues present in a flat, 2-D
display (Figure 7.19A,B,C). This demonstration also reveals another
constraint that vision uses to overcome ambiguity. Figure 7.19D shows
that the appearing/

disappearing shadow could (logically) be produced by movement of the
light source direction (from dashed to solid arrows) rather than 3-D
movement of the object. The fact that observers never saw this
interpretation implies that depth perception uses a "stationary light
source" constraint as well as the "lighting from above" constraint
discussed earlier. Such constraints are not rigid rules, but guides
based on environmental probabilities: two simple facts about our planet
are that the sun is above us, and its movement across the sky is
imperceptibly slow. The finding that perception makes strong use of
shadows prompts the question: how do we know that shadows are shadows,
and not other objects? This has not been much studied, but the likely
cues to recognition are that shadows are

Shadows, depth, and motion: the rising-square experiment. (A,B,C) In a
smooth transition from A to B to C, the grey square is stationary while
the shadow simply slides out from behind it, then slides back again
(C,B,A). Observers see the grey square rise up from the background
surface, then fall smoothly back down. The object's motion in depth must
be derived from the movement of the shadow. Geometrically, when a shadow
is disconnected from its casting object, the object must be some
distance above the background surface (D). People do not see the light
source as moving, even though that is a possible interpretation. The
displaced shadow is also quite a strong depth cue in stationary images
(B,C). After Kersten et al. (1996, 1997).

7. SEEING A 3-D WORLD

both dark and transparent; they are sometimes (but not always) blurred;
their shape is a projection of the object's shape; their movement is
tightly linked to that of the casting object; and they are always
located on a receiving surface away from the light source direction. For
further discussion of shadows in perception, see Mamassian, Knill, and
Kersten (1998).

Atmospheric perspective Not only are the images cast by objects smaller
when they are at a distance, but over long distances they are also less
clear, less bright, and have slightly different spectral properties.
This is because light is scattered and absorbed by particles in the
atmosphere---and different wavelengths are scattered to different
degrees (see Chapter 1). Figure 7.20 gives a powerful impression of
depth because of the way in which the

The image of the near hills is brighter and clearer than the image of
the distant ones, from which the light has been scattered by the
atmosphere. Photograph by Mike Burton.

195

light reflected by the distant hills has been scattered---although there
are also other cues, such as relative size and occlusion, operating
here.

Occlusion Most natural scenes are cluttered with a variety of objects,
such as trees, plants, and rocks, lying at different distances. The view
of one object is often partially obscured by a nearer one, producing the
cue of interposition or occlusion. It provides information about
ordering in depth, but no measure of relative or absolute distance. As a
very simple example, Figure 7.21A is seen as circles, with one on top of
another. This is not the only possible interpretation; we might see the
drawing as representing adjacent shapes, one circular and the others
with "bites" taken out of them. Usually, however, we interpret the
irregular images as representing a "good" shape (cf. Gestalt principles
discussed in Chapter 6), which is partially covered by another good
shape. On this view, detecting occlusion requires a high level of shape
representation, but another view is that line-ends and T-junctions in an
image are low-level features from which an occlusion interpretation may
be derived (see Chapter 6). For example, Figure 6.18 shows that at
T-junctions the leg of the T always belongs to the occluded surface
while the crossbar of the T belongs to the occluding surface. These
occlusion cues are features in an image that point to the description of
one surface lying in front of another. They are pictorial cues, equally
visible to both eyes. This is rather different from our earlier
discussion of stereopsis where features visible to one eye but not the
other served as "half-occlusions" (see Figure 7.12), with the
implication that those features lie on the occluded surface. Thus when
one object partially obscures another it yields both monocular and
binocular cues to occlusion, as well as conventional stereo disparity
cues, and disparity can either reinforce or countermand the pictorial
(monocular) cues, as cross-fusion of Figure 7.21A will show.

Perceptual completion Occluding objects may fragment the image of an
occluded object into several separate regions, and

196

(A) Stereo cues can reinforce, or contradict, occlusion cues for depth.
    With cross-fusion of the centre and left images, both occlusion and
    disparity indicate that the light grey disc is in front, and the
    mid-grey disc behind. All three surfaces seem solid, and in depth.
    With cross-fusion of the centre and right images, disparity is
    reversed and now conflicts with occlusion. Perceived depth order is
    reversed (following the disparity) but the two darker shapes now
    appear as curiously transparent discs with illusory contours ("modal
    completion"). (B) With cross-fusion, centre and right images give
    occluding strips of texture in front of a face, left and centre
    images give strips of face in front of a texture. In both cases,
    note the sense of a complete surface behind the strips. This is
    "amodal completion". Nakayama et al. (1989) found that face
    recognition was better when the face was behind, suggesting that
    amodal completion aids the recognition of occluded objects.

7. SEEING A 3-D WORLD

to maintain the structure of that object the fragments must remain
linked or grouped together, as belonging to a single object. When the
occlusion cues are reinforced by disparity, the grouping of the occluded
parts seems especially robust, and creates the perception of a complete
surface behind the occluding objects, along with the sense that parts of
this surface are obscured from view. Free fusion of Figure 7.21B should
confirm this phenomenon for the reader. This kind of perceptual
filling-in is known as "amodal completion" (to be contrasted with the
"modal completion" of illusory contours that are seen as "visible" not
as "obscured"). Nakayama, Shimojo, and Silverman (1989) found that face
recognition (Figure 7.21B) was better when the fragmented face was
behind the occluding strips, rather than in front, even though the image
information was the same in both cases. This suggests that amodal
completion does serve to assist the recognition of occluded objects. One
might suppose that these perceptual completion processes would operate
at a fairly high level or late stage in perceptual analysis. Recently
however it has been found that cells in visual cortex (V1 and V2) can
show responses that reflect modal and amodal completion. Sugita (1999)
found that some simple and complex cells in V1 of the alert monkey
responded well to a pair of separated, collinear line segments that fell
outside the classical receptive field (Figure 7.22). This was true only
when the gap was filled by a blank patch that was stereoscopically
nearer than the lines, not when the patch was in the same plane, or
behind. Thus, as early as V1, stereo cues to occlusion enable amodal
completion of contours---the grouping and integration of separated image
fragments that belong to the same object. In a similar vein, Bakin,
Nakayama, and Gilbert (2000) found that some cells in V2 (but not V1)
responded to the kind of display that produces modal completion
(illusory contour) from disparity cues that lay entirely outside the
classic receptive field (Figure 7.22G,H). They also found a number of
other influences of "global" depth information on the responses of V2
cells. By contrast, they found little of such effects in V1, and other
studies have suggested that V1 cells

197

encode local image disparities, even when these do not correspond to the
perceived depth determined by more global factors (Cumming & Parker,
2000). Bakin et al. (2000) concluded that V2 cells can integrate
contextual depth information from beyond the classical receptive field,
and that V2 therefore plays a role in coding the surface properties of a
scene, including surface contours, regions of surfaces, opacity, and
transparency. These physiological findings could mean that V2, and to
some extent V1, are much more sophisticated processors than has
traditionally been supposed. But it could also be that these more
global, perceptual computations are carried out in higher areas, and the
results are fed back to V1 and V2 (cf. Chapters 3 and 6, pp. 69--72,
148--151; see Bullier, 2001, for a brief review).

DEPTH FROM MOTION So far we have considered a static observer viewing an
unchanging scene. Most natural scenes, however, are alive with the
movements of animals, plants, clouds, water, and so on. Movement in the
optic array arises not only because external objects move, but also
because the observer's head and body move, generating "optic flow" (see
Part III of this book). Try moving your head from side to side or back
and forth (with one eye closed) to get an impression of the image
motions and changes in relative position and size that occur as your
viewpoint moves through space. Image movement on the retina is further
complicated by the fact that the eyes move in the head, even when head
and body are stationary. Thus, in addition to physiological, binocular,
and pictorial cues to depth, recent research has considered how movement
of an observer, or of objects, can produce information about relative
distance through motion parallax or motion perspective. In Figure 7.23
we see an eye viewing two objects at different distances. As the eye
moves laterally, the image cast by the nearer object B travels further
across the retina than that cast by object A. Similarly, if the eye were
still and

198

VISUAL PERCEPTION

(A--C) Stereo display of the kind used by Sugita (1999) to test for
amodal completion at the level of single cells in primary visual cortex
(area V1). The receptive field of the cell was located entirely within
the central grey area. The flanking lines caused no response when the
central patch was stereoscopically behind or in the plane of the lines.
But for some cells there was a vigorous response when the disparity of
the central patch caused it to appear as an occluding surface, in front
of the line segments. Thus the cell's response correlates with
perceptual completion of the line behind the patch. Cross-fusion of A,B
illustrates perceptual completion of a single long contour behind the
patch. Cross-fusion of B,C reverses the disparity; the patch is seen
behind, and the two line segments are seen as separate things. (D--F)
Arbitrary overlapping shapes can be segmented into two distinct surfaces
using disparity cues, even though most of each shape is obscured by the
other (right). Cross-fusion of D,E brings the dotted shape (right) in
front with the other shape behind. Modal completion is experienced for
the front shape (illusory contours, not apparent in the monocular
images), while amodal completion occurs for the rear shape. Cross-fusion
of E,F reverses the disparity and the perceived depth of the two shapes.
(G--I) Simplified version of D--F, used by Bakin et al. (2000) to test
for modal and amodal completion of surface forms in V1 and V2 of the
monkey. Cross-fusion of G,H creates a vertical bar (with vertical
illusory contours) in front of a large rectangle. Crossfusion of H,I
reverses the disparity and the depth. Some cells in V2 responded to
these disparity-induced illusory contours, and to other forms of global
depth information.

7. SEEING A 3-D WORLD

199

Motion parallax. The observer looks at two objects (A and B) at
different distances. If the observer moves (as in the left diagram), or
the objects move at equal speed (as in the right diagram), the image of
the nearer object B moves further across the retina (b1--b2) than does
the image of A (a1--a2).

objects A and B moved across the line of sight at equal speed, the image
of the nearer object would travel further and faster across the retina
than the image of the more distant one. In fact, the ratio of speeds is
inversely proportional to the ratio of distances from the observer: if A
were twice as far away as B it would move at half the speed, and so on.
Relative speed of motion of different portions of the retinal image
could therefore signal relative depth. If we consider a stationary,
textured 3-D surface, instead of just two points, then the entire field
of relative motions produced by the observer's movement conveys very
detailed information about the 3-D structure of the surface. One way to
appreciate this fact is to realise the close geometric connection
between the motion parallax cue and binocular disparity (also known as
binocular parallax). If a single eye moves laterally through (say) 6 cm,
then the changes in image structure between the start and finish of the
movement are just those that could be picked up simultaneously by having
two eyes separated by 6 cm. Motion parallax and stereo cues therefore
represent temporal and spatial samplings of the same 3-D information.
The experiments of Rogers and Graham (1979, 1982) were instrumental in
showing that human vision does in fact exploit the motion cue to convey
a sense of depth as rich as that obtained

from binocular stereopsis. Observers were shown random-dot patterns (on
a flat screen) in which the pattern of relative movements were those
that would have been produced by a variety of corrugated 3-D surface
shapes. An example of such displays is shown (using stereo in place of
motion) in Figure 7.24. With no motion the surface (not surprisingly)
looked flat, but with the motion cue the different surface shapes
(sinewave, triangle-wave, square-wave) were clearly and reliably
recognised (Rogers & Graham, 1979). The ability to recognise depth from
motion declined as the surface corrugations became narrower (higher
spatial frequency), and a similar restriction applied to stereo vision
tested under similar conditions (Rogers & Graham, 1982). Peak
sensitivity to depth from stereo or motion occurred for fairly broad
corrugations of 0.2--0.4 c/deg. This contrasts markedly with sensitivity
to luminance contrast that peaks at spatial frequencies of 2--5 c/deg in
foveal vision, and extends to much higher spatial frequencies. Perhaps
contour and texture from luminance contrast convey finer details of
surface structure, while motion, stereo, and coarse shading carry
information about larger-scale surface shape. In Part III of this book
we consider in greater detail how motion information may inform an
animal or person about the layout of the world and their own movements
within it.

200

VISUAL PERCEPTION

Surfaces created purely from stereo disparity cues. Cross-eyed fusion of
the pairs of images should give a smoothly corrugated sine-wave surface
in depth (right). (Top) 50% dot density. (Bottom) 0.5% dot density.
Vision evidently has mechanisms for constructing smooth surfaces even
from very sparse data (Grimson, 1981; Julesz, 1971; Treue et al., 1995).

Interim summary The optical projection of a 3-D world onto a 2-D retina
contains many inherent ambiguities, but various signs, cues, or clues
available in the retinal image are correlated with distance, relative
depth, and surface shape and allow these to be recovered by the
observer. At least some aspects of depth perception appear to be innate
rather than learned, even in humans, who are not independently mobile at
birth, and we will be considering some of this evidence in Chapter 12.
It does not seem as though all the cues to depth need to be learned
through associating visual information with the felt positions of
objects as Berkeley (1709) had supposed.

INTEGRATING DEPTH CUES Modules and cue integration We have seen that
depth information is obtained from a variety of sources or cues,
including stereo disparity, motion, texture, occlusion, perspective, and
shading. Marr (among others) proposed that these cues are processed
initially by separate, parallel modules handling disparity, motion
parallax, shape-from-shading, and so on. Yet we do not see multiple
objects from these multiple cues; we do not see separately a stereo
surface, a motion surface, and a shaded surface. The simple fact that we
see one object, or one surface, suggests that

7. SEEING A 3-D WORLD

the multiple cues are integrated in some way. We have also seen that
information is lost in the projection from 3-D to 2-D, and that each cue
is an ambiguous indicator of distance, depth, and surface structure. The
ambiguity might be reduced by combining information from several cues.
For example, if one cue says that the state of the world is either a, b
or c, while a second cue is compatible with states c, d, or e, then a
logical combination of the cues is unambiguous (c). More generally,
there will be a probabilistic relationship between cue values and
external scene properties, and the task of vision may be seen as
(somehow) to combine these probabilities to derive the most likely set
of 3-D structures and lighting conditions that gave rise to the present
image(s). This is the task that Helmholtz (1866) called "unconscious
inference", and in recent years a new theoretical approach has
emerged--- the Bayesian approach to perception---that aims to formalise
these ideas mathematically in order to gain an understanding of one of
the deepest problems in perception. It enables the combination of
different image cues and the exploitation of prior knowledge and
constraints to be integrated in a single theoretical framework. This is
difficult work, but the interested reader may consult a recent
collection of chapters (Knill & Richards, 1996).

Cue integration and surface description: The 2½D sketch Marr (1982)
proposed that the goal of early visual processing is the production of a
description of the visible surfaces of the environment, so that their
dispositions and layouts are described with respect to the viewer. We
have seen here and in Chapter 6 much evidence that the construction and
representation of surfaces is a key intermediate step in the transition
from image features to object descriptions. Marr termed this surface
representation the 2½D sketch, and supposed that it serves to integrate
the multiple depth cues from visible surfaces. Stereopsis, the analysis
of motions present in the image, and the contour, texture, and shading
information available from the full primal sketch would all contribute
to the 2½D sketch. The

201

label "2½D" reflects the idea that the sketch captures a great deal
about the relative depths and surface orientations, and local changes
and discontinuities in these, but is viewer-centred rather than
object-centred and does not make explicit the 3-D structure of spatial
forms. The latter involves parts of an object that may not be visible in
any single view, and involves 3-D descriptive primitives (concerned with
axes and volumes) that are not part of the surface description. Marr's
concept of the 2½D sketch was a computational theory of visual
representation. It set out a programme of research in computer vision,
and provided a possible model for human vision. Whether human vision
actually works in this way is an experimental issue, to which we now
turn. What experimental evidence do we have that human vision integrates
depth cues to form a surface representation? To investigate the
interaction of depth cues, researchers need to be able to manipulate
different depth cues independently, and then to investigate situations
where cues are either in conflict, or in accord with each other.
Pseudoscopic vision (interchanging the left and right eye's views; see
p. 184) is one example of cue conflict, where stereopsis is often
suppressed by contrary pictorial cues. The Ames room (see p. 79) offers
conflict between different pictorial cues. Natural vision, however, does
not generally involve such gross conflict of cues, and the study of
concordant cues may be more informative. Manipulating depth cues
separately was hard to do until the advent of computer-generated
displays. Braunstein (1968) was one of the first to examine the
interaction of texture gradients and velocity gradients in the
perception of surface slant, using computer-simulated objects. A given
display contained a random-dot texture gradient consistent with a slant
of 0, 20, 40, or 60 degrees. The same display also contained a velocity
(motion parallax) gradient indicative of 0, 20, 40, or 60 degrees slant.
The results showed that both cues influenced perceived slant, but the
weight assigned to motion was more than twice as great as that assigned
to texture. The idea that perceived

202

VISUAL PERCEPTION

depth is just a weighted average of the depth provided by a variety of
cues is a simple one, but it receives support from more recent research.
Current computer graphics technology can render precisely shaded,
textured, moving stereograms and has enabled much more sophisticated
studies of this kind, linked to a renewed interest in theories of cue
integration (e.g., Bülthoff & Mallot, 1988; Johnston, Cumming, & Parker,
1993). Stevens and Brookes (1988) examined the perception of slant of a
simulated surface on which a square grid of lines was drawn. The
monocular cues to surface slant (linear perspective, and foreshortening
of the squares; Figure 7.14) were controlled independently of the
disparity gradient across the surface. Surprisingly, it was found that
the monocular cues to slant dominated over the stereo information. In a
further experiment, Stevens and Brookes found that this dominance was
peculiar to plane surfaces, and that stereo was much more effective when
curved surfaces were tested. Thus they concluded that "stereo depth
derives most effectively from disparity contrast; when disparity varies
linearly it is dramatically less salient" (p. 382), and further that
"the effective stereo features correspond to places where the second
spatial derivatives \[of disparity\] are non-zero" (p. 385), at points
of curvature in depth. Consistent with these findings, Johnston et al.
(1993) found that for a textured stereogram of a curved, cylindrical
surface (Figure 7.25A) the contribution of texture information to depth
was significant, but small in comparison to the strength of stereo
depth. Moreover their data support the theory of linear, weighted
averaging of cues: as the texture cue increased in its implied depth, so
the stereo depth cue had to be decreased linearly to maintain a constant
perceived shape in depth. But the weighting (relative importance) given
to texture was only 10--20%, with the remainder (80--90%) given to
stereo. Most of the contribution from texture came from the shape
compression that occurs when viewing a slanted surface element (e.g.,
circular blotches on a surface are imaged as ellipses), rather than from
changes in the imaged area or density of surface elements (Cumming,
Johnston, & Parker, 1993).

In a related study, Young, Landy, and Maloney (1993) studied the
inter-relation of texture and motion cues in the portrayal of a
cylindrical surface. Overall, texture and motion were more nearly equal
partners in determining the 3-D shape of the surface. The linear
combination model was again supported, and it was found that the
perceptual weight assigned to a cue decreased if that cue was rendered
more "noisy" or unreliable. The model emerging from these recent studies
on the integration of stereo, texture, and motion cues to surface depth
is sketched in Figure 7.25B, and it is broadly consistent with Marr's
original concept. Depth values (e.g., surface slant or curvature) are
computed by relatively independent modules, and a weighted average of
these values determines perceived depth. Note that the stereo module
computes disparity values, and to convert these to depth values requires
additional information about viewing distance---an example of "cue
promotion". Such information could come from binocular convergence or
the motion module, or both. It may be that this simple averaging model
applies only when the various cues are not too discrepant. Where
cue-conflict is gross there can be other outcomes, but the averaging
model may be more relevant to natural vision. In a further test,
Johnston, Cumming, and Landy (1994) found evidence for the linear
combination of stereo and motion cues. That is, as the depth implied by
motion was increased, the depth implied by stereo had to be decreased
(and vice-versa) to maintain a constant perceived shape---the
"apparently circular cylinder". Their results are summarised in Figure
7.25C, as the relative weights given to the two cues. Note that the
weight given to stereo was less at a far distance (where disparities in
the real world are smaller), and when the motion cue was stronger (many
frames in the movie sequence). Reducing the movie sequence to two frames
decreased the role of motion in depth perception, but increased that of
stereo. Thus the integrative device (2½D sketch?) of Figure 7.25B may
serve to optimise perception by weighting the cues according to the
validity and reliability they have under

7. SEEING A 3-D WORLD

203

(A) Schematic view of the textured cylinder and background used in
    experiments on integration of depth cues. Black outlines were not
    present, and the cylinder was portrayed by a combination of stereo,
    motion parallax, and texture cues. The implied depth of the cylinder
    could be varied independently for each cue, allowing the effects of
    cue combination to be studied. (B) Linear model of depth cue
    averaging implied by the experiments (see text). This model may hold
    only for moderate discrepancies between cues. Cue promotion refers
    to the idea that cues such as disparity and motion parallax may have
    to be rescaled by other information such as convergence in order to
    yield depth values as output. (C) The relative importance assigned
    to depth cues in perceptual averaging can vary with circumstances.
    The weight given to motion parallax (in combination with stereo
    disparity) decreased with only two frames in the movie sequence, and
    the weight assigned to stereo increased at a nearer viewing
    distance. Data from Johnston et al. (1994).

different circumstances. This points to a rather intelligent perceptual
process. Treue, Anderson, Ando, and Hildreth (1995) give further
experimental evidence and arguments favouring the idea that perceptual
construction of a surface by interpolation is an important component of
the structure-from-motion process. In a companion paper, Hildreth, Ando,
Anderson, and Treue (1995) discussed a variety of computational models
of the process by which stereoand motion-defined surfaces may be
constructed from sparse data. We can not show motion in this book, but
demonstrations of surface

interpolation from stereo disparity are given in Figure 7.24 (bottom)
and Figure 7.26. If there really is a specific visual mechanism that
integrates stereo and motion cues to represent 3-D surfaces (as opposed
to some general cognitive process that serves this and other purposes),
then two psychophysical predictions follow. First, we should be able to
adapt the mechanism with one cue (e.g., motion), but reveal a consequent
loss of sensitivity when we test with either stereo or motion. This
transfer of adaptation would imply a common processing device (see
pp. 116--117 for more on this logic). In just

204

VISUAL PERCEPTION

Illusory surfaces created from stereo disparity and occlusion cues.
Cross-eyed fusion of the centre and left images should give a curved
white surface in front of black discs. Reversing the disparity (by
cross-fusing centre and right images) should reverse the depth of the
surface, and the black discs become holes. In both cases, illusory
contours are formed to complete the boundaries of the front surface.
After Carman and Welch (1992).

such a test, Bradshaw and Rogers (1996) found that adaptation to motion-
or stereo-defined surfaces (Figure 7.24) made observers less able to
detect the corrugations of a test surface. They were about 2× less
sensitive after adapting to the same cue, and 1.5× less sensitive after
adapting to the opposite cue. This implies some common mechanism for the
two surface cues. Second, this conclusion was reinforced by the finding
of subthreshold summation of the two cues: the depth of a surface
defined by stereo and motion parallax combined was just detectable when
the stereo and motion cues were each set to about half their
just-detectable values. The common device, then, appears to sum the two
cues linearly in detection of depth. Bradshaw and Rogers (1996) also
note that the mechanism is exquisitely sensitive to depth difference:
observers can detect corrugations that correspond to tiny depth
differences of 1/20 mm at a viewing distance of 57 cm. These adaptation
studies support earlier ones by Nawrot and Blake (1989) who studied the
perception of 3-D structure from motion using computer simulations of a
rotating sphere that had dots scattered on its surface (Figure 7.27).
When the simulated sphere rotates without stereo disparity, motion
parallax creates the perception

of a 3-D transparent sphere, but the depth from motion is ambiguous
(Figure 7.27, bottom): rightward dots could be on the front surface of a
sphere rotating one way, or on the back surface of a sphere rotating the
other way. Perception "flips" from one to the other, rather as in the
Necker cube and other ambiguous figures. When stereo disparity is added
(Figure 7.27, top), the depth and direction become unambiguous. The key
result, however, was that after a few minutes of adaptation to the
rotating stereo sphere, the motion-defined sphere appeared to rotate
consistently in the opposite direction, instead of its usual ambiguous
rotation. This was true even when the motion test was monocular.
Exposure to binocular stereo depth evidently changed the interpretation
of structure-from-motion, and so it appears that stereo and motion
signals must feed into a common mechanism that represents 3-D
surfaces---broadly in line with Marr's concept of the 2½D sketch.

Cue integration in area MT The nature of this mechanism, and its
location in the brain, have recently been clarified by recordings from
single cells in brain area MT (see Chapter 3, p. 58), combined with
behavioural analysis

7. SEEING A 3-D WORLD

205

(Top) Stereo image pair illustrating the kind of transparent sphere used
by Nawrot and Blake (1989) to test for stereo-motion integration. Front
and back surfaces are visible at the same time. (Middle) Control surface
in which all dots lie on the front of the sphere (when cross-fused).
(Bottom) When the sphere rotates, dots from the front and back surfaces
move in opposite directions, and are intermingled in the image.
Projected image speed is fastest in the centre, and slows down as dots
approach the edges of the sphere. Similar displays (but cylindrical)
were used by Bradley et al. (1998).

206

VISUAL PERCEPTION

of the monkey's perception. MT is known to be deeply involved in the
analysis and perception of motion (see Chapter 8), but MT cells also
show selectivity for binocular disparity in a similar way to cells in V1
and V2 (p. 173). Moreover, when clusters of cells in MT were stimulated
electrically, the monkey's judgements of depth in a stereo display were
consistently biased towards the preferred disparity of the stimulated
cells (DeAngelis, Cumming, & Newsome, 1998). These important findings
strongly imply that disparityselective cells in MT play a direct role in
the perception of depth as well as motion. In combined studies of stereo
and motion, Bradley, Chang, and Andersen (1998) have cleverly revealed
that brain area MT is also an important site for cue integration and
surface representation. They used a rotating transparent cylinder,
similar to the rotating sphere discussed above (Figure 7.27). As we have
seen, its perceived depth and direction are ambiguous ("bistable")
without disparity, but not ambiguous with it. This proved to be as true
for the monkey's judgements as for human ones. Since particular cells in
MT tend to show a preference for both direction and depth, we can ask
how their responses are related to the stimulus display cues, and to the
monkey's perception. For example, one cell may fire most to a display
that contains motion and disparity indicating rightward and "near",
while another cell responds best to leftward and "far", and so on.
Bradley and colleagues arranged the cylinder so that for a given cell
the firing rate was high for one direction of stereo rotation (the
preferred direction) but low for the other. How would the MT cell
respond when the stereo disparity was set to zero to create the
ambiguous rotation? One might expect that it would simply fire at some
intermediate rate. This was true on average, but the crucial finding was
that firing rates were much higher in trials when the monkey perceived
the preferred direction of depth and motion than when it perceived the
opposite direction (Bradley et al., 1998; Parker, Cumming, & Dodd,
2000). This was also true when disparity was present; if the monkey
reported the wrong direction of depth and motion (an error trial), then
firing rates tended to shift up or down accordingly. MT cells'

firing rates were thus closely linked to the perception of depth and
motion, even when the stimulus information was held constant. We cannot
immediately conclude from these data that certain patterns of cell
firing cause a given perception because, as always, correlation does not
establish causation. Nevertheless, when we recall that direct electrical
modification of firing rates in MT does cause changes in perception of
motion and depth (De Angelis et al., 1998; Salzman, Britten, & Newsome,
1990), it seems very likely that random or spontaneous changes of firing
patterns in MT do cause the switches in perceived 3-D rotation of the
ambiguous cylinder (for further detail, see Andersen & Bradley, 1998).

CONCLUSIONS In summary, psychophysical and physiological studies of
depth perception have shown us that human vision uses a great variety of
information sources ("depth cues") that are available to the
eye(s)---image contours, perspective gradients, occlusion cues, shading
and shadow, binocular disparity, and motion parallax. Individual cues
are ambiguous, but by combining them, and by exploiting a number of
rules and constraints (e.g., surfaces are locally smooth, objects tend
to be rigid, lighting comes from above), the visual system usually
settles on a stable and unambiguous solution. We are beginning to see
how extrastriate brain areas such as MT serve to integrate different
cues to surface depth, and that an animal's perception of structure from
motion and disparity is tightly linked to brain activity in this area.
Recent brain imaging (fMRI) results show that other areas of the human
brain (including V2, V3, V3A) are also associated with stereo depth
perception (Backus, Fleet, Parker, & Heeger, 2001). These are genuinely
important advances in our understanding of the relation between brain
and perception. As yet, little is known of the more global processes
that construct and represent 3-D surface shape, although one recent
study has found neurons quite far along the ventral stream (temporal
cor-

7. SEEING A 3-D WORLD

tex, area TE, associated with object recognition, p. 58) that are
selective for elements of surface shape defined by disparity, such as
slant, convexity, or concavity (Janssen, Vogels, & Orban, 1999).
Understanding cue combination remains a challenging problem for which a
new theoretical language is emerging (Knill & Richards, 1996). As
perceivers, we experience the solu-

207

tion---a representation of the structure and layout of surfaces in space
around us---but are quite unaware of the complex, interacting processes
that lead to that solution. Surface representation is the output of
early vision, but early vision is "cognitively impenetrable" (Pylyshyn,
1999). We will return to this wider theoretical point in Chapter 14.

Page Intentionally Left Blank

8 The Computation of Image Motion

ciples of motion computation, along with the visual mechanisms that may
carry out these computations. A key idea here is to consider the visual
input to be a "space-time image". The analysis of motion can then be
tackled with the same concepts that we applied to spatial vision in
Chapters 5 and 6. First we ask how the visual system may recover the
velocity (speed and direction) of local image motion, irrespective of
the external events that may have caused it. The theory of "motion
detectors" is especially well developed, and we explain how visual
filters sensitive to direction of motion are constructed, and how the
responses of such filters may be combined and compared to compute the
velocity of motion. (Readers who want to omit some of the more
mathematical detail can skip Boxes 8.1--8.4 without losing the flow of
ideas.) We discuss recent evidence on the series of processing stages by
which motion computation is implemented in the brain, the role of areas
MT and MST in the dorsal pathway in encoding global patterns of motion,
and the influence of attention on these processes. We shall also see
that there is evidence for several additional forms of motion analysis,
and consider recent accounts of the way in which these

The retinal image is alive with motion from a variety of sources.
Whenever objects or living things around us move, their images move over
our retinas. Even if the world is still, our eye, head, and body
movements cause continual motion of the retinal images. In most
circumstances, these effects will combine to produce a rich pattern of
image motion. In Chapter 7 we saw how the changing view gained by a
moving observer ("motion parallax") provides information about the 3-D
structure of objects and scenes. In Part III of this book we shall see
how "optic flow"--- the complex pattern of motion throughout the visual
field as an animal or person moves about--- can provide important
information for the control of animals' and people's actions. To use
Marr's terms, introduced in Chapter 4 (pp. 80--81), all these
considerations contribute to a computational theory of the information
available in image motion, and the uses to which it can be put. But to
understand how this information is obtained, we need to devise
algorithms capable of extracting it from a time-varying retinal image
and then to ask whether these algorithms are implemented in nervous
systems. In this chapter we therefore discuss some prin209

210

VISUAL PERCEPTION

multiple cues to motion are extracted and integrated in the brain. We
then discuss how local motion information may be combined across space
to yield higher-order motion signals that carry information about the
components of optic flow, and hence carry information about the spatial
structure of the scene and about the observer's own motion. This sets
the scene for more extended discussion of the role of optic flow in
perception and behaviour in Part III of the book.

FIRST PRINCIPLES: MOTION AS ORIENTATION IN SPACE-TIME Movement is a
change of position over time. It might seem obvious therefore that to
perceive movement we must first identify something, note its position,
then identify the same thing a moment later and note its position again.
The change in position (δs) divided by the time taken (δt) gives the
velocity of movement. This simple intuition forms the basis for a class
of models of motion perception known as correspondence models, since
with this approach the major problem is taken to be that of matching
"things" over time. What things should be matched, and how are competing
matches resolved? The problem is discussed later in this chapter, and is
similar to that encountered in stereoscopic vision (Chapter 7), but with
the additional problem of measuring

Two ways of thinking about a moving image. (A) As a sequence of
snapshots where the position of any given feature traces out a path
through space (x) and time (t). Along that path, (x − V.t) = constant.
(B) As a space-time image: an array of image intensities distributed
over space and time, whose orientation reflects the velocity V.

time intervals. However, a different approach holds that simpler
measurements made on the time-varying image can recover motion
information more directly. Based on extensive experimental evidence, a
variety of models for the detection of local motion has been proposed
over the last 30 years or more, and these models appear at first sight
to differ substantially in the means by which they recover motion
information. Our aim in what follows is to show how motion information
can in principle be recovered from the space-time image, and then to
show how the various models form a single family, differing not so much
in principle, but in the details of their implementation. With a little
elementary algebra and calculus we can gain some quite deep insights
into this approach. Let us consider first the simple movement of a rigid
pattern to the right at constant speed, V, as shown in Figure 8.1A. In
general, images are of course two-dimensional (2-D), but for simplicity
we consider here a one-dimensional (1-D) slice through an image, whose
intensity I(X) thus varies in only one spatial direction, say
horizontally. Any point at location X within the pattern traces out a
path through external space (x) and time (t), and that path is defined
by a simple equation: x − V.t = X

or x = V.t + X

8. COMPUTATION OF IMAGE MOTION

This is the equation of a straight line in the coordinates (t,x) with
slope V. All points on a given path have the same intensity I, and as X
varies adjacent points trace out parallel paths at different luminances
determined by the pattern of intensities I(X). Thus a complete
description of the moving pattern is given by its space-time image
(Figure 8.1B). A horizontal slice through the space-time image gives us
a snapshot of the 1D spatial pattern at any one instant, while a
vertical slice tells us about the temporal fluctuation of intensity at a
given spatial position. The tilt of the space-time image away from
vertical gives the speed of motion. A vertical space-time image is
stationary, a tilted one is moving, a horizontal one arises from a
flickering but empty field (a "ganzfeld"). Other, more elaborate
space-time patterns can arise from more complex events. To summarise:
Motion is orientation in spacetime. A moving one-dimensional image can
be thought of as a pattern of intensity in space-time (x,t), just as a
stationary 2-D pattern is an image in "space-space" (x,y). A most basic
task in motion perception is to recover the velocity V from the
time-varying retinal input, and this can be seen as the task of
recovering the orientation of the space-time image. The idea of a
space-time

Box 8.1

211

image may at first be unfamiliar, but once assimilated it proves to be a
simple and powerful way of thinking about movement and movement
analysis. Ideas about spatial image analysis are well developed, and
since we can now think of time as if it were a spatial dimension, we can
immediately transfer ideas about spatial filtering and orientation
detection to the space-time domain. For a more detailed introduction to
these ideas see Adelson and Bergen (1985), and for their extension to
other visual domains, such as colour and stereopsis, see Adelson and
Bergen (1991).

Velocity from space-time gradients We now proceed to see how velocity V
can be recovered by measuring local differences of image intensity in
space and time, without explicitly identifying image features or their
locations. Velocity corresponds to orientation in space-time, and it can
be found from the ratio of temporal and spatial intensity gradients or
derivatives. The basic idea is sketched in Figure 8.2, and derived
without approximation in Box 8.1. The ratio of temporal gradient to
spatial gradient thus yields an estimate of velocity V at all points in
space and time, except that it cannot be calculated where the spatial
gradient ∂I/∂x = 0. In

Deriving velocity from space-time gradients

We have seen that the intensity of the moving image at any point x and
time t is some function I(x − V.t) and so the spatial gradient of the
image is the derivative: ∂I/∂x = I′(x − V.t) where I′(X) denotes ∂I/∂X.
Similarly, applying the chain rule of calculus, we get the temporal
gradient: ∂I/∂t = −V.I′(x − V.t). Hence, combining the two relations, we
get: ∂I/∂t = −V.(∂I/∂x). In words, the rate of temporal change in the
image at a given point is proportional to both the speed and the spatial
gradient at that point. Clearly, if we can measure both gradients, then
V can be recovered directly: V = −(∂I/∂t)/(∂I/∂x).

212

VISUAL PERCEPTION

Velocity V can be recovered from spatial and temporal gradients
(derivatives), without tracking features. The spatial gradient of
intensity ∂I/∂x is (approximately) the spatial change in intensity (I)
between points x apart, divided by the separation x. The temporal
gradient ∂I/∂t is (approximately) the temporal change in intensity (−I)
at a given position, divided by the interval t. Because V = x/t it
follows that V = (I/t)/(I/x) = −(∂I/∂t)/(∂I/∂x) Velocity is given by
(minus) the ratio of temporal to spatial gradients. To exploit this
relationship, and encode local velocity, the visual system needs at
least two kinds of operators, or receptive fields, that respond to local
spatial and temporal differences in image intensity, whose ratio of
output values is then proportional to velocity.

other words, blank, uniform regions carry no motion information.
Similarly, the estimate of V would be unreliable where ∂I/∂x was itself
small or noisy, and so we might expect to get best estimates of V at
edges where the spatial gradient is maximum. This was the insight
offered by Marr and Ullman (1981) in extending the theory of edge
detection (Marr & Hildreth, 1980) into a theory of motion detection with
emphasis on the encoding of direction. Harris (1986) developed the
gradient ratio idea further for the encoding of both speed and
direction. To give a concrete example, whose development will be useful
later, let us consider a moving

sine-wave grating, widely used in visual experiments. The spatial
pattern can be expressed as sin(u.X) where u is the spatial frequency.
It moves at velocity V, and since X = (x − V.t) the space-time image is:
I(x,t) = sin\[u(x − V.t)\].

Examples of space-time images for moving gratings are given in Figure
8.3A. Note how the tilt in space-time increases with speed, through a
lateral shearing of the space-time image (not a rigid rotation). The
temporal and spatial derivatives are

8. COMPUTATION OF IMAGE MOTION

∂I/∂t = −u.V.cos\[u(x − V.t)\] ∂I/∂x = u.cos\[u(x − V.t)\]

and their ratio gives velocity (−V), as expected. It is also worth
noting that we can rewrite I(x,t) as sin(u.x − u.V.t). If we introduce a
new term w = u.V, then I(x,t) = sin(u.x − w.t)

and we can see that the space-time image not only has a spatial
frequency (u) but also a temporal frequency (w). While the spatial
frequency is the number of bars (cycles) per unit distance, the temporal
frequency measures the number of bars (cycles) passing a given point per
unit time, in cycles per second, often expressed as Hertz (or Hz).
Figure 8.3B illustrates the relationship between spatial frequency,
temporal frequency, and velocity for a moving grating. Temporal
frequency describes the rate of luminance fluctuation at a given point
as the grating moves past it, and the temporal waveform can be seen by
examining a vertical cross-section of the space-time image. Just as
spatial acuity in vision is defined by the highest visible spatial
frequency---about 40--50 c/ deg for human foveal vision---so temporal
acuity can be defined by the fastest visible flicker rate---a temporal
frequency of about 40--50 Hz in ordinary viewing conditions. In general,
the visibility of a grating (assessed by contrast sensitivity) depends
on both spatial frequency and temporal frequency as shown by the
experiments of Robson (1966) and Kelly (1979). Human visual sensitivity
is greatest at roughly 5 c/deg and 5 Hz, and there is a pleasing
similarity and symmetry in the spatial and temporal properties of
contrast sensitivity. Since w = u.V, it follows that for a moving
grating, speed V equals the ratio of temporal to spatial frequencies (V
= w/u). For example, a grating whose spatial frequency is 2 c/deg and
whose temporal frequency is 10 Hz has a speed of 5 deg/sec. It is
tempting to think that speed might be encoded by measuring temporal and
spatial frequency, and then taking their ratio. This would work for
gratings, which are synthetic images created in the laboratory, but
would not generalise easily to

213

other patterns that are not periodic and have no dominant spatial
frequency. By contrast, obtaining speed from the derivative ratio
discussed above would apply equally well to all images and so offers an
attractive general theory (Harris, 1986). There is however an important
difficulty in the analysis presented so far. We have assumed that motion
is present, and then deduced that its velocity in the x-direction is
given by the ratio of temporal to spatial gradients. This is fine for a
moving image, but what if motion is not present? If the image is
flickering, but not moving, the derivative ratio can still be computed
in most places, but obviously it should not be taken as a measure of
velocity. This reveals that there are really two problems to be solved
in elementary motion analysis: to establish that motion is present, and
to compute its speed and direction. The solution adopted by nature, and
by computational vision theorists, is to incorporate motionspecific
filters or motion detectors to analyse the space-time image.

MOTION DETECTORS We consider first some of the evidence that animal and
human visual systems have such direction-selective mechanisms, and use
them to represent motion, and then move on to consider how such
mechanisms can be built from simpler filters that are not
direction-selective. We shall then be able to see how motion detector
models can be integrated with the derivative ratio idea both to signal
the presence of motion and to compute its velocity.

The motion aftereffect One of the oldest and most robust observations in
the history of vision research is the motion aftereffect (MAE). If you
look at a moving pattern for a few seconds or minutes, and then stop the
motion, the pattern will appear to be moving in the opposite direction
for some time afterwards. The illusory movement of the MAE typically
appears slower than the motion that induced it, and the duration of the
MAE tends to increase

214

VISUAL PERCEPTION

(A) Space-time image for a moving grating of fixed spatial frequency, at
    several speeds, V. (B) Cross-sections of the space-time image allow
    one to visualise the spatial frequency (u) and temporal
    frequency (w) of the moving grating. Note that for a moving grating,
    w = u.V, and V = w/u.

with the duration of inspection. Thus the MAE is a visual aftereffect,
like the tilt aftereffect discussed in Chapter 5 (p. 116), and many
hundreds of experimental studies of the MAE have been published since
Wohlgemuth's (1911) classic monograph (see Mather, Verstraten, & Anstis,
1998, for a recent review). Researchers have an enduring interest in the
motion aftereffect because of its usefulness in probing the mechanisms
and processes of motion coding in human vision. The emerging picture, as
we shall see, is that motion is analysed in a series of processing
stages, beginning with a very local analysis of 2-D image motions and
building in later stages to a more coherent, global description of 3-D
motion and spatial structure. Physiological, anatomical, and fMRI
studies show that the dorsal pathway

from V1 and V2 to MT and MST (see Chapter 3) is a major route for motion
analysis. There is little doubt that the motion aftereffect results from
temporary de-sensitisation of direction-specific neural mechanisms
(motion detectors) stimulated by the adapting motion. After inspection
of gratings moving (say) to the left, observers' contrast sensitivity
for leftwardmoving gratings is much reduced, but sensitivity for
rightward-moving gratings is only slightly affected (Sekuler & Ganz,
1963; Sharpe & Tolhurst, 1973; Tolhurst, 1973). Thus contrast detection
mechanisms are direction-selective, and adapt selectively, provided the
speed of movement is greater than about 0.5--1 deg/sec (see Graham,
1989, Ch. 12 for a compact graphical summary of the evidence). Similar
direction-

8. COMPUTATION OF IMAGE MOTION

selective loss of contrast sensitivity is found when a moving grating is
masked by a superimposed pattern moving in the same direction (Anderson
& Burr, 1985; Burr, Ross, & Morrone, 1986). But at slower speeds it
appears that the most sensitive mechanisms---those activated at contrast
threshold---are not direction-selective. To see why perceived motion
occurs as an aftereffect, let us consider horizontal motion, and suppose
that there are some mechanisms or "detectors" specifically responsive to
leftward movement, and others responsive to rightward movment. After a
period spent adapting to leftward movement, the detectors for leftward
movement will be less responsive than those for rightward movement (as
first shown by Barlow & Hill, 1963, in the rabbit's retina). Testing
with a stationary pattern will then produce a stronger response from the
rightward than the leftward mechanisms. To explain the appearance of
illusory rightward motion we must suppose that some comparison is made
between the activities of different detectors and that when rightward

215

units are more active than others, then motion is seen to the right. In
humans, the motion-sensitive units are almost certainly located in the
visual cortex, rather than the retina or LGN. Like the tilt aftereffect,
substantial interocular transfer of the MAE is found when one eye is
adapted and the other is then tested, implying that binocular cortical
units are responsible for much of the effect (e.g., Moulden, 1980).
Nevertheless, the MAE has a monocular component too. If the left and
right eyes are adapted to opposite motions, or motions at right angles
to each other, and then the eyes are tested separately, each eye sees
its own aftereffect (Anstis & Duncan, 1983; Grunewald & Mingolla, 1998).
This does not imply that the source of the MAE is literally in each eye,
but it does imply the existence of monocular, direction-specific
mechanisms as well as binocular ones (Figure 8.4). These two sets of
results suggest that V1 is one important site for the effect, where both
monocularly and binocularly driven cells are found. This conclusion is
supported by physiological

Properties of the motion aftereffect (MAE) imply the existence of
different direction-selective motion mechanisms (short arrows), driven
by the left and right eyes (MONOC) and by both eyes (BINOC). These units
are probably located in primary visual cortex, area V1. Interocular
transfer of the MAE demands the presence of BINOC units, while
eye-specific MAEs require the MONOC units. Such variation in
binocularity is consistent with that found physiologically.

216

VISUAL PERCEPTION

studies in cat and monkey showing that substantial numbers of V1 cells
are selective for direction of motion, while cells in the retina and LGN
are not, and by the finding that direction-selective cells do in fact
adapt to motion by reducing their responsiveness in much the way we
would expect from the properties of the MAE (Barlow & Hill, 1963;
Hammond, Mouat, & Smith 1986; Vautin & Berkley, 1977). Since it is
possible to perceive motion in any spatial direction we might expect
there to be cells responsive to many different directions around the
clock. This is indeed found in cat and monkey cortex, and psychophysical
evidence points to the same conclusion. Levinson and Sekuler (1976)
devised a novel form of MAE which we might term the "directional
aftereffect"---a movement version of the tilt aftereffect. Subjects
adapted to a random dot pattern drifting at constant speed in a given
direction, say +30° from horizontal. They were then tested on a similar
pattern drifting horizontally, and asked to judge its perceived
direction. There was a large shift of perceived direction, of up to 10°
away from the adapting direction. This important result implies that
there are different directional units in human vision and that the
distribution of activity across different units serves to encode the
direction of movement in 2-D (x,y) image space. When the response
distribution is biassed by adaptation, so the perceived direction shifts
towards the direction of the unadapted units, away from the adapting
direction.

Contrast sensitivity for movement and ﬂicker Alongside the MAE, several
other experimental procedures have contributed to our understanding of
early, direction-selective filtering. Experiments on the detectability
of low-contrast moving and flickering gratings have been especially
revealing. To appreciate one of the most important results, we need a
little basic trigonometry which will also prove useful later. We
introduced the algebraic expression \[sin(u.x − w.t)\] for a rightward
moving grating above, and we now give an expression for a flickering
grating. If we let the spatial profile be sin(u.x) where u is the
spatial frequency of the grating, and the temporal

waveform be cos(w.t) where w is the temporal frequency of flicker then
the space-time image, I(x,t), (see Figure 8.5A, right) is simply their
product: I(x,t) = sin(u.x).cos(w.t),

and we can now see an important, perhaps surprising, relation between
movement and flicker. In words, a flickering grating is the sum of two
similar moving gratings drifting in opposite directions. In symbols, the
sum of two similar, leftward and rightward moving gratings is: F(x,t) =
sin(u.x + w.t) + sin(u.x − w.t)

and from a standard trigonometric identity it follows that F(x,t) =
2.sin(u.x).cos(w.t),

which represents a flickering grating whose contrast is twice that of
its moving components. In a kind of shorthand, we might say that
(leftward + rightward) = 2.(flicker), as illustrated graphically in
Figure 8.5A. Thus a flickering grating is a space-time plaid, formed
from the sum of two oriented (i.e., drifting) sinusoidal components.
Suppose we present one of these moving components at a very low contrast
that is barely detectable, and then add to it a similar grating moving
the other way. Will the image now be more visible? Since the image
contrast is doubled we might intuitively expect sensitivity to double as
well. In fact, there is little or no increase in visibility when the
second grating is added to the first one (Kelly, 1979; Levinson &
Sekuler, 1975; Watson, Thompson, Murphy, & Nachmias, 1980). This key
result implies that the two moving components of a flickering grating
are detected separately and independently, by directionselective
mechanisms---"movement detectors". This conclusion has been found to
hold for a wide range of spatial and temporal frequencies, again
provided that the equivalent speed (V = w/u) is faster than about 0.5--1
deg/sec (see Graham, 1989, Ch. 12). Similar psychophysical results have

8. COMPUTATION OF IMAGE MOTION

217

Space-time images. (A) If two drifting gratings, each with contrast c,
are superimposed additively, they form a flickering ("counterphase")
grating of contrast 2c. Mathematically, this follows from application of
a standard trigonometric identity: sin(u.x + w.t) sin(u.x − w.t) =
2.sin(u.x).cos(w.t). (B) The reverse is also true: the addition of two
flickering gratings can form a moving grating. In this example, the
rightward components of flicker are in phase and so reinforce each
other, whereas the leftward components are out of phase, and cancel. (C)
Small patches taken from the images of panel B. This shows how an
oriented receptive field can be formed as the sum of two nondirectional
fields, discussed in Figure 8.6D.

been obtained using monkeys as observers (Merigan et al., 1991). The
detection of time-varying contrast is thus direction-selective, except
at slow speeds, and this conclusion is reinforced by experiments that
have compared the detection and identification of moving gratings at
very low contrasts. When

subjects could detect the presence of the moving grating at all, they
could also report its direction of movement (leftward or rightward)
(Thompson, 1984; Watson et al., 1980). Such results are hard to explain
unless the filters operating in these conditions are selective for
direction of movement, and also signal that direction perceptually.

218

VISUAL PERCEPTION

How might such filters be constructed by the nervous system?

Models of motion detectors: Direction selectivity Since a moving image
is oriented in space-time, while a flickering image is not, it follows
that a motion-specific filter must have a receptive field that is
oriented in space-time, in much the same way as filters can be oriented
in (x,y) space. In the (x,t) co-ordinates of Figure 8.3, a filter
oriented clockwise from vertical would respond best to rightward
movements while an anti-clockwise orientation would respond best to
leftward motion. How could visual systems build such movement-sensitive
filters? All attempts to account for the direction selectivity of motion
mechanisms involve combining the outputs of pairs of filters that are
not themselves directionselective, but whose receptive fields are
displaced in space and time, relative to each other, in order that the
combination of the two filters becomes "tuned" to motion. To start with
a simple example, imagine two adjacent receptors, P and Q separated by
distance δx. A moving spot of light stimulates P, then stimulates Q
after a time lag δt, where V = δx/δt is the speed of movement. A
mechanism (let's call it "M" for motion-sensitive) that delayed P's
response by δt, then added it to Q's response would clearly be "tuned"
to velocity V, because at that velocity the responses from P and Q would
coincide and reinforce each other more than at other velocities. At
other speeds the individual inputs to M from P and Q would of course
still exist and so the mechanism would not be highly specific for
velocity. The space-time receptive field of the hypothetical mechanism M
is illustrated in Figure 8.6A. Just like a spatial receptive field, the
spacetime receptive field represents the sign and strength of the
influence that different points in space and time have on the current
response of a cell at a given position. The influence may be positive
(excitation, +) or negative (inhibition, −). Since no cell can
anticipate the future, all inputs that influence a cell at a given time
must come from earlier times. A delayed input is thus repre-

sented by a shift down the time axis, because it arises from even
earlier points in time. The combination of delayed and nondelayed inputs
forms an oriented receptive field in space-time, with some crude
preference for rightward movement (Figure 8.6A, right). Figure 8.6B
shows, however, that a more highly tuned filter can be formed by
exploiting delayed lateral inhibition (Barlow & Levick, 1965) rather
than delayed summation. If the summation device M receives direct
excitation (left) and delayed inhibition (centre panel) from points
shifted somewhat to the right then its receptive field (right panel) is
tuned more specifically to rightward motion, and responds much less to
leftward. Barlow and Levick (1965) obtained evidence from the rabbit's
retina that this interaction occurs separately in many small regions of
the receptive fields of some retinal ganglion cells, and so the cell may
sum the outputs of many units of the kind shown in Figure 8.6B. The
reader may notice from Figure 8.6B that the Barlow--Levick mechanism is
actually computing a space-time derivative, making it sensitive to
space-time gradients in a given direction. It can be seen that the goal
is to build an oriented spacetime receptive field. A third approach is
to start by computing separate spatial and temporal derivatives (Figure
8.6C), and then add them to create the oriented space-time receptive
field. This approach, inspired by the derivative ratio algorithm for
velocity (see above) is the essence of the Marr-- Ullman model (Marr &
Ullman, 1981). They attributed the spatial differencing operation to the
action of "sustained" X-cells and the temporal differencing to
"transient" Y-cells. It is interesting to compare Figures 8.6B and C to
see how similar the outcome of the Barlow--Levick and Marr-- Ullman
schemes for direction selectivity can be. Finally, a fourth highly
influential model for motion coding was proposed by Adelson and Bergen
(1985), based on the "motion sensor" of Watson and Ahumada (1985). It
can be seen as a generalisation of the Marr--Ullman scheme, and like all
earlier schemes it combines two nondirectional filters to form a
directional one (Figure 8.6D). The underlying logic is illustrated in
Figure

8. COMPUTATION OF IMAGE MOTION

219

Building a movement detector in the space-time domain. (A) Simple
delayed summation of the output of two adjacent receptors creates an
oriented space-time receptive field, but better schemes create greater
direction selectivity. (B) Excitation (light blob) combined with delayed
lateral inhibition (dark blob) creates an "operator" oriented to the
right that suppresses any response to leftward movement. (After Barlow &
Levick, 1965.) (C) The same oriented (direction-selective) filter can be
created as the sum of spatial and temporal differencing (derivative)
operations. (After Marr & Ullman, 1981.) (D) Oriented filters can be
created by combining receptive fields that have odd and even symmetries
in space but even and odd symmetries in time. Note that panel C is a
special case of this general strategy. (After Adelson & Bergen, 1985.)

8.5B,C. In brief, the figure shows that just as two flickering patterns
can be added together to form a moving one, so two nondirectional,
flickersensitive receptive fields can be added to form a
motion-sensitive (direction-selective) one. This logic is described
further in Box 8.2. For some years researchers have attempted to
distinguish experimentally between the three or four schemes sketched in
Figure 8.6. Psychophysical experiments discussed so far have implied
only the existence of direction-selective filters, and are therefore
consistent with any of the schemes. Since all the models share the
assumption of space-time oriented filters, it might be thought
impossible to distinguish them. However,

different models arrive at those filters through different stages, and
so experiments that could tap into the underlying stages could, in
principle, establish the most appropriate model. Direct evidence that
cortical simple cells have space-time receptive fields of the kind shown
in Figure 8.6D comes from sophisticated physiological analyses of
responses in the cat and monkey visual cortex (De Valois et al., 2000;
McLean & Palmer, 1989, 1994). All the simple cells that were classed as
having space-time oriented receptive fields were indeed
direction-selective in response to moving bars, and the preferred
(optimal) velocity of these cells was well predicted by the space-time
orientation of the response field

220

VISUAL PERCEPTION

Box 8.2

Creating a direction-selective, space-time oriented ﬁlter

We saw in Figure 8.5A that nondirectional flicker is the sum of
rightward (R) and leftward (L) movements; in shorthand: F1 = R + L.
Inverting the contrast of (say) the L component creates a second,
similar pattern of flicker (F2), which is shifted in space and time
relative to the first one: F2 = R − L. It follows that if we add the two
flicker patterns together the L component is cancelled to yield a
rightward-moving one: (F1 + F2) = (R + L) + (R − L) = 2.R (Figure 8.5B).
To create (simulate) receptive fields using this logic, the only further
step is to restrict the images to a local "window" in space and time.
This can be done by multiplying each of the extended patterns of Figure
8.5B by a localised Gaussian (or similar) window function. The result is
shown in Figure 8.5C. The essential logic of the operation is preserved,
in that the two nondirectional, flicker-sensitive fields respond to both
R and L directions; we then add them such that their L responses always
cancel, but their R responses reinforce each other, and a
directionselective receptive field is created. This process can be
neatly expressed in one equation. If we let the window function be
G(x,t), then the two non-directional fields are G.sin(u.x).cos(w.t), and
G.cos(u.x).sin(w.t). Note that the two spatial profiles are 90° apart in
phase (sine vs cosine) and may be described as "orthogonal" to each
other, or "in quadrature". The same holds for their temporal profiles.
The movement field M(x,t) is their sum: M(x,t) = G.sin(u.x).cos(w.t) +
G.cos(u.x).sin(w.t) = G.sin(u.x + w.t), which defines an oriented
receptive field tuned to spatial frequency (u), temporal frequency (w),
and speed V = w/u. Physiological evidence supporting this scheme for
creating direction selectivity of simple cells in the monkey's visual
cortex (V1) was recently described by De Valois, Cottaris, Mahon, Elfar,
and Wilson (2000).

(McLean, Raab, Palmer, & 1994). Importantly, the spatio-temporal
filtering underlying this direction-selectivity does seem to be
essentially linear (additive) as our discussion so far has presumed,
with nonlinear effects (squaring, or rectification, and contrast gain
control) occurring later at the output of the cell (Albrecht & Geisler,
1991; DeAngelis, Ohzawa, & Freeman,; 1993 Heeger, 1992b). The
nondirectional subunits that carry out temporal differencing probably
receive their inputs from fast, "transient", biphasic Mcells of the LGN,
while the other subunits (Figure 8.6D) are driven by slower, "sustained"
monophasic P-cells (De Valois et al., 2000). This evidence from the
primate visual cortex is directly analogous to Marr and Ullman's (1981)
original proposal about the role of Y- and X-cells in the cat. It
supports other evidence (see p. 50) that M and P inputs are mixed
together quite early on in primary visual cortex.

Psychophysical support for the gradient scheme (Figure 8.6C) or its
generalisation (Figure 8.6D) comes from an intriguing variant of the
motion aftereffect, discovered by Anstis (1990). The observer inspected
a uniform patch of light whose brightness increased smoothly over time,
jumped back down, increased smoothly again, and so on. After adapting to
this for some time there was a negative aftereffect: a steady patch
appeared to grow dimmer over time. This aftereffect suggests that there
are visual mechanisms sensitive to the direction of temporal luminance
gradient at a given location. Second, and most importantly, a stationary
test edge located in the patch appeared to be moving, either to the left
or right depending on the luminance polarity of the edge, even though
the adapting stimulus had not moved at all. This paradoxical motion
aftereffect makes good sense in the Marr--Ullman gradient scheme,
however, since motion is sensed by a

8. COMPUTATION OF IMAGE MOTION

combination of spatial and temporal gradient signals. The real spatial
gradient given by the test edge and the illusory temporal gradient given
by the negative aftereffect of adapting the temporal derivative operator
(Figure 8.6C) combine to signal motion in the usual way. Reversing the
sign of one or other gradient reversed the perceived direction of
motion, as the gradient model predicts. Anstis reported that the
aftereffect was purely monocular (it did not occur if one eye was
adapted and the other eye was tested), consistent with the idea that the
spatial and temporal gradient operators lie at the earliest stage of
motion analysis.

ENCODING LOCAL VELOCITY We now have a framework for thinking about
space, time, motion, and direction selectivity. But the evidence for
direction selectivity, and the models devised for it, do not tell us how
velocity of movement is made explicit. We began with this as a key
problem, and we must return to it. McKee, Silverman, and Nakayama (1986)
found that perceived velocity of moving gratings, and the ability to
discriminate small differences in velocity, was little affected by
randomisation of spatial and temporal frequencies, provided that the
physical velocity V was preserved. This implies that, at some level, an
explicit velocity code is established that allows vision to compare
speeds between different spatial and temporal patterns. The simplest
general algorithm for velocity is the derivative ratio, V = −
(∂I/∂t)/(∂I/∂x), but in this form it suffers two problems. First, this
simple ratio "blows up" (goes to infinity) when the spatial gradient
∂I/∂x = 0. This drawback is illustrated in Figure 8.7. The input
space-time image (A) represents a grating oscillating back and forth
over time, and its temporal and spatial gradients are shown in panels C
and D. Despite the apparent complexity of these gradient maps, the
simple ratio of the gradient values (panel B) correctly captures the
variations of speed and direction. But as expected it shows local
failures of velocity coding (sharp points in B) where spatial gradient

221

= 0. This problem can be overcome, however, by an elaboration of the
ratio rule termed the multichannel gradient model (Johnston, McOwan, &
Buxton, 1992), outlined in Box 8.3 The multi-channel model uses both 1st
and 2nd spatio-temporal derivatives to yield a more robust estimate of
velocity in an image. For moving images it will always deliver a true
estimate of local velocity in the x-direction, up to the limitations
imposed by biological or machine implementation of the algorithm. See
Johnston and Clifford (1995) for its further extension and success in
accounting for three illusions of apparent motion. We show next that, in
order to recover velocity explicitly, the "motion detector" or motion
energy model (Adelson & Bergen, 1985) requires further stages of
processing. We have seen so far that a quadrature pair of filters can be
summed to form an oriented one (Figure 8.6D). In the full motion energy
scheme there are several stages beyond this: the energy, opponency and
ratio stages, summarised in Figure 8.8. In outline, the four stages of
processing are as follows: (i) the initial, nondirectional filters are
combined in pairs to form directional filters; (ii) the outputs of the
directional filters are squared and added in pairs to compute motion
energy for each of the two directions; (iii) the difference between the
two energies (opponent energy) is a rough measure of the "amount" of
movement (and contrast) in the input; (iv) a more accurate velocity code
is obtained by comparing two energy measures whose ratio varies directly
with velocity. Box 8.4 gives further details of these computations,
showing that this more complete version of the energy model can recover
velocity exactly, and can be equivalent to the multi-channel gradient
model. This insight again highlights the family connection between
apparently different accounts of motion coding. The merger combines the
robustness of the energy model with the velocity specificity of the
derivative ratio, and provides an appealing, if somewhat idealised,
system for encoding velocity. Figure 8.9 illustrates all these points in
a worked example, showing how a velocity code emerges correctly at the
ratio stage, without the local failures of the simple gradient

222

VISUAL PERCEPTION

Velocity coding from space-time gradients: elementary model. Input
signal (A) is a grating oscillating back and forth over time. Its
contrast is graded across space (higher towards the left) to reveal any
dependence of coding on stimulus contrast. Temporal and spatial
gradients are mapped in C and D. The simple ratio of these gradients (B)
correctly captures the variations of speed and direction, irrespective
of contrast, but reveals local failures of velocity coding (sharp points
in B) where the spatial gradient = 0. This shows the need for an
elaborated model based on the same principle. In B, lighter points
indicate speed to the left, darker points indicate speed to the right.
Grey borders represent a value of zero.

8. COMPUTATION OF IMAGE MOTION

Box 8.3.

223

Multi-channel gradient model: Encoding velocity via 1st and 2nd
derivatives

We saw earlier that the intensity pattern of a moving image can be
expressed as some function I(x − V.t). Differentiating with respect to
space and time (as indicated by subscript letters, where Ix = ∂I/∂x, Ixx
= ∂2I/∂x2, Ixt = ∂2I/∂x∂t) we can recover velocity V in a variety of
ways:

∴

Ix = I′(x − V.t),

(1) 

It = −V.I′(x − V.t),

(2) 

It = −V.Ix,

(3) 

hence V = −It/ Ix. This re-states the simple ratio rule. Differentiating
(3) again yields: Ixt = −V.Ixx.

(4) 

Multiplying (3) by Ix and (4) by Ixx we get

and

Ix.It = −V.Ix.Ix,

(5) 

Ixx.Ixt = −V.Ixx.Ixx.

(6) 

Adding (5) and (6), with a weighting factor w2, and re-arranging, we get
a new rule for computing velocity: V = −(Ix.It + w2.Ixx.Ixt)/(Ix2 +
w2.Ixx2).

(7) 

This expression exploits the simple ratio rule twice by using second
derivatives as well as first, and does so more robustly because Ix and
Ixx tend not to be zero simultaneously, so that the bottom line of the
ratio is likely to be greater than zero. The formula can be extended
further by including third and higher derivatives too. Combining values
within a spatial neighbourhood is also possible and yields greater
reliability. The simple gradient ratio is a special case of equation
(7), dropping all derivatives higher than the first. Since each
successive derivative operation is implemented by a different
spatio-temporal filter or "channel", the model requires four or more
filters, and has been termed the "multi-channel gradient model" of
velocity coding. See Johnston et al. (1992) for more formal derivation
and extensive analysis.

ratio model (Figure 8.7). Johnston and Clifford (1995) say more about
versions of the multichannel model that are more realistically tuned to
experimental data. In essence, ratio models of this kind encode velocity
by comparing the outputs of "fast" and "slow" mechanisms. The velocity
tuning of the two mechanisms taken individually is not the critical
factor, provided that the ratio of their outputs increases with speed,
to give an unambiguous code for velocity. Psychophysical experiments
give broad support for this type of ratio model. First, there is
evidence from masking, adaptation,

and discrimination experiments (Anderson & Burr, 1985; Hammett & Smith,
1992; Hess & Snowden, 1992; Mandler & Makous, 1984) for just two (or at
most three) temporal "channels" in human vision, unlike the multiplicity
of spatial channels that have been revealed by similar techniques,
discussed in Chapter 5. One temporal filter is low-pass, while the other
one (or two) are bandpass in the temporal frequency domain. Second, the
ratio of the sensitivities of these two channels may depend directly on
velocity (Harris, 1980). It is tempting to associate the slow and fast
channels with P and M systems respectively (see

224

VISUAL PERCEPTION

The motion energy model. The model creates direction-selective filters
by adding and subtracting the responses of nondirectional filters. Each
of the outputs A, B, L1, R1, etc. should be thought of as a space-time
image \[A(x,t), B(x,t), etc.\] that is the output of a given filter. The
outputs are then squared and added to give leftward and rightward
energy. The opponent stage takes the difference of the two energy values
to eliminate responses to flicker. To encode velocity independently of
contrast and other factors, opponent energy must be scaled (divided) by
a suitable measure of "static energy". See text for details. This
diagram is faithful to the Adelson and Bergen (1985) model, except that
it shows a particular choice of input filters designed to produce ideal
coding of velocity at the output. These filters have space-time
receptive fields that are spatial and temporal derivatives of a Gaussian
function, as shown.

8. COMPUTATION OF IMAGE MOTION

Box 8.4

Computing velocity from motion energy

This box explains how nonlinear combination of filter outputs can be
used to compute velocity. It should be read in conjunction with Figure
8.8. There are four non-directional input units whose responses are
denoted by A, B, A′, B′(x,t). These are added or subtracted in
quadrature pairs to form four directional filters, two leftward and two
rightward, whose outputs are denoted L1, L2, R1, R2(x,t). The leftward
(and rightward) pair are 90° out of phase (odd and even) in space-time.
In symbols, the outputs are: L1 = A+B′, R1 = A−B′,

L2 = B−A′, R2 = B+A′.

(8) 
(9) 

Motion energy (leftward and rightward) is defined in the same way as
local energy or contrast energy (p. 107): Leftward energy: Rightward
energy:

EL = L12 + L22, ER = R12 + R22.

(10) 
(11) 

The purpose of this step is to deliver a signal that is smooth across
space and time, reflecting the pattern of motion flow, rather than the
undulating structure of the image in space and time. This emulates the
behaviour of complex cells rather than simple cells (see Chapter 3,
pp. 47--48). The next stage is an opponency stage, analogous to that in
colour vision, which takes the difference between the two motion
energies, to give: Opponent energy, EO = (ER − EL).

(12) 

After simple manipulation of symbols, omitted here, we get: Opponent
energy, EO = 4.(A′B − AB′).

(13) 

This opponent step enhances direction specificity, and eliminates any
response to stationary images or nondirectional flicker. But opponent
energy EO does not estimate velocity, partly because it also varies
enormously with contrast, being proportional to contrast-squared.
Adelson and Bergen (1985) noted that division by a "static energy"
signal S would eliminate the contrast dependence, but even this does not
guarantee the recovery of velocity unless the various filters are
carefully chosen. In an ideal algorithm we would want the motion/static
energy ratio (EO/S) to equal the true velocity, V, for all possible
moving patterns, and in principle this can be done. We have seen that A
and B are nondirectional spatial filters with the same temporal
response, but 90° apart in spatial phase, so let the static energy be: S
= 4.(A2 + B2).

(14) 

EO/S = (A′B − AB′)/(A2 + B2).

(15) 

Hence, dividing (13) by (14), Equation 15 makes an interesting
comparison with the gradient model, equation 7: V = −(Ix.It +
w2.Ixx.Ixt)/(Ix2 + w2.Ixx2).

(7) 

They are ratios of similar form, and in fact they become identical if we
let: A = Ix, B = −w.Ixx,

A′ = w.Ixt B′ = It.

(16) 
(17) 

It follows that with this choice of derivative filters, an energy ratio
model will always compute velocity correctly, because equation 7
guarantees that V = EO/S. Figure 8.8 illustrates the Gaussian derivative
filters used in this merger of the Adelson--Bergen and multi-channel
gradient models.

225

226

VISUAL PERCEPTION

Velocity coding: Two-channel gradient model using 1st and 2nd
derivatives shows reliable and accurate coding of velocity (B). With
this oscillating input, speed varies sinusoidally over time but is
constant across space. Note how the "energy" measures (C,D) decrease
from left to right with the local contrast of the grating, but their
ratio---the velocity code in B---eliminates this contrast dependence.
Based on the similarity between gradient and energy models, we defined
"opponent energy" = −(Ix.It + w2. Ixx.Ixt) and "static energy" = Ix.Ix +
w2. Ixx.Ixx (Box 8.3, Box 8.4), where w2 is a weighting factor, chosen
here to balance the 1st and 2nd derivative contributions. Because the
static energy term does not now fall to zero (black border in D), the
failures of velocity coding seen with the elementary gradient ratio are
eliminated. If the balance (weighting, w2) between 1st and 2nd
derivative terms is altered then the energy maps (C,D) would show
spatio-temporal ripple arising from the input signal, caused by
imbalance in the contribution from 1st and 2nd derivative filters, but
this does not reduce the accuracy of velocity coding (not shown). The
algorithm (Box 8.3) ensures that velocity coding is robust.

8. COMPUTATION OF IMAGE MOTION

Chapter 3, p. 47), since the behavioural contrast sensitivity of the
monkey is reduced for slow speeds of grating movement after
parvocellular (P) lesions in the retino-geniculate pathway (Merigan &
Eskin, 1986), but sensitivity is lost only at high speeds after
magnocellular (M) lesions (Merigan et al., 1991). The M/P sensitivity
ratio may thus depend directly on velocity. Third, adapting to moving
gratings influences the perceived speed of subsequently viewed test
gratings. Adapting gratings moving faster than the test speed tend to
make the test seem slower, and vice versa (Smith & Edgar, 1994;
Thompson, 1981). Smith and Edgar (1994) showed that their results were
broadly consistent with a ratio model for speed, combined with a
subtractive process of adaptation. In summary, then, there is
significant support for a fast/slow energy ratio model of velocity
coding, and this in turn is compatible with a spatiotemporal derivative
approach to motion analysis.

Energy and Reichardt models compared One of the attractions of the
energy model has been that it is consistent with physiological evidence,
giving a functional role to directionselective simple cells (L1, R1,
etc.) followed by complex cells (EL, ER) \[cf. equations 8--11, Box 8.4,
and Figure 8.8\]. Another important model of motion detection was
developed by Reichardt (1969), who used quantitative experiments on the
optomotor response of insects (see Chapter 11, p. 318) to account for
the first stage of motion processing in the insect retina. Reichardt's
"correlation" model has since been shown by Adelson and Bergen (1985) to
be functionally equivalent to the opponent energy model, but without the
final ratio stage that encodes velocity. Its output behaves in the same
way as the energy model, although it does not have the same linear,
direction-selective intermediate stages. The Reichardt model begins with
pairs of nondirectional input signals (A,B) along with delayed versions
of them (A′,B′), then forms nonlinear, direction-selective responses
directly by multiplication (A.B′, A′.B). The final stage takes the
difference of these two signals, to give an output equivalent to
opponent energy in the Adelson--Bergen model: (A′B − AB′)

227

(see equation 13). In other words, given the same input filters, the
outputs of the two models must be indistinguishable. Experiments on
motion perception that give quantitative support to the Reichardt model
(van Santen & Sperling, 1984) must equally support the opponent energy
model. We have already discussed the equivalence of the energy ratio and
extended derivative ratio models, and so it is tempting to conclude that
"all motion models are really the same". Tempting indeed, but also too
simple, because each "model" is actually a class of models whose
performance will depend on details of implementation, such as the number
of stages and choice of filter parameters, as well as the possible
inclusion of intensity nonlinearities, and contrast gain controls. What
we can say is that the three classes of model overlap interestingly and
significantly. Moreover, even where overall performance is equivalent,
the hardware implementation of the models may be distinguishably
different. In a detailed analysis of responses in the cat's cortex,
Emerson et al. (1992) showed that the behaviour of direction-selective
complex cells was nicely consistent with their being the nonopponent
motion energy stage of the energy model, but not consistent with any
stage of the Reichardt multiplier model. They did not find evidence for
an opponent stage in primary visual cortex, but it is now very clear
that motion processing is incomplete in V1, and that further elaboration
occurs especially in the dorsal pathway through V2 to MT(V5) and MST
that we discuss next.

A HIERARCHY OF PROCESSING IN THE MOTION ENERGY SYSTEM: FROM V1 TO MT It
is now evident that the different levels of the motion energy system
(Figure 8.8) are not to be found at a single site in the visual cortex,
but distributed across successive stages of the neural pathway from V1
to MT and beyond.

Motion and area MT A majority of cells in area MT are highly responsive
to moving images, and most are directionselective. Three key findings
illustrate why MT is

228

VISUAL PERCEPTION

now regarded as having a pivotal role in motion perception. First, in a
series of sophisticated experiments, Newsome, Britten and colleagues
evaluated the behavioural (psychophysical) performance of monkeys
trained on a motion perception task, while simultaneously recording the
responses of MT cells to the same moving stimuli. The task is known as
"coherent motion detection" and it has proved to be a key paradigm in
the study of later stages in motion perception. The visual display
contains hundreds of small moving dots in random positions on a screen.
At one extreme, all the dots may be moving in a given direction (e.g.,
upwards); this is 100% coherent motion. At the other extreme all the
dots may move in random directions; this is 0% coherent. The
experimenter can vary from trial to trial the percentage of "signal
dots" (those moving in a given direction) while the rest are random
("noise dots"), and the observer has to indicate the signal dot
direction (e.g., up or down). The monkey indicated its perceptual
decision by quickly moving its eyes to one of two lights around the edge
of the display. Naturally, at 0% coherence the task is impossible, but
surprisingly when the signal coherence is as small as 5--10% both human
and monkey observers can reliably distinguish direction of movement.
Performance clearly cannot be based on attending to single dots or small
areas, because at 10% coherence any randomly chosen dot is most likely
to be an irrelevant noise dot. Instead, this task seems to involve the
gathering of information over quite a wide area. Cells in MT have
receptive fields about 10 times the diameter of those those in V1, and
so MT seems well suited to carry out this spatial pooling of local
motion signals to deliver global motion perception. Newsome, Britten,
and Movshon (1989) worked out (using signal detection theory) how well
the monkey would do if its behaviour were based on the responses of
single MT cells, and found that on average this singlecell performance
was as good as the monkey's actual performance on the task. This most
striking finding implies that, in principle, motion perception and
behaviour in this coherent motion detection task could be guided by the
responses of single nerve cells in area MT.

Second, in another experimental tour-de-force, Salzman et al. (1990)
showed that electrical micro-stimulation of clusters of
directionselective MT cells tended to bias the monkey's perceptual
decisions in favour of the direction coded by those cells (say, upward).
This suggests that electrically induced firing of these cells created or
enhanced the perception of upward motion, just as if the stimulus had
contained more upward dots. Third, small lesions in MT selectively
impaired the ability to discriminate direction of motion in this task
(Newsome & Paré, 1988). Clearly, then, the firing of MT cells is closely
coupled to the detection, and perhaps the experience, of visual motion.
Within the last 10 years several techniques (TMS, fMRI, MEG) have been
developed that enable analogous tests of the human brain's organisation
for motion perception. One method---"transcranial magnetic stimulation"
(TMS)---uses pulses of magnetic energy applied to the scalp to disrupt
neural processing very briefly in the underlying brain tissue. Depending
on how it is done, TMS may be applied to quite a wide area of brain, or
to a localised region. Using the localised method, Beckers and Zeki
(1995) applied TMS over the estimated location for area MT+ (or V5; see
Chapter 3, p. 63). They found that the observer's ability to report
motion direction was completely disrupted if the random-dot motion
occurred within 20 ms of the TMS pulse. When the TMS was applied more
than 30 ms earlier or later than the moving stimulus, it had no effect
on motion detection. TMS applied to MT+ had no effect on a static
pattern detection task, and so these experiments and those above point
to a specific role for MT+ in motion perception for primates---both
humans and monkeys. Functional imaging (fMRI) methods record images of
human brain activation, and enable maps of activation to be related to
the stimulus information presented. In general, it is emerging that
different visual areas are indeed activated by different kinds of
stimulus information, but this activation is by no means a passive,
stimulusdriven affair. As we saw in Chapter 3, the level of activation
also depends on the visual task that the observer is engaged in, or the
degree of attention

8. COMPUTATION OF IMAGE MOTION

that is paid to one location or one kind of information versus another
(Ress, Backus, & Heeger, 2000; Tootell et al., 1998). For example, fMRI
responses from human MT were greater when the observer had to
distinguish between two slightly different speeds of motion ("speed
discrimination") than during passive viewing of the same moving images
(Huk & Heeger, 2000). Moreover, these task-related enhancements of MT
response are based on selective attention to movement. Huk and Heeger
also found that MT responses were greater during speed discrimination
than when observers had to distinguish small differences in contrast of
the same moving images, while ignoring speed. They suggest that when
observers attend to the motion of a stimulus, responses in MT are
enhanced (an increase in response "gain"), and that this will lead to an
increase in signal-to-noise ratio which in turn improves performance on
the motion task. Such ideas are important in bringing cognitive
("top-down") and psychophysical ("bottom-up") approaches into a common
conceptual framework. These attentional effects on human brain responses
are reflected in single-cell studies of MT (and other visual brain
areas). For example, Treue and Trujillo (1999) found that MT cell
responses to moving dots were influenced by where the monkey was
attending, and what motion it was attending to. In one experiment, two
separate areas of dots moving in different directions were placed within
the receptive field. Responses were enhanced by about 30% (relative to a
no-attention baseline) when the monkey attended to the dots moving in
the cell's preferred direction, while responses were depressed by about
30% when the monkey attended instead to the dots moving in the "null" or
nonpreferred direction. Note that the stimulus conditions and direction
of gaze remained the same in both cases, only attention was shifted from
one set of dots to the other. Interestingly, the directional selectivity
or "tuning" of MT cells was not affected by attention. Cells typically
responded to about a ±60° range of movement directions around the
optimal or preferred direction, with similar bell-shaped tuning curves
in all cases. Attention raised or lowered response rates in a
proportional (multiplica-

229

tive) fashion, which further supports the idea (above) that attention
(or inattention) varies the "gain" of visual cell responses.

From V1 to MT With these general ideas in mind, we turn now to a more
detailed consideration of the functional architecture of the motion
system from V1 to MT. Based on the work of Qian, Andersen, and Adelson
(1994a,b) and others, Figure 8.10 provides a schematic summary of this
system that helps to organise ideas, theories, and findings in both the
physiology and psychology of motion perception. This outline scheme
identifies five main levels, and (on the left) identifies four main
properties that are introduced in the transition from one level to the
next. To anticipate, as we ascend the hierarchy, we see progressive
increases in direction selectivity, in binocular summation and disparity
specificity, in motion opponency and velocity specificity, and in
spatial aggregation of local motion signals to achieve "global" motion
perception. At the lowest level, we have already seen that
direction-selective filters can in principle be built by combining
non-directional ones whose receptive fields compute spatial differences
in luminance (broadly labelled "d/dx" in Figure 8.10) and temporal
differences ("d/dt"). This is achieved quite early on, in layer 4 of V1.
In the cat's primary visual cortex (area 17), for example, simple cells
of layer 4 may be highly selective for movement direction, or they may
not, but those that are highly directional tend to have clearly oriented
space-time receptive fields while those that are not directional do not
(DeAngelis et al., 1993; Murthy, Humphrey, Saul, & Feidler, 1998).
Compare this finding with the model filters in the second and first rows
of Figure 8.8 respectively. When inhibitory interaction between these
cortical cells was blocked by local application of bicuculline (which
counteracts the inhibitory transmitter GABA), many cells lost their
direction selectivity, and their space-time receptive fields became
correspondingly non-oriented (Murthy & Humphrey, 1999). This shows that,
for some cells, direction selectivity requires inhibitory (subtractive)
input from other nearby cells. Compare this with the theoretical
directional filters in

230

VISUAL PERCEPTION

The motion processing hierarchy. (Adapted from Qian et al., 1994a, Fig.
8.)

Figure 8.8 that do the same: R1 = A − B′, or L2 = B − A′. The minus sign
here may thus stand for subtractive inhibition between cells; if it were
blocked the responses would revert to just A or B, which are not
direction-selective, as observed. Other cells studied by Murthy and
Humphrey did not lose their direction selectivity when intracortical
inhibition was blocked with bicuculline; these might rely on local
excitatory input from other cells, rather than inhibition: for example,
L1 = A + B′, R2 = B + A′ in Figure 8.8.

Binocular summation and motion In general, as we ascend the processing
hierarchy, cortical areas seem to become more binocularly driven, and to
be increasingly concerned with binocular processing. It becomes
interesting, therefore, to ask how the circuitry for motion processing
relates to that for binocular processing. Some direction-selective
simple cells in V1 are

largely monocular while others are binocularly driven. In binocular
simple cells, the space-time receptive fields for each eye tend to be
very similar in most respects including size, preferred orientation and
speed, and direction selectivity (Ohzawa et al., 1996). This kind of
matching may be necessary for binocular cells to participate in
disparity coding (Chapter 7). More specifically, if the receptive field
for the left eye shows direction selectivity then so does the right eye.
This suggests that direction selectivity precedes binocular summation,
and in Figure 8.10 we have sketched the formation of direction
selectivity as if it were entirely monocular---as if the spatial and
temporal subunits must necessarily come from the same monocular pathway.
Psychophysical experiments have tested this assumption of monocularity
by presenting image sequences that can be seen as moving only if the two
eyes combine their information. We saw in

8. COMPUTATION OF IMAGE MOTION

Figure 8.5B that a drifting grating can be formed by adding together two
flickering gratings. If we present one such flickering grating to the
left eye, and the other to the right, will both eyes together see
movement? According to the monocular hypothesis, they should not. The
experimental answer, however, is that motion direction can be reliably
seen, even though each eye alone sees only flicker (Georgeson &
Shackleton, 1989; Shadlen & Carney, 1986). Thus motion perception can
work between the eyes as well as monocularly. A complication is that we
might see this "dichoptic motion" by tracking the positions of features
seen by either eye over time, rather than by using the motion energy
system. Indeed, using a display that put feature tracking and motion
energy in opposition, Georgeson and Shackleton (1989) found that the
direction predicted by motion energy was seen during monocular viewing
but with interocular (dichoptic) viewing observers saw motion in the
feature direction. This is consistent with motion energy detectors being
mainly monocular, but it does not prove that they always are. Moreover,
a recent study provides clear evidence that dichoptic motion can be seen
even when feature tracking is prevented by another cunningly designed
motion sequence. When a moving grating is added to a stationary grating
(known as the "pedestal"), the pedestal alters the space-time image such
that local features merely oscillate back and forth, rather than moving
in one direction (Lu & Sperling, 1995). Thus the pedestal disables
feature tracking but does not disable the motion energy system because
that is "blind" to the nonmoving pedestal. Carney (1997) exploited this
logic to test for dichoptic motion detection without feature tracking.
He used flickering gratings in each eye, superimposed on stationary
pedestal gratings to prevent feature tracking. With two-eyed viewing,
direction of motion was readily visible across a wide range of
conditions, and so Carney concluded that the early motion system can
after all combine nondirectional information from each eye to create a
directional motion signal. A key factor may be that it needs time (1--2
seconds) to do it, perhaps to overcome

231

binocular rivalry. Using a 1 second display time, Derrington and Cox
(1998) found that contrast thresholds for detecting the direction of
dichoptic moving gratings could be as good as for monocular motion.
Since contrast thresholds of this kind almost certainly tap into the
motion energy system, these results support Carney's conclusion. The
physiological basis for dichoptic motion energy detection may be found
in the linear behaviour of simple cells. We have seen that simple cells
seem to combine space-time signals linearly (additively) to create
direction selectivity (DeAngelis et al., 1993), but perhaps more
surprisingly they also combine inputs linearly from the left and right
eyes (Anzai, Ohzawa, & Freeman, 1999; Smith, Chino, & Cheng, 1997). This
corresponds rather well with psychophysical findings on binocular
summation at threshold. Generally, contrast sensitivity with two eyes
has been found to be about 1.4 times better than with one eye, but this
binocular advantage seems to increase with movement. Contrast
sensitivity for moving gratings was nearly twice as good with two eyes
as with one (Arditi, Anderson, & Movshon, 1981; Rose, 1978), and this
implies linear summation of the two eyes' inputs for moving images.
Hence these linear binocular filters should be able to create a
directional mechanism by adding together two nondirectional inputs
between the eyes just as readily as within one eye. This would serve the
efficient detection of dichoptic motion observed in the laboratory
experiments discussed above, although it is not obvious that there are
any natural circumstances in which it would be needed. A full account of
the psychophysical evidence needs to consider the additional presence of
monocular detectors. In dichoptic experiments these will convey flicker
signals (not motion) that may make dichoptic motion more difficult to
see above threshold. Monocular detectors are also needed to account for
eyespecific effects in the MAE, for the finding that monocular
adaptation reduces contrast sensitivity more for the adapted eye than
the other eye (Anderson & Movshon, 1989; Bjorklund & Magnussen, 1981;
Blakemore & Campbell, 1969), and for some effects of visual noise on
monocular and

232

VISUAL PERCEPTION

binocular grating detection (Anderson & Movshon, 1989). In summary, it
seems that under appropriate conditions the motion energy system (and
other forms of motion coding; see later) can combine information
efficiently, probably linearly, between the eyes to extract direction of
motion, but monocular motion detectors also exist alongside the
binocular ones.

Motion opponency In all sensory systems information is coded by
comparing or combining different sensory inputs. In the motion system,
firing rates of directionselective cells in V1 depend on speed,
contrast, spatial frequency, and direction of movement. To disentangle
these factors, the system must combine the activities of different cells
in appropriate ways. Thus the motion energy model recovers speed and
direction by a three-way comparison that we can encapsulate as "(R −
L)/S": rightward minus leftward energy, divided by static energy
(Adelson & Bergen, 1985, 1986). This is the ratio model for velocity
discussed above. Whether the brain implements velocity coding in
precisely this fashion remains unclear, but two important features of
this model are mirrored by neurophysiological evidence: the subtractive
comparison of opposite motion signals that creates an opponentmotion
signal (R − L), and a divisive normalisation that renders the signal
invariant with contrast. These two features may be weakly present in V1
but appear to be strongly expressed in area MT. First, while V1 cells
show a graded response to different contrast levels, MT cells show much
greater saturation of their contrast response. As contrast increases,
the response at first increases rapidly but then levels off: over most
of the range of visible contrasts, the response of MT cells varies
selectively with factors such as speed and direction (Rodman & Albright,
1987), but does not change with luminance contrast (Sclar, Mounsell, &
Lennie, 1990). Simoncelli and Heeger (1998) have proposed that this
contrast invariance, or "saturation" of the MT response reflects the
action of a divisive contrast gain control, in which the responsiveness
of a given cell is reduced (div-

ided) by the pooled activation of many neighbouring cells. Thus as
contrast increases all responses might (say) double in magnitude, but
the divisive effect would also double, and the net response of the cell
would be unchanged. This can be seen as an active mechanism to make the
cell's response independent of contrast. According to Simoncelli and
Heeger's (1998) model, contrast invariance is more complete in MT than
in V1, because MT cells reflect the action of two gain controls---once
in V1 that provides input to MT and then again in MT itself. It would be
interesting to know whether these gain controls also mediate the
attentional gain effects discussed above (see Treue, 2001, for further
discussion). Second, many cells in MT are activated best by a particular
direction of movement but inhibited by other directions, including the
opposite direction, and this inhibition is often tuned to the preferred
speed of the neuron. For example, a cell that was activated best by a
bar moving to the right at 16 deg/sec would be inhibited most by a bar
moving left at that speed (Rodman & Albright, 1987). Several lines of
psychophysical evidence confirm "motion opponency" as a feature of human
vision. Perhaps the most complete example of motion opponency is to be
seen when we take a sinusoidal grating moving (say) rightwards and
additively superimpose a second grating (of the same spatial frequency,
contrast, and speed) moving in the opposite direction. The display now
contains equal motion energy leftwards and rightwards but all sense of
coherent motion is lost, and the appearance is of a flickering grating.
The complete loss of perceived motion may be attributed to motion
opponency that is part of motion energy analysis (Figure 8.10). Now, the
astute reader may object: we have already seen (Figure 8.5A) that the
combination of two equal and opposite moving gratings is a flickering
grating. Why should we be surprised that it appears so? What else could
it look like? The answer is that it could, in principle, appear
transparent with the two gratings sliding over each other in opposite
directions, but this appearance is never reported. This lack of
transparency becomes all the more interesting when we realise that many

8. COMPUTATION OF IMAGE MOTION

other patterns, whose motion energies are also balanced, do appear
transparent. For example, two superimposed random-dot patterns, drifting
in opposite directions appear as two "sheets" of dots sliding over each
other. Similarly, two superimposed gratings of different spatial
frequencies (e.g., 0.5 and 2 c/deg) moving in opposite directions also
appear transparent, with their own motions clearly visible (Qian, et
al., 1994a). In a key experiment Qian et al. (1994a) discovered that the
transparent motion seen with two moving random-dot patterns is killed
off when the dots moving in opposite directions are arranged in locally
matched pairs, rather than being randomly positioned. In the paired
display, like the case of matched moving gratings, observers see flicker
rather than transparent motion. These results imply that there is
localised opponency between motion signals, and that it is selective for
spatial frequency. Thus when the gratings are matched for spatial
frequency, or the opposite dots are locally paired, responses that would
otherwise signal opposite directions of motion cancel each other: R − L
= 0. When the motion signals are carried by filters tuned to different
spatial frequencies, or at different locations, it is evident that this
cancellation does not occur; the two sets of motion signals survive and
transparent motion is the resulting perception. Qian and Andersen (1994)
examined the physiological basis for localised motion opponency by
recording the responses of V1 and MT cells to the paired and unpaired
moving dot patterns just described. In general, MT cells were more
direction-selective than those in V1. Responses to paired or unpaired
dot patterns (100 dots in each direction) were lower than the response
to a single set of 100 dots moving in the preferred direction. This
suppressive effect was greater in MT than V1, but is not direct evidence
for opponency, because it could arise from the more general gain control
mechanism discussed above (Simoncelli & Heeger, 1998). When there are
200 dots present, the total stimulus energy is greater and so the gain
control will turn down the cell's sensitivity more than with 100 dots.
More specifically, it was the most direction-selective MT cells that
showed the most interesting opponent

233

effects. For these cells (about 40% of the total) the response to paired
dots was less than half the response to unpaired dots, even though the
number of dots (and stimulus energy) were the same. This finding
suggests a localised motion opponency that is strongest for the most
direction-selective cells of MT. Only a small effect of this kind was
found for V1 cells. Qian and Andersen (1994) therefore concluded that
motion opponency takes place mainly within small subunits (see Figure
8.10) that provide the direction-selective input to MT cells. Motion
signals will cancel each other only when they fall into the same
subunits. The role of motion opponency may be to enhance direction
selectivity of cells by suppressing residual sensitivity to the
"nonpreferred" direction, and also to suppress responses to flicker or
other temporal change that is not motion. In psychophysical experiments,
human sensitivity to motion, masked by flicker, was well predicted by
the combined effects of motion opponency and contrast gain control
(Georgeson & Scott-Samuel, 1999). Direct evidence for motion opponency
in the human brain was revealed by an fMRI study of human MT+ (see
Chapter 3, p. 63). Using Qian's paired dot display, Heeger, Boynton,
Demb, Seidemann, & Newsome, (1999) found that the response of MT+ was
greater for the unpaired dots than the paired ones, and this (as we saw
above) is an indicator of localised opponency. This was not generally
true of responses from V1. Likewise, responses from human MT+ were
greater for single moving gratings than for two opposed gratings (which
appear to flicker rather than move); the reverse was true for V1
responses. Overall then, in monkey and human, area MT shows clear signs
of local motion opponency but primary visual cortex (V1) does not.
Receptive fields of MT cells tend to be much larger than those at
earlier stages of the hierarchy, and this suggests that the responses of
many subunits are aggregated over a wide area to form the larger MT cell
field (Qian et al., 1994a; see Figure 8.10). This kind of spatial
pooling of local motion signals may be the basis for several aspects of
global motion perception, to which we now turn.

234

VISUAL PERCEPTION

GLOBAL MOTION There are many natural circumstances in which perception
needs to describe or represent the motion of a large group of items
whose individual motions are different, but which have something in
common. The motion of a flock of birds, corn waving in the wind,
shifting sands, rippling water, and so on, are all examples of this kind
of "global" motion perception. The Gestalt psychologists appealed to the
notion of "common fate" as a principle that would group such local
motions together. Indeed, since motion analysis in early vision seems to
employ quite small receptive fields (Anderson & Burr, 1991), the spatial
aggregation of local motion signals may well be necessary to perceive
the coherent motion of any extended surface or contour. A striking
example of the global capability of motion perception comes from an
experiment in which observers were able to judge the mean direction of a
random-dot display to an accuracy of 2--4°, even when the individual dot
directions were randomly spread over a range up to150° (Watamaniuk,
Sekuler, & Williams, 1989). The most widely studied global motion task
is the coherent motion detection task that we introduced earlier
(p. 228), in which the direction of a

set of signal dots (e.g., up vs down) has to be distinguished in the
presence of distractor ("noise") dots moving in random directions. To
encourage the use of global motion, and to prevent observers tracking a
particular signal dot, the display time or the lifetime of individual
dots is usually brief---a few hundred milliseconds at most. Our ability
to detect the global motion direction is not simply determined by the
number of signal dots presented, but by the proportion of signal dots in
the display. For example, with a display of 50 dots (Figure 8.11A), the
threshold for motion detection was about 5 signal dots, but with 100
dots in the same area (Figure 8.11B) the threshold increased to about 10
dots---about 10% signal dots in both cases (Edwards & Badcock, 1994).
When the signal dots were always light dots, but half the dots were dark
(Figure 8.11C) the threshold was still about 10% of the total dots. Thus
light or dark dots moving in random directions were equally "noisy" for
the detection of coherent motion signalled by light dots. This result
implies that the global detection system pools dot motion signals
without regard to contrast polarity, and is broadly consistent with a
motion energy system that throws away polarity information at the
squaring stage (Figure 8.8). Evidently the observer was unable to
improve performance by selectively attending to the light dots and
rejecting the irrelevant dark dots.

The type of display used by Edwards and Badcock (1994) to study global
motion perception. Signal dots (moving either up or down) were always
light. Dark and light "noise" dots (moving in random directions)
interfered with detection of coherent motion for light signal dots to
the same extent. Detection depended on the proportion of signal dots
present.

8. COMPUTATION OF IMAGE MOTION

Morrone, Burr, and Vaina (1995) studied the spatial extent of pooling in
global motion, and found evidence consistent with large receptive fields
for detection of coherent motion. Noise dots were scattered over a large
(10° diameter) display field, while signal dots were restricted to one,
two, four, or eight narrow sectors, as illustrated in Figure 8.12.
Coherent motion detection (for two highly practised observers) required
about 5% signal dots (18 dots out of 360) in all four cases, regardless
of whether the signal dots were concentrated in one sector or spread
over up to eight sectors of the display. Morrone et al. show that this
is consistent with a large detector that integrates motion signals
linearly over the whole display. Importantly, the observer appears
unable to ignore the noise-only sectors even though they are irrelevant
to the task. The large

The type of display used by Morrone et al. (1995) to study spatial
integration in global motion perception. Dashed lines (not visible in
the experiment) indicate sectors that contained signal dots as well as
noise dots. In different trials there were 1, 2, 4, or 8 signal sectors.
The pattern of motion could be expansion, rotation, or translation (see
Figure 8.13).

235

detector area "picks up" all the noise dots as well as all the signal
dots, and so performance is determined by the proportion of signal dots,
irrespective of their location. Further experiments showed that the area
over which motion signals are linearly summed can be as large as 40° in
diameter, and that fast speeds are best for these large, peripheral
fields---up to 300 deg/s (Burr, Morrone, & Vaina, 1998). Indeed, there
may be separate "global" integrators for slow and fast speeds, since the
masking effect of the noise dots turns out to be speed-specific.
Detection of slow signal dots (1.2 deg/s) was made worse by similarly
slow noise dots, but not by stationary or fast ones; conversely,
detection of fast dots (10.8 deg/s) was masked by fast dots but not slow
ones (Edwards, Badcock, & Smith, 1998). The proposed linear summation of
signal inputs across a wide area also corresponds well with the finding
that firing rates of MT cells increase in direct (linear) proportion to
the percentage of signal dots in such displays (Britten, Shadlen,
Newsome, & Movshon, 1993). This linear response has recently been
confirmed by human brain imaging, where the fMRI response from MT+ was
found to be a linear function of the percentage of signal dots (Rees, et
al., 2000). Britten et al. (1993) also found that when the signal dots
were moving in a cell's nonpreferred direction, then firing rates of MT
cells decreased systematically as the percentage of signal dots
increased. This appears at first sight to be evidence for opponent
inhibition at the "global" level, but it probably isn't. As Britten et
al. point out, when the number of dots in the display is held constant
(as is usual in these dot coherence studies) then increasing the number
of dots moving in the signal direction entails reducing the number that
move in other directions. Hence the apparent suppression by motion in
the nonpreferred direction can be largely explained by the removal of
motion energy from the cell's preferred direction. This interesting
twist of logic, suggesting a lack of global opponency, is consistent
with the evidence (above) that opponency, producing cancellation or
suppression of opposite motion signals, is a localised affair. Further
evidence that specific opponency does not take place between

236

VISUAL PERCEPTION

global detectors comes from psychophysical coherence detection
experiments in which about half the noise dots were moving in random
directions (as usual) while half were replaced by dots moving opposite
to the signal direction, or at right angles to it. If there were
specific opponency at the global level, the presence of these coherent
nonsignal motions should have reduced the detectability of the signal
dots, but in fact there was no change; coherence thresholds were about
10% in all cases (Edwards & Nishida, 1999). This confirms the idea that
opponency is local, not global.

Spatial patterns of global motion Morrone et al. (1995) found extensive,
linear summation of motion signals across space not only for the simple
kind of unidirectional motion ("translation") discussed so far, but also
when observers had to detect more complex 2-D patterns of motion
(rotation, expansion, contraction). This is perhaps surprising because,
for a pattern of rotating or expanding motion, the local motions are in
different directions (sometimes opposite directions) in different parts
of the pattern. Simply pooling the responses of subunits with similar
direction preferences across space would not serve to detect these more
complex, structured patterns of motion. Nevertheless, Burr et al.'s
results are nicely consistent with the finding of large receptive fields
in higher motion areas (MT and MST) that are thought to pool input from
smaller subunits (Figure 8.10). Cells in MST, but not MT, give selective
responses to rotation, expansion, and spiral motions (Duffy & Wurtz,
1991; Tanaka & Saito, 1989), and are thus thought to represent an even
higher stage of motion encoding than MT (Tanaka, 1998) concerned with
the analysis of optic flow produced by self-motion. A recent fMRI study
of the human brain found two distinct "hot spots" in the MT+ region that
were responsive to translation and to rotation or expansion respectively
(Morrone, Toselti, Montanaro, Fiorentini, Cioni, & Burr, 2000), and this
is evidence that the human brain also encodes simple (1-D) and more
complex (2-D) patterns of motion at different stages of the motion
hierarchy.

Linear summation across a set of subunits can also account for these
more complex motion detectors, provided that the direction selectivity
of the subunits is suitably arranged across space. Figure 8.13
illustrates the subunit arrangements that would be needed to create
large-field detectors for expansion and rotation as well as translation.
In psychophysical experiments Meese and Harris (2001) confirmed the
extensive spatial summation for detection of these motions across the
four quadrants of their display (which was similar to Figure 8.12, but
with light dots on a dark background). Moreover, they found direct
evidence that human vision has separate detectors for different complex
motions. When a display contained both expansion and rotation, detection
performance was only slightly better than for expansion or rotation
alone. This lack of summation between different motion patterns implies
that we have separate, independent detectors for rotation and expansion
(Figure 8.13), and when both motions are present the observer has only a
slightly better chance of detecting one or other motion than one alone.
These detectors can be regarded as templates that are constructed to
match different types of optic flow pattern, and provide information
about self-motion---where the observer is heading through the
environment (Chapter 11, p. 337; for review see Perrone, 2001). Whether
we have further specific mechanisms for other types of motion pattern,
such as spirals (Graziano, Anderson, & Snowden, 1994) or deformation, is
a hot topic of current research. A final, but more puzzling, aspect of
the global motion process is its extensive temporal integration. When
contrast thresholds are measured for briefly presented gratings, a
standard finding is that sensitivity improves with the duration of
presentation, but only for a period of about 100 ms or less, depending
on the stimulus conditions (Legge, 1978). This implies that local
contrastdetecting mechanisms are quite brisk, with a short period of
visual persistence, or temporal integration. Global motion detection
appears to be quite different, with evidence of linear temporal
summation for several seconds (Burr & Santoro, 2001). Global motion was
presented for different durations during a 10-second display of

8. COMPUTATION OF IMAGE MOTION

237

(A--C) Spatial arrangement of direction-selective subunits that may
underlie the selectivity of cells in areas MST and MT for complex
patterns of motion, and could account for the complete spatial summation
observed for detection of these motions psychophysically by Morrone et
al. (1995) and Burr et al. (1998). White background indicates positive
(excitatory) input from a subunit. (D) Grey background indicates
negative (inhibitory) input from a subunit. This would create
sensitivity to relative motion between target and background, observed
by Born and Tootell (1992) in some groups of MT cells.

noise dots. As usual, observers had to report the direction of movement.
Coherence thresholds for rotation, expansion, or translation improved
linearly with signal duration up to a limit of about 2--3 seconds---at
least 20 times longer than the usual timescale for integration of
luminance or contrast. This lengthy temporal summation is analogous to
the extensive spatial summation for global motion, and could be a
property of the higher stages of motion analysis, but it is difficult to
see the functional significance. As Burr and Santoro remark, in order to
control navigation through the world, changes in the optic flow field
should be immediately available without lengthy integration. Their
experiments showed lengthy integration for a consistent flow pattern,
not a changing one, and so it remains possible that changes in flow
pattern are immediately detected even against this background of lengthy
temporal integration.

Global motion and the MAE We saw earlier in this chapter how the motion
aftereffect (MAE) has been used to probe motion

mechanisms. In the conventional MAE (which has been known for centuries)
after a period of adaptation to motion a stationary test pattern appears
to be moving in the opposite direction. As discussed earlier, the
conventional MAE may well arise from adaptation of directional cells in
V1. Recently, however, the coherent motion task has been used to test
for the MAE at the level of global motion analysis as well. Observers
spend several minutes adapting to 100% coherent dot motion (translation,
rotation, or expansion), and are then tested on patterns that have some
percentage of signal dots moving in the same direction as the adapter,
or in the opposite direction (Blake & Hiris, 1993). Which pattern will
appear to have no consistent direction of movement? Normally, it is
patterns with 0% coherence that appear to be nondirectional, but after
adaptation these random motions appear to drift in a direction opposite
to the adapter---a motion aftereffect. The test pattern appears
nondirectional, however, when about 40--50% of the dots move coherently
in the adapting direction (Figure 8.14), thus cancelling the MAE. This
"nulling" or

238

VISUAL PERCEPTION

Motion aftereffect (MAE) assessed in the motion coherence task,
expressed as the % signal dots needed to cancel the MAE and make the
dynamic test pattern appear nondirectional (geometric mean of three
observers, from Steiner et al., 1994). Data show the MAE for one eye
(hatched bars) and the extent to which it transferred to the other eye
(open bars). Note that the basic MAE and the degree of interocular
transfer were both greater for expansion (Expand) and rotation (Rotate)
than for simple translation (Trans).

cancellation implies that the strength of this MAE is equivalent to
40--50% motion coherence, a very strong bias in motion perception.
Moreover, this MAE is observed strongly when one eye is adapted and the
opposite eye is tested (Figure 8.14). Such "interocular transfer"
assesses the degree to which the underlying process is binocularly
driven, and Fig 8.14 shows that the transfer was 76% for translation,
and as high as 85--90% for rotation and expansion (Steiner, Blake &
Rose, 1994). Such high binocularity, found especially for rotation and
expansion, suggests that the source of these MAEs may be at fairly high
levels of the motion system where cells are mainly binocular.
Nevertheless, these results do not tell us whether this form of MAE was
arising from local or global mechanisms. Snowden and Milne (1997)
extended the nulling technique to show that the MAE does have a strong
global component. A key part of the argument is that global detectors
for rotation or expansion combine different local motion signals (up,
down, left, right) from different locations (Figure 8.13). Thus if the
MAE can arise from adaptation of these global detectors, it

should be possible to adapt the mechanism using up and down motions in
two quadrants (Figure 8.15, solid arrows) and yet reveal the MAE by
nulling it with left and right motions in the other two quadrants
(Figure 8.15, dashed arrows). This was just what Snowden and Milne
(1997) found. The global MAE showed about 50% transfer to the unadapted
locations, and was nulled by motions that were at right-angles to the
adapting motions. Similar results were obtained for translation,
rotation, and expansion. These findings cannot be understood in terms of
local motion adaptation, but make good sense if much of the MAE arises
from the global motion detectors we have seen in MT and MST.

Motion, attention, and MT A recent fMRI study found that the appearance
of the conventional MAE was accompanied by increased responses in human
MT+, but with little effect located in V2 and none in V1 (Tootell et
al., 1995). This appeared to confirm that the main sites for the MAE are
areas MT and MST. However, science is a tricky business; Huk, Ress, and
Heeger (2001) pointed out that MT responses

8. COMPUTATION OF IMAGE MOTION

The logic used by Snowden and Milne (1997) to reveal a global motion
aftereffect (MAE). Solid arrows represent rotational adapting motion,
restricted to two quadrants of the display. Dashed arrows show the test
motion needed to null the observed rotational MAE, restricted to the
other two quadrants. Since the MAE can be observed and nulled in areas
not exposed to the adapting motion, these results imply that global
detectors for optic flow patterns (see Figure 8.13) contribute
substantially to the MAE.

are modulated by attention (discussed on p. 229) and that the MAE is an
"engaging illusion", so the increased MT responses may have been caused
by extra attention being paid to the MAE, rather than because MT is the
site of the MAE. When the level of attention was controlled, fMRI
responses from MT+ to a static pattern were the same with and without
the MAE (Huk et al., 2001). This result by no means excludes MT+ as a
site of the MAE; rather it implies that the fMRI evidence so far is
rather difficult to interpret, and that task demands and attentional
factors play a surprisingly large part in visual motion processing. In
an elegant psychophysical study, Alais and Blake (1999) demonstrated a
direct effect of attention on the MAE. Observers adapted to a set of
dots moving horizontally, along with a weak (low-coherence) secondary
set of dots moving in another direction (e.g., vertical). It is known
that a second adapting motion of this kind will make

239

the perceived direction of MAE appear oblique rather than horizontal.
But how large will that shift be when the secondary motion is very weak?
Alais and Blake found that it depended whether observers paid attention
or not. When observers performed an attentional task on the secondary
dots (pressing a button each time they were seen), the perceived
direction of the MAE for a stationary dot pattern was shifted by up to
20°, but with passive viewing the secondary dots had little effect.
Alais and Blake showed that the attentional effect was
large---equivalent to a three-fold increase in motion coherence level
for the passive viewing condition, and by comparison with the responses
of MT cells this could reflect a doubling of neuronal firing rate. The
difference between effects of attention and inattention was strong only
when the secondary dot motion was rather hard to see. Thus Alais and
Blake argue that attention boosts the strength of weak coherent motion,
and amplifies its contribution to the MAE. This view is consistent with
the physiological evidence (p. 229) that attention increases the gain of
responses in MT and MST. In summary, we have seen that the coherent
motion detection task reveals higher stages of motion analysis, probably
in brain areas MT and MST, that integrate local motion signals to
represent spatial patterns of motion. The local motions are detected at
an earlier stage (V1, V2) by fairly small receptive fields, followed by
strong localised opponency between opposite directions. The higher
stages combine these signals linearly across wide spatial areas, and
contain a number of different mechanisms organised to respond best to
the kinds of large-scale motion patterns--- expansion, rotation, and
translation--- encountered by animals (and people) who move through
their environment. Attention increases the sensitivity and/or response
strength of neurons that are relevant to the current task.

Relative motion Naturally, not all motion perception can be based on
large-scale patterns of motion. We often need to see small moving
targets whose backgrounds may be either stationary or moving in a
different direction. Movements of the head and eyes create

240

VISUAL PERCEPTION

large background movements across the retina, in addition to the
movement of objects relative to their backgrounds. The response of
large-scale integrating units would be dominated by background motion
that would swamp any response to small moving objects. This is a
question of spatial resolution, and a unit that sums inputs linearly
over large areas would average out local spatial differences in the
input, and so have poor spatial resolution. In practice however, the
spatial resolution of human motion perception appears to be rather good.
When moving displays contained interleaved strips of motion in opposite
directions, observers could resolve the strips, and report the direction
of an individual strip, even when the strips were as small as 0.2° in
height (Georgeson & Scott-Samuel, 2000; Nakayama & Silverman, 1984). The
visual system's solution to this problem of segregating target and
background motions seems to have been to create a parallel subsystem of
analysers sensitive to relative motion between target and background,
rather than to global motion. Instead of integrating across large areas,
as many MT cells do, some groups of MT cells respond well to movement of
a small target area, provided that the background is not moving in the
same direction. These cells do not show broad spatial summation, but
show decreasing responses as the area of a moving display is increased,
and are suppressed by surrounding motion in the cell's preferred
direction (Born & Tootell, 1992). Some are facilitated by surround
motion in the opposite direction. A simple model for such behaviour can
be formed by taking a set of direction-selective subunits, as before
(Figure 8.13C), and reversing the sign of the subunits that surround the
central one, so that they inhibit the cell in question instead of
exciting it (Figure 8.13D). The cell would respond well to a small area
of motion but fall silent when the same motion fell across the centre
and surround areas, as observed. Thus the neural "wiring" for both
global motion and relative motion sensitivity could be based on
combining the same subunits, but with a simple sign reversal for
relative motion. The observed facilitation by opposite surrounding
motion can also be understood as a combin-

ation of this lateral inhibition (Figure 8.13D) with the local opponency
discussed above. The surrounding subunits would be suppressed by motion
in the nonpreferred direction (through local opponency), and so the
inhibition they exert on the central subunit would drop, thus enhancing
the central subunit's response to target motion in the preferred
direction. Such a combination of lateral inhibition and local opponency
is known as "double opponency", and is found in the colour domain as
well (e.g., Michael, 1978). Cells at earlier stages (V1, V2, V3) of the
motion hierarchy can also show this kind of selectivity to relative
motion, but interestingly it depends, at least in part, on feedback from
MT/V5. When MT/V5 was temporarily inactivated by cooling, responses in
V3 to a moving target were decreased, but suppression by background
movement was also decreased (Hupe, James, Payne, Lomber, Girard, &
Bullier, 1998). Thus feedback from MT/V5 may normally serve to enhance
the responsiveness of earlier cells and their ability to signal relative
motion.

SECOND-ORDER AND "LONG-RANGE" MOTIONS We have so far considered
continuous movements of an image through space and time. Such motions
generate luminance contours that are oriented in space-time and can be
detected by suitably tuned spatio-temporal filters in the motion energy
system. In a seminal paper Braddick (1974) contrasted this kind of local
or "short-range" motion detection with a diverse set of results
suggesting that a second kind of "longrange" motion analysis existed. In
this section we trace the fate of the "long-range motion" idea, and the
rise of ideas about "second-order" or "non-Fourier" motion. The common
thread is that motion energy detection is not the only route to motion
perception.

Apparent motion In cinema, television, and video, motion is perceived
from a sequence of still frames which are

8. COMPUTATION OF IMAGE MOTION

produced by taking successive snapshots or samples of a changing scene
at a certain rate. In the cinema the sample rate is 24 frames/sec (hence
film director Jean-Luc Godard's famous dictum that "cinema is truth at
24 frames a second"), but when projected each frame is flashed three
times to increase the flicker rate to 72 Hz, in order to minimise the
distracting appearance of flicker which is invisible beyond 50--60 Hz.
Perceived motion from a sequence of stills (recall the "flick books" of
childhood) is often termed apparent motion, though when the sample rate
is high enough there is every reason to believe that "real" (smooth)
motion and "apparent" (sampled) motion perception are effectively the
same thing. The small-scale differences between their spacetime images
are smoothed away by the limited spatio-temporal resolution of the
receptors (Watson, Ahumada, & Farrell, 1986). Motion, however, can be
reliably perceived at low sample rates, where the spatial displacement
and temporal interval between frames is quite large. Braddick (1980) and
Anstis (1980) concluded that there were two qualitatively different
kinds of motion analysis. "Short-range" motion was based on the motion
detectors discussed in this chapter; "long-range motion" was more
loosely defined by exclusion, as that which was not "short-range". It
seemed that it could operate across large spatial and temporal gaps,
much larger than the likely extent of motion-sensitive receptive fields,
could integrate frames presented successively to left and right eyes,
and depended on the matching of corresponding visible features or shapes
across time. The characteristics of short-range motion, derived mainly
from the visual properties of random-dot image sequences, seemed
different in every way, requiring short spatial and temporal intervals,
monocular presentation, and no prior recognition of coherent contours or
shapes in the image (Baker & Braddick, 1985; Braddick, 1974). Later
research has led to a re-evaluation of each of these distinctions, and
to the conclusion that long-range motion is not a well-defined category
of motion perception (Cavanagh, 1991; Cavanagh & Mather, 1989). For
example, since the spatial range of direction-selective fields

241

almost certainly increases greatly with eccentricity and receptive field
size, there is no fixed range over which "short-range" motion operates,
and so spatial range ceases to be a simple defining characteristic of
the two processes. Additional difficulty has been raised by recent
experiments on the apparent motion of random-dot patterns, once a
standard way of characterising motion detector properties
psychophysically. It now seems that the spatial and temporal limits to
motion discrimination in these experiments may be determined more by the
statistical structure of the stimulus patterns than the properties of
the detecting system (e.g., Eagle & Rogers, 1996; Morgan, 1992; see
Mather, 1994, for a compact review).

First- and second-order motion Despite these uncertainties, interest in
the idea of multiple routes to motion perception has intensified. Chubb
and Sperling (1988) were the first to define clearly several classes of
stimulus sequence that did give rise to reliable motion perception, but
would not activate the motion energy system described above. Consider
for example a small light square that steps frame-by-frame across a grey
background. This is ordinary, "first-order" motion, based directly on
luminance contrast and its orientation in space-time. Now suppose that
on every step the square reverses polarity, from light to dark, then
dark to light, and so on. The average luminance along the path of motion
is the same as it is everywhere else---mid-grey. Thus there is no
oriented luminance contrast, and the motion energy system should be
silent. Nevertheless, motion of a flickering square is seen. As a second
example, consider a field of stationary, random, black and white dots.
In one region or "window" of the field the contrast is reversing over
time at (say) 5 Hz. Now let the window containing flicker be moved
smoothly over time, while the dots themselves remain stationary.
Observers see a region of disturbance or "twinkle" that moves, even
though there are no space-time contours, or equivalently no space-time
Fourier components, corresponding to this motion. In a final example,
let the window define a region of lower-contrast dots, rather than
twinkling dots.

242

VISUAL PERCEPTION

Again, when the window moves, its motion is reliably perceived. These
examples, and there are many others, point to a general class of
"secondorder" or "non-Fourier" stimuli where spatial structure or motion
can be perceived, despite the lack of corresponding structure in the
luminance profile. The structure exists, certainly, but it lies in the
spatial or spatiotemporal profile of some higher-order property of the
image such as local contrast, local flicker rate, and so on.

The role of nonlinearities We touched on a similar distinction between
firstand second-order structure in discussing the perception of texture
edges and illusory contours in Chapter 6. In both texture and motion,
Chubb and Sperling (1988) and others have emphasised that second-order
structure can be readily transformed into a first-order structure by the
application of a simple, but suitable, nonlinearity, and can then be
detected in well-understood ways. The Bergen and Landy (1991) model for
texture segmentation (Chapter 6) was an example of this approach. In our
first motion example above, full-wave rectification of contrast (setting
negative values to positive) is a nonlinearity that would serve
perfectly, since the contrast-reversing square would be rendered as a
simple moving square, with positive contrast in every frame. Motion
analysis applied after the rectification would obviously yield the
correct motion information. Half-wave rectification of contrast (setting
negative values to zero) would also be suitable, since it would render
the stimulus as an oriented path containing only light spots. This also
has strong first-order energy at the true orientation or speed of
motion. Are there special processes in vision devoted to recovering
second-order motion, or could it be a by-product of nonlinearities that
just happen to be there anyway? This is a key question for theories of
motion processing, and the evidence to date seems to favour the "special
process" view. Much of the evidence comes from the study of
contrastmodulated (CM) patterns, illustrated in Figure 8.16A. These are
second-order gratings, since it is the contrast, rather than luminance,
that varies (modulates) sinusoidally across space. This con-

trast profile imposed on a carrier pattern (typically either random
dots, or a sine-wave grating) is often called the envelope of the
pattern. When the envelope is made to drift over time, while the carrier
remains stationary, this creates second-order motion whose space-time
image is shown in Figure 8.16B. Observers may be required to
discriminate the speed or direction of movement of the envelope. Note
that in a naturally moving texture, first- and second-order movements
would normally go hand-in-hand, since the carrier and envelope move
together. The ability to dissociate them in the laboratory allows us to
ask questions about the nature of responses to firstand second-order
movements, and particularly whether or not they arise from separate
mechanisms. Figure 8.17B shows that the Fourier spectrum of a CM pattern
contains no salient peaks of energy at the modulating frequency,
confirming the second-order, non-Fourier nature of the CM image. This is
quite unlike the first-order, luminance-modulated (LM) grating of Figure
8.17A, and it is important because it means that any standard motion
energy mechanism (Figure 8.8) would not respond to this kind of movement
in an image. However, Figure 8.17C shows that halfwave rectification
distorts the CM image and its spectrum, creating prominent energy at the
modulating frequency. That is, the distorted CM image now contains a
first-order grating as well. Figure 8.17D shows that a compressive
(saturating) transformation of intensity values does the same. A
nonlinearity can thus convert second-order modulation into first-order.
If such nonlinearities occurred early in the chain of processing, and
there were no special pathways for second-order motion, then we should
expect first- and second-order motion to behave equivalently in a
variety of ways. For example, adaptation to moving first-order patterns
should reduce sensitivity to second-order motion and vice-versa.
Holliday and Anderson (1994) found that this was not true for rates of
movement up to 4 Hz, but was true at faster drift rates (8 or 16 Hz).
Similarly Scott-Samuel and Georgeson (1999) found that perception of CM
motion could not be cancelled by an opposing

8. COMPUTATION OF IMAGE MOTION

243

(A) Comparison of a first-order, luminance-modulated grating (LM, left)
    and a second-order, contrast-modulated grating (CM, right). The CM
    grating is formed by imposing a low-frequency contrast variation
    (the envelope) onto a high-frequency carrier grating. (B) Space-time
    image for second-order motion stimulus (left), in which the envelope
    moves while the carrier is stationary. Its Fourier spectrum (right)
    has energy at the frequency of the carrier (black points), and
    "sidebands" created by the modulation (grey points), but no energy
    at the low frequency of the envelope; the centre of the plot,
    representing lower frequencies, is blank. Thus second-order motion
    is also called "non-Fourier motion"; the moving envelope has no
    direct counterpart in the Fourier components of the image.

luminance modulation (LM) at drift rates up to 7.5 Hz, but could be
cancelled at a fast speed (15 Hz). These studies show that for slow to
medium speeds responses to CM and LM are not equivalent. If CM were
"converted" into LM, because

of early distortion, we should be able to see motion in animation
sequences composed of alternate CM and LM frames. This has not been
found in general (Ledgeway & Smith, 1994; Mather & West, 1993a), except
at high speeds

(A) Luminance grating (LM) added to a random pattern (left) has strong
energy at the grating frequency (right). The axes of the space-time
Fourier spectrum (right) are spatial frequency and temporal frequency,
as in Figure 8.16. (B) Contrast modulation of a random pattern has no
prominent energy peaks, but the oriented structure is evident in this
spatial image and also when it is presented as a space-time image (as
second-order motion). (C) Half-wave rectification distorts the CM image,
and introduces energy at the modulation frequency. (D) Compressive
transformation of intensity (saturating response at high intensities)
also introduces energy at the modulation frequency. Nonlinear processes
of this kind (C and D) can be used to recover the secondorder structure
in spatial and space-time images. 244

8. COMPUTATION OF IMAGE MOTION

where distortion does play a part (Scott-Samuel & Georgeson, 1999).
Thus, all these results reinforce the idea of a "special process"
encoding second-order motion. The exception occurs with fast,
high-contrast images where early compressive distortion of the waveform,
perhaps at the photoreceptor level, means that CM images produce
significant responses in first-order pathways (Holliday & Anderson,
1994; Scott-Samuel & Georgeson, 1999).

Is the second-order motion system slow and sluggish? The sensitivity for
CM patterns moving at different speeds shows a low-pass characteristic
that rolls off at much slower temporal frequencies than for luminance
pattern detection (Derrington & Cox, 1998; Smith & Ledgeway, 1998).
These results might seem to imply that temporal resolution for CM is
much worse than for LM. In a practical sense this is true, since the
highest visible rate of movement for CM is only around 10 Hz--- much
lower than for luminance gratings. However, much of this difference
stems from the fact that sensitivity for CM is very much lower than for
luminance gratings. This has to be factored out before we can compare
their underlying temporal characteristics. Experimental analysis and
modelling of temporal integration for LM and CM information shows that
these results are consistent with the idea that CM processing operates
on a timescale that is only slightly slower than luminance information
processing (Schofield & Georgeson, 2000). This slight sluggishness may
enable secondorder motion to be detected over longer time gaps than
first-order motion. Boulton and Baker (1993) examined perceived
direction of motion for a display sequence containing two 100 ms flashes
of a CM pattern. The entire pattern (carrier and envelope) was spatially
displaced in the second flash. When there was no blank time gap between
the flashes, perception was evidently dominated by first-order motion
since direction of movement became ambiguous when the spatial
displacement was about half a cycle of the carrier grating. (Note that
displacements of a periodic

245

waveform through half a cycle to the left or right are equivalent, and
therefore ambiguous in direction.) With a 50--100 ms blank gap between
the CM flashes, performance was quite different. Direction was correctly
perceived over a much greater spatial range, until the displacement
reached about half a cycle of the contrast envelope. Thus second-order
motion detection may have a longer "memory" that can span time gaps up
to at least 100 ms. First-order motion gives way to second-order when
the time gap is greater than about 40 ms (Boulton & Baker, 1993;
Georgeson & Harris, 1990).

Second-order mechanisms for motion Second-order motion perception thus
depends on a nonlinear process that recovers the contrast envelope, or
"energy profile" of the CM stimulus (Lu & Sperling, 1995). The evidence
discussed so far, however, does not establish the existence of
second-order mechanisms specific to motion. Success in the motion tasks
might reflect a more general ability to encode the positions of things,
and the way those positions change over time. This has sometimes been
called "feature-tracking" (Pylyshyn & Storm, 1988) or "attention-based
motion perception" (Cavanagh, 1992). Indeed, when Seiffert and Cavanagh
(1998) studied the detectability of oscillating motion (movement back
and forth rather than continuous drift in one direction) they found that
across a range of different rates of oscillation the limiting factor for
luminance patterns was that a certain speed had to be reached before
motion was seen, but for a variety of second-order patterns (including
CM) the limiting factor was a certain positional displacement, not a
fixed speed. This suggests that position-based coding is important in
perception of second-order motion, but does not exclude the existence of
second-order motion detectors. A clear demonstration of selective
adaptation to second-order motion would be positive evidence for
direction-specific, second-order detectors, but until recently such an
effect has remained elusive. In a key experiment, Nishida, Ledgeway, and
Edwards (1997) showed that adaptation to moving gratings (either LM or
CM) did reduce the observers' sensitivity for movement in the same

246

VISUAL PERCEPTION

direction, but only if the test grating was the same type as the
adapting grating (Figure 8.18A). This direction-specific adaptation was
selective for spatial frequency, just as it is for stationary gratings
(Blakemore & Campbell, 1969), but it did not transfer from LM to CM, nor
vice-versa (Figure 8.18B). This strongly suggests that detection
thresholds are mediated by separate, motionspecific mechanisms
responding to the LM and CM content of moving images. It is also
becoming clear that second-order MAEs can indeed be found, but only with
dynamic or flickering test patterns and not with stationary ones
(Ledgeway, 1994; Nishida and Sato, 1995). By contrast, ordinary
first-order MAEs can of course be observed on static test images. The
reason for this difference remains

unclear, but in an ingenious study Nishida and Sato (1995) exploited it
to show that separate first- and second-order MAEs can be induced at the
same time. They adapted to a carefully designed CM grating sequence,
such that the firstand second-order motions were in opposite directions.
The perceived direction of MAE on a static test grating was opposite to
the first-order adapting direction, while on a flickering test grating
it was opposite to the second-order direction. This striking experiment
shows that separate motionspecific mechanisms must exist for first- and
second-order information. (See Mather et al, 1998, for a detailed review
of this area, especially their Ch. 5.) Moreover, second-order MAEs
observed on flickering test gratings exhibited complete

Separate channels for moving patterns defined by luminance modulation
(LM) and contrast modulation (CM). (A) When the adapting and test
gratings were of the same type (both LM or both CM), threshold elevation
was selective for direction of motion and for spatial frequency.
Vertical axis plots the ratio of thresholds measured in the same
direction as the adapter to thresholds measured in the opposite
direction (both measured after adaptation). Thus a ratio of 1.0
indicates no direction-specific adaptation. (B) When the adapting
grating was CM and the test grating was LM (filled triangles) or
vice-versa (open triangles) there was little or no loss of sensitivity.
This lack of transfer strongly implies separate pathways for detecting
LM and CM. Data are means of two observers re-plotted from Nishida et
al. (1997, Figs. 3 and 4) for a test spatial frequency of 0.5 c/deg
(arrowed). Temporal frequency of drift was 4 Hz.

8. COMPUTATION OF IMAGE MOTION

interocular transfer when one eye was adapted and the other was tested,
while the first-order MAE on a static test grating showed about 50%
transfer, as usual (Nishida, Ashida, & Sato, 1994). This result echoes
our discussion of the tilt aftereffect (Chapter 5) and rotational
aftereffects (above), and suggests again that second-order mechanisms
lie at a higher, more completely binocular, cortical site---perhaps
areas V2 and/or MT rather than V1. Physiological studies of cat and
monkey cortex are beginning to confirm this view, as we now describe.
Zhou and Baker (1993) studied the responsiveness of cat cortical neurons
to drifting luminance gratings and to second-order (CM) gratings, where
the carrier was stationary and the envelope moved. Very few cells in
primary visual cortex (area 17) responded to the contrast envelope of
the CM grating, but in area 18 more than half the cells did so. In these
cells the preferred direction of movement was the same for LM and CM
gratings, but the spatial frequency tuning was different; cells were
responsive to lower spatial frequencies of CM than LM. Zhou and Baker
argued from these results that first- and secondorder processes do not
share identical pathways, but run in parallel and converge onto the
cells that are responsive to both LM and CM. Such integration of first-
and second-order processes is even more evident in the responses of
cells in area MT of the monkey. In MT almost all (99%) of cells are
direction-selective for movement of firstorder, luminance contrast, but
87% of cells tested were also responsive to a moving second-order bar,
defined as a twinkling region moving against a stationary background
(Albright, 1992). Again, the preferred direction of movement tended to
be the same for both classes of stimulus. This suggests that MT plays a
strong role in integrating different cues for motion, and in
representing the speed and direction of movement irrespective of the
cue(s) that gave rise to it. This emerging picture of "cue convergence"
is similar to our discussion of orientation cues in Chapter 5. The
psychophysical and physiological evidence has been incorporated in a
well-specified computational model for motion coding proposed by Wilson,
Ferrera, and Yo (1992),

247

sketched in Figure 8.19. After a common stage of oriented, spatial
filtering the model embodies separate first- and second-order pathways
with separate motion energy mechanisms in each path, as discussed above.
In the second-order path, the spatial structure of contrast modulation,
or other textural variations, is captured by the usual device of
rectification or squaring, followed by a secondstage of oriented spatial
filtering. The gain control stage in each path is the equivalent of the
ratio stage in Figure 8.8, which minimises contrast dependence and
renders the response of each oriented mechanism more nearly proportional
to the component of velocity in that direction. The final and crucial
stage in the Wilson model is the integrative stage, ascribed to brain
area MT, that combines velocity vectors from both paths to compute the
final speed and direction of pattern movement in each local area of
space.

THE INTEGRATION OF MOTION MEASUREMENTS Each point in the field of view
(or retinal image) can thus be associated with a movement vector that
describes the speed and direction of motion at that point. The entire
set of such vectors is called the velocity field. To compute the retinal
velocity field, the visual system would need to derive two-dimensional
velocity vectors at each spatial position, and this raises new problems
for our understanding. As we shall see, the problems arise when we try
to get a 2-D vector from 1-D information. This can be the case with
long, smooth, moving contours, for which the local speed and direction
of movement are ambiguous, and may apply also to individual motion
detector systems if they are responsive mainly to one axis of movement
in 2-D space.

The aperture problem First let us consider the aperture problem, in
which an edge or line is observed moving behind a fixed aperture (Figure
8.20A). In what direction will the line appear to be moving? Almost

248

VISUAL PERCEPTION

Motion coding model of Wilson et al. (1992). The model combines first-
and second-order velocity signals from different directional mechanisms,
at a higher stage whose output represents the local velocity of a
complex pattern.

certainly it will appear to move in the direction of the solid arrow, at
right-angles to the line. A moment's thought, however, reveals that its
actual movement could be quite different. The dashed arrows are two of
the possible movement vectors, which could be de-composed into two
components, one along and one at right-angles to the line. Movement of a
line or edge along itself causes no optical change whatever within the
aperture, and so must be undetectable. Only the perpendicular component
is detectable. The line's actual movement vector is therefore ambiguous,
but it must be consistent with the known perpendicular component, even
though the component along the line could be anything. It follows that
the true velocity vector must have its endpoint lying along a velocity
constraint line, as

shown in Figure 8.20A. Clearly, further information or constraints would
be needed to recover the true velocity. Further discussions and
illustrations of the problem can be found in Marr and Ullman (1981),
Hildreth (1984a, 1984b) and Hildreth and Koch (1987). But since we don't
usually look at life through small holes, why is the aperture problem
important? Its significance lies in the idea that motiondetecting
receptive fields might always suffer the aperture problem. They do look
at life through a small hole (the receptive field area) and if they are
orientation-tuned then they pick up essentially 1-D information even
from a 2-D image. This means that individual mechanisms may be capable
of sensing only the component of velocity perpendicular to their own
receptive field

8. COMPUTATION OF IMAGE MOTION

249

(A) The aperture problem. Motion of 1-D contours in a restricted
    aperture is ambiguous. The problem is important because the velocity
    signals of oriented (effectively 1-D) mechanisms may be ambiguous in
    this way. Velocity constraint line defines the set of motion vectors
    consistent with the line's observed (perpendicular) component of
    motion. (B) True velocity vector for a moving 2-D image (bold arrow)
    can be obtained from the intersection of two or more constraint
    lines, given by different directional mechanisms. (C) Intersection
    of constraints (IOC) solution can also be found by combining
    information from different parts of a moving object. (D) Plaid
    formed by adding two moving gratings. Component velocities (V1, V2)
    are different from pattern velocity (V). Cells in MT may combine
    component velocity signals to derive pattern velocity (see text).

orientation. Figure 8.20B summarises this problem. Given a moving 2-D
texture, containing Fourier components at many different orientations,
the motion energy system at a given location may yield a set of velocity
vectors from different oriented mechanisms (dashed arrows) with

different lengths and directions. What is the true motion of the
pattern? The solution can be obtained unambiguously from the velocity
constraint lines introduced in panel A. Since there is only one true
vector (bold arrow in panel B) all the constraint lines must be
consistent with it.

250

VISUAL PERCEPTION

The constraint lines, drawn at right angles to each component vector,
intersect in a single point that reveals the true motion vector. Figure
8.20C shows us that the ambiguity of a moving 1-D contour can also be
solved by combining vectors from different locations, provided they
belong to the same moving surface or object. In summary, then, velocity
vectors from 1-D mechanisms are ambiguous, but the intersection of
constraints (IOC) algorithm can, in principle, be used to find the true
velocity in two dimensions. The IOC idea came to prominence when Adelson
and Movshon (1982) studied the perception of moving plaids, formed as
the sum of two sinusoidal grating components moving in different
directions (Figure 8.20D). They found that if the spatial frequencies or
contrasts of the components were very different then the motion was seen
as incoherent or transparent, with the two gratings sliding over each
other. But with reasonably similar contrasts and spatial frequencies, a
single coherent motion was seen. This suggested that the visual system
does have to combine different 1-D vectors to derive 2-D motion, and
that it might implement a version of the IOC algorithm. Notice also that
the speed (vector length) of the component motions in the plaid is
different from the pattern's overall speed. Welch (1989) asked whether
speed discrimination (the ability to distinguish small differences in
speed) would be determined by the component speed or by the plaid's
speed. She found that performance on plaids was limited by the ability
to detect differences in the component speeds. This surprising result
further supports a two-stage model in which component motions are
integrated to extract pattern velocity, and implies that the noise or
other factors that limit discrimination lie at the earlier component
stage of processing. These perceptual findings prompted further research
to identify the site of this 2-D motion integration physiologically.
Single cell recordings from areas V1 and MT in primates (Movshon,
Adelson, Gizzi, & Newsome, 1985; Rodman & Albright, 1989) revealed that
while most cells in V1 and some cells in MT responded to the 1-D grating
components of a plaid, other cells in MT responded to the direction of
pattern movement.

For example, a cell responding best to horizontal movement of a vertical
grating might also respond best to the horizontal movement of a plaid,
even though its components were moving obliquely. Such cells in MT may
be integrating the outputs of cells responding to the plaid's
components. Indeed there is direct evidence from fMRI studies that MT+
in the human brain is especially involved in this integration of motion
components, while V1 is not (Huk & Heeger, 2002), but whether vision
employs the IOC method or some other algorithm to achieve it is more
controversial.

Intersection of constraints or vector summation? As an alternative to
the IOC rule, the Wilson model (Figure 8.19) combines first- and
secondorder component velocity signals via vector summation. In simple
cases the two rules yield the same predicted direction (though not
necessarily the same speed) but there are patterns, known as type II
plaids, for which the two rules make very different predictions, as
shown in Figure 8.21. The IOC construction always represents the
geometrically correct solution for a rigidly moving

Velocity vectors in a type II plaid. By definition, the pattern vector
(IOC) for the type II plaid does not fall between the two component
vectors (V1, V2). Thus the IOC and vector sum can be in very different
directions, and these two hypotheses about motion component integration
can be tested experimentally (see text).

8. COMPUTATION OF IMAGE MOTION

pattern, and when plaids are presented for durations longer than about
150 ms, the perceived direction matches the IOC direction quite closely.
Nevertheless, Wilson and colleagues build a persuasive case, supported
by detailed modelling of experimental data, that vector summation and
not IOC is the rule used at the integrative stage. First, at brief
durations and in peripheral vision, the direction of type II plaids is
no longer perceived correctly, and instead they are biased towards the
vector sum direction (Yo & Wilson, 1992). This shift is attributed to a
reduced contribution from second-order signals in the periphery and to
the sluggishness of second-order vision (see earlier, p. 245) that
reduces its contribution at brief durations (Wilson et al., 1992).
Acccording to the Wilson model, first- and second-order signals for type
II plaids produce opposite errors that normally cancel each other. But
when the balance between the two types of signal is disturbed, quite
large errors can be revealed. Second, the model predicts that a type II
plaid formed from purely second-order (CM) gratings should appear to
move in the vector-sum direction, not the IOC direction, since the
second-order signals are not now counterbalanced by first-order. This
was just the result observed by Wilson and Kim (1994b). In summary, the
model of Figure 8.19 can account for a wide range of normal and illusory
perceptions of direction in moving patterns. It also predicts correctly
that MT neurons should respond to both first- and second-order motions
(Albright, 1992), but its more specific predictions about MT have yet to
be tested physiologically. There are some conditions, however, in which
the components of a plaid do not cohere at all, giving the appearance of
transparent motion instead. Some psychophysical and physiological
studies have suggested that the rules for combining component motions in
plaids involve a rich range of cues for the interpretation of occlusion,
transparency, and binocular depth (for review, see Stoner & Albright,
1994). In determining whether to integrate early "component" motion
signals, visual processes may be evaluating a wide range of evidence
about whether the components arise from a single object surface, or from
two superimposed surfaces where one is transparent. The

251

Wilson model was modified to incorporate mechanisms that produce a range
of such transparency effects (Wilson, 1994a, 1994b; Wilson & Kim,
1994a). Coherent motion corresponds to a single peak of activity in the
set of pattern units, while transparency corresponds to two distinct
activity peaks in different directions. This enhanced model can account
well for the observed effects of relative component contrast on both the
emergence of coherence or transparency, and the perceived direction of
components when they appear transparent (Wilson & Kim, 1994a).

Role of area MT in integration of local motion components Despite the
successes of the Wilson model, debate over local motion integration is
far from finished. As we have seen, a central issue is how the different
oriented components of a moving 2-D pattern are combined to represent
the pattern velocity (speed and direction), given that the component
motions are ambiguous and are consistent with many different speeds and
directions (Figure 8.20). The intersection of constraints (IOC, Figure
8.20) is a geometrical construction that locates the unique speed and
direction that is consistent with all the individually ambiguous
components, but of course it seems implausible (to say the least) that
the brain literally draws constraint lines in velocity space to solve
this coding problem. Based on physiological evidence, Simoncelli and
Heeger (1998) constructed a detailed model of V1 and MT cells that can
be seen as implementing the IOC idea in neurally plausible form. They
propose that MT cells combine the outputs of local sets of V1 cells,
tuned to different orientations, spatial frequencies and temporal
frequencies (cf. Chapters 3, 5), that are consistent with a given speed
and direction, as illustrated in Figure 8.22. This speed-selective
combination creates MT cells ("pattern cells") that are not selective
for spatial or temporal frequency, but are tuned for pattern speed and
direction. This model suggests that MT cells act as "velocity templates"
rather as MST cells may act as optic flow templates (Perrone & Stone,
1994), and two recent studies give strong support to this view. The
model implies that MT cells should

252

VISUAL PERCEPTION

(A) How frequency-tuned cells ("component cells") such as those in V1
    may be combined to create cells tuned to image speed in MT ("pattern
    cells"). Each disc represents the range of spatial and temporal
    frequencies over which a component cell responds best. Dashed lines
    represent constant speed (corresponding to a particular ratio of
    temporal to spatial frequency). Combining the "slow" subunits
    (cross-hatched) would create a broadband cell sensitive to a range
    of slow speeds. Similarly, combining the "fast" subunits (grey)
    would create a cell sensitive to fast speeds. Based on the model of
    Simoncelli and Heeger (1998). (B) The model combines subunits across
    orientation as well as spatial frequency. To create selectivity for
    speed (rather than temporal frequency), the preferred temporal
    frequency of the selected subunits must vary with orientation, as
    shown. This example assumes a horizontal motion of 10 deg/s (solid
    arrow), with subunits tuned to 1 c/deg. Note that stationary
    components (0 Hz) are part of a moving pattern when their
    orientation lies along the direction of motion.

respond best to gratings whose temporal and spatial frequencies are in a
fixed ratio that corresponds to a particular speed, and so the most
effective gratings for a given cell should lie within oblique regions in
the frequency domain (Figure 8.22A). This is just what Perrone and
Thiele (2001) found in monkey MT. The "best" speed for a given cell was
estimated from this frequency domain orientation and was found to
correlate fairly well with the cell's preferred speed in response to a
moving bar. A second strong prediction of the Simoncelli--Heeger model
is that the MT pattern cell should combine motion energy across all
orientations and frequencies that are consistent with a given velocity
(Figure 8.22B). This was tested psychophysically by Schrater, Knill, and
Simoncelli (2000) in experiments where observers had to detect drifting,
dynamic noise

images that had been filtered to contain various component bands of
frequencies and orientations. Summation across the different components
was near-perfect when they were all consistent with a single velocity,
but not otherwise. Schrater et al. further showed that the "velocity
template" model gave a good account of the detection thresholds in all
the conditions tested. (Note: this is a technically difficult paper;
readers may wish to consult the nontechnical overview given by Braun,
2000.) Figure 8.22B shows that according to this IOC-based model even
stationary components of a pattern should contribute to motion coding
when their orientation lies along the direction of motion. The rather
surprising prediction is that when a drifting grating moves across a
stationary one (with the two gratings at different orienta-

8. COMPUTATION OF IMAGE MOTION

tions), the perceived direction of motion should be along the stationary
bars---the IOC direction that is uniquely consistent with a single
moving surface containing both gratings. The prediction seems to be
supported indirectly by some psychophysical data of Gorea and Lorenceau
(1991), and deserves further study. Physiological support comes from the
finding that MT "pattern cells" responded to stationary bars oriented
along the preferred direction of movement, just as the IOCbased model
predicts (Rodman & Albright, 1989). By contrast, MT "component cells"
responded in the same way as V1 cells, with the best orientation being
at right-angles to the preferred direction. In summary then,
velocity-tuned cells appear to be constructed in MT by combining the
outputs from a set of different frequency-tuned subunits (either V1
cells or MT component cells) that are consistent with a given speed and
direction, including responses to stationary components lying along the
movement direction. This is consistent with the IOC approach, but we
should realise that velocity is not made explicit by the response of any
one cell. Tuning for speed is quite broad, and response rate is not
monotonic with velocity. It therefore remains likely that perceptual
speed judgements are based on a comparison of "fast" and "slow" cells in
MT (Figure 8.22A), as discussed earlier (p. 223).

MOTION FROM FEATURE TRACKING When things move, they change position over
time. This may be a truism, but we have seen in the previous sections
that motion energy mechanisms---either first-order or second-order---do
not explicitly identify things or their positions in order to encode
motion. It remains quite likely, however, that a third route to the
perception of motion does involve tracking the locations of identified
features over time. Evidence comes from experiments using stimuli that
put motion energy detectors and the feature-matching process into
opposition. Georgeson and Harris (1990) did this for first-order (LM),
and Smith (1994) did so for second-order (CM) image

253

sequences. In both cases, with a short timeinterval between frames,
motion was seen in the direction predicted by motion energy, but with a
40--60 ms time gap between frames motion was seen in the opposite
direction, as expected from feature-matching. This sort of evidence
prompted a 2 × 2 classification of motion (Cavanagh, 1991), rather than
a simple dichotomy. Stimulus information may be first- or second-order,
and motion extraction may be based on motion energy or on feature
correspondence. Cavanagh (1992) associated the latter process with
active, attention-driven tracking of feature locations. Recently, Lu and
Sperling (1995, 2001) have developed in greater detail the evidence and
theory of three motion systems; first-, second- and third-order. Their
third-order system has much in common with the idea of feature-tracking,
but with a greater emphasis on the perceptual salience of the features
and their status as "figure" rather than "ground". The task of matching
corresponding features is analogous to the correspondence problem in
stereopsis (Chapter 7), and was tackled in detail by Ullman (1979,
p. 4): The correspondence problem is that of identifying a portion of
the changing visual array as representing a single object in motion or
in change. The notion of a 'correspondence' comes about when the problem
is considered . . . in the context of a sequence of images, such as the
frames of a motion picture. The problem then becomes one of establishing
a match between parts of one frame and their counterparts in a
subsequent frame that represents the same object at a later time. Ullman
argued that correspondences are established on the basis of matches
between primitive elements of figures such as edges, lines, and blobs,
rather than between whole figures. That is, matches are built up between
the kinds of descriptive units found in the primal sketch. (Note the
similarity between this and the theories of Julesz, 1971, and Marr and
Poggio, 1976, 1979, who established matches between elements, rather

254

VISUAL PERCEPTION

The solid lines show the first frame of Ullman's "broken wheel"
configuration, and the dotted lines show the second frame when it is
viewed in apparent motion. Under certain conditions the wheel is seen to
split into three rings, with the outer and inner ones moving
anti-clockwise and the central one moving clockwise, as shown by the
arrows. Adapted from Ullman (1979).

than entire patterns, to achieve stereoscopic fusion.) Ullman presents a
number of demonstrations to support his case. In one of these, observers
were presented with a "broken wheel" display (see Figure 8.23) in which
every other spoke is incomplete. If the "wheel" is rotated by x degrees
between successive frames, where x is greater than half the angle
between the spokes of the wheel, the observer sees the wheel breaking
into three distinct rings. The innermost and outermost rings rotate
clockwise while the middle ring appears to rotate anti-clockwise. This
would be expected if matches were established between line segments, but
would not be expected if the entire figure were being matched from frame
to frame. If figural matching were occurring one would expect to
perceive clockwise rotation of the whole wheel. Ullman provides an
elegant computational account of how correspondence can be achieved by
making use of a principle of "minimal mapping". Suppose that one frame
of a film consists of

elements A and B, and a second frame consists of elements A′ and B′,
displaced relative to A and B. The correspondence problem is to
establish whether A or B is to be matched with A′. Ullman achieves this
by establishing an affinity measure for each possible pairing. The
closer together in space, and the more similar in description are the
two elements in a pair, the greater will be their affinity (based on the
simple assumption that near, similar matches are more likely to belong
together than more distant, dissimilar matches). To solve the
correspondence process for an entire display of several elements a
solution is found that minimises matches with poor affinities and
maximises those with strong affinities. A global solution is thus
obtained through a set of local measures. Once the correspondence
problem has been solved (although Ullman's solution is not necessarily
that used by the human visual system; see Marr, 1982), it is possible to
recover the threedimensional structure that gives rise to a particular
set of motions. The kinetic depth effect (Wallach & O'Connell, 1953)
provides perhaps the best-known example of the recovery of structure
from motion. If a shadow is cast by a rotating wire shape onto a screen
(see Figure 8.24), a viewer can readily perceive the shape of the
structure behind the screen from the dynamic shadow pattern. Ullman's
own demonstration of the recovery of structure from motion involves the
images of a pair of co-axial counter-rotating cylinders (see Figure
8.25). When static, the display looks like a random collection of dots.
Once it moves, however, the observer has a clear impression of one
cylinder inside another, with the two rotating in opposite directions.
Ullman has shown that it is possible to recover structure from motion if
one assumes that the motion arises from rigid bodies. Given this
rigidity assumption, his structure-from-motion theorem proves that
structure can be recovered from three frames which each show four
non-coplanar points in motion. The interested reader is referred to
Ullman's book (Ullman, 1979) for a fuller discussion of the processes of
establishing correspondences and recovering structure from motion.
Throughout his book Ullman, like Marr, attempts to provide a
computational account that makes use of

8. COMPUTATION OF IMAGE MOTION

255

The kinetic depth effect (Wallach & O'Connell, 1953). When a wire shape
is rotated behind a screen on which its shadow falls, observers see the
dynamic shadow pattern as a solid shape in motion.

Illustration of the principles behind Ullman's (1979) counter-rotating
cylinders display. The screen shows the pattern of dots that would arise
if the images of two, coaxial, glass cylinders covered with blobs were
projected orthographically onto a screen. As the cylinders are rotated
in opposite directions, the pattern of dots on the screen changes.
Observers who view a film of the screen can recover the structure of the
counter-rotating cylinders from the pattern of apparent motions present.

general constraints (e.g., assume motion is of a rigid object) rather
than knowledge of specific objects.

SPATIAL VARIATIONS IN THE VELOCITY FIELD Spatial smoothing of the
velocity ﬁeld Where the visual input is sparse, or noisy, or ambiguous
because of the aperture problem, there are advantages to be gained by
pooling motion information across extended regions of space. We have
already discussed (pp. 234--238) the evidence that perception of global,
coherent

motion can be seen in displays containing noisy, variable local motions
of dots, and that it arises through large-scale spatial integration at
higher levels of the motion system. The disadvantage of spatial
averaging, however, is that it may eliminate genuine spatial variations
in velocity. These exist where rigid objects rotate or loom relative to
an observer, or where objects deform, as when a tree blows in the wind
or a person moves. To deal with this problem, algorithms have been
devised which yield correct velocity fields without presuming velocity
to be constant within a region. One approach has been to assume that
velocity can vary over images of objects, but that

256

VISUAL PERCEPTION

it does so smoothly. This is a reasonable assumption for most natural
scenes, as the surfaces of objects are smooth relative to their distance
from the observer. Sudden changes in distance, and therefore in
velocity, will occur only at occluding edges. For example, if a cylinder
rotates around its long axis, the velocity of images of elements on its
surface varies smoothly from zero at the edges to a maximum along the
centre. An algorithm that assumed pure translation would compute uniform
velocities over the surface and so would not distinguish a rotating
cylinder from a translating one, while the smoothness assumption allows
the two to be discriminated. The smoothness assumption does not in
itself allow the correct velocity field to be computed, and it is
necessary to make a further assumption that some particular measure of
velocity variation is minimised. For example, an algorithm that assumes
that velocity variation is minimised over areas of the image is
described by Horn and Schunck (1981). They show that it computes correct
velocity fields for both translating and rotating objects, but also, not
surprisingly, that it yields errors at occluding edges where the
smoothness constraint does not hold. Another model of this kind is
proposed by Yuille and Grzywacz (1988). An alternative approach is that
of Hildreth (1984a, 1984b), who demonstrates that an algorithm
minimising variation in velocity along a contour in the image yields
correct velocities for

(a) An ellipse rotating rigidly around its centre O. The velocities of
    points on its edge are shown by straight lines. (b) The components
    of these velocities perpendicular to the edge. (c) The velocity
    field obtained from the perpendicular components that minimises
    variation in velocity along the edge. Reproduced from Hildreth
    (1984b) with permission of the publishers, MIT Press.

objects with straight edges. Evidence that an algorithm of this kind
operates in human vision is provided by Hildreth's discovery that the
errors it produces for moving curves match a number of visual illusions.
Two of Hildreth's (1984b) examples are shown in Figures 8.26 and 8.27.
The computed velocity field for a rotating ellipse differs somewhat from
the true field (Figure 8.26); in particular it has a stronger radial
component than it should. Hildreth points out that this corresponds to
the illusion of pulsating radial movement seen in a rotating ellipse
under some conditions. Similarly, the computed velocity field for a
rotating cylinder with a helix on its surface (a barberpole) shows
strong vertical components that are absent from the true velocity field
(Figure 8.27). Again, the same error occurs in human vision, as the
rotating barberpole is seen to move upwards or downwards. Further
examples of illusions that seem to reflect a process of minimising
velocity variation can be found in Hildreth (1984a, 1984b).

Local analysis of optic ﬂow? Koenderink and van Doorn (1976) showed that
the pattern of relative motion within any small region of the flow field
can be fully described as the sum of an expansion, a rotation, and a
deformation (see Figure 8.28). Each of these simple local
transformations can be measured by straightforward differentiation of
the flow field

8. COMPUTATION OF IMAGE MOTION

(a) A cylinder with a helix on its surface ("barber pole"), rotating
    around its vertical axis. (b) The two-dimensional projection of the
    helix with the velocities of points along it shown by straight
    lines. (c) Perpendicular components of velocity. (d) The velocity
    field that minimises variation in velocity along the edge.
    Reproduced from Hildreth (1984b) with permission of the publishers,
    MIT Press.

and this might be achieved, in a biologically plausible way, by
comparing the responses of individual motion detectors responding to
neighbouring regions of the field. The main advantage of this analysis
is that the expansion and deformation together capture the useful
information in the flow field and neither is affected by global
rotation. Thus, for example, the rate of expansion can be used to
recover "time-to-collision" (see Chapter 12, p. 350), whilst the
direction and rate of deformation can be used to recover the tilt and
slant of surfaces, respectively. See Harris (1994) for a readable
discussion of these ideas. Despite its mathematical elegance and
apparent biological plausibility, there is, as yet, little evidence that
the vertebrate visual system contains mechanisms capable of measuring
local expansion, rotation, or deformation, as distinct

257

A simple pattern (a) subjected to three different transformations: (b)
pure expansion, (c) pure rotation, and (d) pure deformation. Any
transformation of a patch in the optic flow field can be expressed as a
sum of four components: the rates of expansion and rotation, and the
rate and axis of deformation. Note that only in (d) does the relative
orientation change. Adapted from Koenderink (1986).

from the global measures discussed earlier (p. 236). Moreover, the
required local measures could not be computed when the flow is only
sparsely defined, as it will be in relatively untextured regions of the
visual field. Koenderink (1986) therefore suggests that visual systems
might make use of less general but more robust descriptions of the flow.
In particular, there may be a number of mechanisms operating in
parallel, each computing one particular property for one particular
behavioural purpose. Koenderink further suggests that such computations
may not even be based directly on the velocity field. For example, one
way to measure deformation is to monitor changes in the relative
orientation of texture elements (see Figure 8.28d), which are available
in the output of the visual cortex without any further motion
computation.

258

VISUAL PERCEPTION

Motion-in-depth from optic ﬂow Psychophysical evidence suggests a system
in human vision that is specifically sensitive to expanding and
contracting patterns of local motion, and represents changes in two
different perceptual attributes---size, and depth or distance. Regan and
Beverley (1978) tested observers' thresholds for detecting two types of
motion of a luminous square, either oscillating in position or in size.
They found that 25 minutes exposure to an adapting square oscillating in
size caused subjects to be about five times less sensitive to small size
changes, but scarcely affected their ability to see small positional
changes that had equivalent local motions of the edges. Thus adaptation
was specific to changing size. Regan and Beverley concluded that
adaptation must have occured in some higher level "looming detector"
which is sensitive to the specific pattern of relative motion of the
four edges. Moreover, the adapting effect is specific to the direction
of size change (expanding or contracting) and creates two negative
aftereffects. After adapting to an expanding square, a stationary test
square at first seems to be contracting in size and then after a few
seconds it appears to be moving away in depth (Beverley & Regan, 1979;
for a critical review see Cumming, 1994). From these results Regan &
Beverley proposed a three-stage model of motion-in-depth analysis. The
first stage contains local, 1-D motion analysers, the second stage
combines them in opposing pairs to form filters sensitive to size
change, and the third stage represents motion in depth. These three
stages correspond rather well with the roles assigned to V1, MT, and MST
in the physiological studies discussed earlier in this chapter.
Importantly, the third stage appears to integrate optic flow and
binocular cues, since the depth aftereffect described above could be
cancelled either by imposing a size change on the test stimulus, or by
changing disparity (Regan & Beverley, 1979). This is a further example
of cue convergence, which we have already met in the contexts of stereo,
orientation, and local motion coding. The motion-in-depth stage implied
by these experiments integrates at least two different cues to

motion in depth and so provides some empirical support for the
integrative, 2½D sketch stage of Marr's (1982) model (discussed further
in Chapter 7).

POSITION, MOTION, AND EYE MOVEMENTS We began this chapter by saying that
the retinal image is "alive with motion". Surprisingly, this is true
even when the scene and the observer are stationary, because the eyes
themselves are never at rest for more than a moment. The fact that eye
movements are very frequent raises important questions about how we
perceive a stable visual world, and how we distinguish moving from
stationary objects. These questions are considered in this section, and
we shall see that, in general, visual perception involves the
integration of retinal image information with nonretinal sources of
information about eye position and eye movement. We can distinguish a
number of different types of human eye movement---saccades, pursuit, and
vergence, described in Chapter 1 (p. 20). The role of saccades is to
direct the gaze to different points in the scene, in order that the high
acuity of the fovea can be used to analyse the region of interest.
Pursuit (tracking) movements serve a similar purpose of locking the gaze
onto moving objects. Unlike the image motions produced by head and body
movement, those produced by eye rotation within a stationary head are
not informative about depth and 3-D structure. This is because the
viewpoint does not change significantly, so that the image movements are
almost pure translation, without distortion or change of structure,
i.e., without motion parallax. In addition the eye also undergoes small
movements (tremors, flicks, and drifts) even during "stationary"
fixation, and there is evidence that these small movements are essential
for perception. This has been revealed by experiments that examined
perception without eye movements, using the technique of stabilised
retinal images.

8. COMPUTATION OF IMAGE MOTION

Fading of stabilised images Image stabilisation can be achieved in a
number of ways. One early method was by attaching to the cornea of the
eye a contact lens that carried a miniature projector. Since the contact
lens, and hence the projector, moved with the eye, the images of objects
presented to the eye remained focused on constant retinal positions.
Loss of perception of colour and contour occurs within seconds of
stabilisation (Heckenmuller, 1965) and pattern vision gives way to a
diffuse luminous "fog". Pritchard (1961) claimed that form perception is
disrupted in a rather interesting manner. He presented observers with
patterns, pictures, and words and his subjects reported that they
disappeared, and sometimes reappeared, in fragments, such that
"meaningful" chunks were preserved. Thus the stimulus word BEER might be
reported as PEER, BEE, and BE at different times. While this might
suggest a role for "topdown" processes, the reappearances may have been
produced by occasional slippage of the lens system (Cornsweet, 1970),
while the meaningful nature of the fragments may have resulted from
reporting bias on the part of the observers. More recent work by Kelly
(1979a, 1979b) and Tulunay-Keesey and Jones (1980) used better
stabilisation methods, and quantified the degree of image fading by the
change in grating contrast sensitivity that takes place. It turned out
that fading is substantial but not actually complete, for contrast
sensitivity is low but not absent after stabilisation. Fading takes
about 2--3 seconds for a low-contrast pattern, increasing to as much as
10--20 seconds for patterns of high contrast and high spatial frequency
(Tulunay-Keesey, 1982). Fragmentary reappearance of images was not
reported. A very strong negative afterimage is seen when the faded
pattern is turned off, suggesting that the fading is probably caused by
local adaptation of retinal elements (see Chapter 2, p. 36) at or before
the level of retinal ganglion cells (Burbeck & Kelly, 1984). Kelly's
(1979b) experiments showed that normal contrast sensitivity could be
restored by imposing a slow (0.15 deg/sec) drift on the stabilised
pattern, akin to the slow drifts of eye position that accompany normal
fixation. The general conclusion from the

259

stabilised image experiments is that small movements of the image across
the retina are vital for the maintenance of perception over time. Local
adaptation processes allow each region of the retina to adapt to the
average luminance level in that region, but small eye movements prevent
pattern contrasts within a region from fading out.

Distinguishing movement of the eyes from movement in the world The
larger saccadic and tracking eye movements raise several important
problems for our understanding of spatial perception and movement
perception. How do we know where things are in space? When image
movement occurs, how do we know whether it is due to object movement or
eye movement? The retinal image moves or jumps every time the eyes move,
and yet perceived position of objects does not change or jump about.
Retinal position and movement alone are evidently no guide to an
object's location and movement in external space. This implies either
that absolute retinal position is unimportant (a position adopted by
Gibson, 1966, 1979; see Chapter 10, p. 309) or that the eye movements
are somehow taken into account (a view held most notably by Helmholtz).
Consider a stationary eye viewing an isolated object as in Figure 8.29.
As the object moves across the line of sight, the image it casts will
move across the retina---it will be cast on different receptors as it
travels. In this situation we correctly perceive ourselves as still and
the object as moving. Now suppose we move the eye laterally, but the
object remains stationary. Again the image will move across the
retina---but this time we will perceive the object at rest and ourselves
as moving. Finally, consider what happens when the eye tracks a moving
object. The image is cast on the same part of the retina, just as when
the eye and the object were stationary, but now movement of the object
is perceived. It appears that the information contained within the
retinal image is ambiguous. In order to perceive correctly what is
moving and what is at rest, the visual system may take into account
information about the way the eyes are moving. Following Helmholtz,
Gregory (1972) suggested

260

VISUAL PERCEPTION

(Grusser, Krizic, & Weiss, 1987), even though its retinal position
cannot change. Perceived movement must in both cases come from the
eye--head system (EEPS) alone.

The nature of the eye position signal (EEPS)

At (a), a stationary eye views a moving object O. As O moves to O1, its
image I moves across the retina to I1. At (b) the same image movement
(I--I1) is produced when the eye moves but the object remains
stationary. At (c) the eye moves to track the movement of the object. O
moves to O1, but its image remains at the same place, I.

that two systems must be involved in movement perception---the
image--retina system and the eye--head system. For a stationary head,
Figure 8.30 illustrates how an object's lateral (angular) position is
the sum of (a) its position in the visual field (i.e., relative to the
fovea) and (b) the direction of gaze (eye position in the head). If
internal estimates were available for both these quantities, or their
time-derivatives (angular velocities), they could be summed to compute
the object's true lateral position or movement. Several experimental
phenomena imply that an "extra-retinal eye position signal" (EEPS) is
indeed available for use in judgements of position and movement. If a
moving light is tracked by the eyes in a dark room it appears to move
smoothly even though its retinal position is (nearly) stationary. When a
small afterimage is viewed in total darkness and saccades are made to
the left and right, the afterimage is seen to change position
accordingly

Although saccades are very rapid movements, up to 500 deg/sec, it is
evident that the EEPS is a sluggish or low-pass filtered version of the
actual saccadic movement. Grusser et al. (1987) found that the apparent
extent of afterimage displacement decreased progressively as subjects
were asked to make faster saccadic alternations of gaze direction. The
perceived displacement decreased with increasing frequency of movement,
even though the actual eye displacement did not decrease. This is the
sign of a low-pass filter that cannot follow high-frequency changes. At
a sufficiently high rate (around 3.5 saccades per second) the afterimage
appeared stationary in space, even though the eyes were moving. The
sluggishness of the EEPS is confirmed by very different experiments on
the judged position of spots of light flashed in darkness at different
times during a saccade (Honda, 1991; Matin, 1976). Suppose for example
that a saccade was made to the right. Targets flashed up to 100 ms
before, or in the early part of, the saccade were perceived too far to
the right (compared with their true position), while later target spots,
up 150 ms after the saccade, were seen too far to the left. This implies
that the EEPS is a temporally blurred (low-pass filtered) version of the
real saccadic displacement (Figure 8.31), running ahead of the saccade
in the early phase, but lagging behind it in the later stage of the
movement. The pattern of errors was more complicated when the target
spots were flashed against a dimly visible background, reflecting
additional influences of relative visual position (Honda, 1993). Because
the EEPS is a blurred replica of the actual eye position, we might
expect position perception to be very unstable. With every saccade,
objects would appear to lurch to the left and right before settling
down. This does not happen, and so it may be that the EEPS is not used
continuously to derive object position from retinal position. If the
EEPS were used only during fixations (where it is stable), and not
during saccades, the

8. COMPUTATION OF IMAGE MOTION

261

For a stationary head, the angular position of an object relative to the
observer's straight ahead direction is the algebraic sum of the position
of the object in the visual field, (image), and the direction of gaze,
(gaze). The same holds for angular velocities (the timederivatives of
the angular positions).

resulting code for object position would be more accurate, but sampled
discontinuously over time. It is not unreasonable to suppose that visual
information received during saccades is ignored, because it is of poor
quality, and is actively suppressed. The high speed of saccades means
that information is grossly blurred or smeared by temporal integration
(like "camera-shake" in photography). This rapidly moving blur or smear
is not usually seen, because it is masked by the strong, clear images
received during fixation before and after the saccade (Campbell & Wurtz,
1978; MacKay, 1970). The properties of such forward masking and backward
masking have been intensively studied in laboratory experiments (see
Breitmeyer, 1984, for a review). In addition to suppression by visual
masking, saccadic suppression also occurs via some neural process that
actively inhibits visual detection during saccades and eye blinks
(Riggs, Merton, & Morton, 1974; Riggs, Volkmann, & Moore, 1981).
Saccadic suppression appears to affect the magnocellular pathway (see
Chapter 3, pp. 45--50) most strongly, since the loss of sensitivity is
greatest at low spatial frequencies (Burr, Holt, John-

stone, & Ross, 1982; Volkmann, Riggs, Moore, & White, 1978), and because
chromatic patterns, detected by the parvocellular system, do not suffer
saccadic suppression (Burr, Morrone, & Ross, 1994). We should remember,
though, that in natural viewing all patterns would be degraded during
saccades by temporal smear, and suppressed by forward and backward
masking. In summary, it is surprisingly reasonable to think of vision as
a sequence of discrete "snapshots" obtained during periods of fixation,
with the EEPS being employed discontinuously to update the encoded
position of objects across a sequence of saccades in which image
position is changing.

Source of the EEPS: "inﬂow" or "outﬂow"? What is the source of the EEPS?
Eye movements could be taken into account in one (or both) of two ways.
Sherrington (1906) originally proposed the inflow theory in which
afferent (sensory) signals from the eye muscles are taken into account
when movement in the retinal image is interpreted. Helmholtz (1866)
proposed instead the outflow theory where motor commands sent to the eye
muscles (described by Helmholtz as an

262

VISUAL PERCEPTION

Comparison of the time-course of a saccadic eye movement (solid curves)
with the time-course of the extra-retinal eye position signal (EEPS;
dashed curves). The EEPS was deduced from data on the misperception of
the position of dots flashed during a saccade. Note that the EEPS is a
sluggish, temporally blurred version of the real saccade. Thus errors
arise when the EEPS is combined with image position information (see
Figure 8.30) to compute the objective position of a target. Redrawn from
Honda, Copyright (1991), with permission from Elsevier Science.

"effort of will") are used in interpreting image movement. Early
evidence seemed to favour Helmholtz's theory. If the eye is moved
passively by pressing on the side of the eyeball, the visual world
appears to move, as the reader can easily confirm. Here an eye movement
is not accompanied by the normal oculomotor command (outflow). If
sensory signals from the muscles were being processed, such passive
movement (it was argued) should still be compensated, but it appears not
to be. The converse condition (motor command present, without actual eye
movement) would provide a crucial test of the outflow theory. This is
the "paralysed eye experiment" that has had a long and somewhat
chequered history. Several attempts were made to immobilise the eye
using paralysing drugs or a mechanical wedge (Brindley & Merton, 1960;
Mach, 1914), and subjects reported that the visual world appeared to

move in the direction of the attempted eye movement. Here it appears
that the commands to the muscles are being taken into account, even
without actual eye movements. Unfortunately, it seems unlikely that
these experiments truly immobilised the eye. The only study to do this
successfully was by Stevens et al. (1976), who went to the extreme of
whole body paralysis to stop any eye movement. Following an attempted
saccade, their heroic subject reported a kind of displacement, or
relocation of the visual world without movement. Stevens et al. suggest
that a spatial system compares information from the retina with commands
sent to the muscles, in the way suggested by Helmholtz, but that this
system is responsible for maintaining a perceptually stable spatial
world without being involved in motion perception. Normally there will
be a great deal of informa-

8. COMPUTATION OF IMAGE MOTION

tion from the background against which any object appears that can
disambiguate the interpretation of image movement. Following Gibson
(1966; see Chapter 10, p. 309), Stevens et al. suggest that movement in
part of the retinal image is interpreted as movement of an object in the
world, while movement of the whole image is interpreted as an eye
movement, without any need to involve efferent information at all. When
a large structured background is present, the system appears to rely
more on an object's position and motion relative to the background, and
to give less weight to the EEPS (Pelz & Hayhoe, 1995). This illustrates
how some apparent ambiguity in the retinal image may disappear when one
considers the information available in the entire image, rather than a
restricted portion of it, a point to which we will return later in the
book.

Motion perception during eye movements We have seen that the external
position of an object may be coded as the sum of two positional codes:
image position + eye position (EEPS) (or if eye and head are considered,
then image position + gaze direction; Figure 8.30). In the same way,
object movement could be encoded as the sum of image velocity + eye
velocity. This would require the use of an extraretinal eye movement
signal (EEMS) analogous to the EEPS, perhaps derived from the eye
pursuit command signals. Two complementary phenomena suggest that human
vision does use an EEMS to encode object movement. First, in the
Aubert--Fleischl effect, the perceived speed of a moving pattern can
seem up to 40% slower when the eye is tracking it, than when the eye
fixes on a stationary point. This can be understood if the eye movement
signal (EEMS) is about 0.6 of the actual eye velocity (Freeman & Banks,
1998). During this tracking, the retinal image of the moving object is
stationary, and so its perceived speed is underestimated because it
depends entirely on the EEMS. Second, in the Filehne illusion, a
stationary background pattern appears to move leftward when the eye
tracks a point moving to the right, and vice versa. Here the background
would appear stationary only if the EEMS were equal and opposite to the
image

263

velocity, but because the EEMS is too low, the background appears to
drift. Freeman and Banks (1998) extended this account by showing that
both illusions of movement could vary in magnitude and sign because the
coding of image velocity, as well as EEMS, is subject to misestimation.
A dramatic demonstration of the importance of EEMS in motion perception
comes from the clinical study of a patient whose MRI scan showed
bilateral extrastriate cortex lesions (Haarmeier, Thier, Repnow, &
Petersen, 1997). The patient (RW) appears unable to use the EEMS at all,
and so cannot compensate for eye movements. He has normal motion
perception when the eye is stationary, but suffers vertigo and nausea
during pursuit eye movements or selfmotion. He can track a moving
pattern normally, showing that his pursuit system is intact. But when he
does so, the moving target appears stationary---he has no EEMS to make
it move. While he is tracking, a stationary background appears to move
in the opposite direction at the speed the eye is moving; that is, RW
suffers 100% Filehne illusion. He evidently cannot combine visual and
gaze signals to compensate for the image movements that accompany eye
pursuit, and his world is grossly unstable unless the eye is stationary.
Studies of brain areas along the dorsal stream (MST, 7a, LIP) show that
visual responses of cells in these higher areas are modulated by head
and eye position in a way that suggests they are beginning to transform
visual information from a retinal co-ordinate system into body- and
worldcentred co-ordinates (Andersen, Snyder, Bradley, & Zing, 1997; see
Chapter 3, p. 69). Haarmeier et al. therefore suggest that RW's lesions
have left image motion analysis intact up to the level of MT, but have
eliminated his ability to combine this information with eye movement
signals in MST and beyond. There was no report that RW had unstable
vision during saccadic scanning of a stationary scene, and so we surmise
that EEPS and EEMS are separate signals; for RW the eye position signal
(EEPS) is presumably intact while his use of the eye movement signal
(EEMS) is abolished.

264

VISUAL PERCEPTION

And finally, RW must also be living proof of the incorrectness of
Gibson's assertion (e.g., 1966, p. 256) that optic array information
alone is sufficient for active vision in the world. People really do to
take account of their own eye movements. For a detailed review of
extraretinal signals in the context of perception and action systems,
see Bridgeman (1996).

CONCLUSIONS We have considered in some detail how the simplest variables
of image motion---speed and direction---may be computed from intensity
values in the space-time image, and have seen that considerable progress
has been made on this problem. Several mechanisms, based on first- and
second-order motion energy and on feature

correspondence, appear to operate initially in parallel, followed by an
integrative stage that combines the different cues, and the signals from
different directions, to yield a local estimate of velocity. The middle
temporal area of the brain (MT/V5) plays an important role in this
integration of motion signals, and in the integration of motion signals
across space. MT sends signals to MST, even higher in the dorsal
pathway, where very large receptive fields are responsive to different
spatial patterns of motion such as rotation and expansion that are
produced by moving objects and by the moving observer. At these higher
levels, retinal motion signals begin to be combined with information
about eye, head, and body movements in order to encode object motion
relative to the observer, and not simply motion across the retina. In
Part III we shall consider the particular purposes of motion computation
in perception and in the control of action.

9 Object Recognition

written, how large it is, the angle at which it is seen and so on
(Figure 9.1). Yet somehow we recognise all these patterns of light as
corresponding to the same letter. Or consider the problem of recognising
a friend's face; the image of their face on the retina will depend on
the lighting conditions and their distance, angle, and facial
expression. Again, all these images are classified together, even though
some (such as a full-face and a profile view) are quite dissimilar and
more like the same views of different faces than they are like each
other (Figure 9.2). These are both illustrations of the problem of
stimulus equivalence; if the stimulus controlling behaviour is a pattern
of light, or image, on the retina, then an infinite number of images are
equivalent in their effects, and different from

An essential aspect of the behaviour of animals and people is their
ability to recognise objects, animals, and people that are important to
their survival. People are able to recognise large numbers of other
people, the letters of the alphabet, familiar buildings, and so on.
Animals may need to recognise landmarks, suitable prey, potential mates
or predators and to behave in the appropriate way to each category. If
we assume that the information available to a person or animal is a
static two-dimensional image on the retina, a problem immediately arises
in explaining visual recognition. Take the example of a person
recognising letters of the alphabet; the problem is that an infinite
number of possible retinal images can correspond to a particular letter,
depending on how the letter is

All these different shapes are classified as the letter A. 265

266

VISUAL PERCEPTION

(a) and (b) show two different views of the same person, Patrick Green.
    View (b) is in many ways more like picture (c), which is of a
    different person, than it is like view (a). Photographs by Sam
    Grainger.

other sets of images. Many influential treatments of object recognition
assume that some or all the images corresponding to a particular thing,
whether letter of the alphabet or face, have something in common. The
problem is to find just what this is and how this thing in common is
detected. It is this problem that we will be considering in this
chapter.

SIMPLE MECHANISMS OF RECOGNITION Many animals, particularly simpler ones
such as insects and fish, solve the stimulus equivalence problem by
detecting something relatively simple that all images corresponding to a
particular object have in common. A good example is the three-spined
stickleback. Males of this species build nests and defend them against
other males by performing threat displays. A stickleback must therefore
be able to recognise rival males and discriminate them from other fish
and from objects drifting by. The retinal images of rival males will

obviously vary greatly, depending on the other fish's distance, angle,
and posture, and it seems that classifying these images separately from
those of other fish will need elaborate criteria. In fact, as Tinbergen
(1951) discovered, the stickleback manages successfully with quite
simple mechanisms of recognition. Tinbergen observed the strength of
sticklebacks' aggressive responses to a range of models and found that
they would readily attack a crude model of another fish, provided it had
the red belly colour characteristic of male sticklebacks. Indeed, a
crude model with a red belly elicited more attack than an accurate one
without (Figure 9.3). A feature of an object or animal---such as the red
belly of a stickleback---that elicits a response from an animal, is
called a key or sign stimulus, and it greatly simplifies the problem of
recognition. As long as red objects and fish with red markings are rare
in the stickleback's environment, it can use the key stimulus to
recognise rivals and does not need to use information about another
fish's detailed structure and colouration. The stickleback's recognition
of a rival male

9. OBJECT RECOGNITION

267

An accurate model of a stickleback without a red belly (top), is less
effective as a stimulus to elicit aggression from a male stickleback
than any of the cruder models below. Adapted from Tinbergen (1951).

does depend on more than just the presence of a patch of red of a
certain size in the retinal image, as Tinbergen also found that a model
with a red patch on its back was attacked less than one with an
identical red patch on its belly, and that a model in the "head-down"
posture of an aggressive fish was attacked more than one in a horizontal
posture. Even so, the presence of this distinctive feature allows a much
simpler means of recognition to be effective than would otherwise be the
case. Many other examples are known of key stimuli being important in
the recognition by animals of other members of their species, and we
will mention two other examples from Tinbergen's work. One is the
recognition of female grayling butterflies by males. Tinbergen found
that males would fly towards crude paper models moving overhead and that
their responses were not affected by the colour or shape of the model.
The key stimulus turned out to be the pattern of movement of the model;
males would fly towards it if it imitated the flickering and up-and-down
movements of a butterfly, but not if it moved in a smooth way. Although
butterflies do waste time

chasing other males, or butterflies of the wrong species, this simple
mechanism of recognition does prevent responses to other kinds of
insect. Another example is the recognition by nestling thrushes and
blackbirds of their parents. When the parents bring food to the nest,
the young birds turn towards them and gape, opening their mouths wide to
be fed. Tinbergen found that a moving dark silhouette above the birds'
eye level will elicit gaping, whatever its shape and size. Presumably
this simple mechanism of recognition is adequate because the chances of
anything other than a parent resembling the key stimulus are low. Key
stimuli may also be important in the recognition of prey. Toads feed by
snapping at insects flying past them, capturing them with their long
sticky tongues, and Ewert (1974) found that they recognise insects by
fairly simple criteria, as they will snap at small cardboard squares.
Although Ewert's experiments used moving targets, toads will also snap
at stationary models (Roth & Wiggers, 1983). While toads are selective
for the size and speed of movement of model prey, these results show
clearly that they are not

268

VISUAL PERCEPTION

able to recognise insects on the basis of finer details of their
appearance. Thus for some animals the problem of recognising significant
objects may be reduced to the problem of detecting localised key stimuli
or features that in the natural world are unambiguous cues to
appropriate action. Such local features may be quite simple. It is easy
to see how a "redness" detector might function in the stickleback, and
not too difficult to conjecture how this might be coupled with a rather
crude configurational analysis to explain observed preferences for the
location of the red patch and the posture of the model. However such
mechanisms are also relatively inflexible, and depend for their success
on the predictability of the natural environment. When a scientist
introduces a red dummy fish, a paper butterfly, or pieces of cardboard
into an animal's surroundings, the assumptions about the properties of
mates or prey on which the perceptual mechanism relies are violated.
Other animals, especially primates, have more flexibility in their
perception and action and are able to recognise and discriminate on the
basis of more complex and subtle criteria. In these cases, as in human
perception, the problem of how stimulus equivalence is achieved is a
difficult one, as we will see in the remainder of this chapter.

and friends from strangers, family pets from strays, and the long
process of formal education enables most to decipher the intricacies of
written language. What kinds of internal representations allow for the
recognition of complex configurations, and what kinds of processes
operate on the retinal image to allow access to these internal
representations? These have been the questions posed in the study of
human pattern and object recognition. Much early work on pattern
recognition focused on the problem of recognising alphanumeric patterns.
There was good reason for such work, since researchers in computer
science had the applied aim of making computers able to recognise such
patterns so that they might, for example, achieve automatic sorting of
letters with handwritten postal codes. The emphasis on alphanumerics was
unfortunate in other ways, since the problem of stimulus equivalence is
rather different for alphanumerics than for objects. Letters must be
recognised despite changes in their form, but they are only
twodimensional patterns, so that other problems in object recognition
are minimised. Nevertheless the area of alphanumeric recognition is
worth discussing briefly since it serves to introduce certain
theoretical approaches to the broader area of object recognition.

MORE COMPLEX RECOGNITION PROCESSES TEMPLATE MATCHING We may speculate
that at least some behaviour in humans may be under the control of key
stimuli. For example, it has been shown (e.g., Goren, Sarty, & Wu, 1975)
that human neonates show innate following of face-like patterns, and we
discuss this evidence in more detail in Chapter 13. On the whole,
however, it is through a process of learning that we come to classify
certain configurations as equivalent and distinct from others. The human
infant learns to recognise the faces of its parents irrespective of
angle, expression, or lighting. A mother will still be "mummy" to her
child after she has curled her hair, and a father will still be "daddy"
if he hasn't shaved for a few days. Later, the child will learn to
distinguish teachers

The simplest account that we could offer of how we recognise
alphanumeric characters would be that of template matching. For each
letter or numeral known by the perceiver there would be a template
stored in long-term memory. Incoming patterns would be matched against
the set of templates, and if there were sufficient overlap between a
novel pattern and a template then the pattern would be categorised as
belonging to the class captured by that template. Within such a
framework, slight changes in the size or angle of patterns could be
taken care of by an initial process of standardisation and
normalisation. For example all patterns could be rotated so that their

9. OBJECT RECOGNITION

major axes (as discovered by other processing operations) were aligned
vertically, with the height of the major axis scaled to unity (see
Figure 9.4). In addition, some pre-processing or "cleaning up" of the
image would be necessary. Both humans and other animals (Sutherland,
1973) cope very well with broken or wobbly lines in the patterns they
recognise. Such a template-matching scheme could work provided that such
normalising procedures were sufficient to render the resulting patterns
unambiguous. Unfortunately this is almost impossible to achieve, even in
the simple world of alphanumerics. An "R" could match an "A" template
better than its own, and vice versa (see Figure 9.5). The bar that
distinguishes a "Q" from an "O" may be located in a variety of places
(see Figure 9.6). It is clear that at the very least, we would need to
have multiple templates for each letter Template-matching schemes also
fail readily to account for the facts of animal discrimination.
Sutherland and Williams (1969) showed that rats trained to discriminate
an irregular from a regular

269

chequerboard pattern readily transferred this learning to new examples
of random and regular patterns (see Figure 9.7). As Sutherland (1973)
points out, the configuration in Figure 9.7d should match better with a
"template" for pattern 9.7a than for b, but it is treated by the rats as
though it were more like b than a. It is also difficult to see how this
kind of template-matching model could be applied to the more general
area of object recognition, where the problem of stimulus equivalence is
magnified. However, later in this chapter we will consider some recent
variants of the template-matching model that appear quite successful.
Moreover, a template-matching process can operate successfully if the
form of the characters it must recognise can be constrained. Thus the
computer that recognises account numbers on the bottom of cheques
matches these to stored templates. The character set has been
constrained however, so that the numerals have constant form, and in
addition are made as dissimilar from one another as possible to avoid
any chance of confusion. The characters that humans recognise are not
constrained in this way.

Before matching to a template, a pattern could be standardised in terms
of its orientation and size. This could be done by finding the major
axis of the figure, rotating this to vertical, and scaling its size to
some standard.

The bold figures show possible templates for an A (left) and an R
(right). The dashed figures show how an R (left) and an A (right) could
match another letter's template better than their own.

270

VISUAL PERCEPTION

What distinguishes the Qs from the Os? Not the precise form of the
circle, nor the precise location or orientation of the bar.

Rats trained to respond in one way to pattern (a), and another way to
pattern (b), later treat pattern (c) in the same way as (a), and pattern
(d) in the same way as (b). This is not consistent with a
template-matching model (Sutherland & Williams, 1969). Reprinted with
permission from the author and the Experimental Psychology Society.

FEATURE ANALYSIS When we consider how it is that we know the difference
between an A and an R, or a Q and an O, it seems that there are certain
critical features that distinguish one from another. The bar that cuts
the circular body of a Q is essential to distinguish it from an O,
whereas the precise form of

the circle is less crucial. Perhaps a model in which combinations of
features were detected would be more successful than one based on
templates. Feature analysis models of recognition were popular with
psychologists and computer scientists during the 1960s while
physiologists such as Hubel and Wiesel were postulating "feature
detectors" in the visual cortex of cats and monkeys (see Chapter 3,
p. 47). Perhaps the most influential model for psychology was
Selfridge's

9. OBJECT RECOGNITION

(1959) Pandemonium system, originally devised as a computer program to
       recognise Morse Code signals, but popularised as a model of
       alphanumeric recognition by Neisser (1967), and Lindsay and
       Norman (1972). An illustration of a Pandemonium system is shown
       in Figure 9.8. The system consists of a number of different
       classes of "demon". The most important of these for our purposes
       are the feature demons and the cognitive demons. Feature demons
       respond selectively when particular local configurations
       (right-angles, vertical lines, etc.) are presented. The cognitive
       demons, which represent particular letters, look for particular
       combinations of features from the feature demons. Thus the
       cognitive demon representing the letter H might look for two
       vertical and one horizontal lines, plus four right-angles. The
       more of their features are present, the louder the cognitive
       demons will "shout" to the highest level, the decision demon, who
       selects the letter corresponding to that represented

271

by the cognitive demon who is shouting the loudest. Thus in this system
individual characters are represented as sets of critical features, and
the processing of any image proceeds in a hierarchical fashion through
levels of increasing abstraction. It is this kind of model that Barlow
(1972) and others used to interpret the properties of simple cells in
the visual cortex (see Chapter 3, p. 54). Simple cells were thought to
be acting as the feature demons in the Pandemonium system, passing
information on to cells that would supposedly respond to increasingly
abstract properties. Such hypothetical cells were dubbed "Grandmother
cells" or "Yellow Volkswagen detectors" to express the abstract nature
of the stimuli exciting them. A Pandemonium system can learn to give
different weights to different features according to how well these
features discriminate between different patterns, and so this simple
model can be seen as an early type of connectionist model. A

A Pandemonium system for classifying letters. Each of the feature demons
responds selectively to a different feature in the image, and signals
the number of features present to the cognitive demons. Each of the
cognitive demons represents a different letter, and "shrieks" louder the
more of its features are present. (Extra features inhibit the responses
of cognitive demons.) The decision demon selects the letter that is
being shouted the loudest. Liberally adapted from Selfridge (1959) and
Lindsay and Norman (1972).

272

VISUAL PERCEPTION

system of the Pandemonium type can in principle accommodate certain
kinds of contextual effect. These are a ubiquitous feature of human
pattern recognition, and Figure 9.9 shows one example of how context
affects the recognition of letters. The same shape can be seen as H or
as A depending on the surrounding letters. Within a Pandemonium system
we might allow higher-level demons to "arouse" those at lower levels
which correspond to particularly likely patterns, so that they would
need less sensory evidence to make them shout sufficiently loudly to win
over the decision demon. However, as a general model for human pattern
and object recognition the Pandemonium system is unsatisfactory.
Ultimately it rests on a description of patterns in terms of a set of
features, which are themselves like mini-templates. One of the reasons
that Pandemonium was so popular was that it seemed consistent with the
neurophysiology of the visual cortex; but we have already seen that
single cells cannot be thought of as "feature detectors" (see Chapter 3,
p. 54). While this may not matter for a purely psychological or
computational theory of recognition, there are other problems.
Feature-list descriptions fail to capture overall structural relations
that are captured, but too rigidly, by more global templates. Thus the
Pandemonium system depicted in Figure 9.8 would confuse an F with F and
a T with , confusions that humans typically do not make. In addition,
the Pandemonium system, in classifying patterns, discards all
information that distinguishes different instances of the same pattern.
The output of the decision demon would be the same irrespective of the
particular version of the letter A shown. We need a way of talking T

The same shape may be seen as an H in one context and an A in another
(from a demonstration by Selfridge).

about recognition that allows us to describe the differences between
patterns as well as being able to classify together those that are
instances of the same type. We need to preserve such differences so that
other kinds of classifications can be made. We recognise someone's
handwriting, for example, by the particular shapes of the letters they
produce. Thus we need a representational format that captures aspects of
structure that are essential for the classification of an item but
preserves at some other level structural differences between different
instances of the same class.

STRUCTURAL DESCRIPTIONS A general and flexible representational format
for human pattern and object recognition is provided by the language of
structural descriptions. Structural descriptions do not constitute a
theory of how recognition is achieved, they simply provide one kind of
representation with which to construct such a theory. A structural
description consists of a set of propositions (which are symbolic, but
not linguistic, although we describe them in words) about a particular
configuration. Such propositions describe the nature of the components
of a configuration and make explicit the structural arrangements of
these parts. Thus a structural description of a letter T might look like
Figure 9.10a. Using the language of structural descriptions it is
possible to construct "models" for particular concepts and categories
against which any incoming instance can be matched. Such models capture
obligatory features of the structure but may be less particular about
other details. Thus the "model" for a letter T might look like Figure
9.10b. It is essential that a horizontal line is supported by a vertical
line, and that this support occurs about half way along the horizontal
line. But the lengths of the two lines are less important. Figure 9.10c
shows examples that would be classified as letter Ts by this model, and
those that would fail. Structural descriptions are also easier to apply
to object recognition than templates or feature

9. OBJECT RECOGNITION

273

(a) A structural description for a letter T. The description indicates
    that there are two parts to the letter. One part is a vertical line,
    the other a horizontal line. The vertical line supports and bisects
    the horizontal line. (b) A model for a letter T. This is like the
    description at (a), but the essential aspects of the description are
    specified. For something to be a T, a vertical line must support,
    and must bisect, a horizontal line, but the relative lengths are not
    important. (c) Shapes that would be classified as Ts by the
    model. (d) Shapes that would fail to be classified as Ts.

representations. A picture of an object can be described by a series of
structural descriptions at increasing levels of abstraction from the
original intensity distribution. There are thus a number of possible
"domains" of description (Sutherland, 1973). Take for example the two
drawings shown in Figure 9.11. These drawings can be described

within a number of distinct domains, which can broadly be grouped
together as being either "twodimensional" or "three-dimensional". The
2-D descriptions describe the picture or image present, and this image
can be described in increasingly abstract or global terms. It may be
described as a collection of points of different brightnesses, as a
collection of lines, or as a group of regions.

These two forms are quite different in terms of their two-dimensional
description. They are equivalent only in the three-dimensional domain.

274

VISUAL PERCEPTION

These different levels of description are reminiscent of the different
stages of elaboration of the primal sketch, through the aggregation of
small edge segments up to larger contours or aggregated texture regions
(see Chapters 5 and 6). Whatever the level of description in the 2-D
domain, whether points, lines, or regions, the representations
established for these two pictures would look very different. It is
within the domain of 3-D description that the equivalence of these two
pictures can be established. 3-D descriptions are couched in terms of
surfaces, bodies, and objects. The two pictures shown in Figure 9.11 are
equivalent only at the level of an object description that is
independent of the vantage point. The description above again
illustrates the thrust of Marr's term "2½D" sketch for the
representation of surfaces, from the point of view of the observer.
Marr's 2½D sketch falls somewhere in between the 2-D and 3-D groups of
descriptions in Sutherland's scheme. Thus two different projections of
the same object will have different structural descriptions in the
picture domain, but will be equivalent in the object domain (see Figure
9.11). Provided that structural descriptions are established at all
levels simultaneously, we can capture both the equivalences between
different views of the same object and their differences. Our problem
now is to consider how structural descriptions at the 3-D level can be
constructed, stored, and matched, and to examine the extent to which the
construction of 3-D representations can proceed in a "bottomup" fashion.
Winston (1975) provided an early illustration of the use of structural
descriptions in object recognition to show how object concepts might be

Three of the toy block structures learned by Winston's program. Adapted
from Winston (1973) with his permission.

learned by giving examples. His program learned to recognise simple toy
block structures such as those illustrated in Figure 9.12, which
contains examples of an "arch", a "pedestal", and a "house". The
computer program was presented with examples of each, as well as
"near-misses", in order to build up models for each concept. The
procedure for a pedestal might go as follows. First, the program would
be presented with an example of a pedestal (Figure 9.13a) to which it
would assign the structural description shown in Figure 9.14a. Thus a
pedestal is described as having two parts, with one part being a "brick"
and the other part being a "board", with the former supporting the
latter. Then the program would be presented with the sequence of "near
misses" shown in Figure 9.13b--d. For Figure 9.13b, the description
would again show two parts, with one a brick and the other a board, but
the relationship between these is now different. The board is beside the
brick, and the program was told that this is not a pedestal. By
comparing this description of the near miss with that of the structure
labelled pedestal, the program can construct a model for a pedestal in
which the support relation is made obligatory. For something to be a
pedestal, one part must be supported by the other. The other examples in
the training sequence (Figure 9.13) further constrain the eventual model
for a pedestal (Figure 9.14b). The eventual model shows that for
something to be a pedestal, an upright brick must support a lying board.
Our choice of a pedestal to illustrate this process of learning a
structural model from examples was deliberate. The pedestal is like a
threedimensional letter T (see Figure 9.10), and the

9. OBJECT RECOGNITION

275

A pedestal training sequence. Adapted from Winston (1973) with his
permission.

(a) A description of the pedestal in Figure 9.13a.
(b) A model for a pedestal built up after training on a sequence of
    pedestals and near-misses. Adapted from Winston (1973) with his
    permission.

structural description for a pedestal is very similar to that described
for a T, except that the parts of the pedestal are themselves
three-dimensional objects like a brick and a board, instead of the
horizontal and vertical lines in the letter T. Thus, this kind of
representation can be used for twodimensional written characters, or
threedimensional objects. Of course, the structural descriptions for
brick and board must themselves be specified at a different level of the
program. At an even lower level, the line drawing that serves as input
must be

parsed into separate objects using the procedures described in Chapter
6. The initial stages of the program rely on this previous stage of
grouping regions of the picture together. The problems with Winston's
system are buried within these lower-level programs that furnish the
descriptions on which the learning program operates. As we noted in
Chapter 6, scene analysis programs of the kind developed by Guzman,
Clowes, and Waltz worked by making use of the constraints inherent in
the kinds of scene they describe. But the constraints of the mini-world
of

276

VISUAL PERCEPTION

matt prismatic solids are not the constraints of the natural world.
While something similar to Winston's learning program might provide a
theory of visual object classification, we need a better way of
furnishing structural descriptions for such procedures to operate
on---one that is not restricted to an artificial world. To do this, we
must return to consider the fundamental problem of object recognition.
To recap, the projection of an object's shape on the retina depends on
the vantage point of the viewer. Thus, if we relied on a viewer-centred
coordinate system for describing the object (one in the picture domain,
to use Sutherland's terminology), descriptions would have to be stored
for a number of different vantage points. Later in this chapter we will
consider some recent theories of recognition that do involve the storage
of discrete viewpoints, an approach that is now gaining considerable
empirical and computational support, at least for certain kinds of
recognition task. However, if we can describe the object with reference
to an object-centred coordinate system, (i.e., build a structural
description in the "object" domain) then it would be possible to reduce
the number of object models stored, ideally to only a single one per
distinguishable object. This was what Winston attempted to do with an
artificial world. The problem is then to find a way of describing the
object within its own coordinate system without confining the discussion
to an artificial world, and/or using knowledge of an object-specific
kind. If one has to rely at the outset on objectspecific knowledge then
we would have to know what an object was before we could recognise it---
an obvious paradox. However it seems likely that knowledge of some
constraints is essential to parse objects---the question is, how
specific are these?

MARR AND NISHIHARA'S THEORY OF OBJECT RECOGNITION Marr and Nishihara
(1978) outlined the foundations for one possible solution to this
problem.

An object must be described within a frame of reference that is based on
the shape itself. To do this, we must be able to set up a canonical
coordinate frame (a coordinate frame that is determined by the shape
itself) for the shape before the shape has been described. The
appropriate set of descriptive elements (primitives) for describing a
shape will depend in part on the level of detail that the shape
description is to capture. The fingers of a human hand are not expressed
in a system that uses primitives the size of arms and legs. To get
around this problem, Marr and Nishihara suggest that we need a modular
organisation of shape descriptions with different--sized primitives used
at different levels. This allows a description at a "high" level to be
stable over changes in fine detail, but sensitivity to these changes to
be available at other levels. First we need to define an axis for the
representation of a shape. Shapes that are elongated or have a natural
axis of symmetry are easier to describe, and Marr and Nishihara restrict
their discussion to the class of such objects that can be described as a
set of one or more generalised cones (after Binford, 1971). A
generalised cone is the surface created by moving a cross-section of
constant shape but variable size along an axis (see Figure 9.15). The
cross-section can get fatter or thinner provided that its shape is
preserved. The class of generalised cones includes "geometric" forms
like a pyramid or sphere, as well as natural forms like arms and legs
(roughly). Objects whose shape is achieved by growth are often
describable by one or more generalised cones, and so we can talk about
object recognition in the natural world, rather than an artificial one.
In the discussion that follows we will generally be talking about the
recognition of shapes composed of more than one generalised cone, so
that there will be more than one axis in the representation. For
example, a human figure can be described as a set of generalised cones
corresponding to the trunk, head, arms, and legs. Each of these
component generalised cones has its own axis, and together these form
the component axes for a representation of a human. A description that
uses axis-based primitives is like a stick figure. Stick figures capture
the relative

9. OBJECT RECOGNITION

277

lower limb segments and the foot. At a still finer level, we can capture
the details of toes with a set of much smaller sticks. At each level of
description we can construct a 3-D model where each 3D model
specifies: 1. A single model axis. This provides coarse

information about the size and orientation of the overall shape
described. 2. The arrangements and lengths of the major component axes.
3. Pointers to the 3-D models for the shape components associated with
these component axes.

One example of a generalised cone. The shape is created by moving a
cross-section of constant shape but variable size along an axis.

lengths and dispositions of the axes that form the components of the
entire structure. The relative thicknesses of these components (e.g.,
the human trunk is thicker than a leg) could also be included in the
representation, although for simplicity we will omit this detail here.
Information captured by such a description might be very useful for
recognition since stick figures are inherently modular. We can use a
single stick to represent a whole leg, or three smaller sticks to
represent the upper and

This leads to a hierarchy of 3-D models (illustrated in Figure 9.16)
each with its own co-ordinate system. The first "box" in Figure 9.16
shows the single model axis for a human body with the relative
dispositions of the component axes (corresponding to head, body, legs,
and arms). The axis that corresponds to the arm forms the major axis for
the "arm model" (next box in the figure), in which the component axes of
upper arm and forearm are shown, and so on through to the details of the
fingers of a human hand. Such a hierarchy of 3-D models is called a 3-D
model description. Recognition is thought to be achieved when a match is
established between a 3-D model description derived from an image, and
one of the stored catalogue of 3-D model descriptions corresponding to
known objects. These may in turn be organised hierarchically, in terms
of the specificity of

A hierarchy of 3-D models. Each box shows the major axis for the figure
of interest on the left, and its component axes to the right. From Marr
and Nishihara (1978). Reprinted with permission of the Royal Society.

278

VISUAL PERCEPTION

A catalogue of 3-D model descriptions at different levels of
specificity. Reproduced from Marr and Nishihara (1978) with permission
of The Royal Society.

their descriptions (see Figure 9.17). Thus a human figure can be matched
to the general model for a biped, or the more specific model for a
human. Ape and human are distinguished by the relative lengths of the
component axes in the model description for a biped. At this point we
should note that there is some limited evidence for the psychological
validity of axis-based representations. For example, Humphreys (1984)
asked subjects to decide whether or not two presented objects were the
same shape (both elongated triangles or both parallelograms). Humphreys
found that when subjects did not know exactly where the second shape
would appear relative to the first, judgements were faster if the
orientation of the major axis of the shape was preserved, suggesting
that this aspect of the shape played a role in the comparison process.
While such results lend some support to Marr and Nishihara's theory,
axis-based descriptions do not seem to be constructed when the position

of the second shape is known in advance (Humphreys, 1984), nor is there
evidence that axis-based descriptions are used for all elongated shapes
(e.g., Quinlan & Humphreys, 1993). However, while the evidence for the
primary role of axis-based representations is limited, it is also the
case that these studies have explored the perception of 2-D shapes
rather than the 3-D objects addressed in Marr and Nishihara's theory.
Humphrey and Jolicoeur (1993) reported that the identification of line
drawings was markedly disrupted when the objects were depicted with
their main axis oriented directly towards the viewer so that the main
axis appeared foreshortened. This disruptive effect of foreshortening
occurred even though the main components of the objects were salient at
all viewing angles. Lawson and Humphreys (1996) used a matching task
with line drawings of objects rotated in depth. With relatively long
intervals between the stimuli there was little effect of the angle
between consecutive

9. OBJECT RECOGNITION

objects until the to-be-matched stimulus had its main axis
foreshortened. These studies lend some support to Marr's theory that
object recognition will be disrupted if the major axes of elongation of
the object are not visible. How could such 3-D model descriptions be
derived prior to accessing the catalogue? The problem is to derive the
axes from an image without knowing what object it is that the image
represents. A possible solution is provided by Marr's (1977)
demonstration that we can make use of the occluding contours of an image
to find the axis of a generalised cone, provided the axis is not too
foreshortened. The only assumption needed is that these contours come
from a shape that comprises generalised cones. We have already seen, in
Chapter 6, how Marr's early visual processing program derived contour
information from an image without knowing what shape it was looking for.
Occluding contours in an image are those that show the silhouette of the
object (see the outline of the head of the bear in Figure 6.26, or the
donkey in Figure 9.20). As Marr points out, silhouettes are infinitely
ambiguous, and yet we interpret them in a particular way (1982, p. 219):
Somewhere, buried in the perceptual machinery that can interpret
silhouettes as three-dimensional shape, there must lie some source of
additional information that

279

constrains us to see silhouettes as we do. Probably . . . these
constraints are general rather than particular and do not require a
priori knowledge of the viewed shapes. Let us examine the assumptions
that Marr suggests allow us to interpret silhouettes so consistently: 1.
Each line of sight from the viewer to the object

should graze the object's surface at exactly one point. Thus each point
on a silhouette arises from one point on the surface being viewed. We
can define the contour generator as the set of points on a surface that
projects to the boundary of a silhouette (see Figure 9.18). 2. Nearby
points on the contour in an image arise from nearby points on the
contour generator on the viewed object (see Figure 9.19). 3. All the
points on the contour generator lie in a single plane. This third is the
strongest assumption, but is necessary in order to distinguish convex
and concave segments in the interpetation process. If this assumption is
violated, then the wrong conclusion might be reached. For example, the
occluding contour in the image of a cube, viewed corner on, is hexagonal
(see Figure 9.19). Because we assume the contour generator is planar, we
could interpret such a silhouette wrongly. In the absence

An object, its silhouette, and its contour. The set of points that
projects to the contour (the contour generator) is shown. For this
figure, all three assumptions (see text) hold for all distant viewing
positions in any one plane. Adapted from Marr (1977) and Marr (1982)
with permission of The Royal Society.

280

VISUAL PERCEPTION

A cube viewed corner-on gives rise to the silhouette and contour shown.
The contour generator (a-b-c-de-f) is not planar. This silhouette might
be seen simply as a hexagon, or interpreted as one of the spindle shapes
shown.

of any other information from internal lines or motion, we might
interpret the contour as belonging to a spindle shape like one of those
drawn, or simply as a flat hexagon. In fact the points on the cube that
gave rise to this contour do not lie in a single plane. It is this
assumption of a planar contour generator that may lead us (wrongly!) to
interpret the moving silhouette of someone's hands as the head of a
duck, or an alligator, while playing shadow games. Marr has shown that
if a surface is smooth, and all the above assumptions hold for all
distant viewing positions in any one plane (see Figure 9.18), then the
viewed surface is a generalised cone. Thus shape can be derived from
occluding contours provided the shape is a generalised cone, or a set of
such cones. Vatan (cited by Marr 1982) wrote a program to derive the
axes from such a contour. Figure 9.20 shows how his program derived the
component axes from an image of a toy donkey. The initial outline was
formed by aggregating descriptions from the raw primal sketch, in the
same way as for the teddy bear's head (Chapter 6, p. 136). From this
initial outline, convex and concave segments were labelled and used to
divide the "donkey"

into smaller sections. The axis is derived for each of these sections
separately, and then these component axes are related together to form a
"stick" representation for the entire figure. Now these axes derived
from occluding contours are viewer-centred. They depend on the image,
which in turn depends on the vantage point. We must transform them to
object-centred axes, and Marr and Nishihara (1978) suggested an
additional stage to achieve this by making use of the "image-space
processor". The image-space processor operates on the viewer-centred
axes and translates them to object-centred coordinates, so that the
relationships between the different axes in the figure are specified in
three, instead of two dimensions. Use may be made of information from
stereopsis, texture, and shading to achieve this, but it may also be
necessary to use preliminary matches with stored 3-D model description
to improve the analysis of the image. Thus, for recognition, Marr does
envisage that there is a continuous interplay between the derivation of
an object's description and the process of recognition itself: (1982,
p.321) We view recognition as a gradual process

9. OBJECT RECOGNITION

281

(a) An outline of a toy donkey. (b) Convex (+) and concave (−) sections
    are labelled. (c) Strong segmentation points are found. (d) The
    outline is divided into a set of smaller segments making use of the
    points found at (c) and rules for connecting these to other points
    on the contour. (e) The component axis is found for each
    segment. (f) The axes are related to one another (thin lines).
    Reproduced from Marr and Nishihara (1978) with permission of The
    Royal Society.

that proceeds from the general to the specific and that overlaps with,
guides, and constrains the derivation of a description from the image In
summary then, Marr and Nishihara outlined a scheme in which an
object-centred representation, consisting of an axis-based structural
description, could be established from an image and used to access a
stored catalogue of 3-D model descriptions in order for recognition to
be achieved. Once an initial match has been established, use may then be
made of downwardflowing information to refine the analysis of the image.
These ideas of Marr's were speculative. Only a few isolated details of
these derivation and recognition processes were specified sufficiently
clearly to implement them, and the system itself rests on a number of
assumptions and observations about the perception of stick figures and
silhouettes, which have a rather ad hoc flavour.

Nevertheless, in the years since Marr and Nishihara's (1978) theory,
there have been a number of developments of these basic ideas.

BEYOND GENERALISED CONES: RECOGNITION BY COMPONENTS An important step in
the development of Marr and Nishihara's theory was the suggestion that
complex occluding contours formed from objects comprising several
generalised cones are segmented at regions of sharp concavity. In
Chapter 6, we described the work of Hoffman and Richards (1984) who have
illustrated the importance of such concavities in segmenting contours to
reveal parts for recognition, and thereby have supported one aspect of
Marr and Nishihara's theory. However, Hoffman and Richards' scheme is
independent of the nature of the "parts" within

282

VISUAL PERCEPTION

the image. It will work if these are generalised cones, but it will work
too if they are quite different kinds of shapes. Since Marr and
Nishihara's theory of recognition was formulated, a number of authors
have suggested extensions to their basic approach, to encompass a wider
range of shapes among the component parts. For example, Biederman (1987)
offered a theory of human object recognition that is clearly related to
early ideas from Marr and others, although with some key differences,
and which he supports with evidence from a variety of psychological
experiments. In Biederman's theory, complex objects are described as
spatial arrangements of basic component parts. These parts come from a
restricted set of basic shapes such as wedges and cylinders. Biederman
calls these shape primitives "geons" (a shorthand for the phrase
geometric ions), suggesting an analogy with words, which are likewise
constructed from combinations of primitives--- phonemes. Like Marr and
Nishihara, Biederman suggests that the first stage of object description
involves the segmentation of the occluding contour at regions of sharp
concavity. This divides the contour into a number of parts that can then
be matched against representations of the primitive object shapes
(geons). The nature and arrangements of the geons found can then be
matched with structural models of objects. The representation of each
known object is a structural model of the components from which it is
constructed, their relative sizes, orientations,

A selection of the volumetric primitives called "geons" (left-hand
panel) are used to specify objects in the righthand panel. The relations
between the geons are important, as shown by the difference between a
pail and a cup. Reproduced from Biederman (1987b) with permission © 1987
IEEE.

place of attachment, and so forth (see Figure 9.21). Where members of
the same basic object category (e.g., piano) may have quite different
shapes (e.g., grand piano vs upright piano) then more than one
structural model would be stored for the object. The main point of
departure of Biederman's theory from Marr and Nishihara's is the
suggestion that geons are defined by properties that are invariant over
different views. According to this theory, it is not necessary to make
use of occluding contours to recover an axis-based three-dimensional
shape description. Instead, each different kind of geon has its own
"key" features in the 2-D primal sketch level representation. Thus in
Biederman's theory, unlike Marr's, object recognition can be achieved
directly from the 2-D (primal sketch) level representation with no need
to construct an explicit representation of 3-D shape. Biederman argues
that there a number of "nonaccidental" properties of edges in images
which can be used as reliable cues to related properties of edges in the
world (cf. Kanade, 1981; Lowe, 1987). The "nonaccidental" principle is
an assumption that when a certain regularity is present in an image,
this reflects a true regularity in the world, rather than an
"accidental" consequence of a particular viewpoint. We can illustrate
this with the example of a straight line in an image. This will usually
result from an extended straight edge in the world, but it could result
from other "accidental" consequences of viewpoint;

9. OBJECT RECOGNITION

for example, a bicycle wheel viewed end-on will give rise to a straight
line image, even though it is actually curved. The nonaccidental
assumption would lead to the wrong answer in this case, but will usually
be correct, and the general assumption is required in order to constrain
the interpretation of essentially ambiguous image data. The
nonaccidentalness assumption leads to assertions such as that curved
lines in images result from curved edges in the world; parallel edges in
an image derive from parallel edges in the world; symmetry in the image
signals symmetry in the world; and so forth. Nonaccidental properties
include collinearity, curvilinearity, symmetry, parallelism, and
co-termination (see Figure 9.22). A geon is identified by a particular
set of defining features (such as parallel edges) which can be accessed
via these nonaccidental properties. Biederman suggests that the
assumption of nonaccidental properties could explain a number of
illusions such as the Ames chair (see Chapter 4, p. 79), and
"impossible" objects, where, for example, the cotermination assumption
is violated. Biederman also provides new evidence for the importance of
concavities in defining the part structures of objects (cf. Hoffman &
Richards, 1984). Biederman (1987) describes experiments in which objects
were presented with regions of contour deleted---either at places where
there were concavities in the occluding contour that should help define
the part's structure, or from segments between these concavities.
Contour deletion had a

283

far greater detrimental effect on recognition when information about
concavities was removed than when this was preserved. Biederman and Ju
(1988) also produced evidence that supported the proposal that it is
edge properties, rather than surface or texture properties, that are
used to classify objects into basic categories. In Biederman and Ju's
experiments object recognition was affected rather little by whether or
not appropriate or inappropriate colour was added to line-drawn objects
in a recognition test, suggesting that the recognition processes ignore
such surface properties. Biederman has also performed a series of
empirical studies that appear to support the geon theory of object
recognition. When a picture of an object is presented twice for naming,
the naming latency on the second occurrence is much faster than on the
first. This speeding up of responses from one presentation to the next
is known as repetition priming. Biederman and Cooper (1991) investigated
how repetition priming was affected by a change in the way a line
drawing of an object is depicted. The amount of priming obtained was
reduced when the repeated presentation shows a different exemplar of the
category which would access a distinct structural model (e.g., an
upright piano followed by a grand piano), compared with the amount of
priming shown when the same exemplar is repeated (e.g., another picture
of an upright piano). This difference gives a measure of "visual"
priming at the

Nonaccidental differences between a brick and a cylinder. From Biederman
(1987a). Copyright © 1987 by the American Psychological Association.
Reprinted with permission.

284

VISUAL PERCEPTION

level of the structural model itself over and above additional
"conceptual" priming that might occur as a result of re-accessing the
same object meaning or category label. Biederman and Cooper (1991)
showed that the magnitude of this visual priming of object
identification was unaffected if the second view of the same object
exemplar showed the same object components, represented by complementary
but nonoverlapping image edge features. However, visual priming was
reduced if the depicted components (geons) themselves were changed from
first to second presentation (when different volumetric parts of the
same object exemplar are shown on the two occasions) (see Figure 9.23).
Further experiments have shown that priming is invariant over other
changes that alter the image but preserve its components, such as size,
location, and moderate changes in viewpoint. In contrast, these same
manipulations do affect memory for line-drawn pictures (Biederman &
Examples of materials used in Biederman and Cooper's (1991) experiment.
The top panel shows two complementary images of a piano, created by
deleting alternate segments of contour in each image. The amount of
visual priming obtained when one member of this pair was followed by the
other was as great as when identical images were repeated. The bottom
panel shows two complementary images produced by deleting alternate geon
components. The amount of priming obtained when one member of such a
pair was followed by the other was much reduced and attributed to
conceptual rather than visual processes. Adapted from Biederman and
Cooper (1991) with permission of the authors.

Cooper, 1992; Cooper, Schacter, Ballesteros, & Moore, 1992; Humphrey &
Khan, 1992), suggesting that variations in object components that are
irrelevant for identity may be processed and maintained by other parts
of the visual system, perhaps those to do with spatial layout and
action. The location of an object in the visual field does not affect
its identity, but will affect how an observer reacts to it (e.g., if
reaching out to grasp it, or ducking to avoid being hit by it). Cooper
and Biederman (1993, see also Biederman, 1995) furnished other evidence
supporting the geon theory. In one study, people were asked to decide
whether two successive objects shown were the same or different in name.
When objects shared the same name (e.g., both were wine goblets), the
two exemplars could differ in terms of the geon shown (e.g., the bowl of
the goblet could have rounded or straight sides) or they could differ in
a way that did not involve any change in accidental properties and hence
geons

9. OBJECT RECOGNITION

(e.g., the bowl of the goblet could be stretched in the second view
compared with the first) (see Figure 9.24). They found that matching was
slowed more (and became more error-prone) by a change in geon than by
other metric changes that left the geons unchanged. This suggests that
it is the categorisation of the shape parts, rather than holistic or
metric properties of shape, that determines ease of matching. Recent
work has shown that the sensitivity to geon rather than metric variation
of such shapes is reflected in the response characteristics of IT
neurons (see Chapter 3, p. 61). For example, Vogels, Biederman, Bar, and
Lorincz (2001) found that IT neuronal responses were more sensitive to
changes in the nonaccidental properties than to the metric properties of
shapes to which they responded. Hummel and Biederman (1992) produced a
computational model of the stages of mapping from retinal features to
geon-based object descriptions. The model, shown in Figure 9.25,
comprises a series of layers in which units repre-

285

sent increasingly complex image properties, from edges in layer 1
(roughly analogous to Marr's primal sketch level) up to individual
objects such as "nuke plant" in layer 7. The challenge in the
implementation was to find a way of solving the "binding" problem that
is inherent in object recognition theories that use part-based
structural descriptions. The problem is as follows: if there is a unit
that represents a "brick" geon and another one that represents a "cone"
geon, units that represent "above" or "below", and others that represent
"horizontal" or "vertical", then we need some way to represent the
different spatial relationships between the brick and the cone that
characterise different classes of object. We need distinct
representations for a "vertical cone above a horizontal brick" (the
description that activates the "nuke plant" object in Figure 9.25) and
for "vertical cone above a vertical brick" (a "tree", perhaps), and so
on. One way round this would be to store all possible combinations of
attributes, but this is both uneconomical and---more
importantly---inflexible. The way proposed by

Examples of the geonchanged (VIC change) and metric-changed shapes used
by Cooper and Biederman (1993). Object name matches were disrupted more
by a change of geon than by a metric change even when the metric change
was 50% greater (see far-right column) than the amount of metric change
that was rated as subjectively equal to the geon change. Reprinted by
permission of the authors.

286

VISUAL PERCEPTION

Hummel and Biederman's (1992) neural network model of shape recognition.
Distinct representations are activited at each layer, from image edges
at layer 1, through to objects (such as "nuke plant") at layer 7.
Reproduced from Hummel & Biederman (1992). Copyright © 1992 by the
American Psychological Association. Reprinted with permission.

9. OBJECT RECOGNITION

Hummel and Biederman is to bind together temporarily conjunctions of
items by synchronising the activity of independent units that contribute
to these conjunctions so that they fire in the same time intervals. This
approach draws on evidence that cells in visual cortex may be linked
into cell assemblies by synchronisation of their rhythms of firing
action potentials (see Chapter 3, p. 71), and extends it to the activity
of hypothetical recognition units. In the model, synchronisation is
achieved by using a novel kind of connection between different
attributes, called a Fast Enabling Link (FEL). FELs are a different
class of link from the usual excitatory or inhibitory link, and operate
only to synchronise activities between units. When a unit representing a
particular attribute (e.g., "cone") becomes active, an enabling signal
travels very quickly along its FELs to other units. This enabling signal
will make other active units fire in synchrony with each other, by its
effect on the refractory component of the cell's activity, but has no
effect on inactive units. So, to represent that "above" goes with
"cone", "horizontal" goes with "brick", and so forth, the activities of
"cone" and "above" are synchronised, and the activity of "brick" and
"horizontal" are synchronised, and these temporary conjunctions between
units in layers 3, 4, and 5 together activate appropriate descriptions
in layer 6 to trigger the recognition of an object such as the "nuke
plant" in Figure 9.25. Hummel and Biederman trained their model to
recognise single views of each of 10 different simple objects composed
of two or three geons each, and showed that it was able to recognise new
instances of these objects in novel locations, sizes, and viewpoints as
accurately as the original test images. Recognition performance was
reduced as the orientation of the test images was varied in the picture
plane (i.e., as objects were shown in different orientations from
upright). Human performance in recognising basic-level objects is
similar, as it is largely unaffected by size, location, and (to some
extent) viewpoint, but is sensitive to orientation. Hummel and
Biederman's model thus represents a promising implementation and
extension of Biederman's (1987) theory of object recognition. The
limitation of

287

the model at present is that it has been tested with a very limited set
of objects. Hummel and Biederman tested their model by training it on
the geon feature assemblies of single views of a set of objects and then
looking at its ability to generalise to novel instances. The model
system is therefore capable of learning new objects, and Hummel and
Biederman discuss how novel object categories can be learned through the
recruitment of unused object recognition units (an idea also developed
in models of face recognition by Burton, 1994).

VIEWPOINT-DEPENDENT RECOGNITION The theories of object recognition we
have discussed here have emphasised the recognition of objects
irrespective of viewpoint. In fact, there is evidence that not all views
of objects are equally easy to recognise. Palmer, Rosch, and Chase
(1981) described how each of the different objects they examined appears
to have a "canonical" viewpoint, which is often, though not always,
something like a three-quarters view. People asked to imagine such
objects report imaging them in their canonical views, and people asked
which view they would choose to photograph, or which view of an object
is "best", select canonical views. Importantly, Palmer et al. also found
that these canonical views could be named more quickly than other views,
suggesting that such views play a privileged role in object recognition.
The advantage of canonical viewpoint could quite easily be accommodated
by the theories described earlier, even though these stress the
recognition of objects independent of viewpoint. Marr and Nishihara
(1978) emphasise that certain viewpoints will conceal important major
axes that are needed to derive a shape description. For example, a top
view of a bucket conceals the axis of elongation that is probably
crucial to its description. For Biederman, certain views may conceal the
nonaccidental properties that define the "geons", and other views may
better reveal them.

288

VISUAL PERCEPTION

Biederman and Gerhardstein (1993) conducted a series of experiments
using the repetition priming method to investigate whether object
recognition was invariant across viewpoint. In their experiments they
examined how priming was affected by a change in orientation of the
object from the view experienced in the first phase. They found that,
provided different viewpoints revealed the same geon components, the
amount of repetition priming was affected very little by an angular
change of up to 135° between its first and second presentation. If
successive viewpoints revealed different geons then the amount of
priming was affected to a greater degree. Other experiments, however,
seem to reveal a much greater dependence on viewpoint than do
Biederman's. For example, Bülthoff and Edelman (1992) (also Edelman &
Bülthoff, 1992) showed that when people were asked to try to recognise
rather complex unfamiliar shapes they showed very poor abilities to
recognise them in novel viewpoints, even when they had been studied
under conditions that ought to have promoted the formation of a 3-D
viewpoint-invariant description. There is some dispute about whether
such effects of viewpoint-dependence arise only when objects are drawn
from a very restricted set within which there are no distinguishing
"geons" (see Biederman & Gerhardstein, 1993; and Tarr & Bülthoff, 1995,
for discussion). However, effects of viewpoint-dependence have also been
found with the kinds of familiar object categories studied by Biederman.
For example, Lawson, Humphreys, and Watson (1994) reported experiments
in which subjects were required to identify an object from a series of
briefly presented pictures. Priming effects were strongly influenced by
the visual similarity of successive views, a result that should not be
expected if each recognisable view contacts a viewpointindependent
description. A number of authors therefore suggest that our usual
ability to recognise objects across a range of viewpoints arises as a
result of our experiencing and storing different viewpoints separately,
rather than through the recognition of viewpoint-invariant features
(Biederman) or the storage of a viewpoint-invariant model (Bieder-

man, Marr). If discrete viewpoints are stored, recognition of novel
views may be achieved by alignment of a novel image with one of those
stored (e.g., see Bülthoff & Edelman, 1992; Tarr, 1995; Tarr & Pinker,
1989; Ullman, 1989), or by using the set of discrete stored views to
encode the novel view (e.g., Edelman, 1995; Ullman, 1998). Some of these
approaches are quite similar to the kind of template-matching scheme we
described earlier, using multiple templates to capture different
viewpoints. Tarr and Bülthoff (1998) provide a recent review of object
recognition with an emphasis on theories that stress storage of discrete
views. However, it is important to note that object recognition is but
one of the tasks accomplished by vision. If recognition can be achieved
directly from 2-D features, as Biederman suggests, or through storing a
number of viewpoint-specific exemplars (Tarr & Bülthoff, 1995) or
"prototypes" (Edelman, 1995), this does not imply that 3-D descriptions
of objects are not constructed to guide other actions such as picking up
the object. Different kinds of representation are needed for different
kinds of visual task. Even within the task of object recognition it is
possible that flexible representational systems are used depending on
task demands, and that view-based and more abstract representational
systems may both play a role (see Schyns, 1998; Tarr, 1995). Whatever
the resolution of the rather intense debate about the mechanism by which
viewpointinvariance is achieved, the theories of Marr and Nishihara, and
Biederman, presented here are rather limited in scope, since they
describe the recognition of basic categories of object from different
configurations of parts. Humans can recognise much more subtle
distinctions within classes of objects that share a similar
configuration. We can recognise our individual dogs and houses, not just
tell a dog from a horse or a house from a church. Indeed some of the
strongest evidence for view-dependent recognition processes has been
obtained in situations where the objects to be discriminated belong to a
single class within which variations are largely captured by what
Biederman describes as metric changes--- variations in the size or
relative spacing of object

9. OBJECT RECOGNITION

parts (see Figire 9.24). Biederman, Subramaniam, Bar, Kalocsai, and
Fiser (1999) provide a critical analysis and review of the
representational bases of a variety of superordinate and subordinate
categorisations, and argue that most objectrecognition
processes---including many withincategory discriminations such as
distinguishing makes of car or types of bird---are mediated by geon
structural descriptions. However they argue that for certain kinds of
sub-ordinate recognition, particularly face recognition, it is metric
properties that may prove critical.

DISCRIMINATING WITHIN CATEGORIES OF OBJECTS: THE CASE OF FACE
RECOGNITION Human faces must all be similar in their component parts and
their overall configuration because of the other functions of signalling
(e.g., expressions) and sensing (e.g., seeing) that they subserve.
Individual identity must be stamped upon this basic configuration. What
do we know about the basic form of the representations used to tell one
individual face from another? In contrast to basic-level object
recognition, face recognition is not very successful if based on simple
"edge" features alone, and seems to require information about surface
characteristics such as the pigmentation and/or the texture of skin and
hair. One example arises from an experiment by Davies, Ellis, and
Shepherd (1978), who showed that famous faces were very poorly
recognised from outline drawings that traced the features of faces. In
Chapter 5 (p. 103 and Figure 5.16) we described Pearson and Robinson's
work on the development of an algorithm to produce an effective sketch
of a face. Bruce et al (1992a) conducted recognition experiments showing
that application of the "full" cartoon algorithm, which adds dark areas
of shadow and hair to the outlines found by "valledge" detection,
yielded images almost as recognisable as the original photographs.
Moreover, Bruce et al. (1994) showed that repetition priming of faces
was considerably

289

reduced if there was a change in the image characteristics between the
first and second presentation of faces. Priming was reduced if faces
were initially seen as photographic images, and then tested as
computer-drawn sketches produced by the Pearson and Robinson (1985)
algorithm, or vice versa, compared with the amount of priming produced
when the format of the images remained constant between the prime and
test phases of the experiment. The viewpoint, expression, and face
features remain the same between the photographic and sketch
versions---what varies is the details of the grey-levels across the
image. This sensitivity to image format in face priming is in apparent
contrast with basic-level object recognition, where Biederman and Cooper
(1991) found that priming was insensitive to changes in the image
features. Biederman and Kalocsai (1997) demonstrated an even more
surprising dependence of face priming on image characteristics. They
decomposed faces into a set of spatial frequency components of different
frequency and orientation (see Chapter 2). By adding back together
subsets of these different components, it was possible to produce
complementary images of faces, each built from distinct, nonoverlapping
spatial frequency components. At the level of conscious vision, such
complementary images look remarkably similar to us (see Figure 9.26).
However in same-- different matching tasks, or in repetition priming
tasks, the two versions of the face behave as though they were quite
different images. In contrast, when similar manipulations are made to
exemplars from a different object class (Biederman & Kalocsai, 1997),
the complementary images prime each other perfectly. Such findings
suggest that representations for face recognition are based on much
lower-level image properties than those for object recognition. A
geon-based structural description would be unaffected by the different
spatial frequency components that generated the edge descriptions
underlying the assignment of geons. However, a representational
description based directly on the image itself would show sensitivity to
the precise image characteristics, and thus priming would be affected by
changes in these underlying image

290

VISUAL PERCEPTION

Examples of images used by Biederman and Kalocsai (1997). In each set of
four, image complements of the same item appear in horizontal pairs.
These are created from complementary sets of spatial frequency
components derived from the same starting image. Different exemplars of
the same category are shown on different rows. (a) Image sets of
unfamiliar faces used in same different matching tasks (this page). (b)
Image sets of familiar faces (O.J. Simpson) as used in repetition
priming tasks (p. 291). (c) Image sets of chairs as used in
same-different matching tasks (p. 292). (d) Image sets of dogs as used
in repetition priming tasks (p. 293).

9. OBJECT RECOGNITION

properties (Biederman & Kalocsai, 1997; Bruce et al., 1994). Consistent
with these observations is the wellknown finding that faces are
extremely difficult to recognise from photographic negatives (e.g.,
Phillips, 1972), although a negative of a face preserves the spatial
layout of edges from the original image. There are at least two possible
sources of the photographic negation effect. One problem is that
negative images reverse the contrast of pig-

291

mented areas---a dark-skinned person with blonde hair becomes a
dark-haired person with light skin. The effect of negation also inverts
shading patterns---a shadow becomes a highlight. Bruce and Langton
(1994) were able to show that the impairment of face recognition did not
occur when three-dimensional surface shapes of faces were negated (see
Figure 9.27). This finding suggests that the critical factor in the
negation effect is the reversal of the relative brightness of

292

VISUAL PERCEPTION

pigmented areas such as hair and skin, which are absent from such
surface shapes. Kemp, Pike, White, & Musselman, 1996, provide a
demonstration that it is not pigmentation (i.e., colour) per se that
matters, since reversing just the hue has no effect on recognition of
familiar faces---it is the relative brightness that matters. These
studies of the identification of linedrawn and negated faces suggest
that the surface properties of skin and textured areas such as

hair---in particular their relative lightness and darkness---play an
important role in face recognition. This need not imply that faces are
represented in a radically different way from other objects, as object
recognition also seems more dependent on surface properties when the
task of discriminating within categories becomes more difficult. For
example, Price and Humphreys (1989) showed that when objects to be
recognised were drawn from structurally similar categories

9. OBJECT RECOGNITION

(such as animals or vegetables) there was a greater advantage in
recognising them if they were coloured appropriately rather than
inappropriately. So, one contrast between face recognition (and,
perhaps, some other varieties of within-category recognition) and the
recognition of basic object types is the extent to which representations
preserve information about surface properties. Moreover, a further
difference seems to arise in the extent to which different kinds of
object discrimin-

293

ation involve decomposing the object shapes into parts, or analysing
them more holistically. The theories of Marr and Nishihara, and
Biederman, which we have discussed at length in this chapter, emphasise
the decomposition of object shapes into discrete parts, followed by the
identification of these parts and their spatial relationships. In
contrast to the evidence for a part-based representational scheme for
objects, face representation seem to be more "holistic", or at least

294

VISUAL PERCEPTION

Examples of the surface images used by Bruce and Langton (1994) to
explore effects of negation in the absence of surface pigmentation.
Positive (left) and negative (right) versions are shown of two of the
different viewpoints used in the experiments. Reprinted from Bruce and
Langton (1994). © Pion Ltd. Used by permission.

the relationships between parts (their configuration) seem to be more
important in the coding of faces than in that of most objects. Of
course, in Biederman's terms, objects sharing the same overall
configuration must share the same geon structural description and thus
some other way of discriminating them that is based upon metric and/or
surface properties must be invoked. The main observation favouring the
holistic processing of faces is that it seems to be difficult or
impossible to encode a particular part, or "feature" of an upright face
without some influence from other, more distant features. It is not just

that the spatial arrangement of face features is important---after all,
we have seen that the spatial arrangement of geons is crucial for the
definition of an object. For faces, it seems either that the internal
description of the parts themselves is influenced by that of other
parts, or that parts are not made explicit in the description that
mediates face identification. For example, Young, Hellawell, and Hay
(1987) took pictures of famous faces and divided them horizontally
across the centre. They showed that subjects were able to identify these
halves in isolation. When halves of different faces were recombined,
however, it became extremely difficult for subjects to name

9. OBJECT RECOGNITION

the people who contributed to the composites if these were aligned. New
(and unfamiliar) faces seemed to emerge from the combination of the top
half of, say, Margaret Thatcher's face and the bottom half of, say,
Princess Diana's. However, when the composite faces were presented
upside down, subjects' abilities to identify the halves improved.
Further evidence for the specific use of nondecomposed facial properties
in face identification has been obtained by Tanaka and Farah (1993).
They asked subjects to learn the identities of individuals constructed
from Mac-a-Mug, an electronic "kit" of face features, available for the
Macintosh computer. After learning the faces, subjects were asked
questions such as "Which is Larry's nose?", where they had to choose the
nose that went with the face they had learned to identify as Larry (see
Figure 9.28). Subjects were much better at making this judgement when
the noses were shown in the context of the whole face, then when
presented in isolation. However, this advantage for presentations of the
whole face was not shown when identities had initially been learned for
scrambled faces, upside-down faces, or houses (in the latter case,
questions about windows, doors, etc. replaced those about face features
such as the nose). These results suggest that memory for intact, upright
faces is not based on a representation in which parts are made explicit,
in contrast to memory for jumbled or inverted faces. Note, though, that
the results do not necessitate the view that facial representations are
nondecomposed; the results are also consistent with the idea that memory
representations for faces are based on emergent, configural descriptions
in which parsed features are no longer represented independently (Bruce
& Humphreys, 1994). The above evidence suggests that, even if face
identification does involves part-decomposition, there may be a
difference in the relative importance of parts versus their
configuration, in the representation of basic kinds of objects versus
faces. One theory suggests that the relative emphasis on configural
and/or holistic processing of faces emerges as a function of expertise
with this object class (e.g., see Carey, 1992), and is
orientation-specific. Upside-down faces, which

295

are very difficult to recognise, seem to induce a more parts-based
analysis compared with upright faces (see also Bartlett & Searcy, 1993;
Leder & Bruce, 1998; Young et al. 1987). Diamond and Carey (1986) showed
that people who were dog experts also showed dramatic effects of
inversion of dog pictures, comparable to the effects of inverting faces.
They suggested that the special "configural" mode of processing faces
was something that might emerge with expertise within any class of
objects sharing the same basic-level configuration. On this argument,
face recognition is "special" only in so far as it is a task of
withincategory recognition at which we are all highly expert, and face
recognition can be used to exemplify the more general process of
withincategory object recognition. (For further discussion and evidence
about whether or not face recognition involves specific mechanisms or
neural networks not shared with other objects, see Bruce & Humphreys,
1994). In the past decade, Gauthier, Tarr, and their associates
(Gauthier & Tarr, 1997; Gauthier, Williams, Tarr, & Tanaka, 1998) have
sustained this line of argument by their investigations of the
recognition of "Greebles", a family of artificial objects that share a
similar overall configuration but where individuals are distinguished by
subtle variations on this (see Figure 9.29 for some examples). Using
Greebles, it is possible to vary experimentally the degree of expertise
of experimental participants, and the studies have thus provided an
innovative way to explore the development of perceptual expertise in a
controlled way. The Greeble studies provide some (rather limited)
evidence that people exposed to Greebles in an extensive training period
are more sensitive than novices to the configural properties of
Greebles. Somewhat more clearly demonstrated is that those trained on
Greebles are relatively more impaired than novices when images of
Greebles are shown in photographic negative, and also appear more
susceptible to detrimental effects of inversion of the images than
novices (Gauthier et al., 1998), though unfortunately these effects are
still fairly weak. Gauthier has also demonstrated that expertise with
cars and birds (Gauthier, Skudlarksi, Gore, & Anderson, 2000) and with

296

VISUAL PERCEPTION

Examples of isolated part, intact face, and scrambled face test items
used by Tanaka and Farah (1993). Subjects in these experiments were
better able to distinguish the correct version of a feature (such as
Larry's nose) when it appeared in the context of the whole face (centre)
than on its own (top row) or in a scrambled face (bottom row).
Reproduced from Tanaka & Farah (1993) with permission of the authors and
the Experimental Psychology Society.

Greebles recruits areas of the fusiform gyrus and occipital lobe that
are involved in face processing (Gauthier, Tarr, Anderson, Skudlarski, &
Gore, 1999). The above review of representations used to mediate face
recognition and other within-

category discriminations reveals some apparent inconsistency in terms of
approach. On the one hand, evidence of image-specificity in face
recognition is consistent with faces being represented in a very raw,
unsophisticated way. Biederman and Kalocsai (1997) argue that faces are
represented

9. OBJECT RECOGNITION

297

Examples of "Greebles". In the top row, four different "families"
(Camar, Vomsi, Nalli, Masio) are represented. For each family, two
members of different "genders" are shown (e.g., Ribu is one gender and
Pila is the other). The bottom row shows a new set of Greeble figures
constructed on the same logic but asymmetrical in structure. Images
provided courtesy of Michael J. Tarr (Brown University, Providence, RI),
see www.tarrlab.org.

on the basis of filter outputs from V1 simple cells (see also Hancock,
Bruce, & Burton, 1998). On this view, face processing is based on
"holistic" image analysis in the sense that the image is not analysed in
any parts-based way at all. In this way, face recognition in some sense
requires "less" work than other kinds of object recognition. On the
other hand, the "expertise" approach seems to suggest that there is
something "extra" about faces (and Greebles, cars, and birds to people
trained to discriminate them) which yields a rather elaborate form of
configural processing of the relationships between their different parts
that requires extensive experience to develop. Leder and Bruce (2000)
provide some evidence that detailed configural relationships between
facial features, not just "whole" images, are coded when faces are
learned. This perspective suggests that face processing (and other kinds
of expert within-category discrimination) involve something more than
other kinds of object recognition. The truth is probably a bit of both.
Just like object recognition more generally, face recognition most
probably depends on a multiplicity of representations, with the
specialist

"face" (or expertise) areas building on the outputs of image analysis
from V1 areas, in ways that differ from the kinds of representation
derived from objects.

STATIC vs MOVING FORMS All theories of object recognition discussed in
this chapter have assumed that representations for object recognition
are based on an analysis of static forms. However, recent work has
demonstrated that time-varying information may also be represented
and---in certain circumstances at least---influence recognition. Stone
(1998) showed that participants who learned novel object shapes that
moved in a particular direction were disadvantaged when the image
sequence was reversed at test, suggesting that they had represented the
sequence as well as the shapes themselves. Knight and Johnston (1997)
showed that when famous faces were viewed as photographic negatives they
were much easier to recognise if shown moving rather than static. Lander
(Lander

298

VISUAL PERCEPTION

& Bruce, 2000; Lander, Christie, & Bruce, 1999) showed that this
advantage of a sequence over a static view of a face was due to rather
subtle dynamic properties. Faces made difficult to recognise by
thresholding were facilitated most by viewing them at a natural tempo.
When the dynamic properties of the sequences were altered by slowing
them down, manipulating relative frame rate, or speeding up or reversing
the sequence, recognition was less good. These results suggest that
characteristic motion either of individual faces, or of faces in
general, is represented in memory and can assist the recognition
process. Such observations are important for two reasons. First, it is
possible that there is an overemphasis on the representations that may
be extracted from single viewpoints. We normally encounter objects in
multiple viewpoints in structured sequences, as the objects themselves
move, or as we move around them. Such sequences may facilitate rather
different kinds of representational mechanism from those that have been
emphasised to date. Second, such observations suggest there may not be
such a clear-cut distinction between the kind of perceptual analysis
that underlies recognition, and the perceptual analysis of dynamic
characteristics that guides our actions in the world. We

return to consider this in the final chapter of this book.

CONCLUSIONS In this chapter we have outlined some of the problems posed
by the recognition of objects from retinal images, and have seen how
contemporary work in cognitive science has attempted to overcome these
problems. We are still a long way from developing a computer program
that can recognise everyday objects with the ease that we do, and some
way off understanding how we ourselves perform everyday tasks of natural
object recognition. The theories of recognition we have discussed in
this chapter differ in the extent to which objects are thought to be
recognised via abstract models that are viewpoint-independent, or by the
storage of particular instances or viewpoints seen on distinct
occasions. Most approaches, however, are dominated by the perception of
static form. While there is considerable evidence for independence of
"action"-based perception from "recognition", the last section has
suggested that they may not entirely be separable.

Part III

Vision for Action

Page Intentionally Left Blank

10 Introduction to the Ecological Approach to Visual Perception

is a perfectly reasonable assumption to make. Psychophysical methods
would be an adequate means of studying perception, using verbal or
nonverbal reports of people's perceptual experiences to probe a common
set of processes that are involved in all uses of vision. In Chapter 3
(p. 54) we saw that physiological evidence has challenged this
assumption, by showing that visual information is processed in at least
two partially independent neural pathways. One pathway provides a fast
link between the early stages of visual processing and the control of
actions such as reaching and grasping, and in some circumstances this
pathway can operate independently of conscious visual awareness. The
implication of these discoveries is that the role of visual perception
in the control of action is a problem for research in its own right.
Some years before these physiological advances were made, theoretical
arguments were being developed by James Gibson that would lead to a
stronger conclusion still, that perception and action are intimately
bound up with each other and cannot be treated as separate problems. In
this chapter, we will review these developments from a historical
viewpoint. We will first describe

The approaches to understanding visual perception that we discussed in
the second part of this book rest on a common assumption that the
function of perception is to provide us with awareness of the
surrounding world. With our eyes open, we have an immediate impression
of a solid world of surfaces, objects, and people, and of their shapes,
movements, or identities. From the origins of thinking about visual
perception through to modern vision science, the goal has been to
understand how such impressions are obtained from a fluctuating pattern
of light falling on the retina. Obviously, perception must do more than
create a flow of inner, private impressions of the world. Much of the
time, we are engaged in activities such as walking, handling objects, or
communicating with other people. These actions all require body
movements to be controlled by information obtained through perception.
Traditionally, this role of perception has been seen as a secondary one,
which can be addressed once we understand how the representation of the
world underlying our conscious awareness of it is created. If all the
processes involved in controlling our actions draw on this
representation, then this 301

302

VISUAL PERCEPTION

the theory that Gibson developed over a 35-year period in three major
books (Gibson, 1950a, 1966, 1979; see also Reed & Jones, 1982), and then
outline the further developments made by his followers in the
"ecological" school of psychology. In the following chapters of Part
III, we will describe theory and research that address specific aspects
of the links between visual perception and the control of action. While
early studies of these problems often started from a strongly ecological
theoretical position, interest in the visual control of action has
recently spread more widely. We will therefore draw on a wider range of
theoretical approaches to these problems in later chapters than the
specifically Gibsonian one that we sketch in this chapter.

J.J. GIBSON'S THEORY OF PERCEPTION During World War II Gibson addressed
himself to the problem of how to train pilots quickly, or how to
discriminate potentially successful from unsuccessful pilots prior to
training. The most difficult, and hence dangerous, aspects of flying are
landing and take-off. To land a plane successfully you must know where
you are located relative to the air strip, your angle of approach, and
how to modify your approach so that you are aiming for the right
position at the right speed. Gibson therefore felt that good depth
perception was likely to be a prerequisite of good flying. He
discovered, however, that tests based on the pictorial cues to depth,
and training measures devised to make people capitalise on depth
information, had little success when applied to the problem of training
pilots. Here was a clear practical example of the perception of relative
distance, and yet attempts to improve "depth perception" were fruitless.
Such observations led Gibson to reformulate his views of visual
perception radically. In his 1950 book he began by suggesting that the
classical approach to "depth" or "space" perception should be replaced
by an approach that emphasised the perception of surfaces in the
environment. This emphasis remained throughout

his subsequent books. Gibson's theory emphasises the ground on which an
animal lives and moves around, or above which an insect, bird, or pilot
flies. The ground consists of surfaces at different distances and
slants. The surfaces are composed of texture elements. Pebbles, grains
of sand, or blades of grass are all elements of texture that, while not
identical, possess statistical regularity---the average size and spacing
of elements of the same kind of texture will remain roughly constant for
different samples. Some surfaces surround objects, and these objects may
be attached to the ground (e.g., rocks and trees), or detached and
independently mobile (e.g., animals). Object surfaces, like ground
surfaces, have texture. The environment thus consists of textured
surfaces that are themselves immersed in a medium (air). Gibson argues
that we need an appropriate geometry to describe the environment, which
will not necessarily be one based on abstractions such as "points" and
"planes", as conventional geometries are. An ecological geometry must
take surfaces and texture elements as its starting point. A surface is
substantial; a plane is not. A surface is textured; a plane is not. A
surface is never perfectly transparent; a plane is. A surface can be
seen; a plane can only be visualized (Gibson, 1979, p. 35). The
structure that exists in the surfaces of the environment in turn
structures the light that reaches an observer, as we saw in Chapter 1.
Gibson argues that it is the structure in the light, rather than
stimulation by light, that furnishes information for visual perception.
Stimulation per se does not lead to perception, as evidenced by
perceptual experience in a Ganzfeld (Metzger, 1930; Gibson & Dibble,
1952; Gibson & Waddell, 1952). Diffuse unstructured light, as might be
obtained by placing halves of table-tennis balls over the eyes and
sitting in a bright room, produces perception of nothingness. To
perceive things, rather than no-thing, the light must be structured. In
order to describe the structure in light we need an "ecological" optics
(Gibson, 1961), rather than a description at the level of the physics of
photons, waves, and so on. The physics of photons coupled

10. THE ECOLOGICAL APPROACH

with the biochemistry of photoreceptor action can be used to explain how
light is emitted and propagated and how receptors are stimulated by it,
but not how the world is perceived. An ecological optics must cut across
the boundaries between physical and physiological optics and the
psychology of perception. Gibson rejected the claim that the retinal
image is the starting point for visual processing. He argued that the
whole array of light rays reaching an observer, after structuring by
surfaces and objects in the world, provides direct information about the
layout of those surfaces and objects, and about movement within the
world and by the observer. In Chapter 1 we described how light is
structured in the optic array, and here we remind you briefly of the
important points. The ambient optic array at any point above the ground
consists of an innumerable collection of light rays of different
wavelengths and intensities. Some have been emitted from light sources
such as the sun, while others have been reflected from surrounding
surfaces. These rays form a hierarchical and overlapping set of solid
angles. The solid angles corresponding to the tiniest texture elements
are nested within those that correspond to the boundaries of larger
regions or objects. Changes in the pattern or properties of the light
from one solid angle to another signal boundaries in the world, where
for example one object partially conceals or occludes another object, or
the ground. In his early work, Gibson considered in particular how
textured surfaces structure the light reflected from them, and how the
resulting structure in the optic array could inform an observer about
the shapes and orientations of those surfaces. The ground surfaces on
which we move usually have statistically regular texture, made up of
elements of similar size and shape. Light reflected from such surfaces
will form a structure in the optic array known as a texture gradient.
Figure 10.1 shows examples of how texture gradients (of artificially
regular proportions) can give impressions of surfaces receding into the
distance. Figure 10.2 shows how the local shape of a surface may be
given by the change in texture gradient.

303

Examples of texture density gradients.

Surface shape and slant can be revealed by texture.

In Chapter 7 (p. 187) we saw that simulations of textured surfaces can
give strong impressions of solid shape and enable people to make
judgements of the relative depths of the objects that they suggest. For
Gibson, however, texture gradients are not just a cue to depth that can
be combined with others, but a more fundamental property of ecological
optics that specifies unambiguously the layout of a perceiver's
surroundings. For example, changing the slant of a

304

VISUAL PERCEPTION

textured surface relative to a perceiver will produce changes in the
texture gradients in the optic array that could specify the slope of the
ground leading away from a perceiver. Gibson and coworkers (Beck &
Gibson, 1955; Gibson 1950b; Gibson & Cornsweet, 1952) showed that the
perceived slant of a simulated surface is influenced by its texture
gradient. The later research that we described in Chapter 7 has
identified more precisely the properties of texture gradients that are
responsible for the perception of surface orientation, using
sophisticated computer graphics techniques to build on Gibson's original
insights. A second way in which Gibson argued that texture gradients are
important for perception is in providing information about the distances
and sizes of objects. A traditional analysis of the problem of
perceiving size and distance would resemble that shown in Figure 10.3,
which illustrates how the same image could potentially be cast by an
infinite number of objects of different sizes, inclinations, and
distances from the observer. Gibson argued that this kind of analysis is
mistaken because it treats objects as geometric forms floating in empty
air rather than as solid, textured objects lying on a textured ground
surface. Because he stressed the importance of considering objects in
the context of a background surface, Gibson (1950a) termed his own
theory a "ground" theory of perception in contrast with traditional
"air" theories. An "air" theory implies that the size or distance of an
object can only be perceived if the other quantity is known in advance,
but Gibson argued that this problem disappears as soon as a textured
ground surface is introduced. Gradients of texture in a receding surface
provide a continuous scale, and the dis-

The kind of drawing typically used by students of visual perception to
illustrate the ambiguity of the retinal image.

tance of an object is given by its point of attachment to the ground
relative to this scale. Recently, Sinai, Ooi, and He (1998) demonstrated
that people's judgements of the distances of objects are influenced by
the structure of the ground surface on which they rest. Sinai et al.
asked participants to judge the remembered distance of an object several
metres away by matching the distance of another object or by walking the
same distance blindfolded. Judgements were accurate when the object lay
on the same flat, uniform ground surface as the participant, but showed
systematic errors if there was a hole in the ground or a boundary
between two textures (e.g., concrete and grass) lying between the
participant and the object. The problem of perceiving the size of an
object can also be solved, Gibson argued, because an object will cover
the same amount of background texture whatever its distance from the
observer (strictly, this is true only for the base of an object that is
standing on the ground). This theory provides an explanation of the
phenomenon of size constancy; the fact that we see an object as the same
size whatever its distance from us. Traditional theories sought to
explain size constancy in terms of a scaling of retinal image size using
cues to distance. In the laboratory, the perceived size of distant
objects tends to decrease with increasing distance, in the same way as
image size does, suggesting that a scaling mechanism produces a
distortion in these conditions. However, Gibson (1947) showed that in an
open, ploughed (and hence textured) field, estimates of the height of a
distant stake merely became more variable at great distance rather than
decreasing. Size constancy also fails if we view objects from a

10. THE ECOLOGICAL APPROACH

height, rather than at a horizontal distance. Thus, from the top of a
high building people on the pavement below us appear insect-like in
their proportions. Traditional theory would explain this in terms of the
absence of cues to distance. Gibson would say that when viewing from a
height, the absence of the ground removes the continuous scale of
texture necessary for accurate size perception. These examples of the
role of texture gradients in perception illustrate an important concept
in Gibson's theory, that of invariants in the structure of the optic
array. An invariant is a property of the array that is determined by
some specific property of the world and remains constant as other
conditions vary. For example, gradients in several properties of optic
texture depend in regular and predictable ways on the slant of a
surface, but are invariant with changes in the nature of the texture
elements themselves, for example whether they are pebbles, cobbles, or
tiles. Similarly, the amount of background texture covered by an object
increases with its size but is invariant with its distance. The
cornerstone of Gibson's theory was the argument that lawful
relationships such as these imply that properties of the optic array can
unambiguously specify properties of the world. Traditional theories,
Gibson argued, ignored the structure of a perceiver's natural
surroundings, and erroneously concluded that the structure of light is
inadequate to specify properties of the world and has to be supplemented
by inference and stored knowledge. A full discussion of the concept of
invariants and its significance for visual perception can be found in
Cutting (1986). So far, we have described Gibson's analysis of how the
optic array sampled by an observer at any instant of time can provide
information about the surrounding world. In doing so, we have omitted
entirely another important aspect of his theory, namely his argument
that transformations in the optic array produced by the active movement
of an observer are essential for visual perception. What is clear to me
now that was not clear before is that structure as such, frozen

305

structure, is a myth, or at least a limiting case. Invariants of
structure do not exist except in relation to variants (Gibson, 1979,
p. 87). Variants in information are produced by movement of the observer
and the motion of objects in the world. The fact that observers actively
explore their world allows powerful information from motion perspective
to tell them both about their position relative to structures in the
world and about their own movements. When an observer moves (as in
Figure 10.4) the entire optic array is transformed. Such transformations
contain information about both the layout and shapes of objects and
surfaces in the world, and about the observer's movement relative to the
world. Perception of the world and of the self go together and only
occur over time (Gibson, 1975, p. 49). Figure 10.5 shows an example of
motion perspective. As an observer walks past a collection of objects at
different distances the relative motions present in the changing optic
array will be specifically correlated with the layout of such objects.
Indeed as an observer moves in any way in the world this locomotion will
always be accompanied by flow in the optic array. The nature of optic
flow patterns is specific to certain types of movement, (see Figures
10.6--10.8). If a pilot is trying to land an aeroplane (Figure 10.6)
there will be streaming in the optic array radiating out from the point
at which he or she is aiming. This point is known as the centre of
expansion, or pole, of the optic flow field. The array of optical
texture elements (produced by light reflected from the texture elements
in the world) expands centrifugally, with elements successively passing
out of the bounded visual field of the observer and new elements
emerging at the centre of expansion. If one was sitting on the roof of a
train facing backwards there would be a continuous inward streaming of
optical texture elements towards the point from which one was travelling
(Figure 10.7). If you chose the softer option of remaining seated

306

VISUAL PERCEPTION

When an observer moves the entire optic array is transformed. From
Gibson (1966). Copyright © 1966 by Houghton Mifflin Company. Reprinted
with permission.

Successive views of a row of fence posts as an observer moves past them.
The observer travels from right to left between each of the frames from
left to right. From Gibson (1950a). Copyright © 1950 by Houghton Mifflin
Company. Reprinted with permission.

at a train window the flow pattern would be as in Figure 10.8. Gibson
(1979) described the relationship between optic flow and locomotion more
formally in the following way (abridged from Gibson, 1979,
pp. 227--229): 1. Flow of the ambient array specifies locomotion

and nonflow specifies stasis. 2. Outflow specifies approach and inflow
specifies

retreat from. 3. The focus or centre of outflow specifies the dir-

ection of locomotion in the environment.

4.  A shift of the centre of outflow from one visual

solid angle to another specifies a change in the direction of
locomotion, a turn, and a remaining of the centre within the same solid
angle specifies no change in direction. These arguments for the power of
optic flow to provide animals or people with information about their own
motion through the world have been very influential, and have inspired a
large body of research on the visual control of movement that we will
consider in Chapters 11 and 12. However, Gibson saw the role of
perceiver movement as

10. THE ECOLOGICAL APPROACH

307

The optic flow field for a pilot landing an aeroplane. From Gibson
(1950a). Copyright © 1950 by Houghton Mifflin Company. Reprinted with
permission.

The optic flow field for a person sitting on the roof of a train, facing
backwards.

more fundamental, relevant not just to the control of locomotion but to
all aspects of visual perception. This is expressed in his notion of
perceptual systems to contrast with the traditional "senses". Gibson
(1966, 1979) claimed that it was an entire perceptual system whose job
it is to "see" (1979, p. 53):

Receptors are stimulated whereas an organ is activated . . . the eye is
part of a dual organ, one of a pair of eyes, and they are set in a head
that can turn, attached to a body that can move from place to place.
These organs make a hierarchy and constitute what I have called a
perceptual system.

308

VISUAL PERCEPTION

The optic flow field for a person sitting on a train and looking out of
the window as they travel from right to left through this terrain.

Movement by the observer, whether of body, head, or eyes, is one way in
which variant information is obtained. The other way is through motion
or change in objects in the world---i.e., through events. Events include
objects or animals translating, rotating, colliding, or growing,
changing colour, or disappearing. All such events are accompanied by
disturbances in the structure of the optic array. Rigid translation of
an object across the field of view involves the progressive accretion,
deletion, and shearing of texture elements. An object will progressively
cover up (or "wipe out") texture elements in the direction of its
movement, uncover (or "unwipe") them from behind and shear the elements
crossed by the edges parallel to its movement (see Figure 10.9).

As an object moves, elements of background texture are progressively
wiped out (covered up) by its leading edge, unwiped (revealed) by its
trailing edge, and sheared by edges parallel to its direction of
movement.

If the object changes its distance from the observer this change will be
accompanied by magnification (if approaching) or minification (if
receding) of the texture elements of its own surface, and the covering
up or uncovering of texture elements of the background. Texture elements
that are covered up by object motion in one direction are uncovered by
motion in the reverse direction. The same is true of observer movement.
Texture elements that pass out of the observer's view when movement is
in one direction will reappear if the movement is reversed. Gibson
claims that this principle of reversible occlusion underlies the
observer's impression of a constant and stable visual world where even
those surfaces momentarily hidden are still "perceived".

10. THE ECOLOGICAL APPROACH

Once one considers the total array of light there is no ambiguity about
whether it is oneself or objects in the world that are moving. Eye
movements do not change the structure of the ambient optic array, but
only allow a different portion of the array to be sampled. Movement of
the head and body is always accompanied by a systematic flow pattern in
the total array. Movement of an object within the world produces local
disturbances in the structure of the array. Thus the major distinction
between movement within the world or by the observer can be specified
unambiguously by different flow patterns in the optic array. On the
basis of this logical argument, Gibson concluded that eye movements are
not taken into account when we perceive motion. As we saw in Chapter 8,
however, empirical evidence tells us that they are (see p. 263).
Gibson's approach to the psychology of perception became progressively
more radical. Although his 1950 book was a major departure from earlier
approaches, it did not directly challenge the assumption that the
function of perception is to create an awareness of the world that can
be tapped by psychophysical methods. In his later work, however, Gibson
(1966, 1979) developed more and more strongly the argument that
perception is "direct". If there are properties of the transforming
optic array that can unambiguously specify properties of the surrounding
world, Gibson argued, then there is no need for cognitive processes to
interpret visual input. In time, he extended this argument from specific
contexts such as the perception of distance, size, or slant to a general
rejection of cognitive processes and representations in the explanation
of perception. He claimed that the end product of perception is not an
internal representation of the visual world---a "percept" of which a
person is consciously aware---but the detection of affordances. The
affordance of some surface or object in the environment is what it
offers the animal---whether it can be grasped or eaten, trodden on or
sat upon. The notion of an affordance can be traced back to the Gestalt
psychologists, and particularly Koffka's idea of the "demand character"
of an object (Koffka, 1935, p. 7):

309

To primitive man each thing says what it is and what he ought to do with
it . . . a fruit says 'Eat me'; water says 'Drink me'; thunder says
'Fear me'. A sawn-off tree trunk of the right flatness and size affords
"sitting-on" by a human, or "hopping-on" for a frog; if a surface is
flat, extended, and substantial its property of affording support to
terrestrial animals is implicitly given. Gibson makes the strong claim
that there is information in the light to specify the affordances of the
environment (Gibson, 1979, p. 127): This is a radical hypothesis, for it
implies that the 'values' and 'meanings' of things in the environment
can be directly perceived. An illustration of the concept of affordance
is provided by Warren (1984), who studied people's judgements of whether
staircases with differently proportioned steps, depicted in pictures,
could be climbed in the normal way or not. Participants taller and
shorter than average differed in their judgements, which proved to be
determined by the ratio of step height to the individual participant's
leg length. Their judgements were therefore influenced both by the
geometry of the steps that they viewed and by the mechanical properties
of their own limbs; in Gibson's terms, they were sensitive to the
affordance of "climbability". While these are important findings, they
cannot test Gibson's wider theory directly. They may imply that people
pick up directly an invariant property of the pattern of light reflected
from a staircase that specifies "climbability", but it could equally
well be that inferential processes are involved in relating the
perceived dimensions of the steps to information about stride length
held in memory. While it is relatively easy to appreciate that
affordances like "climbable" or "graspable" might be specified in the
optic array, it is much less easy to appreciate how qualities such as
"eatable" or "writable-with" could be contained within the light. At the
point where Gibson (1979) claims that a letter-box affords the posting
of letters by humans of western culture, his theory is the most
controversial. Nevertheless, Gibson

310

VISUAL PERCEPTION

claimed that the concept of affordances can bridge the gap that exists
in more cognitive theories between perception and action. Within the
theory of affordances, perception is an invitation to act, and action is
an essential component of perception. However, Gibson's claim that all
aspects of perception can be understood without appeal to the concept of
representation is problematic, and we will return to this issue in
Chapter 14. Gibson thus asserts that optical information specifies
surfaces of support, falling-off places, impending collision, and so on.
And, he claims, affordances are perceived directly, without the need for
mediation by cognitive processes. The major task for the ecological
psychologist is to discover the invariant information that animals have
evolved to detect, and to discover the mechanisms by which they become
attuned to this information. Gibson denies the need for memory in
explaining perception. Incoming percepts are not matched against
previously laid-down traces, rather the perceptual system has evolved to
"resonate" to certain invariant information. The concept of "resonance"
is left rather vague by Gibson, but presumably it implies that there
should be neurons or neural networks sensitive to variables of higher
order than features such as lines and edges. Gibson decries traditional
laboratory experiments in perception in which observers are presented
with "stimuli", devoid of context. In such situations the optical
information is indeed impoverished but this will only tell us how a
human observer copes with artificially impoverished inputs, and may tell
us nothing of perception in the optically rich real environment. He
denies that ambiguous figures and illusions should be the starting point
for a psychology of perception. While these may be interesting, and may
be analysed in terms of the invariant information that they contain,
they are not characteristic of normal perception. In the real world such
perceptual distortions are rare. Additionally, Gibson regards the
perception of pictures (the focus of much research in perception) as
involving two components---the direct perception of the picture as a
picture, i.e., as a flat surface, and the indirect

perception of what it is that the picture represents. For example, a
picture of an apple, as a flat surface, affords little apart from
inspection. The affordances of the object depicted, that it can be
grasped, thrown, or eaten, are perceived indirectly and without ever
fooling an adult observer into actually trying to reach for and eat the
picture. Everyday perception is of the direct, not the indirect kind.
Gibson's final theoretical position is radical indeed. It stands apart
from the mainstream of perceptual theory. Some have likened Gibson's
ideas to those of the Gestaltists, who took a similar phenomenological
approach to seeing. However, the Gestaltists were nativist in
philosophy, while Gibson sees learning as important; and the Gestaltists
sought to explain perceptual phenomena in terms of the physiology of the
brain, unlike Gibson. It would be as legitimate to compare Gibson to the
behaviourists, who looked at stimuli and responses but did not care to
speculate on intervening stages of processing. On the other hand, the
behaviourists saw animals as prodded into action by discrete stimuli or
sensations--- while for Gibson, perception and action are intimately
interlinked. Thus Gibson's approach is unique and for many years was
ignored by the vast majority of perceptual psychologists. This is
largely because the difference between Gibsonian and traditional
accounts of perception is more profound than might be appreciated from
the preceding pages. The differences between the two approaches are not
just psychological but verge on the philosophical. Traditional
perceptual theory holds that perception is indirect and mediated by
higher cognitive processes. We do not "just see" the world but actively
construct it from fragmentary perceptual data. Gibson is a "direct
realist". He holds that perception is direct and unmediated by inference
and problem solving. We will discuss some of these theoretical
controversies further in Chapter 14, but for now our aim is to describe
how the relationship between perception and action continued to be
explored within the approach that Gibson founded. The goal of this
school of "Ecological Psychology" has been to explain how perception and
action are linked without invoking the idea of a

10. THE ECOLOGICAL APPROACH

representation of the surrounding world that stands between them. In
order to give an outline of the kind of alternative that it proposes, we
need first to consider some basic questions about the control of
movements of the body and limbs.

THE CONTROL OF HUMAN ACTION A theory of the control of action must
explain its flexibility; the fact that animals and people are able to
achieve equivalent ends in a variety of ways. Turvey (1977) gives as an
example the observation that we can draw a letter A with a pen on paper,
with a finger on someone's back, or with a toe in the sand. At least
some of these activities may be novel, but are not difficult for us. In
the same way we can recognise a letter A drawn in different ways by
different people (see Chapter 9). Some abstract representation of the
letter seems to allow for the way in which we can generalise both when
perceiving and when acting. Suppose then we wished to draw an A in the
sand with a toe. Somehow the abstract representation that we wish to
particularise must be translated into a specific pattern of motor (i.e.,
muscle) activity. It might be possible, in principle, to conceive of
each muscle involved in this action being independently instructed by
commands issued from a high level in the nervous system. Thus our
abstract conception of the letter A might be translated into a series of
independent commands to a variety of muscles in the leg, foot, and toes.
This kind of "push-button" metaphor for the control of action has been
criticised by Bernstein (1967) whose arguments have been summarised and
extended by Turvey and his colleagues (Fitch, Tuller, & Turvey, 1982;
Tuller, Turvey, & Fitch, 1982; Turvey 1977; Turvey, Fitch, & Tuller,
1982; Turvey, Shaw, & Mace 1978). There are two different, but closely
associated problems with the push-button metaphor. The first is known as
the degrees of freedom problem. An "executive" issuing independent
commands to all those muscles involved in even the simplest of movements
would have a very great deal of moment to moment computation to perform.

311

This first problem is possibly compounded by the second---that of
context-conditioned variability (Turvey et al., 1982). The context in
which any particular muscle contraction occurs affects the actual limb
movement achieved. The movement produced by a given contraction depends
on the current configuration of the parts of the limb, the current
motions of the adjoining limb segments, and the external forces against
which each muscle must work. An executive pressing the buttons, in such
a model, would have to have moment to moment information available about
the external forces and the dynamic and static aspects of the current
configurations of the limb segments. Turvey et al. (1978) liken the
problem of the push-button executive to that which an air pilot would
face if he or she had to control individually each of the mechanical
segments used to guide the flight of an aeroplane. At a minimum, an
aeroplane has two ailerons at the back of the wings, which can be moved
up or down to control roll; two elevators on the tail, which if moved up
or down control pitch; and a rudder at the back which can be moved left
or right to control yaw (cf. Figure 11.3, p. 317). There is thus one
degree of freedom for each of these five hinged parts. If each of these
parts had to be altered individually the pilot would be faced with an
impossible informational load. Even if the mechanical parts could only
be moved to one of eight positions the control system would still have
to keep track of 85 (32,768) independent states. Of course no air pilot
actually has to cope with this task because the mechanical components of
the guidance system are in fact linked. The ailerons are yoked so that
when one moves up the other moves down. The rudder is linked to the
ailerons so that it moves left when the right aileron goes down, and the
elevators on the tail section move together---both up or both down. This
linkage reduces the degrees of freedom to two, and the guidance of the
aircraft can be achieved with a joystick which also has two degrees of
freedom (it can be moved forward or backward for ascent or descent and
from side to side to bank or turn.) Turvey (1977) and Turvey et
al. (1978) suggest that combinations of muscles in animals are

312

VISUAL PERCEPTION

similarly linked and constrained to act together as co-ordinative
structures. To some extent, these can function autonomously, without
control from higher levels in the nervous system. Spinal reflexes can
work in this way, even though they may involve quite complicated
actions. For example, an animal with the upper part of the spinal cord
completely sectioned will still repeatedly scratch an itch on its body
with whichever foot can most easily reach it. The concept of
co-ordinative structures goes beyond simple reflex acts, however, to
include patterns of interlimb co-ordination in voluntary acts. An
everyday example is given by the difficulty we experience if we try to
beat out two quite different rhythms simultaneously with different
hands. The hands seem constrained to act together in this situation.
Kelso, Putnam, and Goodman (1983) have demonstrated this more formally.
If two hands are required to make movements of different difficulties
and directions, the movement of each hand is influenced by that of the
other. Such patterns of mutual constraint and interaction would not be
expected if an "executive" independently commanded each muscle. Instead,
Turvey (1977) suggests, the executive commands groups of muscles that
function co-operatively together, and so the number of degrees of
freedom can be greatly reduced. Co-ordinative structures can also solve
some of the problems of context-conditioned variability. For example, a
co-ordinative structure can take care of the local context in which an
action takes place if it behaves like a mass--spring system. The
equilibrium point of a spring to which a mass is attached is not
affected however the mass is pushed or pulled. The spring always returns
to rest at the same length, without any executive monitoring its
movements over time. Further models with similar properties have been
developed to describe the behaviour of coordinative structures, using
concepts from dynamical systems theory (for an introduction, see
Abraham, Abraham, & Shaw, 1991). For example, some systems of muscles
show the properties of limit-cycle oscillators, tending to return to a
particular pattern of oscillation, rather than a set point, after being
disturbed. It is possible to

test models of this kind by measuring changes in rhythmic actions, such
as finger tapping, after a perturbation. An example of this approach can
be found in Kay, Saltzman, and Kelso (1991), who provide evidence that
finger tapping is driven both by a peripheral oscillator arising from a
co-ordinative structure of arm and hand muscles and by a central neural
oscillator. The interaction between the two oscillators appears to work
in both directions; the peripheral system is not driven passively by the
central one, but can also influence its activity. This two-way
interaction underlying finger tapping provides one example of a more
general argument made by Turvey et al. (1978) that motor control is not
organised hierarchically, with the executive issuing commands that pass
unidirectionally and without modification to lower levels. Rather they
suggest that the system must be organised as a heterarchy, or, more
radically, as a coalition. In a heterarchical organisation no one part
of the system should be seen as dominating the others. All levels in a
heterarchy contribute equally to hypothesis testing and decision making.
An important structure that contributes to the organisation and control
of action is the segmental apparatus of the mammalian spinal cord. The
spinal cord is made up of a series of segments (marked out by the
vertebrae), within each of which there are neuronal loops that control
simple reflexes (like the knee jerk), without involving any
communication with the brain. Complex voluntary activities may involve
the recruitment, modification, and elaboration of these simple reflexes,
which form the bases for co-ordinative structures. This may be achieved
in part by tuning of the segmental apparatus prior to a movement
occurring. Turvey (1977) cites evidence from Gurfinkel et al. (1971) in
support of the notion of tuning. If a person is asked to flex one leg,
it typically takes about 170 ms between the command and the flexion
occurring. If, during this latency period, the knee-jerk reflex is
elicited, its amplitude is enhanced relative to a control condition
where no command is present. It therefore appears that an instruction
issued from the brain to the leg involves the preparation or tuning of

10. THE ECOLOGICAL APPROACH

the segmental apparatus prior to the actual movement of the leg
occurring. If we continue to consider leg movements, in the more complex
activities of walking or running, there is evidence that the
organisation of the segmental apparatus of the spinal cord allows the
initiation and maintenance of stepping movements of the limbs without
sensory input. However the form of the stepping pattern must be tailored
to the external forces. This can be achieved by using afferent
information obtained from reflex structures and also by tuning the
segmental apparatus on the detection of relevant information obtained
primarily through vision. In this way a basic pattern of activity can be
attuned to the current contextual demands.

Implications for the role of perception The traditional metaphor of a
central executive controlling action in a push-button fashion
complements the view that perception acts to provide a representation of
the surrounding world for the central executive to use in making
decisions. Having discussed the objections to such metaphors, what does
the ecological approach outlined above imply about the role of
perception? Turvey and his colleagues argue that visual information of
particular kinds must be injected into unfolding activities at
appropriate points, after which the co-ordinative structures that have
been activated and tuned can take care of themselves to a large extent.
They devolve the responsibility for these "injections" of visual
information to the coordinative structures themselves (Fitch et al.,
1982, p. 272): We do not want a model in which the brain interprets the
perceptual information, decides what portion of the information to
supply a given coordinative structure, and when to supply it. Instead,
the organisation of the coordinative structure should be such as to
accept only certain information at certain times. An implication of the
ecological approach is that the information obtained through vision is
not independent of the motor activity it controls.

313

Consider someone catching a ball: as the action proceeds, the ball is
tracked by head and eyes, the arms move, and the fingers open and then
close on the ball. At each stage, the activity of coordinative
structures is modulated by particular kinds of optical information. In
the ecological view, we should not assume that perception delivers a
representation of the trajectory, speed, and size of the ball, and then
leaves the rest to a motor system. Evidence in support of the
interdependence of perception and motor tasks comes from experiments
showing that people use visual information differently when they are
performing different actions. For example, Bootsma (1989) compared the
behaviour of people hitting a falling ball with a bat, closing a switch
to release an artificial arm holding a bat, and closing a switch to
signal when the ball passed a marker. From a traditional point of view,
all these tasks seem equivalent: in each one, the subject must detect
when the ball reaches a particular point and then execute some action.
From an ecological perspective, however, the different actions involved
may use visual information in different ways. The results gave some
support to the ecological view, as the timing of the natural batting
movements was less variable than that of the switch-closing movements.
The implication is that the control of the natural perception--action
coupling is more accurate than that of an apparently "simpler" movement.
The most progress towards a detailed understanding of the links between
perception and coordinative structures has been made in cases of
rhythmic movements. Schmidt, Carello, and Turvey (1990) studied the
synchronisation of limb movements between two people, by asking pairs of
participants to sit with their legs crossed, in view of each other. They
were then both asked to swing one lower leg up and down, keeping its
movement in time with the other person's leg movement. The interesting
result from these experiments was that the coupled oscillations of the
two legs of different people behaved in the same way as those of two
fingers of the same person (Kelso et al., 1983; see earlier). Visual
information specifying the movement of the other person's leg therefore
seems to become coupled to

314

VISUAL PERCEPTION

the observer's leg rhythm in the same way that different motor systems
become coupled.

CONCLUSIONS The analysis of the control of human action that we have
discussed in this chapter is an important extension of Gibson's theory,
and represents the achievements of ecological theorists in developing
tools for understanding the mutual coupling of perception and action.
The approach aims to understand the structure of action, and its
modulation by environmental information, in terms of the behaviour of
dynamical systems governed by physical laws, rather than the behaviour
of a "central executive" controlling the muscles (see Kelso, 1995). The
aim is clearly an ambitious one, and the progress made has so far been
on a few specific problems such as the control of rhythmic limb
movements and the coupling of the underlying dynamics to optical
information. The problems of the degrees of freedom in motor control,
and of context-conditioned variability, have been tackled from other
theoretical perspectives, and the reader may be interested in comparing
these with the ecological approach sketched here. These include the
analysis of the kinematics (trajectory and speed) of human movements to
discover underlying invariants (e.g., Lacquaniti, 1989), and use of
single-cell recording methods to determine how parameters of movement
are coded in the activity of single cells in the spinal cord and brain
(e.g., Bizzi, Mussa-Ivaldi, & Giszter, 1991; Fetz, 1992; Georgopoulos,
1991). Connectionist modelling techniques (see Chapter 4) have also been
applied to the problem, attempting to train a network to

control a multi-jointed model person or limb without any explicit coding
of movement parameters. A successful example is Hinton's (1984) model of
reaching forwards without losing one's balance, using a simple
two-dimensional model person. The problem is to avoid swinging the arm
out in a way that shifts the centre of gravity of the person as a whole
to an unstable position---to avoid this, other limb and trunk movements
must occur to compensate. Hinton found that a connectionist model could
satisfy these two constraints (touch the object, while maintaining
centre of gravity above the foot) simultaneously, and that the solution
was much more elegant when combinations of joint angles were adjusted
synergistically (cf. our earlier discussions of coordinative
structures). An important difference between the ecological approach and
other approaches to understanding the control of action is that it
treats perception and action as interlocked processes and tries to avoid
separating out motor control as a distinct problem. Although it is
possible to imagine how this principle might be carried through into
more detailed models of those aspects of perception involved in the
moment-bymoment control of movements such as reaching, walking, or
jumping, it does not seem directly relevant to understanding how
perception yields awareness. In particular, it is difficult to see how
the principle is relevant when we acquire knowledge of the world through
perception that is used to guide action at a much later time---or indeed
might never be used in this way at all. We will return to these problems
in Chapter 14, but in the following three chapters we will describe
research on the visual control of movement that has been carried out
both inside and outside the ecological school that Gibson founded.

11 Optic Flow and Locomotion

chapter. Because Gibson regarded the starting point for perception as
the changing optic array and not a retinal image, he defined an optic
flow field in terms of the motions of rigid objects and surfaces
relative to a moving observer (or, equivalently, the motions of cones of
light rays in the optic array reflected from objects and surfaces). We
can illustrate this by replacing a pictorial description of an optic
flow field such as the one in Figure 10.6 with a schematic one such as
Figure 11.1. As an observer moves forwards, the angle θ between his or
her direction of motion and the direction of any visible object
increases. The rate of increase of. this angle, or the angular velocity
of the object, θ is given by:

According to Gibson's theory of visual perception, changes over time in
the optic array, such as the optic flow created by our movement, provide
us with the information that we need about the structure of the world
around us. Gibson also argued that perception and action constrain one
another; perception is the pick-up of information currently needed to
guide action, and action in turn causes transformation of the optic
array that yields further information for perception. In this chapter,
we will bring these two aspects of the theory together to explore the
role of optic flow in control of animal and human locomotion. We will
consider whether global patterns of optic flow are used to solve such
problems as maintaining a stable position in space, or controlling the
speed and direction of movement. The evidence that we will discuss comes
not only from within the Gibsonian tradition, but also from research in
a range of other areas of psychology, and in biology, that has converged
on the same problems.

. V sin θ θ= D

(11.1)

where V is the observer's velocity and D is the distance of the object.
The angular velocities of each object or patch of texture in the
surroundings define the structure of the optic flow field. A picture
such as Figure 10.6 illustrates a relatively simple optic flow field.
The observer is a pilot flying over a distant surface, with variations
in height that are small relative to its distance. As a result, the
angular velocity of texture on the ground surface varies smoothly, from
zero far ahead to a maximum directly below. More

OPTIC FLOW AND RETINAL FLOW Before turning to detailed evidence, we need
to analyse the properties of optic flow in a little more detail than in
Chapter 10, and to make some distinctions that we will use in the rest
of this 315

316

VISUAL PERCEPTION

A person (viewed from above) walks to the right with velocity V. An
object X lies at a distance D and an angle  from the person's direction
of motion.

usually, however, people move on foot or in vehicles through cluttered
surroundings such as rooms or city streets, in which objects lie at many
different distances. Although the structure of the optic flow field is
determined by the same equation, the result is now more complex. There
will be abrupt changes in angular velocity, or motion parallax, at the
boundaries between surfaces lying at different distances (see Figure
10.9). Although we have so far treated optic flow as a pattern of
relative motions between observer and objects, we can also consider the
projection of an optic flow field on to an image plane such as a retina
(see Figure 11.2a). The image of each object or patch of texture will
move across the retina away from the image of the centre of expansion of
the optic flow field, and its velocity will be proportional to the
angular velocity of the corresponding object or patch of texture. This
centrifugal pattern of image motion can therefore provide any
information that is available in the optic flow field. However, retinal
flow is only equivalent to optic flow if the eye is undergoing linear
motion through the surroundings. Consider a different situation, where
the eye remains in the same place, but rotates around its axis (see
Figure 11.2b). Now, all parts of the image move over the retina at the
same velocity. This pattern of rotary image flow is quite different from
the linear flow pattern generated by the projection of optic flow on to
the retina. As a person moves about, rotary flow will be generated by
eye and head movements, and by curvature of their path,

and will be superimposed on the linear flow generated by their movement.
We will see later in the chapter some consequences of this for the use
of optic flow to control locomotion.

THE VISUAL CONTROL OF INSECT FLIGHT Insects depend heavily on vision to
control their flight behaviour, and possess a variety of neural systems
that are sensitive in particular to patterns of image flow across their
compound eyes. In this section, we will describe how behavioural and
physiological research has identified some of these systems, and how a
Gibsonian analysis of the role of optic flow in the control of action
has contributed to it.

Steering a straight course An insect flies by beating its wings rapidly,
twisting them as it does so, so that on each downstroke air is driven
backwards and downwards. This generates a force on the insect with two
components---an upwards force, or lift, and a forward force, or thrust.
The aerodynamic principles by which these forces are produced are well
understood, at least for larger insects (Pringle, 1974). Simply beating
the wings will not ensure stable flight, however. The direction and
magnitude of the force produced by the wingbeat must be controlled to
prevent the insect rolling, yawing, or pitching (see Figure 11.3). A
degree of stability is

11. OPTIC FLOW AND LOCOMOTION

317

(a) An eye undergoing linear motion to the right.
(b) An eye rotating around its axis. In both diagrams, the small arrows
    show the motions of images of surrounding objects over the retina.

The three orthogonal axes through a flying insect. Rotation around axis
x is rolling, around axis y is yawing, and around axis z is pitching.

provided by an insect's anatomy. The abdomen, particularly if it is long
as in the locust, acts as a rudder to counteract pitch and yaw, and in
all insects the centre of lift is above the centre of gravity, giving
stability against roll. In these ways, deviation from a stable flight
attitude generates a correcting force. This inherent stability is
augmented by neural control systems linking input from sensory receptors
to motor output to flight muscles. These receptors include cells
sensitive to the flow of air over the insect. In the locust, the rate of
air flow over either side of the head during flight is detected by
sensory hairs. If the insect yaws, the two rates of air flow differ, and
the resulting signals from the sensory hairs cause steering movements of
the legs and abdomen (Camhi, 1970). As long as the surrounding air is
still, or is moving uniformly, this change in the pattern of air flow
over the head unambiguously specifies a rotation caused by the insect's
own movement, and the corrective movements it triggers will keep the
insect on a straight and stable course.

318

VISUAL PERCEPTION

Often, however, an insect flies through air that moves in irregular
currents, gusts, and eddies too small for us to detect but large enough
to deflect a flying insect. In this situation, air flow over the body no
longer provides information about the insect's path relative to the
environment. An insect that could only correct turns relative to the air
around it would fly in an irregular, "drunkard's walk" path as it was
blown about by changing air currents, and would be unable to fly any
distance through the environment in order to reach new food sources or
other goals. To maintain a straight and stable course through
fluctuating air currents, an insect needs information about its turns
relative to the rigid objects and surfaces surrounding it, and this
information can be provided by vision. Locusts maintain stability in the
rolling plane by detecting both the direction from which diffuse light
intensity is the greatest and the angle of the horizon relative to the
body axis (Goodman, 1965). In a locust's natural environment, these two
sources of information unambiguously specify the direction of the force
of gravity, whatever the air around the insect is doing. Even so, these
two means of ensuring stable flight have their limitations. First, the
orientation of the horizon is useful only to insects flying over open
country, as locusts do when migrating, but not to insects flying through
a cluttered environment of vegetation. Second, neither mechanism can
correct yawing turns and prevent an insect flying round and round in
circles. A further means by which insects can maintain stable flight,
which overcomes both these problems, is through the optomotor response.
The first demonstration of the optomotor response (Kalmus, 1949) studied
the control of walking rather than flying, and a typical experiment is
shown in Figure 11.4. A fly walks on a platform surrounded by a cylinder
with vertical stripes on its inside surface. Turning the cylinder causes
rotary flow of the image on the insect's retina, simulating the effects
of the insect itself making a turn. When the cylinder turns, the fly
turns in the same direction, so as to minimise this image flow.
Quantitative analyses of the relationship between parameters of the
rotating striped

Experiment to demonstrate the optomotor response of a walking fly.

pattern and of flies' turning responses were the basis of Reichardt's
(1969) model of the first stage of motion detection in the insect
retina, later extended to the more general model of motion detection
described in Chapter 8 (see p. 218). The ability to detect rotary image
flow is also used by many insects to correct turns during flight. The
optomotor response can be demonstrated in flight by suspending an insect
in the air by a rod glued to its back. In these conditions, it will beat
its wings as if flying, and its turning responses to artificial flow
fields can be measured. When "tethered" in this way, the fruitfly
Drosophila melanogaster not only responds with a yawing turn to a
striped pattern moving sideways across its visual field, but also makes
a pitching or rolling turn in response to the rotation of a drum around
the other two perpendicular axes (Blondeau & Heisenberg, 1982; see
Figure 11.5). These

11. OPTIC FLOW AND LOCOMOTION

319

The optomotor response of a tethered fly to rotation of a drum in all
three planes. In each case, the fly turns to minimise velocity of flow
of optic texture. Adapted from Blondeau and Heisenberg (1982).

rotating patterns simulate the image flow caused by rotation of the
insect, whether the rotation arises from its own movement or from an air
current or from both. Since all these situations require the same
corrective manoeuvres, there is no need for an insect to discriminate
between them. It is obvious how these optomotor responses act to
maintain stable flight by a fruitfly or other insect, but can they also
keep an insect on a straight path through fluctuating air currents? In
principle, they can play a part in doing this. Each time the insect is
rotated by an air current, image flow will cause it to generate an
opposing torque, or turning force. The insect will therefore keep the
direction of its thrust constant relative to the environment. Its actual
path will be determined by the resultant of its thrust and the air
current, and the insect will therefore follow a zig-zag path.
Nevertheless, this path will have a component in a constant direction,
and the insect will not fly around in circles.

Flying towards objects We have seen how the ability to detect image flow
can help an airborne insect to stabilise its flight and to steer a
roughly straight course. From time to time, an insect will also need to
orient its flight towards particular objects, such as decaying matter on
which houseflies feed, flowers from which bees or butterflies take
nectar, or potential mates flying nearby. We shall next discuss evidence
that insects use image flow to discriminate such objects from cluttered
backgrounds, and to orient their flight towards them. As well as showing
an optomotor response to a revolving striped drum, the housefly Musca
domestica will turn so as to follow the movement of a single vertical
stripe on a drum (Reichardt & Poggio, 1976). As the drum rotates, the
fly follows its movement in such a way as to keep the stripe in the
centre of its visual field. Reichardt and Poggio found that the yawing
torque of a fly in this situation is determined by three factors. One is
the angle between the stripe and the long axis of the

320

VISUAL PERCEPTION

fly's body. The second is the angular velocity of the stripe. For a
given position of the stripe, the fly will turn more quickly if the
stripe is moving away from the centre of the visual field more quickly.
In this way, the fly has a simple ability to extrapolate from the
stripe's rate of movement and predict its future position. Finally, the
fly's torque fluctuates irregularly, and this component can be observed
either when the fly is placed in a homogeneously lit environment, or
when it is fixating a stationary stripe. The properties of the motion
detectors in the first stage of the fly's visual pathway (Reichardt,
1969) are matched to the temporal characteristics of this fluctuation so
as to provide inherent stability of the optomotor control system against
its effects (Warzecha & Egelhaaf, 1996).

Record of a fly chase. Circles show positions of flies at 20 ms
intervals (open circles, leader; closed circles, pursuer). Corresponding
positions are numbered at 200 ms intervals. Reproduced from Land and
Collett (1974) with permission of authors and publishers, ©
Springer-Verlag.

Land and Collett (1974) were able to show that the same parameters of
image flow control turns when houseflies are in free flight. If two
airborne flies come close to each other, they may buzz around in a brief
flurry and then separate. Land and Collett filmed encounters of this
kind between flies of the species Fannia canicularis and found that they
take the form of a chase, lasting between 0.1 and 2 seconds, in which
the leading fly is closely followed by the pursuer. The record of one
chase (Figure 11.6) shows how each time the leading fly turns, the
pursuer manoeuvres so as to follow it. Land and Collett were able to
reconstruct a pursuing fly's path accurately, given the leading fly's
path, by applying two rules governing the pursuer's behaviour. First, as
the leader's angular

11. OPTIC FLOW AND LOCOMOTION

deviation from the pursuer's axis increases, the pursuer turns to reduce
the angle. Second, when the leader is within 30° on either side of the
pursuer's axis, the pursuer detects the leader's angular velocity and
turns to reduce it also. As a result, the pursuer can begin its turn
before the leader crosses its midline. These parameters of angular
position and velocity are exactly those that Reichardt and Poggio (1976)
demonstrated as controlling turns made by tethered flies. One possible
reason why a fly may need to track small nearby targets is that this
behaviour enables male flies to locate and contact females. How, though,
does such a simple form of visual guidance allow males to distinguish
females from males, or to discriminate between females of different
species? Part of the answer lies in the context of the fly's behaviour;
Land and Collett (1974) suggest that a male fly can fly and turn more
quickly than a female, so that a male pursuer can catch up with a female
but not with a male. Likewise, a pursuer will be unlikely to catch up
with a fly of another species because its aerodynamic properties will
differ from those of a female of its own species. If the leader is
smaller than a conspecific female and therefore able to turn more
quickly, or if it is larger and therefore able to fly faster, the
pursuer will fall behind. Whether the pursuer catches up depends on the
size and aerodynamic properties of the two flies. If the leader is a
female of the same species, these properties will be matched and the
pursuer will catch up and mate. It seems unlikely, however, that this
mechanism is foolproof, as flies of other species may well be present
that are of the same size as females of the pursuer's species. Unless
they have some further means of discriminating targets at a distance,
houseflies will therefore waste some time in pursuit of the wrong
target. A second possible reason why flies track small targets is that
they need to orient towards landing surfaces in order to find food. If a
fly passes close to a surface, it will give rise to a patch of optic
texture moving rapidly in the flow field. By turning towards this patch,
the fly will approach the surface and land on it. If this mechanism is
to work in flies' natural environments, however, it must also be
sensitive to objects with textured sur-

321

faces against similar backgrounds, such as a nearby leaf against a
background of foliage. The relative motion of such an object as a fly
passes near it will give rise to a boundary between regions of similar
optic texture moving with different speeds. Reichardt and Poggio (1979)
have demonstrated that flies are able to detect and orient towards
boundaries of this kind. They presented tethered flies with a vertical
stripe of a randomdot texture against a background of the same texture
(similar to that illustrated in Figure 7.5). If both stripe and
background either remained stationary or moved together, the flies did
not fixate the stripe, confirming that there were no detectable
differences in texture between it and the background. However, relative
movement between stripe and background did cause flies to fixate the
stripe. Reichardt, Poggio, and Hausen (1983) found that flies respond in
the same way to a difference in spatial structure (such as dot size)
between a small patch and its surround as they do to a difference in
velocity of motion, and so apparently a fly cannot discriminate between
the two situations. In the fly's natural environment, however, a nearby
object against a cluttered background will give rise to either, or
usually both, of these optical features, and so a mechanism built to
steer the fly towards a landing surface need not distinguish them. A
simple perceptual mechanism can control behaviour adequately without
delivering a full and accurate description of the environment. More
direct evidence that insects use relative motion to orient towards
landing surfaces has been obtained by Srinivasan, Lehrer, and Horridge
(1990) from experiments in which honeybees were trained to feed from a
sugar solution on artificial "flowers" consisting of small discs raised
above ground level. The bees were easily able to find discs when they
and the ground surface below them were both covered with the same
random-dot texture. The higher a disc was raised above the ground, the
more likely bees were to land on it, and they also showed a strong
tendency to land at the edge of a disc, facing inwards. These results
suggest strongly that bees

322

VISUAL PERCEPTION

find landing surfaces such as flowers by detecting the boundary of a
patch of optic texture moving faster than the surrounding texture.
Further experiments (Lehrer & Srinivasan, 1993) confirmed that bees are
much more likely to approach and land at a motion boundary where image
speed increases suddenly, than one where it decreases. Sensitivity to
small patches of texture moving more quickly than the background
provides insects such as houseflies and bees with a robust,
general-purpose mechanism for orienting towards landing surfaces, but
how can this mechanism be compatible with the optomotor response? If an
airborne fly turns towards a small object against a textured background,
the turn will cause the image of the background to move in the opposite
direction, and it seems that the resulting optomotor response will
cancel out the original turn, locking the insect into its flight path.
The solution to this problem turns out to be surprisingly simple. Two
distinct flight control mechanisms operate in parallel in the housefly
(Egelhaaf, Hausen, Reichardt, & Wehrhahn, 1989). First, a "large-field"
system is sensitive to relatively low-speed image motion over a large
region of the retina, and is responsible for the optomotor response. At
higher image speeds, caused by oscillating an optomotor drum at a higher
frequency, this response to uniform image motion disappears. Second, a
"small-field" system also generates a yawing turn in response to image
motion, but is sensitive to faster motion over a small part of the
retina, and is responsible for the tracking of small moving objects
described earlier (Land & Collett, 1974; Reichardt & Poggio, 1976). The
critical difference between the "largefield" and "small-field" systems
is in the ranges of image speed to which they are sensitive. When the
small-field system detects an object, the fly fixates it in a series of
fast turns rather than a slow, smooth turn. As a result, image speed is
too high to be detected by the large-field system, and no stabilising
response occurs. In effect, the fly is able to turn to fixate objects by
doing so at a speed too high for its flight stabilising mechanism to
respond. The fly's yawing torque is therefore con-

trolled by at least two systems working independently, each designed to
work in a particular range of situations without interfering with the
operation of the other. As a fly approaches a landing surface, a further
mechanism comes into play to control landing. A fly does not simply
crash into a surface (unless it is made of glass---why should be clear
in a moment!) but performs a stereotyped series of landing manoeuvres,
in which it decelerates and extends its forelegs forwards. The
centrifugal optic flow produced by a looming surface straight ahead can
be simulated by presenting a fly with a rotating disc on which a spiral
is painted. Depending on the direction in which the disc is rotated,
either inward or outward movement of edges is generated. If a housefly
is suspended in front of a disc rotated so as to produce an expanding
pattern, it will immediately give the landing response, whereas a disc
rotating in the opposite direction elicits no response (Braitenberg &
Ferretti, 1966).

Control of ﬂight speed and height How could a flying insect keep its
speed relative to the ground constant, despite variations in its air
speed caused by varying air currents? It might control the speed of the
image of the ground surface as it flows backwards over the lower part of
the retina, increasing or decreasing its forward thrust as image speed
falls or rises. However, this mechanism would also be sensitive to the
insect's height above the ground, as image speed is a function of both
an observer's speed and the distance of an object (see Equation 11.1,
p. 315). Even so, it might still be adequate as a means of controlling
an insect's lift and thrust together, and David (1985) found evidence of
a mechanism of this kind in fruitflies. During flight, a fruitfly can
alter the angle of its body relative to the horizontal, thereby changing
the ratio of the upward lift to the forward thrust generated by its
wingbeat. If the retinal speed of the image of the ground increases, a
fruitfly tilts its body axis so as to increase lift and decrease thrust.
This is the appropriate response whether the change in image speed is
caused by the fly flying too fast, or too close to the ground, or both.
Conversely, a

11. OPTIC FLOW AND LOCOMOTION

decrease in image speed, caused by flying too slowly or too high, causes
the opposite effects on lift and thrust. This simple control mechanism
may therefore provide an accurate enough means of controlling both the
speed and the height of an airborne fruitfly. Another flight control
mechanism that is sensitive to image speed underlies the "centring"
behaviour of honeybees. When bees fly through an aperture, they follow a
path roughly equidistant between its two sides. In order to do this, a
bee only needs information about its relative distance from each of the
two sides, and could potentially obtain this from the relative speeds of
their images. Kirchner and Srinivasan (1989) trained bees to fly through
a tunnel to a food source, and then moved one of the tunnel walls. If
the wall moved in the same direction as the bees' flight, they moved
closer to it, while if it moved in the opposite direction, they moved
towards the other wall (see Figure 11.7a, b, c). These results could be
explained if bees equalised either the speed or the frequency with which
stripes passed by on the two sides. Srinivasan, Lehrer, Kirchner, and
Zhang (1991) showed that differences in the spatial frequencies of the
patterns on the two walls of the tunnel had no effect on bees' flight
paths (see Figure 11.7d), confirming that they do use image speed, and
not temporal frequency, to gauge relative distance. The centring
mechanism therefore contrasts with the

323

optomotor response, which is a function of temporal frequency and not
stripe speed (Reichardt, 1969). The implication is that image motion is
processed in parallel by at least two different systems, each
responsible for the control of a different aspect of flight (Srinivasan,
Poteser, & Kral, 1999). The control of bees' landing flight also
involves regulation of the retinal velocity of the image of a surface.
We saw earlier that the centrifugal optic flow produced by direct
approach is sufficient to trigger a housefly's landing response. Bees
often make "grazing" landings on extended surfaces, approaching on a
shallow trajectory, and in this situation optical expansion of the
landing surface will be relatively weak. Srinivasan, Zhang, Chahl,
Barth, and Venkatesh (2000) filmed bees making grazing landings at
angles between 20° and 40°, and found a close correlation between height
and horizontal speed, which is in turn correlated (though less strongly)
with vertical speed. They argue that this result reflects a simple
system of landing control, in which the velocity of the image of the
ground is measured by the same processing system as controls the
centring response. As the bee starts to lose height, image velocity is
kept constant by reducing horizontal flight speed, while vertical speed
is reduced in proportion. The end result is that the bee touches the
ground just as both components of its speed reach zero.

Schematic view from above of a bee flying along a tunnel with striped
walls. (a) Both walls are stationary. (b) The lower wall is moving in
the same direction as the bee's flight. (c) The lower wall is moving in
the opposite direction to the bee's flight. (d) The walls are stationary
but the stripes are of different widths. The bee flies centrally between
the walls in (a) and (d), but moves to equalise optical speed of the
walls in (b) and (c). Adapted from Srinivasan et al. (1991).

324

VISUAL PERCEPTION

Physiological mechanisms of ﬂight control The flight control mechanisms
that we have described so far have been inferred from behavioural
evidence. Neurophysiological methods have made it possible to identify
some of these mechanisms with specific networks of nerve cells. The
nervous systems of insects contains far fewer cells than those of
vertebrates, and these are arranged in a highly regular way that allows
particular cells to be identified in any individual animal. These
characteristics make it possible to investigate neural circuitry in more
detail than would be possible in vertebrates. The outputs of
photoreceptor cells in the housefly eye (see p. 11) are processed in a
series of three retinotopically arranged layers of neurons; the lamina,
medulla, and lobula. In the first two stages, cells respond in a
direction-selective fashion to local image motion within small receptive
fields. In the third, these signals are integrated to yield responses to
more complex patterns of image motion. A part of the lobula, the lobula
plate, contains approximately 60 "tangential" cells, which have large
dendritic fields receiving inputs from many local motion detectors
across wide areas of the visual field. These cells fall into several
groups, with different responses to image motion. "Figure detection"
(FD) cells respond to differences in the direction of image motion
between small regions of the retina and their surrounds, while
"horizontal" (HS) cells respond to uniform horizontal motion across
large areas of the retina. These two groups of neurons underlie the
"small-field" and "large-field" optomotor responses that we described
earlier (Egelhaaf et al., 1989; see p. 322). Another population of
lobula plate neurons, known as VS cells, respond to patterns of image
flow across large areas of the retina that would arise from pitching or
rolling rotations of the fly, or rotations around axes intermediate
between these (Krapp, Hengstenberg, & Hengstenberg, 1998). Each cell is
driven by a differently weighted combination of inputs from local motion
detectors in the previous stage of the visual pathway, which determines
its particular preferred axis of rotary image flow. Krapp et al. (1998)
also obtained evidence that one of the HS cells is

tuned in a similar way to translation of the fly in a particular
direction. Overall, the neural circuitry of the lobula plate appears to
be organised to separate local from wide-field image motion, and to
identify the axis of the rotation or translation that is causing
widefield motion (for further details see Egelhaaf et al., 2002). Lobula
plate neurons send outputs to motor control centres in the thoracic
ganglia, which control flight by causing deflection of the hindlegs and
abdomen, and differences in wingbeat amplitude (Zanker, Egelhaaf, &
Warzecha, 1991). At least in outline, therefore, the entire sequence of
neural computations involved in a fly's optomotor response is
understood, from photoreceptors to motor neurons. There is evidence that
the neural pathways controlling flight in birds share some basic
features with those of insects. In pigeons, two groups of neurons in
different parts of the vestibulocerebellum respond to patterns of
wide-field image motion arising either from rotation or translation of
the bird. In each group, individual cells respond best to image motion
corresponding to a particular axis of rotation or translation, and these
axes fall into the same three categories, aligned at right angles to one
another, in both groups (Wylie, Bischof, & Frost, 1998). It may be that
the common demands of fast visual control of flight have led to the
evolution of basically similar neural control systems in two very
different groups of animals.

Adaptive properties of insect ﬂight control The mechanisms of insect
flight control that we have described appear to be based on networks of
nerve cells that are "wired up" under genetic control during early
development and operate in a fixed way throughout an insect's life.
Although this is true of the basic organisation of these mechanisms
(Karmeier, Tabor, Egelhaaf, & Krapp, 2001), recent research has found a
surprising degree of flexibility in their operation, which adapts an
insect's behaviour to fluctuating environmental conditions. In this
section, we will look in detail at evidence for adaptive flexibility in
the optomotor response, and then mention briefly

11. OPTIC FLOW AND LOCOMOTION

several other ways that learning is known to affect insect flight
control. In an early model of the optomotor response, Von Holst (1954)
argued that it must involve a comparison between the "reafferent" visual
input caused by a movement of an insect and the input predicted from
that movement. For example, if an airborne insect initiates a turn for
some reason, the reafferent image flow that results will match that
predicted, and there will be no optomotor response. On the other hand,
if the insect is turned by an air current, the resulting image flow will
not be predicted and an optomotor response will be triggered. Note the
similarity between this model and the one we discussed in Chapter 8
(p. 259), which attempted to explain how the human nervous system
compensates for "reafferent" effects of eye movement on the retinal
image. Von Holst argued that, without a process of comparing predicted
and actual visual input, an insect would be unable to initiate a
movement and would be locked permanently into a fixed position or flight
path by its own optomotor response. This need not be the case, however.
As we have already seen (p. 322), the different time courses of turns in
flight to fixate targets and of the optomotor response can solve this
problem (Egelhaaf et al., 1989). The stabilising effect of the optomotor
response to global motion cannot develop fast enough to suppress a turn,
and in this situation at least there is no logical necessity to compare
the predicted and actual effects of movements on visual input. Reichardt
and Poggio (1976) obtained evidence that appeared to reject Von Holst's
(1954) model, from experiments in which they compared the optomotor
responses of houseflies under two conditions. In one, the normal effect
of the fly's flight behaviour on optical motion was mimicked by
measuring the torque exerted by the tethered fly, and then using these
measurements to control the rotation of the drum. This condition is
termed "closed-loop", as the link between behaviour and its visual
consequences is intact. In the other, "open-loop" condition, the fly is
also tethered, but its torque has no effect on the movement of the drum.

325

A passive "transduction" model predicts that the same visual stimulation
will yield the same yaw torque whether the fly is in closed- or openloop
conditions, whereas the efferent copy model predicts a difference
between the two situations. Reichardt and Poggio (1976) determined the
relationship between the position and motion of a target stripe and yaw
torque under open-loop conditions (see p. 319), and showed that it
accurately predicted behaviour under closed-loop conditions. These
results therefore appeared to show that, for this particular optomotor
response, the "transduction" model is adequate (see Figure 11.8).
However, more recent evidence has reopened the possibility that the
efference copy model may still be correct, in at least some cases.
Heisenberg and Wolf (1988) repeated Reichardt and Poggio's (1976)
experiments, using the fruitfly Drosophila and a more accurate technique
which allowed single yaw torque traces to be obtained, rather than
time-averaged traces. They found that, in closed-loop conditions, torque
followed visual motion closely, whereas in open-loop conditions the
torque response was weaker and showed much larger fluctuations. It
therefore appears that the optomotor response becomes less stable under
open-loop conditions. Heisenberg and Wolf (1988) interpreted this result
as showing that endogenous fluctuations in motor output cannot be
treated just as a source of noise added to yaw torque, as in the
transduction model (see Figure 11.8), but instead must interact with the
optomotor controller. In particular, Heisenberg and Wolf propose that
these fluctuations block the response of the controller to motion in the
opposite direction to the resulting turn. This is simply another way of
expressing the central idea of the efference copy model, that the insect
responds only to visual motion that is not predicted from its own
actions. It is not yet clear whether the discrepancy between the two
sets of results arises from differences between insect species, or
between measurement techniques, or from some other source. Some evidence
that Heisenberg and Wolf's (1988) results reflect a general mechanism
has been obtained by Möhl (1989), who recorded action

326

VISUAL PERCEPTION

potentials from the motor neurons driving the left and right wing
muscles of locusts during tethered flight. The difference in timing of
the bursts of potentials driving downstrokes of the wings was computed
in real time and the result was used to control the rotation of the
optomotor drum. This technique made it possible to set any arbitrary
interval between the motor commands to the two sides to be the "correct"
value that kept the drum stationary and so simulated stable flight. We
might expect that motor impulses to the right and left wing muscles
would be closely synchronised, so that the downstrokes of the wings
occur together and yaw torque is minimised. If this were true, then
setting any other value of the interval between bursts of impulses to be
"correct" would prevent the insect from stabilising the drum, and it
would continue to rotate indefinitely. Instead, Möhl (1989) obtained a
striking result. The interval between bursts of motor impulses to the
left and right sides gradually shifted to the new "correct" value and
the drum was stabilised. Each time the "correct" value was changed, the

locust responded in the same way. Apparently, a locust is able to learn
the correct timing of motor commands to minimise visual motion. Möhl
(1989) argues that this happens because the locust nervous system
actively generates fluctuations in motor output and then detects
correlations between output and the sensory feedback resulting from it.
Those changes in motor output that cause "desirable" feedback, such as
stabilisation of image motion, are maintained. In this way, the nervous
system can actively "search" for those parameters coupling sensory input
to motor output that achieve the correct result. It is easy to see why
this flexibility in flight control should exist. The aerodynamic
properties of a locust's wings and body are, to some extent,
unpredictable over its lifetime. If the wings grow in a slightly
asymmetric way, or are damaged by a predator, then the relative timing
of flight muscle activity on the two sides of the body needed to prevent
yawing will change, and the nervous system appears to be designed to
modify motor output so as to compensate. As Möhl states (1989, p. 81):

The "transduction" model of the optomotor response proposed by Reichardt
and Poggio (1976). The torque generated by the insect causes a rotation
of the visual surroundings f, determined by aerodynamic factors
summarised in the coefficient K. The difference between f and the
imposed rotation of the drum d is the image slip over the retina , which
is transduced into torque output by the optomotor controller.
Fluctuations in torque are assumed to arise from noise added to the
controller output.

11. OPTIC FLOW AND LOCOMOTION

Fluctuations in motor patterns are not just an imperfect result but a
clever strategy of evolution to explore permanently the exact function
of each muscle for flight stability. By means of this strategy the
system is able to adjust specifically the sensory input to different
motor neurones, thus increasing precision of the genetically
predetermined network of flight control. The models of insect flight
control proposed by Möhl (1989) and by Heisenberg and Wolf (1988) share
the same basic principle as Von Holst's (1954) efference copy model. In
all three, changes in motor output are produced endogenously and the
resulting change in visual input is compared in some way with an
expected change. The important development in the more recent models is
the new functional role they propose for the comparison of input and
efference copy. Rather than being a means of discriminating
selfgenerated and externally caused visual motion, this process has a
deeper role in adjusting the functioning of sensorimotor systems to keep
their performance optimal. The mechanisms that stabilise insect flight,
guide approach to surfaces and control landing, as we have described
them so far, are generalpurpose ones that will operate in any
environment containing solid objects and surfaces that an insect passes
through. Adaptive processes in the central nervous system not only
adjust the parameters of these mechanisms to changes in an insect's
aerodynamic abilities, but also enable it to recognise and orient
towards specific structures in its environment. These include familiar
food sources, such as nectar-bearing flowers, nest sites, and landmarks
associated with such goals. Many experiments with ants, wasps, and bees
have demonstrated remarkably sophisticated abilities to learn in this
way. Insects are able not only to learn to recognise particular visual
patterns defined by shape, size, and colour, but also to transfer this
learning to tasks where the same pattern must be recognised when
degraded or camouflaged, or defined by different visual features. They
can also learn to recognise sequences of objects along a route to a goal
and can use this knowledge in

327

navigation. Some recent introductions to these research topics are given
by Giurfa and Capaldi (1999), Judd and Collett (1998), and Srinivasan
and Zhang (1998). The results imply that, for some insect species at
least, there is considerable flexibility in the neural systems linking
visual input to the control of locomotion. The small numbers of neurons
possessed by insects do not seem to prevent them from carrying out
perceptual processing comparable to that in some vertebrates.

VISUAL CONTROL OF POSTURE AND LOCOMOTION For insects, visual control is
essential to keep a stable flight posture or to follow a particular path
in relation to the solid structures of the world around them. The same
will be true of other airborne animals such as birds, and also of
animals that swim. In order to navigate long distances through varying
winds, a bird must use visual references including landmarks, sun, and
stars, while a hovering hummingbird or kingfisher, or a trout holding
its position in a fast-moving stream, presumably use image flow to
correct the effects of air and water currents on their position. When we
turn to consider how legged, terrestrial animals such as humans maintain
a stable posture or keep a straight course on solid ground, it seems
that vision need not be so important. We possess a variety of receptors
sensitive to mechanical forces that can provide information about the
positions of parts of the body relative to one another and relative to
the force of gravity. These are the stretch receptors in the muscles and
tendons, which signal the angles around joints, and the receptors of the
vestibular system in the inner ear, which signal angular accelerations
of the head, and its orientation relative to gravity. The fact that we
can stand, walk, and run, or even ride a bicycle, with our eyes closed
suggests that these receptors provide ample information for the control
of posture and locomotion, and that vision is not as important as it is
for flying insects. Gibson was led to argue against this view, and in
favour of an important role for vision in

328

VISUAL PERCEPTION

human locomotion, by his concern to understand how pilots learn to
control aeroplanes. Like insects and birds, a pilot must rely on vision
to keep an aircraft stable and to control its course, despite the
effects of wind. A similar situation, familiar to more of us, is
steering a boat across moving water. Here again, we must use vision to
adjust the rudder or oars relative to surrounding objects and so
compensate for the effect of water currents. We are able to control
locomotion in these artificial situations where kinaesthetic information
from stretch receptors and the vestibular system is not useful, and this
suggests an underlying ability to use vision in similar ways to insects
and birds. Further support for this view comes from the phenomenon of
vection, the powerful impression of self-movement that can be produced
by visual stimulation alone. This is familiar in some everyday
situations, such as sitting on a stationary train as a neighbouring
train starts to move. Despite the lack of any change in mechanical
forces on the body, the visual motion causes a brief but strong
sensation of self-motion. Wide-screen cinemas rely on the same effect,
producing strong and sometimes disturbing sensations by simulating over
a wide region of the visual field the image motion caused by translation
and rotation of the body. An early forerunner of this technology was the
"haunted swing", a fairground amusement of a century ago, in which
customers sat in a stationary swing while the room rotated around them.
Only by closing their eyes could they escape the powerful and sometimes
nauseating sensation of being turned head over heels. Another piece of
evidence that was important in establishing the case that vision is
important for the control of human locomotion was obtained from
experiments using the visual cliff, first designed by E.J. Gibson
(Gibson & Walk, 1960). The apparatus is shown in Figure 11.9. It
consists of a raised platform that divides two checkerboard surfaces
(giving optical texture). One of these (the shallow side) is at a
similar level to the platform, while the other (the deep side) is
considerably lower. Both sides provide surfaces of support, however,
since the deep side is covered with a sheet of glass that is at the same
level as

that covering the shallow side. Thus both the shallow and deep sides
have a surface that could safely support an animal or child and could be
detected by touch. In contrast, the optical information given by the
deep side specifies a sharp drop. Gibson and Walk showed that when young
animals of many species, including chicks and lambs, were placed on the
central platform, they strongly avoided venturing onto the glass
covering the deep side. Other species, such as rats, showed no
preference between the two sides, indicating that they use tactile
information from whiskers and paws rather than vision to guide their
movement in this situation. Human infants aged 6--14 months would not
crawl or walk across the glass covering the deep side, even when
encouraged by their mothers who were standing on the other side of it.
These observations suggest that by the time they are mobile (immediately
for chicks and lambs, after 6 months or so for humans), young creatures
can make use of optical information specifying sudden drops in order to
remain on safe surfaces of support. The results are also of interest for
understanding how perception develops, as they suggest that some
appreciation of depth, or relative depth, may be inborn rather than
learned in the way the empiricist philosophers suggested (see Chapter
4). Bower (e.g., 1966, 1971) describes other observations of infants
that are relevant to this issue. Gibson's theoretical arguments,
together with early evidence such as vection and the behaviour of
children on a visual cliff, argue for a role for vision in the control
of human posture and locomotion alongside the role played by receptors
sensitive to mechanical forces on the body. In the remainder of this
chapter, we will describe recent research that analyses the role of
vision in more depth, beginning with the contribution of vision to
maintaining a stable standing posture.

Maintaining balance As adults (at least when sober) we take for granted
our ability to remain upright on two feet. As every parent knows,
however, the ability to stand and eventually to walk unsupported is an
achievement that is gradually mastered by the

11. OPTIC FLOW AND LOCOMOTION

329

The visual cliff (Gibson & Walk, 1960). (a) Side view. (b) Shows
textured surfaces beneath the glass.

infant with months of unsteadiness and falls on the way. The gymnast on
the narrow beam, the ballet dancer on points, or the circus artiste
standing on a cantering horse must all learn to maintain balance in new
and changing circumstances. Vision is obviously not essential for
balance, since we can stand upright with our eyes closed, but a simple
demonstration shows that it plays a role. Try standing on one leg with
eyes open and then with them closed. With eyes closed you will probably
sway and perhaps even fall over, despite the information still being
received from receptors in your feet, ankles, and vestibular system. The
importance of vision in maintaining balance was demonstrated more
formally by Lee and his colleagues (Lee & Aronson, 1974; Lee & Lishman,
1975; Lishman & Lee, 1973) in an experimental arrangement known as the
"swinging room". The room essentially consists of a bot-

tomless box suspended from the ceiling. The participant stands on the
floor, and the walls of the room can be moved backwards or forwards
around the participant, without his or her knowledge. The walls are
covered with wallpaper to provide a visual texture. When the room is
moved towards the participant, this produces the same expanding optic
flow pattern that would be produced if he or she were in fact swaying
towards the wall. If the room is moved away from the participant this
produces a contracting optical flow pattern as though he or she were
swaying away from the wall (see Figure 11.10). Just as an optomotor
experiment simulates the image flow produced when an insect turns, the
swinging room simulates that generated by a person swaying. Using this
apparatus, Lee and his colleagues conducted a number of experiments in
which they showed that vision could provide information

330

VISUAL PERCEPTION

An adult or child stands in the swinging room. (a) The room is moved
towards the participant, who sways or falls backwards. (b) The room is
moved away, and the participant sways or falls forwards. (c) The
expanding optic flow pattern that would result from movement of the room
towards the participant. (d) The contracting optic flow pattern that
would result from movement of the room away from the participant.

that could be used to control balance. In one experiment Lee and Aronson
(1974) placed toddlers (aged 13--16 months) within the swinging room.
After a period of acclimatisation in which the infant's normal stability
while standing could be assessed, they tested the infant's reactions to
movement of the room. When the room was moved towards the child, he or
she was observed to sway, stagger, or fall in a direction away from the
wall. This was not merely a defensive reaction to the "looming" pattern
because the child staggered or fell towards the wall when it was moved
away from them. Indeed some of the children became distressed during the
procedure and for them the experiment had to be prematurely terminated.
The responses shown by these children were entirely consistent with
those that would be expected if the child interpreted the optic flow
produced by movements of the room as resulting from its own postural
sway. An outward flow, obtained when the room is moved towards the
child, is consistent with sway towards the wall. The child then
compensates for its apparent sway by moving backwards, and vice versa.
It appears that in children acquiring the skill of balancing on two
feet, visual information can override the

veridical information about actual posture obtained from receptors
sensitive to mechanical forces (proprioceptors in the lower body, and
the vestibular system). Lee and Aronson suggest that vision "tunes up"
the sensitivity of these mechanical receptor systems, and point out that
vision is a better source of information for the child to rely on while
the feet and ankles are maturing and growing. Learning to stand upright
is such an achievement for the child that we tend to forget earlier
achievements, such as sitting upright. Vision seems to play a
fundamental role in maintaining posture whether standing or sitting.
Butterworth and Hicks (1977) showed that infants who have just learned
to sit without being supported will sway with movements of the swinging
room, and Butterworth (1983) showed that even 2-montholds will move
their heads with the room. Even for adults it appears that visual
information may override that obtained from mechanical receptor systems.
Lee and Lishman (1975) were able to affect body sway and stability in
adults by small movements of the swinging room. Participants' body sway
was measured accurately when they stood with eyes open or eyes closed,
or within the swinging room. Their body sway with

11. OPTIC FLOW AND LOCOMOTION

eyes open could be "driven" by movements of the swinging room. Thus if
the room was moved backwards and forwards in a regular, sinusoidal
manner, the body was also seen to sway sinusoidally, linked to the
movement of the walls. Participants were often unaware of the movement
of the room, or of their own sway in response to it. Lee and Lishman
(1975) also compared participants standing normally, on a sloping ramp,
on a pile of foam pads (a "compliant" surface), or on their toes.
Although visual driving of sway was observed in all four conditions, it
was greatest for participants standing on the compliant surface, where
the information from the foot and ankle receptors was the least
reliable. In a further experiment, Lee and Lishman had subjects adopt
novel balancing postures such as the "Chaplin" stance (feet aligned at
180°) or the "pinstripe" stance (one foot angled behind the calf of the
other leg while holding a weight in the hand opposite to the supporting
leg). In such circumstances the adults, like the children in Lee and
Aronson's study, could be made to stagger and fall by movements of the
swinging room. They described the participant as like "a visual puppet;
his balance can be manipulated by simply moving his surroundings without
his being aware of it" (Lee & Lishman, 1975, p. 94). Wann, Mon-Williams,
and Rushton (1998) have made more systematic comparisons of the
influence of optic flow on standing posture at different ages. They used
response gain (the ratio of the amplitude of body sway to amplitude of
room movement) as a measure of the strength of coupling between visual
input and postural correction. Response gain was considerably higher in
children aged 3--4 years than in adults, extending Lee and Aronson's
(1974) findings to children with several years of walking experience,
and still slightly higher in 10--12-year-olds than in adults. We can
conclude from these experiments that infants and toddlers learning to
control their balance are strongly influenced by optic flow, which is
used to tune other control systems relying on mechanical information
from joints, muscles, and the vestibular system. While the body is
growing rapidly during childhood, this tuning role of vision appears to
continue. When growth ceases,

331

adults are able to maintain balance using mechanical information alone,
except in novel or demanding conditions, when the greater dependence on
optic flow characteristic of childhood reemerges. The developmental
interplay between mechanical and visual systems can be disrupted when
people suffer from sensory impairments. If vision is absent altogether,
mechanical systems cannot be tuned as accurately, and this may explain
why congenitally blind children are slower than sighted ones in learning
to stand and to walk, and why blind adults show more body sway than
sighted adults, even when sighted people close their eyes (Edwards,
1946). Conversely, if disease affects the vestibular system of the inner
ear later in life, vision can regain its earlier role in controlling
posture. People suffering from such disorders often complain of unusual
sensitivity to movement in their surroundings, which may cause dizziness
and loss of balance, and Redfern and Furman (1994) found that such
patients also sway more strongly in the swinging room than do controls.
The control system that integrates mechanical and visual information for
the control of posture therefore shows long-term adaptation to changes
in the properties of these inputs caused by growth of the body or by
sensory loss. There is also evidence that it can change its
characteristics over much shorter time periods, to adapt to changes in
the surrounding environment. Dijkstra, Schöner, and Gielen (1994) tested
a simple model of postural control that assumes independent "intrinsic"
and "visual" systems interacting in a fixed, additive manner. The
intrinsic system represents the network of muscles and mechanical
receptors involved in postural control, and its operation is assumed to
cause a certain amount of sway. The visual system generates a control
signal proportional to the amplitude of this sway, and this signal in
turn causes a corrective response that subtracts from the sway and
increases postural stability. In their experiments, Dijkstra et al. used
a "virtual" swinging room, in which participants stood in front of a
large screen on which a pattern of dots could be projected. The display
could be controlled on-line by a

332

VISUAL PERCEPTION

signal measuring the participant's body sway, and so could simulate the
optic flow caused by this sway for a real wall at any specified
distance. The effects of superimposing an oscillation on the display
could also be examined. Dijkstra et al. (1994) obtained two results that
were inconsistent with the simple model of postural control. First, the
gain of the visually driven sway decreased as the simulated distance of
the "wall" increased, but not as steeply as would be expected if the
coupling between the visual signal and body sway was constant. This
result implies that the sensitivity of the control system to retinal
flow increases the further away the surface that gives rise to the flow.
The significance of this adaptive property of the system may be in
maintaining optimal sensitivity to retinal flow as we move between
cluttered areas with nearby objects and more open areas. Second, the
amplitude of visually driven sway did not peak at a particular frequency
of display oscillation, as would be predicted if the "intrinsic" system
had a fixed resonant frequency. This result implies that the intrinsic
system does not have a constant resonant frequency independent of visual
input, but instead changes it properties in response to changes in the
frequency characteristics of visual input. This may be important as a
means of adjusting intrinsic mechanical control of posture to changes in
the properties of the surface on which a person is standing. Maintaining
optimal control of posture while standing on different surfaces such as
solid ground, a tree limb, or a trampoline may benefit from changes in
the intrinsic control system driven by characteristics of visual
feedback. Further details of these experiments and their theoretical
implications are given by Schöner, Dijkstra, and Jeka (1998), and there
is an important parallel between them and the work of Möhl (1989) on
insect flight control that we described earlier (p. 326). In both cases,
there is evidence that changes in the sensory feedback arising from
locomotion can be used to track changes in the properties of either the
environment or of the body, and so to keep the properties of visuomotor
control systems optimally adjusted. In both these contexts, there appear
to be dynamic, adaptive

processes operating that so far are poorly understood.

Control of walking and running gait An animal or person walks or runs
with a smooth and cyclically regular sequence of limb movements
(Bernstein, 1967). It propels itself forwards by applying force
backwards against the ground as each foot strikes it. When walking, one
or more feet (depending on how many the walker possesses) remain in
contact with the ground all the time; when running, the animal or human
progresses by a series of leaps. The length of each leap ("flight") is
determined by the speed at which the animal is travelling and the
vertical thrust applied at each stride. Figure 11.11 illustrates this
further for those familiar with vectors. For both walking and running it
is important that the thrust being applied to the foot on contact is
coordinated with the swing-through time of the foot that will next
contact the ground, so that this meets the ground in the right way for
maximum thrust. For example, if the foot is travelling forward relative
to the ground when it contacts it, this will have a braking effect. Thus
trained human runners try to lift their knees high so that they can
thrust down hard and backwards relative to the ground as each foot
strikes it just in front of the hips. The thrust applied at each stride
must then give the runner sufficient vertical lift to ensure a long
enough flight time so that the next foot can be swung through to its
optimum strike position, and so on (Lee, Lishman, & Thomson, 1982).
Provided the ground surface is smooth and unobstructed, we can walk or
run with our eyes closed, and so the basic organisation of gait does not
require vision. However, vision plays many roles in the fine adjustment
of gait in relation to the surroundings, in addition to its obvious ones
in avoiding collisions, steep drops, and other hazards. One such role is
to maintain upright posture. The centre of gravity of the body moves
continuously during walking and running, and its movement must be
controlled to avoid unnecessary mechanical stresses on the limbs and
joints, or toppling over. Bardy, Warren, and Kay (1996) used a variant
of the swinging room technique to

11. OPTIC FLOW AND LOCOMOTION

333

The distance travelled in each stride when running depends on the
vertical thrust (V) and a force reflected in horizontal velocity (H). At
(a), both V and H are large, and the flight is long. At (b) the vertical
thrust is reduced, and at (c) the horizontal velocity is reduced. In
both cases, flight length is reduced. The flight path is not the
resultant (R) of vectors V and H, because other forces are operating.

investigate the visual control of posture during walking. Their
participants walked on a treadmill while oscillating optic flow patterns
were presented on surrounding walls. As they walked, they swayed along
the same horizontal axis as that of the optic flow. Further experiments
demonstrated that this sway could be driven either by smoothly expanding
or contracting flow (as produced by movement of a swinging room) or by
motion parallax between surfaces at different distances from the
observer. Another way in which vision contributes to the control of gait
is to regulate the forces applied to the ground on each stride. This may
be necessary to modify force according to the type of surface
encountered. If the ground surface is slippery then a large force will
produce skidding rather than the desired propulsion forwards. If the
surface is compliant rather than firm then much of the force will be
absorbed by the surface rather than moving the animal (try running fast
on soft sand), and so more force must be applied in order to maintain
speed. At other times, force must be adjusted so as to control stride
length, either to bring the leading foot on to a desired point on the
ground (such as a step) or to keep it away from a hazard (such as a
pothole or dog droppings).

Despite these demands, humans and other animals can normally maintain
fairly smooth progress through an environment of variable surfaces
provided they can see properly. At night it can be an uncomfortably
jarring experience to run across hilly or uneven ground, demonstrating
how vision is needed to make fine adjustments to planned foot positions,
postures, and forces (Lee, 1980a). Experiments on the visual control of
stride length have demonstrated some striking properties of the system
involved. One is the speed with which complex adjustments can be made to
alter foot placement. Patla, Prentice, Rietdyk, Allard, and Martin
(1999) used on-line recordings of participants' foot placements as they
walked at a normal, steady pace to compute in real time the location of
the next placement, and then to project a patch of light on to the
ground near it. Participants were asked to avoid stepping on the patch
of light, and the experiment was designed to ensure that they could not
predict when and where it would appear. In one series of experiments,
the patch was about half a foot-length square and was projected either
where the front or the back half of the foot was about to strike the
ground. Without slowing their normal walking pace, participants were
able to adjust the length

334

VISUAL PERCEPTION

of the stride in the most efficient way (shortening or lengthening it,
respectively) as long as the light appeared no less than 400--500 ms
before the foot was due to strike the ground. When the patch was made
larger, to cover the whole area of a footfall, participants narrowed
their stride to avoid it. These remarkable results show that, just as we
begin to make a stride, we are able to detect where an obstacle lies on
the ground in relation to the anticipated position of our foot when the
stride ends, and to use this information very rapidly to choose the most
biomechanically efficient adjustment to stride length and direction.
When we have more advance warning of a ground feature that we must
either step on or keep off, we are able to avoid making the fast and
large changes in gait required by Patla et al.'s (1999) procedure, by
"spreading" the required adjustment over several strides. This was first
demonstrated by Lee, Lishman, and Thomson (1982) in the context of a
highly skilled athletic task, that of long-jumping. A long-jumper needs
to maximise his or her horizontal velocity and vertical thrust at the
point of take-off (refer back to Figure 11.11). Since the jump is
measured to the take-off board, and is disqualified if the athlete steps
over it, he or she must try to reach maximum speed as the launching foot
strikes the board, with the body postured appropriately to give maximum
lift. Long-jumping therefore places strict demands on the precision with
which the launching foot strikes the board, in a situation where run-up
speed must be kept as high as possible. Lee et al. (1982) filmed the
training sessions of three female athletes who ranged from club to
Olympic standard, and measured the positions of each foot placement
during run-ups to the jumping board. Their analysis revealed that the
run-up consisted of two distinct phases. Until the athletes were a few
strides from the board, their stride patterns were remarkably constant.
This consistency broke down over the last few strides however. It
appeared that the cumulative effect of small inconsistencies during the
first phase meant that the athlete had to adjust her final few strides
in order to strike the board accurately. There was a dramatic increase
in the variability of stride

lengths for the last three strides, while the standard error (a measure
of variability) of the footfall positions decreased dramatically over
these same few strides, to reach 8 cm at the board for the Olympic
athlete. The skills of long-jumpers represent extreme examples of
precisely timed locomotion, but there is evidence that we all possess
the ability to control gait "prospectively" to make contact with a
target several strides ahead. Even those of us of meagre athletic
ability are able to run to catch a bus, jumping over puddles and
negotiating kerbs and other minor hurdles on the way, and it appears
that the control systems responsible can be adapted by experts to
particularly demanding tasks such as long-jumping. One piece of evidence
for this view is that novice long-jumpers home in on the board by
adjusting their strides in just the same way as experts (Berg, Wade, &
Greer, 1994). Another was obtained by Laurent and Thomson (1988), who
found that the same control strategy is used during ordinary walking as
in long-jumping, despite the large difference in speed in the two cases.
They asked participants to walk towards a target and place either their
left or their right foot on it, and found that they achieved this by
adjusting stride lengths smoothly over the last three strides. Laurent
and Thomson (1988) also used a "visual denial" method to determine
whether there is a specific point in the step cycle when people use
visual information to modulate stride length. As participants walked,
the room lights were switched on and off in synchrony with their gait,
so that they had only brief glimpses of the target at a particular point
in each step cycle. Surprisingly, participants showed no loss of
accuracy in placing a foot on the target when they were only able to see
for 300 ms during each step cycle. These brief glimpses clearly provided
enough visual information to carry out the task normally. However, there
was a subtle difference between conditions where the flash of light
occurred while the foot aimed at the target was on the ground, and where
it occurred while the foot was in midswing. Although the foot was placed
accurately in both cases, the control of the last three strides was
considerably less smooth in the second case, and

11. OPTIC FLOW AND LOCOMOTION

was achieved in a series of irregular, variable stride lengths. Laurent
and Thomson's (1988) results suggest that, while a foot is on the
ground, there is a "window" in which information needed to modulate the
next stride with that foot is acquired, before the current stride (with
the other foot) is completed. Hollands, Marple-Horvat, Henkes, and Rowan
(1995) have obtained more evidence in support of this hypothesis. They
recorded participants' eye movements as they walked from one irregularly
spaced "stepping stone" to another, and found a rhythm of saccadic eye
movements tightly linked to the stepping rhythm. While one foot is on
the ground, a saccade takes place towards its next target (not the
target the other foot is currently swinging towards), and is completed
before the foot leaves the ground or soon after. It seems that fixation
of the target of each step, while the foot concerned is still on the
ground, is important in obtaining whatever visual information is needed
to adjust the length of the step. Surprisingly, it appears that fixation
of the next stepping target is not important simply to obtain a foveal
image of it. In a visual denial procedure, with the stepping targets
only visible intermittently, participants still made accurate saccades
just before they stepped, even to a target that was invisible just
before it was fixated (Hollands & Marple-Horvat, 1996). This implies
that head and eye movements involved in shifting gaze to fixate targets
play some role in the control of stride length.

Control of speed By varying the force we exert on each step, we can
control not only the length and direction of individual strides, but
also our walking or running speed over many strides. As in the control
of posture, both visual and mechanical information are potentially
available for this purpose, and in this section we will consider the
role of vision, and particularly of optic flow. Just as for flying
insects (p. 322), the velocity of optic flow can provide a person with
information about their speed, provided the distances of surrounding
surfaces are known. We might therefore expect optic flow arising from a
textured ground surface to be useful in

335

this respect, as the ground lies at a known distance from a person's
eye. Konczak (1994) explored the contribution of optic flow to the
control of walking speed, using a "swinging tunnel". As participants
walked through the tunnel, its walls were moved and any effects on
walking speed were measured. As predicted, there was a tendency for
participants' speed to increase if the walls of the tunnel moved with
the direction of walking (therefore slowing optic flow), although the
effect was small, and inconsistent from one person to another. Movement
of the walls in the opposite direction had no effect on walking speed,
and so overall there was no strong evidence that people adjust their
walking speed by regulating optic flow, at least that arising from
nearby vertical surfaces. The implication is that we largely ignore
changes in velocity optic flow as we walk, provided that the signals
from receptors sensitive to mechanical forces on our bodies remain
unchanged. However, there is evidence to suggest that optic flow plays a
more subtle role in the control of walking speed. It seems that the
central nervous system does not simply use mechanical and visual
information independently for the control of speed (giving more weight
to the former), but tracks changes in the correlations between them.
This can be demonstrated by artificially changing these correlations,
for example by arranging for the same cycle of forces exerted by the
walking muscles to yield either faster or slower optic flow than normal.
The first happens on the moving walkways used in airports, and the
second is caused by running or walking on a treadmill, to the point of
reducing optic flow to zero. Both of these can produce short-lived
sensations of moving more slowly or more quickly than we expect when we
return to walking on a normal surface. Rieser, Pick, Ashmead, and Garing
(1995) obtained evidence that such aftereffects actually influence
people's walking behaviour. In their experiments, participants first
spent a period of time walking on a treadmill, which was mounted on a
trailer and could be towed at a particular speed. Rieser et al. could
therefore manipulate independently the forward speeds of the walker
specified by mechanical and by visual feedback,

336

VISUAL PERCEPTION

by varying the speeds of the treadmill and the trailer respectively.
After walking for a time at a mechanically specified speed either 3 kph
faster or 10 kph slower than their visually specified speed,
participants' ability to judge distance was tested. In this test,
participants were shown a target 16 m away and were then asked to walk
to it blindfolded and to stop as close to it as they could. People
normally perform this task accurately, showing that they are able to
relate a visually specified distance to the distance they have covered
over the ground, when this is specified by mechanical feedback alone
(Thomson, 1983). Rieser et al. (1995) found a difference in behaviour in
the test between the two groups. People who had previously experienced
mechanical feedback specifying faster walking than visual feedback
overshot the target by 1 to 1.5 m, while the other group undershot it,
by a smaller distance (0.5 to 1 m). This result would be expected if the
first group had learned to expect a lower gain, of distance covered
relative to mechanical effort, than the second group. These results
suggest that the visual and mechanical signals generated by walking gait
are continually calibrated against one another, so that we learn to
expect a particular speed of optic flow given a particular pattern of
feedback from mechanical receptors. When the relationships between them
suddenly change---as when we step off a treadmill---we misjudge our
walking speed until recalibration takes place. Even in natural
circumstances, correlations between optic flow and mechanical feedback
will vary somewhat, as the slope or the compliance of the ground surface
changes, as the distances of surrounding surfaces change, and as the
physiological state of the body changes with fatigue, growth, or ageing.
For reasons such as these, the calibration of different signals for the
control of walking speed must be continually adjusted. In similar
experiments on treadmill walking, Pelah and Barlow (1996) found that
effects on judgement of walking speed were specific to particular visual
contexts, suggesting that in natural circumstances many different
calibrations are learned, tied to particular situations. When we travel
in a vehicle, the mechanical feedback generated by walking gait is
entirely

absent and we must rely on other sources of information to regulate our
speed. For pilots, this is a specialised skill that relies on the use of
instruments, but many of us drive cars and have some ability to judge
our road speed even without a speedometer. Optic flow arising from the
road and other surfaces obviously provides one source of information
about speed, and vibration of the car and wind noise may be at least
partly reliable sources as well. Research on drivers' judgements of
their road speed has demonstrated adaptation effects similar to those
that we have described when people walk. In particular, speed is
perceived in relation to a driver's recent average speed, and so
judgements are biased after a large change in speed. A driver who has
been travelling for some time at a high speed and then drops to a lower
one will tend to underestimate his or her speed, and therefore drive
faster then he or she believes, while a change in the opposite direction
causes overestimation and slower driving. These effects of adaptation on
drivers' speed judgements have been demonstrated both in reallife
driving (e.g., Schmidt & Tiffin, 1969) and in driving simulators. Here,
a person sits in a stationary car and uses the steering wheel and pedals
to control the motion of a wide-screen display simulating the road ahead
(e.g., Denton, 1976). It is easy to see the potential dangers of
adaptation to driving speed for road safety. As a driver accelerates to
join a high-speed motorway (freeway) from a congested road, they will
overestimate their new speed and drive more slowly than they intend at
first. A more dangerous situation arises when a driver leaves a
high-speed road. As they decelerate on a slip road (off-ramp) they will
underestimate their speed and may not decelerate sufficiently to stop to
give way to other vehicles. When high-speed roads were constructed in
the United Kingdom in the 1960s, there were frequently accidents where
they ended at roundabouts (traffic circles). To counteract effects of
adaptation on speed judgement, lines were painted across roadways at
approaches to roundabouts, at steadily decreasing intervals. These
provided conspicuous visual information to drivers, indicating that
their speed was not decreasing quickly enough, and had some success

11. OPTIC FLOW AND LOCOMOTION

in reducing the number of accidents (see Denton, 1976). Fog is another
factor that can bias drivers' judgements of speed in a dangerous way.
Snowden, Stimpson, and Ruddle (1998) tested participants' judgements of
speed in a driving simulator, and found an overall tendency to
underestimate speed slightly. They then went on to show that this error
increased to an underestimate of about 10 mph (16 kph) when the contrast
of the display was reduced to a level simulating the effects of dense
fog. These results have important practical implications, suggesting
that the dangerous tendency of motorists to drive too fast in fog may
not arise from deliberate recklessness but from an inherent
characteristic of visual motion processing that causes a bias in motion
perception under low contrast. These effects will be exacerbated by the
need to pay close attention to the road when driving in fog, making it
dangerous to shift gaze to the speedometer.

Controlling direction In order to reach particular goals or avoid
obstacles as we move about, we must be able to control the direction in
which we are moving. When we are walking, we do this by adjusting our
posture during the stance phase, when both feet are on the ground
(Patla, Prentice, Robinson, & Neufeld, 1991). When driving, we achieve
the same thing by turning a steering wheel. Explanations of how we use
vision to make these adjustments have been strongly influenced by
Gibson's insight that the centre of expansion of a linear optic flow
field specifies an observer's heading. This is the direction of a
person's motion relative to some earth-fixed frame of reference, and so
can equally well be defined as the point in the environment towards
which the observer is moving. Heading remains constant while movement is
in a straight line, but continually changes along a curved path. Note
that heading is not defined in an egocentric frame of reference (e.g.,
relative to the eyes, head, or body). Your heading would be the same
whether you approached a goal by walking forwards or backwards, and
whether your gaze was directed to the goal or away from it. We begin our
discussion of the control of dir-

337

ection with the "optic flow" hypothesis derived from Gibson's analysis,
which can be stated as follows: To control our direction, we first
locate the centre of expansion of the linear optic flow field generated
by our movement. We now have our heading (or instantaneous heading if we
are not following a straight path). We then adjust our limb movements so
as to bring our heading into alignment with a goal, or away from an
obstacle that we need to avoid. This hypothesis seems attractive because
of its robustness. Provided that optic flow arises from motion through a
rigid environment, the centre of expansion will always specify heading,
whatever the layout of the surroundings, and so apparently no other
visual processing is needed for the control of direction. A large body
of research has assumed that the "optic flow" hypothesis is correct, and
has concentrated on explaining how heading is obtained from the
projection of the optic flow field on to a retinal flow field. As we
have seen (p. 316, Figure 11.2), this will be made up of two components;
linear flow, which contains a centre of expansion specifying heading,
and rotary flow, generated by eye and head movements and by curvature of
the path of travel. When these two components are added together, there
is no simple relationship between heading and a centre of expansion of
retinal flow. The retinal flow field may contain no centre of expansion,
and if it does, it may be some distance away from the direction of
heading (see Figure 11.12). One possible solution to this problem is to
add a preliminary step in which retinal flow is decomposed into linear
and rotary components, so that heading can be obtained from the linear
component. There are two possible ways in which this could be done. One
is to use an "extraretinal eye position signal" to specify the direction
and speed of the eye movement causing the rotary component. As discussed
in Chapter 8 (p. 261), this could either be reafferent information from
oculomotor commands, or signals from stretch receptors in the eye
muscles. On the other hand, because rotary flow does not produce motion
parallax, it is possible in principle to analyse retinal flow into
linear and rotary components

338

VISUAL PERCEPTION

Three examples of retinal flow fields produced during forward movement
over a ground plane. The lines attached to each dot show the speed and
direction of optic motion at that point in the flow field. In each
case, + denotes heading and × the direction of gaze. (a) No eye
movement; there is a centre of expansion that coincides with heading.
(b) Eye movement to fixate a point on the ground while moving forwards;
there is a centre of expansion, but it does not coincide with heading.
(c) Eye movement to track a moving object; there is no centre of
expansion. Reprinted with permission from Lappe and Rauschecker (1994),
Nature Vol. 369, pp. 712--713. Copyright © 1994 Macmillan Magazines
Limited.

without any extraretinal information about eye movement (Longuet-Higgins
& Prazdny, 1980). Much effort has been devoted to finding out whether or
not people use extraretinal information in detecting heading. The
experiments concerned have used patterns of moving dots on a monitor
screen which simulate the optic flow caused by movement with a
particular heading through various kinds of environment. People watching
such displays are able to estimate the simulated direction of heading
with an accuracy

of 1--2°. Warren and Hannon (1988) used this method to determine whether
extraretinal information is needed to detect heading, by comparing
heading judgements in two conditions. In the first, participants watched
a display similar to Figure 11.12a that simulated forward movement over
a ground surface, while making a smooth eye movement to track a moving
fixation point. In the second condition, participants fixated a
stationary point but the display now contained an added rotary component
simulating the effect of the eye

11. OPTIC FLOW AND LOCOMOTION

movement in the first condition; this display resembled Figure 11.12b.
The retinal flow was therefore identical in the two cases, but
extraretinal information to resolve rotary flow was only available in
the first condition. Warren and Hannon found that judgements of heading
were equally accurate in the two cases, implying that extraretinal
information is not required. Further experiments of this kind have
sometimes obtained different results, finding poorer heading estimation
with simulated than with real eye movements (see Royden, Crowell, &
Banks, 1994). It seems likely that heading can be obtained in more than
one way, and that extraretinal information is used in some circumstances
but not in others. The speed of eye rotation is an important factor in
this respect; extraretinal information is required when actual or
simulated rotation is greater than about 1° s−1, which corresponds
roughly to the rotation caused by fixating an object 15 m away, 10° to
one side of heading, while walking. There is also evidence that
experience in performing heading judgement tasks may be another
important factor. A review of these and other issues arising from
experiments on judgement of heading can be found in Banks, Ehrlich,
Backus, and Crowell (1996). Neither of these proposed mechanisms for
decomposition of retinal flow makes any assumption about the nature of
the shifts of gaze that give rise to the rotary component, and can work
whatever form these take. A different approach to the problem has been
to consider whether constraints on gaze patterns, and therefore on the
rotary component of retinal flow, would allow heading to be obtained
more directly, without achieving a complete decomposition. For example,
if gaze is fixed on a point in the environment as a person walks or
drives a vehicle then the eyes will turn to maintain fixation. Assuming
that rotary flow arises only from eye movements of this particular kind,
it can be shown that heading can be obtained directly from retinal flow
(Glennerster, Hansard, & Fitzgibbon, 2001; Perrone & Stone, 1994). At
this point, we will take a step back from the problem of how retinal
flow is decomposed into linear and rotary components, and assume for the

339

sake of argument that heading can be recovered from optic flow in some
way. This would seem to imply that no other visual information is needed
in order to detect heading. However, there is evidence to suggest that,
when other information is provided in heading detection tasks, it is
used. For example, heading judgements are more accurate if binocular
disparity or texture gradients are added to provide independent evidence
about the depth relations of parts of the scene (van den Berg & Brenner,
1994). Knowledge of the ways that familiar objects change their apparent
shape with changes in perspective also contributes to heading
judgements. Beusmans (1998) set observers a task in which they had to
decide whether their simulated path through a display passed to the left
or the right of an object such as a cube. The display could be
programmed so that the centre of expansion of optic flow and the
perspective transformation of the object specified different headings
(which would be impossible in a natural situation). When this was done,
observers' judgements were more strongly influenced by the perspective
transformation of the object than by optic flow. These results indicate
that detection of heading does not arise from a single process analysing
image flow, but from interactions between a number of processes
recovering depth from a dynamic image in different ways. These include
processes driven in a top-down way by knowledge of the solid structure
of particular objects and scenes. Although linear optic flow fields
offer, in principle, a single, robust way of detecting our heading, it
seems that we do not take advantage of the apparent economy that they
provide. As we suggested at the beginning of this section, research has
often followed the "optic flow" hypothesis in assuming that heading must
be relevant to the problem of controlling direction, and that the
results of psychophysical experiments on heading judgement can inform us
about the control of walking, running, or driving. Next, we consider
challenges to these assumptions. Imagine that you wanted to walk over an
open area to a goal; for example, across a field to a gate. Could you
achieve this simply by fixating the gate and walking while keeping your
gaze on it?

340

VISUAL PERCEPTION

Everyday experience suggests that you could, and therefore implies that
you would have no need to obtain your heading from optic flow at any
stage. Throughout your walk, the centre of expansion of linear optic
flow, and therefore your heading, would both remain aligned with the
gate, but this would be a secondary effect. The system controlling your
movement would use the egocentric position of the gate relative to the
axes of your eyes as input, and not your heading. A Gibsonian analysis
would immediately make two objections to this argument. First, it
assumes that your direction of movement relative to the gate is the same
as your movement relative to the ground. This would normally be true
when someone walks on a solid surface, but not in cases such as flying
an aeroplane or rowing a boat, when air and water currents influence the
direction of motion. Except in still conditions, using the same method
to row a boat towards a harbour would be a mistake. Controlling the
egocentric direction of a goal therefore cannot be a single,
general-purpose means of controlling direction in all circumstances.
Nevertheless, it could still be useful when we walk on solid ground.
Second, the argument seems to assume that gaze is always aligned with
our direction of walking or other movement. This is obviously not true;
as we walk, our eyes and head move continually, and our direction of
gaze only corresponds intermittently to the direction in which we are
moving. However, the same is not true of the

An eye moves straight towards a target T. In the normal case (left), the
centre of expansion of optic flow is aligned with the target. Walking
directions predicted by the "optic flow" and "egocentric direction"
hypotheses (centre and right, respectively) if a prism is placed in
front of the eye. f = fovea.

anterior--posterior axis of the body, which is always aligned with our
direction of motion unless we walk crab-wise or in some other very
unusual way. We could therefore cross a field and reach a gate by
keeping our body axis aligned with it while walking normally, provided
that from time to time we aligned our eyes and head with our body axis,
so that any deviation in the egocentric direction of the gate could be
detected and corrected. A simple test of the "egocentric direction"
hypothesis was carried out by Rushton, Harris, Lloyd, and Wann (1998),
who observed the paths taken by participants walking for about 10 m over
level ground towards a target, while wearing prisms. Since prisms will
displace the images of both the target and the centre of expansion of
optic flow by the same distance over the retina, the "optic flow"
hypothesis predicts that they will have no effect (see Figure 11.13).
Participants would be expected to walk a straight course to the target
(apart from a small initial error before optic flow can be obtained).
Rushton et al. did not find this. Instead, participants walked to the
target at a normal steady pace, but in a curved path along which their
instantaneous heading deviated from the direction of the target by
approximately the angle of the prisms they were wearing. This is exactly
what the "egocentric direction" hypothesis would predict if gaze were
aligned with the target and then the body axis with the direction of
gaze (Figure 11.13).

11. OPTIC FLOW AND LOCOMOTION

Rushton et al. also reported some intriguing effects that further
supported this hypothesis when they asked participants to try to locate
the centre of expansion of optic flow as they walked and keep it aligned
with the target. The participants walked with an unusual gait, twisting
at the waist, and reported that their feet "seemed to be trying to do
something different". Rushton et al.'s (1998) results demonstrate that
the direction of walking can be controlled by egocentric direction
alone, and does not necessarily require the recovery of heading from
optic flow. Even so, it could be argued that optic flow is used
additionally in some circumstances, for example in more richly
structured surroundings than the open areas used in Rushton et al.'s
experiments. Harris and Bonas (2002) obtained evidence to the contrary,
however, showing that prisms caused the same deviation in walking paths
towards a luminous target whether participants walked in lit
surroundings or in darkness. On the other hand, there is some evidence
that the restricted field of view caused by wearing prisms may prevent
the use of optic flow to control walking direction (Harris & Carre,
2001). Egocentric direction may also be important in the control of
direction when people steer cars along roadways. In this situation, the
hypothesis that a driver steers safely by keeping the centre of
expansion of optic flow aligned with the centre of the lane in which he
or she is travelling faces a number of problems (Beusmans, 1998). During
driving, the centre of expansion will often be obscured by the car in
front. Even if it is visible, the method will not be sufficiently
accurate, as a 2° error in judging heading---common in psychophysical
tasks---would cause a car travelling at 90 kph to leave the road within
a second. Beall and Loomis (1996) propose that instead drivers make use
of constraints specific to the steering task, including their
characteristic eye height above the road and the availability of kerb
and lane markings. The "splay angles" of lane boundaries relative to the
horizontal---a form of egocentric direction---will change as the car
moves from side to side in its lane, and these changes could be used to
detect and correct course deviations. Beall and Loomis (1996) tested
possible ways

341

in which drivers might steer accurately by using a driving simulator in
which "preview distance" could be varied. Participants could see only
part of the road ahead, and their steering accuracy was measured as the
distance in front of the car of this section of road was varied. When
the display simulated movement over a ground surface without lane
markings, steering accuracy decreased with increasing preview distance.
When lane markings were added to the display, this decrease in accuracy
was not found. The implication of these results is that drivers do use
the angular positions of lane boundaries to control steering, and to
achieve the accuracy required for safe driving that cannot be obtained
from optic flow alone. Research on the control of direction as we walk,
drive, or move about in other ways has been dominated until recently by
the "optic flow" theory and by psychophysical methods for measuring
judgements of heading in displays. While such results may be relevant to
understanding the control of natural locomotion, we cannot necessarily
assume that they are. As Wann and Land (2000, p. 324) put it: There is a
danger that heading may be a post-hoc percept; one that can be recovered
by observers if they are required to do so, but that is not actually
used in active naturalistic tasks. Wann and Land (2000) provide a
theoretical analysis of the conditions in which the egocentric direction
of a visual goal, and changes in it available directly from retinal
flow, can be used to control direction in natural locomotion tasks. One
important implication of this approach is that shifts of gaze are no
longer seen as a nuisance that imposes an added processing burden of
filtering out rotary retinal flow, but as a critical link in the control
of body orientation. This in turn implies that when we walk the control
of heading cannot be continuous, since our gaze is only intermittently
aligned with heading, and we presumably make small corrections to
walking direction each time that they are aligned. There is evidence
that gaze also plays a critical part in the adjustments

342

VISUAL PERCEPTION

to direction that drivers make as they negotiate bends in roads. Land
and Lee (1994) recorded the direction of gaze and the angle of the
steering wheel while people drove a real car along a real road. During
straight stretches of road, drivers' gaze shifted about erratically to
fixate objects of interest around them, but showed a strikingly regular
pattern each time a curve in the road was approached. Just before each
turn of the steering wheel began, the gaze of the drivers "locked on" to
the inside edge, or "tangent point", of the approaching curve in the
road (see Figure 11.14). The participants were surprised by these
results,

Distribution of drivers' fixation points relative to the road surface
while negotiating right-hand bends (top) and left-hand bends (bottom),
and when driving on a straight road (centre). When steering around
bends, the distributions of fixations are closely centred on the tangent
point. Reprinted with permission from Land and Lee (1994), Nature Vol.
369, pp. 742--744. Copyright © 1994 Macmillan Magazines Limited.

and had been completely unaware of fixating the road in this precise
way. Analyses of the visually guided behaviour of birds suggest that a
role for gaze in the control of direction may be widespread in animals,
and may have an early evolutionary origin. As pigeons fly to land on a
perch, they maintain a head orientation closely related to their flight
trajectory, fixating the perch above the beak tip (Green, Davies, &
Thorpe, 1994). Just before young chicks jump across a gap, they adopt a
head orientation that correlates with the trajectory of their subsequent
take-off, again suggesting that fixing the orienta-

11. OPTIC FLOW AND LOCOMOTION

tion of the head in some way with respect to surrounding objects is a
component in the control of direction (Green, 1998). A common mechanism
may operate in bird flight and jumping, and in human walking, in which
the direction of gaze relative to a target, set by eye--head and head--
neck control systems, is in turn used to set the correct motor output to
the limbs to achieve a particular heading. We are able to rely on this
mechanism in some contexts, but are also able to adopt other
methods---including recovering heading from optic flow---to perform
other tasks such as steering vehicles, flying aeroplanes, or judging
heading in simulations. One strong implication of recent research on the
control of direction is that it cannot be regarded as a single task,
based on a general-purpose visual mechanism. Instead, we are able to use
a variety of control mechanisms, according to the demands of different
tasks, and the ways in which we achieve this are an exciting topic for
future research.

CONCLUSIONS As an animal or person moves about, the complex pattern of
motion of the retinal images is related in a strict and precise way to
the individual's movement and to the structure of the surroundings. This
provides a prime example of the kind of lawful relationship between
dynamic optical structure and the interaction of an individual with
their surroundings that Gibson and his followers argued is crucial for
understanding visual perception and its links to action. Some aspects of
this argument are generally accepted. The geometrical principles
relating environment, locomotion, and retinal flow are obviously
valuable tools to use in analysing problems such as the

343

visual control of flight, posture, walking, or driving. What is less
clear, however, is whether the nervous systems of animals and people are
organised so as to use information in retinal flow in consistent,
general-purpose ways. Although the theoretical analysis begun by Gibson
suggests that this ought to be possible, some of the evidence we have
reviewed in this chapter suggests that it is not actually so. Instead,
the implication of recent evidence is that the control of locomotion is
organised in a more "opportunistic" way than Gibson's analysis
suggested. In any particular context, many sources of visual and other
sensory information may be potentially available to control movement. As
Cutting (1986) argued in a critique of ecological theory, optic
structure often overspecifies the world and provides multiple sources of
information to specify such things as distance, direction, size, or
slant. Perception must therefore be active, or "directed", in order to
select between them. From one context to another, the reliability of a
particular source of information will vary, depending on such factors as
illumination, the richness of texture of surfaces, or the presence of
familiar structures such as roads. We have seen a range of recent
evidence pointing to the conclusion that the visual control of movement
involves dynamic processes that track changes from one situation to
another in the correlations between different visual and other sensory
variables. Perhaps our visual and motor systems are not organised to
carry out a single analysis of optic flow that can control movement
efficiently in all circumstances, but instead to learn in each of very
many different contexts which particular features of sensory information
are most useful in achieving particular tasks. In the next chapter, we
will see evidence pointing to similar conclusions when we consider how
vision is used to time actions.

Page Intentionally Left Blank

12 Vision and the Timing of Actions

that this distinction is too simple to describe at least some visually
guided movements, but we will adopt it for the time being as we look
first at forms of visual control that correspond closely to the
open-loop model.

To produce even the simplest organised movement, the central nervous
system must generate a pattern of motor activity in which changes in the
strengths of contractions of different muscles are precisely timed in
relation to one another. Almost always, this pattern must be adjusted to
the surrounding environment using visual and other sensory information.
These adjustments may alter the path of the body or a limb through
space, and we saw in Chapter 11 some ways in which this can be achieved
through vision. In this chapter, we will consider a different aspect of
the visual control of action, asking how the timing of movements is
controlled in relation to events unfolding around an animal or person.
In Chapter 11 (p. 325) we introduced the distinction between open- and
closed-loop control of movement, and mechanisms of both kinds can
achieve timing of actions. In open-loop control, sensory information is
used to construct a specification of the movement to be carried out,
which can be translated into a correctly timed pattern of activity
across a number of muscles. Once it has started, the movement carries on
to completion in a "ballistic" fashion, without any further influence of
sensory input. A closed-loop control system uses feedback from sensory
input during the course of a movement to generate the pattern of muscle
activity in "real" time. We will see later

SCALING AN ACTION WITH DISTANCE In a variety of situations, an animal or
person may need to move their whole body, or a limb, to make contact
with a stationary target. Such a movement could be controlled in a
closed-loop fashion, by monitoring continuously the discrepancy between
the positions of the target and of the body or limb. If the movement
must be executed quickly, however, there may not be enough time for this
sensory feedback to be used, and open-loop control will be necessary. In
such a case, the distance of the target must be obtained in some way
before the movement begins, and used to generate the correct pattern of
muscle contraction that will carry the body or limb accurately to the
target. One situation in which movements towards a target are performed
too quickly for closed-loop control is when animals strike at prey,
relying for success in obtaining food on their ability to move

345

346

VISUAL PERCEPTION

so quickly that the victim does not have enough time to respond by
escaping. In these circumstances, a predator must be able both to orient
its strike towards the prey and to match the distance of its strike to
the position of the prey. The first can be achieved straightforwardly
using the retinal position of the target, although one exception is when
an animal hunts for aquatic prey from above the water surface. Unless
the predator is directly above the prey, refraction of light at the
boundary between water and air will cause a discrepancy between the
actual and apparent positions of the prey. Some birds strike at fish
from above the water surface, and reef herons are known to correct for
the effects of refraction and to make accurate strikes at fish from any
angle (Katzir, Lotem, & Intrator, 1989). The scaling of strikes to prey
distance has been investigated in the praying mantis by Rossel (1983),
who measured the effect of placing prisms in front of mantis' eyes on
strike accuracy. The results showed that binocular disparity is used to
obtain target distance. Since the range over which disparity can provide
depth information is limited by the separation between the eyes (see
p. 171), it will be only be useful to insects for very close targets.
Presumably it is only useful to a mantis because its camouflage enables
it to get close to prey before striking. Chameleons, frogs, and toads
feed in a similar way, making a very fast strike at small prey with a
long, protrusible, sticky tongue. Although all these animals can scale
their tongue movement with distance, they do so in different ways. The
chameleon obtains distance information from the degree of accommodation
of its lens when the prey is optimally focused (Harkness, 1977), while
toads and frogs use both accommodation and binocular disparity to
produce a weighted average estimate of distance (Collett, 1977). There
are two ways in which an animal could detect the state of accommodation
of the lens; by monitoring efferent commands to the muscles that move
the lens, or by feedback from stretch receptors in the eye muscles
(cf. discussion of eye movements, Chapter 8, p. 261). One way to
distinguish between these alternatives is to use drugs that partially
paralyse the lens muscles, so that for a given

strength of efferent command the movement of the lens is reduced. If
feedback from receptors is monitored, distance estimation will not be
affected. In frogs, however, the result is underestimation of distance
(Douglas, Collett, & Wagner, 1986), implying that efferent commands to
the muscles are monitored rather than the degree of movement that is
actually achieved. Another situation in which actions have to be scaled
with distance is when an animal jumps over a gap. Accurate landing will
rely on an open-loop mechanism that can set the correct take-off
trajectory using information about the distance of the landing surface.
Locusts obtain this information from image motion generated by
side-to-side swaying movements of the head just before jumping. If the
landing surface is moved as a locust sways its head, it will either
under- or overshoot, depending on the relative phase of head and surface
movement (Sobel, 1990). In general, provided that a locust or other
animal moves its head either at a specific speed or over a specific
distance, then the speed or extent of target image motion will specify
its distance (see p. 315, Equation 11.1). Similar head movements are
seen in gerbils, which sometimes make vertical "bobbing" movements of
the head before jumping over a gap. Goodale, Ellard, and Booth (1990)
found that gerbils do not head-bob when they jump on to familiar
targets, and that in this situation they use the angular size of the
target to gauge the correct jumping distance, under- or overshooting if
its actual size is changed. Head-bobbing is only observed when gerbils
are confronted with an unfamiliar target, or when it changes in size
from one jump to another. Goodale et al. suggest that these animals use
image motion generated by stereotyped head movements when they have to
jump on to a novel target, or one that has changed in appearance. By
pausing to head-bob, a gerbil is likely to increase the risk that it
will be detected by a predator, and so it learns to calibrate the
angular size of the target to its distance, enabling it to switch to
this means of controlling its jumps on later occasions. The control of
gerbil's jumps is a little more complex still. Goodale et al. (1990)
also found

12. VISION AND THE TIMING OF ACTIONS

that the error in jumping caused by changing the size of a familiar
target was not as large as predicted if angular size alone were used to
give its distance. This implies that gerbils integrate angular size with
some other source of distance information, in the way that toads and
frogs integrate accommodation and binocular disparity. In both cases,
multiple sources of depth information are used, presumably to increase
the accuracy and reliability of control. Gerbils have the further
ability to change the combination of sources that they use, in an
adaptive way. We saw in Chapter 7 (p. 203) that human depth perception
has similar characteristics, and we will see further evidence in cases
of visually controlled human movement later in this chapter. Stereotyped
swaying or peering movements of the head occur in many species,
including humans, and they may play a role in generating image motion to
support a variety of visual processes. A well-known example of a
stereotyped head movement is the head-bobbing of many bird species,
including doves, pigeons, and chickens. During walking, the head moves
backwards and forwards relative to the body, and there is a brief "hold"
phase in each cycle in which the head is almost stationary relative to
the surroundings, followed by a "thrust" phase in which the head moves
forward more quickly than the body. Notice that a walking bird's head
never actually moves backwards relative to the surroundings; the
powerful illusion that it does is an example of how we see motion of a
figure in relation to nearby, larger figures. It is known that
head-bobbing in doves and pigeons is controlled by the optic flow
produced by walking (Friedman, 1975; Frost, 1978), but its possible
significance for vision is not fully understood. It may be easier to
detect moving objects while the head is stabilised during the hold
phase, but this cannot be the only function of the behaviour. Pigeons
maintain rhythmic head movement in fast running or slow flight without
achieving stabilisation of the head (Davies & Green, 1988; Green et al.,
1994). In these situations, it may be that faster head motion in the
thrust phase amplifies relative motion between the images of small food
items and the image of

347

the ground surface, so increasing the chances that the bird will detect
them.

Reaching For primates, the nearest equivalent to the prey capture
techniques of mantids or toads is to reach out and grasp objects with
the hand. We have already seen in Chapter 3 (p. 65) how the hand is
shaped during the course of a reach, so that the fingers and thumb are
separated by the correct distance when the hand arrives at an object. As
well as this "grasp" component, reaching must also involve a "transport"
component, which controls the contraction of muscles around the
shoulder, elbow, and wrist so as to bring the hand to the correct
position. This transport mechanism must be largely open-loop, generating
a pattern of motor commands before the reach begins that will carry the
hand over the correct distance. This is because reaches typically last
between 300 and 800 ms, and so the hand moves too quickly for a
closed-loop mechanism to operate smoothly. By the time a discrepancy
between hand position and target had been registered, and a corrective
motor signal generated, the hand would have moved a considerable
distance. Closed-loop control would therefore yield an erratic pattern
of hand movement. In fact, hand velocity during a reach changes in a
strikingly smooth way, rising to a peak and then falling, sometimes with
small corrective "blips" at the end of the reach. As the distance of the
target increases, the initial acceleration of the hand increases while
the duration of the reach remains constant (Jeannerod, 1984; see Figure
12.1). This shows that a specification of the whole movement must be
constructed before it begins, using information about target distance.
Further evidence for this open-loop form of control is that interference
with visual or proprioceptive feedback during the early stages of a
reach has no effect on its accuracy. These considerations all support a
two-stage model of the transport component of reaching, in which a
pre-planned, ballistic arm movement first takes the hand close to the
target, and then small corrections are made under closed-loop control at
the end of the movement. Recent evidence has shown that this model is
too simple, and that the

348

VISUAL PERCEPTION

monocular conditions, Servos et al. (1992) found that reaches were still
scaled to target distance, indicating that some other source of distance
information could be used, albeit to support less efficient control of
movement. There is evidence that this information can be obtained both
from the elevation of the target in the visual field (Marotta & Goodale,
1998) and from the motion parallax generated by small head movements
(Marotta, Kruyer, & Goodale, 1998).

TIMING ACTIONS FROM OPTIC FLOW

Velocity of the hand plotted against time over the course of three
reaches to objects at different distances from the body (upper trace, 40
cm; middle trace, 32 cm; lower trace, 25 cm). Adapted from Jeannerod
(1984).

open-loop planning of a reach does not result just in a sequence of
motor commands being "fired off", but also in a "forward model" that
predicts the resulting trajectory of the hand. Further control is then
achieved by a closed-loop mechanism that compares sensory feedback with
these predictions (for further details see Desmurget & Grafton, 2000).
Despite these complexities in the control of reaching, we can still ask
what visual information is used to scale the initial plan for movement
to the distance of the target. Servos, Goodale, and Jakobson (1992)
showed that binocular information contributes; when participants reached
for objects with one eye covered, the movement took longer and had a
lower initial acceleration and peak velocity than in control trials.
There was also a longer period of deceleration of the hand, with more
small adjustments. Further evidence has shown that the binocular
information concerned is the vergence angle of the eyes (Mon-Williams &
Dijkerman, 1999). Even under

We have seen that a stationary animal or person can strike at, jump on
to, or reach towards a stationary object by scaling their movement to
its distance using a variety of different sources of visual information,
singly or in combination. We now go on to consider another kind of
situation, in which either the actor or the target, or both, are moving
in such a way that they are closing with each other. At some point as
they get closer, the actor must perform some particular action. This
might be extending the legs if the actor is a fly approaching a landing
surface, or beginning to swing a racquet if the actor is a tennis player
trying to return an approaching ball over the net. In any case like
this, the control problem is to begin an action at the correct time as
actor and object approach one another. In principle, this could be
achieved by using the same sources of depth information as when both are
stationary, but we will see in this section that the relative motion
between actor and object opens up other possible means of visual
control. We will introduce these by first considering how objects on
collision courses are avoided.

Avoiding collisions Interest in the visual control of avoidance
responses was first prompted by developmental questions. From an
empiricist point of view (see Chapter 4, p. 78), infants would be
expected to learn the visual properties of objects approaching them on a
collision course as a result of experiences with their unpleasant
tactile consequences.

12. VISION AND THE TIMING OF ACTIONS

However, Bower, Broughton, and Moore (1970) demonstrated that babies as
young as 8 days old would show defensive distress reactions when a
foam-rubber cube was pushed towards them. It appears that babies who are
too young to have experienced the effects of colliding objects can
respond appropriately to ones apparently on a collision course. Of
course their reactions might be based on the change in air pressure
created by the real approaching object rather than on the optical
information specifying collision. However Bower et al. (1970) and Ball
and Tronick (1971) also tested young babies' reactions to dynamic
optical displays in which no air pressure changes were present. The
displays were created by casting the shadow of a real object onto a
screen in front of a supported infant. As the object was moved towards
the light source, the shadow cast by it expanded in size, creating a
"looming" image. Babies showed characteristic reactions to such
displays. Their heads went back and their arms and hands were raised to
cover their faces. Distress was also evident. While Bower et
al. reported that the reactions exhibited were somewhat less strong to
an apparent than to a real object, Ball and Tronick reported no
difference in the strength of the reactions in the two cases. The
reactions given to these looming patterns were in marked contrast to
those shown when the pattern cast specified an object that was
approaching on a noncollision path, or when it specified an object
receding from the child (a shrinking as opposed to a looming pattern).
Schiff, Caviness, and Gibson (1962) reported similar responses in infant
rhesus monkeys presented with looming patterns. Adults are strongly
influenced by knowledge of their surroundings, and do not show defensive
responses to looming optical patterns when they believe there are no
sources of danger nearby. King, Dykeman, Redgrave, and Dean (1992) found
that people would only respond to a looming object with a "flinching"
movement of the head when they were concentrating hard on a distracting
task (a computer game) and also believed that they were alone in a room.
During the course of development, it seems that the simple defensive
reactions to optic flow seen in babies come to be

349

inhibited by attention and by knowledge of the physical and social
environment. What do experiments using looming optical patterns tell us
about the visual information that triggers avoidance responses? Since
there is no change in the distance of the pattern from the observer, or
its texture, the only information available to specify its approach is
in the motion of its outline. While other distance information may be
important when real objects approach, this motion on its own is
sufficient to trigger avoidance. It is straightforward to see how it can
provide information about the trajectory of an object. If it is
approaching the eye directly, the image of an object will undergo a
simple expansion, with its edges moving at the same speed in opposite
directions. If it is passing the observer on a perpendicular path, the
motion will be a pure translation, with both edges moving at the same
speed in the same direction. For trajectories at any intermediate angle,
these two motions will be superimposed in varying proportions, and their
ratios can specify whether an object is on collision course and, if not,
how far from the eye it will pass. This information can be obtained from
the retinal velocities of the opposite edges of the image (Regan & Gray,
2000, Box 1). Assuming that an object is on a course close enough to a
collision to be of concern, an observer will also need some information
about how close to collision it is. If the object is a long way off and
moving slowly, it may be wasteful or dangerous to jump out of its way. A
useful concept in thinking about this question is "time to contact", the
time that will elapse before such an object hits the observer. We might
expect this variable to be useful in deciding whether, and when, to make
an avoidance response. If the object's velocity is constant, then time
to contact is given by the ratio of its distance to its velocity, and it
is possible that animals or people might combine a number of sources of
visual information to compute these quantities and so obtain time to
contact. There is no evidence that this actually happens, and we will go
on to look at two alternatives; that simpler heuristics are used, and
that time to contact can be obtained from dynamic optic patterns in a
more direct way.

350

VISUAL PERCEPTION

In Chapter 11 we saw that a tethered fly presented with an expanding
optical pattern filling most of its visual field responds by extending
its legs to land. Robertson and Johnson (1993) carried out similar
experiments with tethered locusts, moving objects directly towards their
heads. When the insects responded late in an object's approach, they did
so with landing movements, but if their response was earlier they
carried out a co-ordinated set of changes in posture and wingbeat that,
in free flight, would take them on a course avoiding the object.
Robertson and Johnson measured the time of onset of these avoidance
responses in relation to variation in the size and approach speed of
objects, and found that they did not begin at a particular time to
contact. Instead, the data were consistent with a model in which the
response begins at a reaction time of 65 ms after the angular size of
the target reaches 10°. This implies that it will begin at different
times to contact depending on the size and approach speed of the object.
Robertson and Johnson suggested that this simple heuristic may be
matched to the typical size and flight speed of other locusts, and show
that it would be adequate to trigger avoidance soon enough to avoid
collision with another insect flying nearby in a swarm. Neurons have
been identified in the ventral nerve cord of the locust that link cells
in the lobula (see Chapter 11, p. 324) sensitive to visual movement to
networks of cells that drive muscles involved in flight and jumping. The
responses of these "descending contralateral movement detectors" are
strongly selective for the motion of an approaching object, decreasing
by 50% if the trajectory of the object deviates from a direct approach
by only 3° (Judge & Rind, 1997). The firing rate of the neurons builds
up steadily during the approach of an object, and it is likely that they
play a role in triggering avoidance manoeuvres when the angular size of
an object reaches a large enough value. It is an open question how
widely simple heuristics of this kind are used to control avoidance
responses, without achieving precise timing in relation to collision.
However, this question does set the scene for examining a more powerful
means by which the

optic flow produced by approaching objects or surfaces could be used to
obtain time to contact.

Time to contact from optic ﬂow: Theory Lee (1980b) has demonstrated that
time to contact can be obtained from the image of an approaching object
or surface without first obtaining either its distance or its velocity.
Consider a flat surface approaching an animal's eye along a path
perpendicular to the surface, with constant velocity. This situation is
represented schematically in Figure 12.2, in which a surface on the
right is approaching an eye on the left with velocity V. At time t it is
a distance Z(t) away, in units of the diameter of the eye, for
convenience. A texture element P on the surface has an image P'
projected on the retina. At time t, P' is a distance r(t) from the
centre of the expanding optic flow field and moving outwards with
velocity v(t). From similar triangles, 1 Z(t) = r(t) R

Differentiating with respect to time, and inverting, r(t)2 R = v(t) V

Since R = Z(t)r(t), r(t) Z(t) = v(t) V

This ratio r(t)/v(t), which Lee called τ ("tau"), is the ratio at any
instant of the distance of any point from the centre of an expanding
optical pattern to its velocity away from the centre. It is equal to
Z(t)/V, and therefore, if V is constant, to the time elapsing before the
eye and surface collide. If the visual system can obtain τ from an
expanding retinal image, then information about time to contact with a
surface is directly available to time actions. Notice that this can be
obtained without having to determine either the distance of the surface
or its closing velocity,

12. VISION AND THE TIMING OF ACTIONS

351

Schematic representation of a surface approaching an animal's eye with
velocity V(t). Adapted from Lee (1980b).

and that it does not matter whether the closing velocity arises from
motion of the surface, the perceiver, or both. On the other hand, the
centre of expansion of the image of the surface does have to be
determined. In the case of a perceiver moving through cluttered
surroundings, the value of τ for each texture element flowing outwards
from the centre of expansion of the optic flow field specifies its "time
to passage" (Kaiser & Mowafy, 1993). This refers to its time to contact
with a plane passing through the perceiver's eye and lying perpendicular
to their direction of heading. It is easy to see this by imagining that
point P in Figure 12.2 lies on an object off to one side of the line of
travel of the eye; τ will give the time elapsing before it passes to one
side of the eye. The term "global τ" is used to refer to the ratio
defined as above in relation to the centre of expansion of an optic flow
field. Other ratios of the same form can be defined that do not refer to
a centre of expansion and that also specify time to contact. Imagine an
object on a collision course with an eye. If P1 and P2 are two points on
the contour of its image, then the ratio of the distance between them to
the rate of increase of their distance gives time to contact (Lee &
Young, 1985). This can be expressed equivalently in terms of the angle
subtended at the eye by the object, and is known as "local τ"
(Tresilian, 1991).

Lee's (1980b) analysis establishes a means by which actions could
potentially be timed as objects approach us on collision courses. These
actions could include not only avoidance responses but also a variety of
other actions, such as a hawk's strike as it swoops on to prey, a
horse's jump over a fence, or a person's reach with a hand to push a
door open as they walk towards it. Several factors need to be taken into
account in considering how useful global or local τ might be in any
particular situation like these. Like any other optical variable, its
usefulness will be limited by the accuracy with which it can be
determined. If the accuracy of locating the centre of expansion of optic
flow were low, as it might be in sparsely textured surroundings, then
the estimates of time to contact from global τ would be unreliable. If
an approaching object is small and distant, the rate of increase of the
angle that it subtends at the eye will be low and so it will not be
possible to determine local τ accurately. Further, τ specifies time to
contact assuming closing velocity is constant, and so will not be useful
where closing velocity is likely to change rapidly. As we will see
later, it may still be useful where closing velocity changes, but in a
predictable way such as a smooth acceleration under gravity. Finally, in
many situations a person or animal may need to know the time that will
elapse before an object reaches not their eye (as in Figure 12.2) but
their

352

VISUAL PERCEPTION

hand, foot, or some other part of their body. In some of these, "time to
passage" provided by τ may be useful; for example, a horse approaching a
fence might use the time elapsing before the fence passes under its head
to time its jump. In other cases, this is less likely; when we need to
know time to contact between an object and our hand, it may lie at any
position relative to our eyes. These constraints on the potential of τ
to provide timing information have prompted further analyses of dynamic
optic variables that could pay the same role across a wider range of
situations. Bootsma and Oudejans (1993) consider the case of an object
approaching some target at a distance from the perceiver's eyes. They
show that time to contact between object and target can in principle be
obtained from the rates of change of the angular size of the object and
of the angular separation of object and target. This result could be
relevant to visual control in cases where the target is the perceiver's
hand, among others. So far we have only considered ways that time to
contact could be obtained from a single retinal image. Binocular vision
provides several further possibilities. If the eyes converge to keep an
approaching object fixated, then the ratio of vergence angle to its rate
of change at any instant specifies time to contact (Heuer, 1993). Notice
that this ratio has the same mathematical form as local or global τ (a
variable divided by its rate of change), and all can be described as
τ-functions of particular optical variables. The rate of change of
relative disparity between the images of an object as it approaches the
eyes can also be used to obtain time to contact, provided that the
distance of the object is known (Regan & Gray, 2000, Box 2). These
theoretical analyses leave us with a range of possible ways in which
actions could be timed visually. First, there are various heuristics for
achieving adequate timing without actually obtaining time to contact,
such as using the angular size or the distance of an object. Second,
there are a number of ways of obtaining time to contact from optic flow,
each valid under different sets of constraints. These may depend on
either monocular or binocular information, and may or may not involve
τ-functions. Given this variety of

means of timing actions, it would not be surprising if we found
different ones being used in different circumstances. We turn next to
look at empirical evidence on the visual timing of actions, which we
will see does point towards that general conclusion.

Time to contact from optic ﬂow: Evidence Much of the recent research on
the visual timing of actions has aimed to test the hypothesis that
variables of optic flow, and particularly local and global τ, are used
generally across many situations. Wang and Frost (1992) obtained
evidence that τ is computed in a visual pathway, from single cell
recordings in the nucleus rotundus of pigeons (a region of the brain
homologous to the pulvinar nucleus of mammals; see Chapter 3, p. 65).
They presented pigeons with graphical simulations on a monitor screen of
a solid, patterned ball moving in depth, and found a group of cells that
were strongly selective for the simulated direction of the ball,
responding only when the ball appeared to be on a course directly
towards the eye. Furthermore, each of these cells began to fire when the
simulated time to contact of the ball with the eye reached a specific
value. This value remained constant over changes in both the size and
approach speed of the ball, implying that each cell is tuned to a
particular time to contact with an approaching object. In further
experiments, Sun and Frost (1998) found that some of the cells selective
for objects on a collision course had different response properties,
appearing to signal the rate of increase of the angular size of an
approaching ball (i.e., v(t) in Figure 12.2) rather than τ. Of these,
some continued to increase firing rate until simulated contact, while
others gave a peak response some time before. These results suggest that
cells in the nucleus rotundus are part of a system that extracts a
number of properties of a looming image in parallel. While one of these
is local τ, specifying time to contact, it cannot be concluded that this
is the only variable that can control a pigeon's avoidance behaviour.
The properties of the τ-selective cells contrast with those of the cells
in the locust that we described earlier (p. 350). While both respond to
objects on collision courses, those in

12. VISION AND THE TIMING OF ACTIONS

the pigeon are selective for more complex properties of image motion.
Rind and Simmons (1999) argue that selectivity for τ is possible because
of the higher spatial resolution of the pigeon eye than of the locust
eye. This allows a pigeon to obtain the time to contact of an object too
small and/or distant for a locust to resolve, and presumably therefore
to begin avoidance manoeuvres sooner to a wider range of objects and
with greater precision of control than is possible in locusts. Evidence
that cells in the visual pathways of animals are selective for τ leaves
open the question whether their outputs are used directly in controlling
the timing of actions. To answer that question, we must turn to
behavioural evidence, and we will begin by describing ways that
observational data have been used. One method relies on the assumption
that an action is timed by a single optical variable and is initiated
when it reaches a particular value that is the same every time the
action is performed. If this is so, then we would expect this variable
to show less variation than any other, over many occasions that the
action occurs, at the point that the action is initiated. Davies and
Green (1990) applied this method to the landing flight of two bird
species, the pigeon and the Harris hawk (Parabuteo unicinctus). During
landings by pigeons, there was less variation in distance between eye
and perch than in τ at the point where the feet were extended. Although
physiological evidence indicates that τ is computed in the pigeon's
visual pathway, it appears that it is not used to time behaviour in this
specific context. Instead, the feet are extended towards the perch when
the distance of the perch from the eye falls to a specific value, and
this distance could be given by a number of variables including
binocular disparity. This result is consistent with the fact that
pigeons head-bob during landing flight (see p. 347), which causes large
fluctuations in the value of τ that would make it an unreliable means of
timing landing. In contrast, data from a small number of landing flights
by a hawk, which does not head-bob, showed that τ varied less than perch
distance when the feet were extended, suggesting that the two species
may dif-

353

fer in the way that they control this component of landing. Lee and
colleagues developed another means of using observational data to test
between variables that might time actions, by using cases where a person
or animal closes with an object or surface as one of them accelerates
under the force of gravity. The advantage of such a situation is that
values of velocity and other parameters during the approach can be
obtained from simple equations of motion. Lee and Reddish (1981) studied
the gannet (Sula bassana), a seabird that hunts by flying over the sea
at heights of up to 30 m. When it detects a fish below the surface, a
gannet dives almost vertically into the water to seize the fish in its
beak. At the start of the dive, the bird assumes a swept-back wing
posture (Figure 12.3) which allows it to steer towards the moving fish.
Gannets enter the water at speeds of up to 24 m/sec (54 mph) and would
be injured if they kept their wings extended at this speed. When less
than a second away from the water surface, they therefore stretch their
wings back into a streamlined posture. If the wings are streamlined too
soon, steering accuracy will be lost, whereas if they are streamlined
too late, the bird will be injured. It is crucial to the bird's hunting
success and survival that this action is timed accurately. As a gannet
accelerates under the force of gravity during a dive, its velocity
increases continuously, and therefore the value of τ at any instant does
not specify time to contact but overestimates it by an amount dependent
on the bird's current velocity. Even so, it is possible that by
streamlining its wings when τ reaches a margin value the bird could
achieve sufficiently accurate control of its dive. Lee and Reddish
(1981) derived an expression for the time to contact with the water
surface at which streamlining would occur on this hypothesis, in terms
of the duration of the dive and the assumed values of τ and of the delay
between its detection and streamlining. They then filmed gannets' dives
and obtained values of time to contact and dive duration for each of a
large number of dives. Figure 12.4 shows the data they obtained and the
curve generated by their model that best fits the points. Lee and
Reddish show that strategies of timing

354

VISUAL PERCEPTION

Successive wing positions of a diving gannet. The wings are streamlined
as the bird strikes the water. Drawing by John Busby, reproduced from
Nelson (1978) with the permission of the illustrator.

streamlining that involve computation of the actual time to contact
(from height, velocity, and acceleration), or streamlining at a
particular velocity or at a particular height, would all give
relationships between time to contact and dive duration that match the
data less well than does the τ strategy. In a second study, Lee, Young,
Reddish, Lough, and Clayton (1983) tested the τ hypothesis in a
situation where human participants had to leap up and punch balls that
were dropped from varying heights above them, as in volleyball. A ball
dropping towards the ground accelerates at a constant rate under the
influence of gravity, and so the value of τ at any instant again
overestimates the actual time to contact. Even so, τ

could still provide an adequate means of initiating jumping and
punching, as its value converges with time to contact when the latter is
less than 250 ms (see Figure 12.5). Lee et al. (1983) measured the
angles of the knee and elbow as these were flexed and extended in the
punching act. When these angles were plotted against time to contact,
the profiles of the flexion phase obtained with different drop heights
did not coincide, as the actions began sooner and lasted longer the
greater the drop height. When plotted against τ, however, they were more
closely aligned, showing that a strategy of controlling flexion by τ
accounts better for the data than one of computing time to contact from
ball height and speed. Furthermore, participants could not have geared
their actions to the height of the ball above them, as this would yield
the opposite relationship between drop height and initiation of the
action to that observed. These conclusions from landing pigeons, diving
gannets, and people jumping to punch balls all rely on indirect methods
to test the τ hypothesis. They determine the relationship between the
kinematics of approach to a surface and the timing of some action, and
then compare it to the relationships predicted by various strategies,
including the use of τ. This method may show that, of two or more
alternative means of timing an action visually, the use of τ best
explains the data. However, this does not necessarily imply that τ is
actually used, because some further means of visual control may be
consistent with the data. Wann (1996) makes this point in a
reinterpretation of Lee and Reddish's (1981) findings from gannets,
arguing that the results are equally consistent with the hypothesis that
the birds fold their wings when they have fallen some fixed proportion
of the height of their dive. Testing this hypothesis against the τ
strategy requires more records of dives from large heights than are
available in Lee and Reddish's sample, because it is only at large
values of td that they generate substantially different predictions (see
Figure 12.4). Similar problems arise with the results from Lee et al.'s
(1983) ball-punching task. Michaels, Zeinstra, and Oudejans (2001)
carried out a similar experiment in which seated participants

12. VISION AND THE TIMING OF ACTIONS

355

Relationship between the duration of a gannet's dive (td) and the time
to contact at which it streamlines (tc). The curve is the best fit
generated by the model to the data. From Lee and Reddish (1981).
Reprinted by permission of the author and publisher from Nature Vol.
293, pp. 293--294. Copyright © 1981 Macmillan Magazines Ltd.

The relationship between τ (instantaneous time to contact) and tc
(actual time to contact) for balls dropped from three different heights
to yield three different drop times. From Lee et al. (1983). Reprinted
with permission of the Experimental Psychology Society.

reached upwards to punch a ball as it fell towards them. The results did
not support the prediction that the whole punching action was initiated
at a margin value of τ, but could be better explained by the use of a
different optical variable, the rate of expansion of the angular size of
the ball (i.e., v(t) in Figure 12.2). Even so, this explanation required
the assumption that the criterion value of this variable was different
from one person to another, and from one occasion to another. Further,
these conclusions only applied to the timing of the initial flexion of
the arm; its subsequent extension towards the ball appeared to be
controlled in different ways. These results from observational methods
suggest that more experimental control may give clearer answers about
the visual timing of actions. As we saw in Chapter 11, it is relatively
straightforward to control optical information and measure its effects
on locomotion in insects. For example, a tethered fly will perform a
landing response when presented with a large looming pattern (see
p. 322). Borst and Bahde (1988) used this method to investigate the
timing of the landing response. They found that landing begins sooner
the faster the simulated rate of approach of an object, and this is
consistent with the hypothesis that flies obtain τ from optic flow.

356

VISUAL PERCEPTION

However, they found that the size of the object and its spatial
structure (the density of contours over its surface) also influenced the
timing of landing, which should not be the case if τ is used. Borst and
Bahde conclude that landing is controlled in a simpler way; the outputs
of motion detectors across the whole visual field are summed and then
integrated over time and the fly begins landing when this integral
reaches a threshold value. As with locusts' avoidance responses in
flight (see p. 350), behaviour is not triggered at a particular time to
contact, but somewhere within a range of times. This simpler mechanism
is presumably adequate for survival. Provided a fly's feet are extended
and its velocity is within a low range, its exact velocity when it
contacts a surface is probably not critical; we have all seen flies
survive head-on collisions with windows! The behaviour of vertebrates
cannot be so easily controlled, but one experiment that succeeded in
gaining control over relevant optical information as an animal closed on
a target was performed by Sun, Carey, and Goodale (1992). They trained
gerbils to run to a food dish in front of a monitor screen displaying a
circle. The size of the circle, and its distance from the food, were
varied to prevent the gerbils using the angular size of the circle as
distance information. In test trials, the circle was changed in size
while the gerbils were running towards it. When it expanded, the animals
decelerated sooner than in the control condition, whereas when it
contracted, their deceleration was delayed. These results imply that
gerbils use τ to control their speed of approach to a goal. However, the
effect of changing the size of the circle was smaller than would be
predicted by a pure τ strategy. Gerbils must therefore integrate τ with
some other source (or sources) of depth information, in the same way as
they integrate the angular size of a landmark with motion cues (Goodale
et al., 1990; see p. 347). Savelsbergh, Whiting, and Bootsma (1991) made
a comparable attempt to control τ experimentally, by giving participants
the task of catching a ball that could be deflated as it approached
them. The shrinking ball optically mimics one of fixed size approaching
more slowly, and so, if sub-

jects used τ, their grasp should be delayed relative to a control
condition in which the ball has a fixed size. Savelsbergh et al. found
that the point at which the closing speed of the fingers reached a
maximum was indeed delayed, by 5--6 ms. Even so, participants had no
difficulty in grasping the deflated ball, suggesting that other sources
of visual information must be used in parallel with τ. The skill of
catching a ball requires particularly precise timing of arm and hand
movements, and its visual control has been widely investigated. In order
to catch a moving ball, the hand or hands must be oriented correctly and
appropriate grasping movements must be initiated before the ball makes
physical contact with the hand, while it is still in flight. Alderson,
Sully, and Sully (1974) studied one-handed catching of tennis balls and
found that the fine orienting movements of the hand began 150--200 ms
before the ball struck the palm and the grasping movement started 32--50
ms before contact. Whiting and Sharp (1974) measured the accuracy of
such catches when the ball was only visible for a brief time (80 ms)
during its approach. They showed that there is an optimal point in the
trajectory during which to view the ball in order to catch accurately,
when time to contact is about 250-- 300 ms. If the ball is seen too
early, it gets "lost", possibly because of limitations on immediate
memory. If it is seen too late, there may be insufficient time to
process the flight information and then begin the correct series of
orienting movements. What optical information is used during this 300 ms
"window" to control the movements of the hand so as to grasp a ball
accurately? In a real ball-catching task, the prospects of being able to
manipulate τ seem even more remote than in Savelsbergh et al.'s (1991)
somewhat artificial one. It is easy, however, to manipulate binocular
sources of timing information such as vergence angle and disparity, or
their rates of change, by using telestereoscopic spectacles. This
optical device mimics the effect of increasing the distance between the
eyes and so will cause the distance (and therefore time to contact) of a
ball to be underestimated (see Chapter 7, p. 171). Judge and Bradford
(1988) found that people wearing this device were

12. VISION AND THE TIMING OF ACTIONS

unable to catch balls thrown towards them slowly and on low
trajectories. Their catching accuracy returned to normal after about 20
attempts, presumably because the control of arm and hand movements by
binocular information was recalibrated. In a control condition,
participants had no difficulty in catching balls while wearing
spectacles that reduced their field of view to the same extent as the
telestereoscope. Since we can catch slowly travelling balls with one eye
covered, binocular information clearly cannot be necessary for visual
control, and some monocular variables must be available as well.
However, Judge and Bradford's experiments show that when binocular
information is available (even if it is distorted), we prefer to use it.
The visual control of ball catching therefore appears to have something
in common with that of reaching to grasp a stationary object, where
binocular information is necessary for optimal speed and accuracy but
monocular variables can be used in addition (see p. 348). The
difficulties involved in trying to control optical information as people
perform visually guided actions can be sidestepped by substituting a
judgement of time to contact for a natural action such as catching a
ball. In one such technique, people are shown films or animations that
simulate an object approaching them at constant velocity on a collision
course. The simulation is stopped at some point, and the observer is
asked to signal with a button-press or some other response when
collision would have occurred. People are able to make fairly accurate
judgements of time to simulated contact in such procedures (e.g.,
McCleod & Ross, 1983; Schiff & Detwiler, 1979). While there is often a
tendency to underestimate time to contact in simulations, this effect is
not found with all displays (Freeman, Harris, & Tyler, 1994). Because
this task makes it possible to control the optical variables that can
specify time to contact, various versions of it have been widely used in
research on visual timing; recent reviews can be found in Kerzel, Hecht,
and Kim (1999) and Regan and Gray (2000). One general conclusion is that
people may adopt heuristic strategies, basing their judgements on
variables such as image

357

velocity that do not provide valid time to contact information, even
though τ is available in the display. Another is that people alter the
weighting that they give to monocular and binocular sources of timing
information according to the nature of the task. Binocular information
is necessary for accurate time to contact judgements when the simulated
approaching object is small, whereas accuracy is equally good under
monocular conditions when it is large (Gray & Regan, 1998). At the same
distance, a small object will yield lower values of image size and its
rate of change than a large object, and therefore computation of τ will
be more susceptible to noise. The detection of rate of change of
binocular disparity is not sensitive to image size, and so accuracy of
judging time to contact is improved by switching to this information
when an object is small. More evidence for adaptive switching between
sources of timing information is provided by Gray and Regan (2000), who
show that binocular variables are weighted more strongly if the
simulated object is nonspherical and rotating than if its spherical. As
we have seen (p. 351), local τ does not provide an accurate estimate of
time to contact in the former case. It is important to be cautious in
assuming that judgements of time to contact are controlled in the same
way as body and limb movements in more natural tasks. It is always
possible that the different constraints applying in the two kinds of
task lead people to adopt different strategies of visual control (see
Tresilian, 1995, for a discussion of this issue). One recent method has
attempted to achieve more control of optical information in the context
of a natural, timed action by using immersive virtual reality (VR)
techniques. Rushton and Wann (1999) presented participants with a VR
display in which a tennis ball seemed to approach them, and asked them
to squeeze a real ball held in their hand at the moment that they would
have caught the virtual ball, had it been real. They first found that
the timing of the squeeze response in relation to the approach of the
virtual ball was similar to that in real ball catches (see p. 356),
suggesting that the display succeeded in engaging the same control
mechanism.

358

VISUAL PERCEPTION

Rushton and Wann (1999) went on to use the VR technique to manipulate
independently monocular τ and binocular disparity in the display, so
that a participant would obtain a different estimate of time to contact
with the virtual ball depending on which of these variables they used.
When τ specified contact with the ball 100 ms before disparity, the
response occurred 70--80 ms earlier, while if τ specified contact 100 ms
later, the response was delayed by 30--40 ms. While the results
demonstrate that τ is used to time the catching response, the fact that
timing did not change by the full 100 ms implies that τ must be
integrated with binocular disparity to yield a combined estimate of time
to contact. We have already seen evidence for such integration of
multiple sources of distance or timing information at several points,
but these results give us further insights into this process. They
indicate in particular that τ and binocular disparity do not have fixed
weightings in the integration process; instead, τ has a stronger effect
when it predicts earlier arrival of the ball than when it predicts later
arrival. Rushton and Wann use this result to develop a model of the
integration process in which the variable specifying the lowest value of
time to contact has the greatest weight. They argue that in natural
situations there will be limits on the reliability of any potential
source of timing information, and that an adaptive strategy for
integrating different sources is to give greatest weight to the one
specifying the most immediate need to carry out an action. Our survey of
research on the timing of actions as an animal or person closes with a
target has revealed a complex picture. It seems clear that no single
optical variable is used in a generalpurpose way to achieve precise
timing, and that the problem is instead solved by fast integration of a
number of variables that are available simultaneously. Like those
involved in the integration of depth cues (see Chapter 7, p. 201), these
mechanisms are adaptable, altering their characteristics so as to give
greatest weight to the variables that are most informative in any
particular situation. Research on this problem has perhaps reached a
point where we know a good deal about potential means of timing actions
visually, and now need to

know more about how these processes can become attuned to different
contexts so efficiently.

CONTINUOUS VISUAL CONTROL In discussing the timing of actions as a
person or animal closes with a target, we have so far assumed that the
problem is to initiate an action at the point in time that allows it to
be carried out most effectively. If the action is a purely ballistic
one, controlled in an open-loop fashion, then identifying the variable
(or variables) that control its initiation would be a complete
explanation of its visual control. This may be true in some of the cases
that we have considered; wing folding by a diving gannet is a likely
one, as the action may begin less than 100 ms before entering the water
and so further modification of it by visual feedback is unlikely to be
possible. In other cases, there is almost certainly more to visual
control than a single open-loop mechanism. Closing our fingers to grasp
a ball can only be one component in catching it, and must be preceded by
movement of the hand to the correct place to intercept the ball. This
movement must be controlled by visual information about the ball's
trajectory, probably in a closed-loop fashion. In the case of Lee et
al.'s (1983) ball-punching task, it is likely that once the legs start
to flex there is further, closed-loop control of flexion and extension
by visual information, to achieve a jump of the correct height. Our next
topic will be ways in which actions could be timed by controlling
optical variables in a continuous fashion.

Controlling deceleration One approach to this problem attempts to
provide a general-purpose mechanism for continuous timing of actions, by
extending Lee's analysis of time to contact information in optic flow.
To introduce this approach, consider a familiar situation for those of
us who drive; braking to bring a car to a stop at a particular point
such as a traffic light or a stationary car. Except in emergencies, we
do not simply slam the brakes on fully at a particular distance or time
to contact from the obstacle, but

12. VISION AND THE TIMING OF ACTIONS

instead modulate the force that we apply to the brake pedal more or less
smoothly as we slow to a stop. How is this force controlled to achieve a
stop at the correct place? At first, it would seem to involve detecting
both the distance of the target and current speed, and then computing
the deceleration needed to reduce speed to zero over just that distance.
Lee (1976) has argued that it is not necessary to compute the
deceleration required in order to

359

control braking. Instead, if the value of τ that specifies instantaneous
time to contact with the target can be obtained, then braking so as to
hold . the rate of change of τ (denoted by τ) constant can solve the
problem. Figure 12.6 shows how deceleration and velocity change with
time over . braking periods in which τ is held constant at different
values, using values generated from simple . equations of motion. With a
constant value of τ between 0 and −0.5, a person will stop precisely at

(Top) Relationship between deceleration and time during approach to an
obstacle, with braking controlled so as to . keep τ constant at various
values between −0.1 and −0.9. (Bottom) Relationship between velocity and
distance from the obstacle for the same approaches. Note: for .
consistency with the text, the τ values in these graphs should be
multiplied by −1. Reproduced from Lee, Young, and Rewt (1992). Copyright
© 1992 by the American Psychological Association. Reprinted with
permission.

360

VISUAL PERCEPTION

a target, while with a value between −0.5 and −1 they will collide with
it at a particular speed (see Figure 12.6, bottom). To stop just at a
target by . keeping τ at a value between 0 and −0.5, braking force and
thus deceleration must decrease during approach (see Figure 12.6, top).
Deceleration is . constant only in the particular case where τ is kept
at −0.5. Do drivers actually control braking force by . holding τ
constant? Spurr (1969) obtained kinematic data from 15 braking
manoeuvres and found that the time course of deceleration did not follow
a standard pattern. In one case, deceleration was approximately constant
throughout, and this would be consistent with a strategy of .
maintaining τ close to −0.5. In others, deceleration increased steadily,
or rose and then fell again, which are not patterns that would be .
expected if τ were held at or above −0.5 (see Kaiser & Phatak, 1993, for
further discussion of these and other data from braking vehicles). In a
different approach, Yilmaz and Warren (1995) used a . simulated driving
task to test the τ hypothesis. Participants used a mouse to control
their simulated speed in a display depicting approach along a road to a
stop sign, and were asked to stop at the sign without using repeated or
last-moment applications of the "brake". In most trials, τ decreased
over the course of braking in a linear fashion, apart from large changes
in magnitude at the end of the trial. The linear portions of the plots
of τ against time had slopes ranging from −0.61 to −0.35, with a mean of
−0.51, which Yilmaz and Warren concluded was the value of . τ being
maintained to control braking. Although their method may have narrowed
down a wider range of strategies that could be used in simulated (and
perhaps real) braking, their evidence is con. sistent with continuous
control of τ as one means of control. Although we have used the example
of braking a car to illustrate Lee's (1976) analysis, it can apply to
any situation where an animal or person must regulate their speed either
to stop just at a target or to achieve a controlled collision that will
impart momentum to it. An example of the second case is when we walk
towards a spring-loaded door and reach out to push it open without

stopping. Wann, Edgar, and Blair (1993) measured values of τ while
people ran up to and stopped at a target. They found that, in some
versions of the task, τ decreased linearly with a slope . (i.e., τ
value) of −0.5 or a little less. In other tasks, particularly where
people had to extend an arm to touch a target as they stopped, this
pattern was followed by a second phase in which τ behaved differently.
As when braking a car, it appears that more than one strategy of
controlling speed is possible when slowing from a run, and not all are .
consistent with the τ hypothesis. Another case, which supports the
hypothesis more strongly, is the approach of hummingbirds to a hovering
stop . at a feeder. Here, τ remains constant at a value less than −0.5
throughout the birds' approach, implying that they collide with it in
order to penetrate the feeder with the beak tip (Lee, Reddish, & Rand,
1991). Even where τ changes, as drivers, runners, or birds slow to a
stop in a way that is consistent . with control of speed by τ, such
evidence is open to the same criticism as the use of observational data
to test the hypothesis that τ is used to initiate actions (see p. 354).
Some other mechanism of control may be involved, which yields the same .
kinematic pattern as predicted by the τ hypothesis. Zaal and Bootsma
(1995) provide an illustration of this problem in their discussion of
how deceleration of the hand is controlled during a reach (cf. p. 347).
Their data are consistent with . control by τ, but equally consistent
with a dynamical model of the arm musculature, and kinematic data alone
cannot distinguish between these possibilities.

Running to catch a ball For further models of the continuous control of
actions by optical information, we return to the context of ball
catching. Often, catching a ball involves more effort than just reaching
out an arm, and we may need to run quickly to intercept the ball's
trajectory closely enough to have a chance of reaching for it. Here,
both person and target are in motion as they close with one another, but
the control involved is more complex than in cases we have looked at so
far. The speed and direction of running must itself be controlled

12. VISION AND THE TIMING OF ACTIONS

in relation to the ball's path, in order for closing to occur. We might
expect that the skill of running to catch a ball involves some
higher-level cognitive process of predicting from the ball's trajectory
where it is going to land, and then running to that place to wait for
the ball to arrive. McLeod and Dienes (1996) considered a simplified
version of this situation, in which a ball is launched into the air
towards a fielder. Because of air resistance, the ball does not follow a
perfectly parabolic path, but it rises smoothly to a peak and then falls
to the ground somewhere in front of or behind the fielder's starting
position. His or her problem is to run either forwards or backwards to
intercept the ball. A skilled baseball or cricket fielder will typically
start to run either forwards or backwards within about 800 ms of the
ball being launched. At this point, balls on trajectories that will end
a large distance apart on the ground differ only slightly in their
position and velocity (see Figure 12.7a). Are we really able to use such
small differences in trajectory to predict the point where the ball will
land? McLeod and Dienes propose that the problem is not solved in this
way, but by continuously controlling running direction and speed in such
a way that the optical velocity of the

361

ball remains at some constant value. Optical velocity is the rate of
change of the tangent of the ball's elevation above the observer's eye
(see Figure 12.7b). As long as a fielder keeps this variable at a
constant value (and therefore keeps optical acceleration at zero), they
will arrive at the ball's landing point at the same time that it does.
This form of control of running to catch a ball is known as the OAC
(optical acceleration cancellation) model. There are several ways of
testing the OAC model against the alternative hypothesis that a fielder
predicts the interception point. McLeod and Dienes (1996) filmed skilled
fielders running forwards or backwards to catch balls fired at various
speeds from a machine. They began to run about 500 ms after the ball was
fired, sometimes making a quick correction in their direction, and were
always running when they reached it, regardless of the distance that
they had run. These observations are consistent with the OAC model, but
not with prediction of the interception point. Furthermore, the profiles
of running velocity showed no consistent pattern from one trial to
another, but optical acceleration was close to zero throughout each
trial. Deviations from zero

(a) Trajectories of three balls launched from point L in the direction
    of a fielder at F. All three are launched at 45°, but at different
    speeds. Circles show the positions of the balls 840 ms after
    launching, the average time at which fielders begin to run. Marks on
    the vertical and horizontal scales are at 10 m intervals. Adapted
    from McLeod and Dienes (1996). (b)  is the angle of elevation of a
    ball at an observer's eye (E). tan  is the ratio of the vertical and
    horizontal distances of the ball from the eye. Optical velocity of
    the ball is d(tan )/dt.

362

VISUAL PERCEPTION

were always within a range that would allow an error of no more than 2 m
in intercepting the ball. The OAC hypothesis provides an illustration of
how a person could solve both a spatial and a timing problem through a
single strategy. If it is correct, then there is no need to detect
separately where a ball will land (in order to control running
direction) and time to contact with this point (in order to control
speed). Closed-loop regulation of a single optical variable is
sufficient to bring the fielder to the right place at the right time.
While it explains existing evidence better than a prediction strategy,
the OAC hypothesis is still open to the same criticism as others we have
discussed, that other strategies may give rise to the same pattern of
behaviour. One limitation of the OAC strategy is that it only explains
the control of running in a single dimension. In cricket, baseball, and
other games, the fielder's problem is two-dimensional, and MacLeod and
Dienes suggest that separate mechanisms may control running in two axes.
Another analysis of the problem of running to catch a ball avoids this
problem. Imagine standing still and watching a ball hit high into the
air; it will rise above the horizon on an optical trajectory that will
curve to one side or the other (unless it is hit straight towards you,
when its trajectory will appear straight). By running fast enough in the
right direction, it is possible to keep this trajectory straight, and
McBeath, Shaffer, and Kaiser (1995) argue that this strategy will
automatically bring a fielder into the path of the ball and to the point
on the ground where it will land. Like the OAC model, this linear
optical trajectory (LOT) hypothesis involves continuous control of an
optical variable, but offers the advantage that running direction can be
controlled in two dimensions at the same time. McBeath et al. (1995)
filmed fielders as they ran to catch balls and found that the curvature
and velocity profiles of their running paths were consistent with the
predictions of the LOT model. Even so, the model has been criticised on
two grounds. Dannemiller, Babler, & Babler (1996) show that maintaining
a linear optical trajectory of the ball will yield the wrong running
direction in some circumstances, while Chodosh, Lifson,

and Tabin (1995) describe observations of professional baseball players
that contradict the model. In the majority of the catches that they
filmed, the fielder did not maintain fixation of the ball throughout its
trajectory, and caught it while standing still at the interception
point. The first observation is not consistent with precise control of
the ball's optical trajectory, while the second contradicts both the LOT
and OAC models, which predict that the ball will be caught on the run.
The theoretical and experimental analyses that we have described
certainly show that the control processes involved in running to catch a
ball have yet to be fully explained. They also hint strongly that there
may not be any single control mechanism that is used in all
circumstances. Chodosh et al. (1995) suggest that their observations
differ from those of McBeath et al. (1995) because their participants
were more expert baseball players, and that the LOT strategy could be a
useful one for a novice fielder while learning various means of
predicting the path of a ball. These may include cues specific to the
game of baseball, such as the way that the ball is pitched to the
batter, the form of the batter's swing, or the layout of the pitch and
surrounding buildings. Given this approach, McLeod and Dienes' (1996)
evidence may reflect the ability of skilled fielders to fall back on a
general-purpose control mechanism for intercepting a ball, when faced
with a ballfiring machine that provides none of the cues that they would
normally use to predict a ball's path. We have looked at two contexts
where continuous, closed-loop visual control can be used to intercept
objects in specific ways; to come to a stop (or bring a vehicle to a
stop) at a target, and to run to intercept a ball. Although
generalpurpose mechanisms for solving these problems, . such as control
of τ or optical velocity, have been proposed, it is not clear that these
are used in consistent ways. In particular, skilled visual control may
involve learning to use a variety of cues tied to specific contexts that
enable a person to predict how their path will close with the paths of
objects. In the next section, we will see evidence that provides further
support for this view.

12. VISION AND THE TIMING OF ACTIONS

PREDICTIVE CONTROL

Continuous control of optical variables provides one means of closing
with the path of a moving target such as a ball, but it seems that in
some situations people are also able to predict an interception point
and move towards it. In its simplest form, predictive control of this
kind has openloop properties. The direction and distance of a run or
other movement are first computed, and then the action is performed
ballistically. However, we saw earlier that the control of reaching even
for a stationary object is more complex than this. The initial
specification of the movement is used to generate a "forward model"
which is then compared to sensory feedback in order to achieve maximum
accuracy of control (see p. 348). The experiments on the control of foot
placement during walking that we described in Chapter 11 (Patla et al.,
1999; see p. 333) also demonstrate that control of this kind is
involved. The corrections made to a stride to avoid a suddenly presented
obstacle are based on a comparison between the position of the obstacle
and the predicted position of the foot at the end of the stride. It
seems very likely that predicting where to intercept a ball will involve
not only advance planning of the direction and distance to run, but also
subsequent closed-loop modification of the run by comparing the ball's
optical trajectory with that expected. Although we know little at
present about the processes underlying predictive control, there is
evidence that it operates remarkably early in life. Babies first reach
for stationary objects at about 18 weeks of age, and at this time they
are able to reach accurately for moving objects as well (von Hofsten &
Lindhagen, 1979). Over the next few months, infants' readiness to reach
for fastmoving objects increases, and their ability to catch them
improves, until by the age of 9 months they can achieve 50 ms precision
in grasping a moving target (von Hofsten, 1980, 1983). At all ages,
those reaches that babies make are almost always accurate, suggesting
that they only initiate a reach when they are likely to be successful. A

363

detailed film analysis of reaches made by babies (von Hofsten, 1980)
revealed that they did not pursue a moving target with their hand in a
continuous way. Instead, they made a series of ballistic arm movements
until the hand arrived at the point where contact would be made with the
target. Babies at all ages were remarkably accurate at predicting the
path of the target, but the older infants reached for it more
"economically", using fewer ballistic steps. Van der Meer, van der Weel,
and Lee (1994) tested infants in a situation where a target could not be
grasped simply by following it with the hand. They moved an attractive
toy in a straight line behind a transparent screen, so that the infant
participant could only grasp it as it passed a gap in the screen. Just
before reaching this gap, the object disappeared briefly behind an
opaque screen. Before the toy disappeared, infants aged 11 months both
looked at and reached towards the "catching point" at the edge of the
gap. Reaching movements with the hands were always directed towards the
catching point, and never towards the toy. These results show that the
control of both eye and hand movements is predictive, and not based
simply on the current position of the target. By testing infants on this
task from the age of 16 weeks onwards, van der Meer et al. (1994) went
on to show that gaze is controlled predictively as early as 20--24
weeks, while similar control of hand movement follows at 32 weeks.
Recording eye movements during interception tasks provides a sensitive
test for predictive control. Since the inertia of the eyeball is low, a
saccade made to fixate a point in the surroundings is completed while
even a fast limb movement towards it is still under way. If an
interception point is fixated before either the target or the hand
reaches it, as van der Meer et al. (1994) demonstrated in babies, then a
form of predictive control is involved. This method has been used to
study the control of interception in fast ball games, recording a
player's direction of gaze as he or she moves to intercept the ball with
a bat. Land and Furneaux (1997) found that amateur table tennis players
generally maintained fixation of the ball during play with a series of
saccades, but

364

VISUAL PERCEPTION

that at certain points they would make eye movements that anticipated
the ball's trajectory. Just after the opponent's strike, a player often
fixates the place where the ball will bounce on his or her own side of
the table (the "bounce point"). The saccade to fixate the bounce point
begins approximately 200 ms before the ball actually reaches it,
implying that the player is able to make a fast prediction from the
ball's trajectory as it passes over the net. In cricket, batsmen are
able to predict bounce points in a similar way to table tennis players
(Land & McLeod, 2000). A batsman fixates the ball as it leaves the
bowler's hand, but then makes a saccade to a point on the ground before
the ball bounces there. Land and McLeod found that expert batsmen begin
these saccades sooner in the course of the ball's flight than less
accomplished players. Part of the skill of a table tennis player or
cricketer therefore lies in extracting enough information from the
trajectory of a ball in the first few hundred milliseconds after it
leaves the opponent's bat or the bowler's hand to predict where it will
bounce. This may be important information because it allows the player
to fixate the bounce point and so gain accuracy in obtaining the
information about the ball's subsequent path that is needed to time the
swing of the bat. In cricket, some balls may be bowled so fast, and
bounce so close to the batsman, that there is not enough time to obtain
and use such information (McLeod, 1987). In this situation, the ball can
only be hit by predicting the bounce point and moving the bat close
enough to it to have at least a chance of intercepting the ball after it
bounces. We do not yet know what information table tennis or cricket
players obtain from the initial trajectory of a ball in order to predict
where it will bounce. It is possible that predictions can be based on
more information than just the path of the ball. In cricket, Regan
(1992) argues that an expert batsman learns the characteristics of an
individual bowler's throwing style during the first few deliveries of
the ball, picking up individual cues in the bowler's actions that
predict to some extent where the ball will bounce. By adding this to
information from the ball's trajectory, the

batsman can gain accuracy in predicting where the ball will bounce.
Regan goes on to suggest that expert bowlers can in turn exploit this
strategy by first delivering a series of balls in a standard way,
allowing the batsman to build up a predictive model. Then, the bowler
uses one of several surreptitious techniques to deliver the ball at the
same angle but at a slightly different speed, so that it bounces in
front of or behind the point that the batsman expects. This skill is
known as the "art of flight", and seems to exploit the limited ability
of a batsman to predict the bounce point of the ball on the basis of its
trajectory alone. Using cues from the bowler's typical pattern of
movement when delivering the ball improves the batsman's predictions,
but also leaves them open to manipulation by an expert bowler's
deceptive tactics. The skilled control of eye movements in ball games
seems to be acquired unconsciously, and indeed sometimes in spite of
advice or instructions to the contrary. Cricket and baseball players are
often advised to "keep their eye on the ball" when it is bowled or
pitched towards them. Like cricketers, baseball players actually only
follow this advice during the early part of the balls' flight towards
them. Even professional players only maintain fixation of the ball until
it is about 2 m away from them (Bahill & LaRitz, 1984); after this point
the angular speed of the ball is too high for tracking to be possible.
Results such as these raise intriguing questions about the relationship
between conscious monitoring of performance and the control processes
that underlie it. For example, does a deliberate attempt to fixate the
ball actually help a player to acquire the skill of making anticipatory
saccades? Would attempts to learn the skill deliberately be effective,
or might they even be detrimental to performance? Similar questions
arise in the case of longjumping discussed in Chapter 11 (p. 334).
Longjumpers train by practising standard run-ups to the take-off board,
in the belief that this allows maximum accuracy in reaching the board.
As Lee et al. (1982) demonstrated, however, the skill relies on large
adjustments in the length of the last few strides, of which the jumper
may not be aware.

12. VISION AND THE TIMING OF ACTIONS

CONCLUSIONS Throughout this chapter, we have been concerned with the
visual control of interception; how animals and people control the
timing of their body and limb movements so as to make contact with
objects and surfaces in the most effective ways. The rapid growth of
interest in this problem over the last 20 to 30 years was certainly
prompted by Gibson's approach to visual perception, and specifically by
the expectation that variables of optic flow could be identified that
directly specify correct timing of actions. Lee's (1980b) analysis of
time to contact information available in optic flow was a landmark in
this approach and set in motion a large body of theoretical and
experimental work, which we have sketched in outline in this chapter.
One general theme emerging from this research is that there is no single
solution to the problem of visual timing of actions that is

365

generally powerful across many contexts. The argument that optical
information overspecifies the structure of the world and an observer's
movement in it (Cutting, 1986) applies here just as it does in the case
of perceiving and controlling heading (see Chapter 11, p. 343). In most
natural situations where an action must be controlled visually, multiple
sources of timing information are available, each subject to a different
set of constraints on its reliability. Research is now beginning to
identify ways in which animals and people select and integrate these
different sources of information, in accordance with both general and
situation-specific constraints. Perhaps the cleverness of the systems
linking vision to the control of action lies not in sensitivity to
powerful, high-level invariant properties of optic flow, but in the
ability to make fast, adaptive switches between different combinations
of optical variables according to the demands of each situation and
task.

Page Intentionally Left Blank

13 Perception of the Social World

ition that we described in Part II of this book, treating it as a means
of gaining knowledge of the world around us. In this chapter, we will
describe some areas of research that have given more emphasis to the
role of social perception in the moment-by-moment control of actions
towards other people. Because we share this aspect of social perception
to some extent with other species, we will begin by considering how
animals detect the behaviour of other animals. We will then ask how
people interpret simple displays in causal and social terms, and how
they decipher social signals from other people---particularly from their
faces---that help shape and guide their social interactions.

In Chapters 11 and 12 we considered how animals and people use vision to
control their actions in the physical world of surrounding surfaces and
objects. For people and for many animals, vision is also important in
moving through the animate world of other creatures with which they must
interact in some way. For animals, these can include prey or predators,
and members of their own species with which they mate, fight, or
cooperate. For people, the social world is more complex still. Our
interactions with other people are guided by shared knowledge and
understanding, and mediated largely through language. Even so, they do
rely on the same basic abilities that some animals possess; to recognise
the identities, actions, and intentions of others and to respond
appropriately. Although we obtain much of this information from what
other people say to us, visual perception of their actions is also
important, especially in situations where we adjust our own behaviour
rapidly in response. Imagine for example a football (soccer) player
watching the glances and changes in posture and running direction of
other players, and using this information to make rapid decisions about
where to run, how to kick the ball, and so on. Much research on "social
perception"---the perception of other people's characteristics such as
identity or personality---falls within the trad-

PERCEIVING OTHER ANIMALS' BEHAVIOUR In Chapter 9, we saw examples of how
insects, fish, and birds use simple visual mechanisms to recognise other
members of their species, relying on coloured markings, stereotyped
movements, or other cues. Many animals need to do more than just
recognise conspecifics, and must also make fine discriminations between
the different patterns of behaviour that they perform during a social
interaction, such as courtship or a 367

368

VISUAL PERCEPTION

Threat (a) and appeasement (b) postures of the lesser black-backed gull.
Drawn from photographs by N. Tinbergen in Manning (1978).

territorial dispute. One animal may need information about another's
posture---the orientations of different parts of its body relative to
each other. For example, the angles of a dog's ears relative to its
head, and of its tail relative to its body, provide information about
its aggressiveness. Similarly, gulls involved in aggressive encounters
threaten other gulls by adopting an upright posture with the bill
pointing downwards and the wings held forwards, and signal submission
with the opposite, crouching posture (Figure 13.1). Although male
three-spined sticklebacks use a simple visual feature---the red belly of
another male---to detect a territorial intruder (see Chapter 9, p. 266),
they use more subtle information about a female stickleback's posture in
deciding whether to court her. Models of females swimming in a "head-up"
posture, with the back curved so as to raise the tail, elicit more
courtship approaches than models in other postures (Rowland, Grindle,
MacLaren, & Granquist, 2002). An animal may also need to be able to
detect the orientation relative to itself of another animal's whole body
or of a part of its body such as its head or a limb. An example is
provided by the aggressive displays of Siamese fighting fish. In a
threat display, a fish spreads its dorsal, tail, and anal fins (Figure
13.2). It may turn broadside to its opponent and lower and twitch its
pelvic fin (Figure 13.2c). At the same time it may beat its tail and
flashes of bright colour may occur on the tail and body. Alternatively,
it may face its oppon-

(a) and (b) Nondisplaying Siamese fighting fish. (c) Display posture
    with fins spread out and pelvic fin lowered.
(b) Display posture with gill covers opened. Adapted from Simpson
    (1968).

ent head-on and open its gill covers (Figure 13.2d). Simpson (1968)
analysed these encounters and discovered that a fish's behaviour is
influenced by the orientation of its opponent relative to itself. A fish
is more likely to turn to a

13. PERCEPTION OF THE SOCIAL WORLD

broadside orientation if its opponent is facing it than if it is
broadside, so that the two fish often take up a "T" shaped
configuration. Also, a fish is more likely to flicker its pelvic fin if
the opponent is facing it than if it is broadside. Perception of other
animals' behaviour can involve not only static configurations of the
limbs and body but also transformations of posture and orientation as
animals move during an interaction such as courtship or a territorial
contest. Turner (1964) carried out a simple experiment with chicks that
demonstrates this. Young chicks learn to feed by pecking at small
objects on the ground and they have a strong tendency to peck close to
the spot where the mother is pecking. Turner made a model hen (Figure
13.3) that could be made to "peck" at the ground and found that chicks
would approach it and peck around its bill as readily as they would
approach a real hen. This only happened when the model made "pecking"
movements, and the chicks were much slower to approach a stationary
model in either an upright or head-down posture. Less direct evidence
suggesting that animals detect changes in posture and orientation of
other animals is provided by experiments that demonstrate a close
temporal relationship between such a change and some specific response,
particularly if animals respond differently to different transformations
of posture that lead to the same end result. Davis (1975) filmed the
responses of small groups of pigeons when one of the group received a
mild footshock. The

Flat cardboard model of a hen. "Pecking" movements of the model elicit
pecking by chicks. Adapted from Turner (1964).

369

alarmed bird immediately took flight, followed in most cases by the
other birds in the group. The delay between the first bird and the next
one starting to take off was very short, typically 100 ms. Davis then
asked why it is common in normal circumstances to see one pigeon in a
group take flight while the others remain completely unaffected. He
compared films of take-offs that did and did not induce flight in other
pigeons and could find no visual or auditory differences between them.
The differences turned out to lie in the pigeon's behaviour immediately
before takeoff; if the bird crouched, stretched its neck, or looked
upwards just before taking off, other birds rarely responded. If these
movements did not occur, they did respond. Davis' (1975) experiments
demonstrate clearly that pigeons do not just recognise a snapshot-like
configuration of a bird with outspread wings, and suggest instead that
they can distinguish two different transformations over time of another
pigeon's posture. A similar response to other birds' behaviour occurs
when birds flying in a flock all execute a turn in the same direction
together. The apparent degree of synchrony of such turns is striking and
has even inspired speculation about thought transference in birds! There
is a rational explanation, which was discovered by Potts (1984), who
examined films of turns made by large flocks of dunlin in response to an
approaching object. Turns made by a whole flock of dunlin begin with a
turn by one bird, followed by a wave of turns in the same direction
spreading in all directions through the flock. The first birds to
respond to the initiator of the turn did so with a delay of 67 ms, well
above the reaction time of 38 ms recorded to a startling stimulus in
laboratory conditions. However, the average interval over the whole
flock between a bird's turn and that of its nearest neighbour was only
14 ms, which shows that the birds cannot simply be responding to their
neighbour's turns. Potts (1984) proposed instead that each bird detects
the wave of turns approaching it and times its turn to coincide with it,
in the same way as dancers in a chorus line do. This behaviour suggests
that animals are able to time their actions with respect not only to

370

VISUAL PERCEPTION

approaching physical objects or surfaces (as we saw in Chapter 12) but
also an approaching wave of other animals' movement. We have seen
examples from chicks, pigeons, and dunlin where transformations in
birds' postures are tightly locked to responses by other birds. In all
these cases, the interaction involved is brief. One individual responds
to another's movement by approaching to peck, by taking flight, or by
turning. Some social interactions between animals involve longer periods
of tightly meshed movement of two individuals, in which each
individual's changes in posture and orientation are closely followed by
the other. An example can be seen in the mating behaviour of a tropical
fish, the bluehead wrasse. When a female wrasse enters the territory of
a male, he performs a "circling" display, swimming round and round in a
tight circle above the female. Both fish then swim rapidly to the
surface and spawn, releasing egg and sperm cells into the water. To
fertilise the eggs, it is essential that both fish dash to the surface
and spawn simultaneously, and Dawkins and Guilford (1994) examined
videos of the circling display to find what information about the male's
behaviour could be used by the female to time her own behaviour
correctly. During the 10 seconds before he dashes to the surface, the
rate of flickering of the male's pelvic fins increases steadily. Since
there is little variability in this rate at any one time, it provides
reliable information for the female about "time to spawning", which
could be used to time her own dash to the surface. The mating behaviour
of bluehead wrasse involves a relatively simple meshing of the swimming
behaviour of two individuals. More complex situations, where many
actions are co-ordinated simultaneously, can be found in mammals, and
the rough and tumble play of two puppies provides a familiar example.
Golani (1976) analysed social interactions of this kind in more detail
from film records, and Figure 13.4 shows drawings that he made from
single frames of a film of two Tasmanian devils (dog-like marsupial
mammals) play-fighting. In this sequence, the animals roll and tumble
about in elaborate ways, but through much of the sequence a constant
relative

orientation of the animals' heads is maintained. Golani terms such a
constant relative configuration a "joint", around which the animals
move, and says (1976, p. 117): The heads of a pair of Tasmanian devils
'wag' their bodies into a multitude of postures and movements. In a
context of cheek-to-cheek joint maintenance, the two animals move in
unison as one kinetic chain. Another illustration comes from an analysis
of wolf social interaction carried out by Moran, Fentress, and Golani
(1981). They filmed "supplanting" interactions, in which a dominant wolf
approaches a subordinate one, they interact for a period, and then the
subordinate moves away. Moran et al. found that the relative
orientations of the two wolves' bodies fell into four main categories.
In each of these, as the animals moved, some aspects of their relative
orientations remained constant. Notice that these patterns identified in
the interactions of Tasmanian devils or wolves are not static displays
such as the threat displays of gulls or fighting fish described earlier.
Instead, they are descriptions of those aspects of the joint orientation
of two animals that remain constant as the animals move. For such
stability to occur, each animal must continually monitor the positions
of parts of the other's body relative to its own and adjust its own
movement to keep the appropriate variables constant. It is likely that
senses of smell, sound, and touch play a role in achieving this, but
likely too that it involves the use of vision to detect elaborate
transformations of posture and orientation. As well as raising questions
about perceptual processes, the close meshing of movement in social
interaction illustrated by Tasmanian devils and wolves poses interesting
problems for theories of the control of movement (see Chapter 10). An
indefinite number of different patterns of commands are sent to the
Tasmanian devil's muscles, all with the effect that its head keeps the
same joint orientation with the other animal's head. The concept of
"coalitional organisation"

13. PERCEPTION OF THE SOCIAL WORLD

371

Drawings from single frames of a film of two Tasmanian devils
play-fighting. Numbers refer to frames of film taken at 16 frames/s.
Until frame 249, the male (in the background until frame 147, in the
foreground after frame 212) keeps the female's head adjacent to his
right cheek. Reproduced from Golani (1976) with permission of the
publishers, Kluwer Academic/Plenum Publishers.

372

VISUAL PERCEPTION

which we outlined in Chapter 10 (p. 312) may be relevant to this
problem.

Perceiving paths of travel In interactions between animals, one or both
may move over the ground or through the air or water, as well as
changing their posture and orientation. What information can animals
obtain about each other by perceiving paths of travel? In some
situations, this may not be necessary, as when one animal pursues and
attempts to catch up with another. This can be achieved simply by
keeping moving as fast as possible while maintaining orientation towards
the target, as a fly does when pursuing a potential mate (Chapter 11,
p. 320). The visual physiology of the pursuing animal may force a
slightly more complex variant of the same tactic. Like many other birds
(see p. 19), the peregrine falcon has two foveas in each retina, and the
one providing maximum visual acuity projects approximately 45° to one
side of the axis of the head. In order to dive in a straight line on to
a prey animal while keeping its image fixated on this lateral fovea, it
would have to turn its head to one side. Tucker, Tucker, Akers, and
Enderson (2000) demonstrated that this posture causes a large increase
in drag that would slow the dive considerably. They went on to observe
falcons' dives and obtained evidence that they follow a "logarithmic
spiral" path, which would be expected if they kept the target fixated
and their head aligned with the path of the dive. In this way, falcons
presumably achieve the shortest dive time that is consistent with
keeping the prey fixated in order to maximise the accuracy of contact
with it. What tactics could a prey animal use to escape as quickly as
possible from a predator? The very simplest would be to move as fast as
possible while orienting away from it. Woodbury (1986) showed that the
escape behaviour of blue crabs is slightly more complex than this. These
animals swim in the shallow water of the intertidal zone and, if a
predator approaches, start to swim rapidly. Acting as a model predator,
Woodbury walked towards crabs and recorded the bearing they took when
they first responded to him. He discovered that they do not simply swim
directly away from a predator, but that their bearing is an

average of two component directions weighted differently. The stronger
component is a directly offshore bearing carrying the crab into deeper
and, presumably safer, water, while the weaker one is a bearing away
from the predator. Woodbury showed that the resulting escape route
maximises the distance offshore the crab attains before being
intercepted by the predator. Woodbury (1986) always approached crabs
directly, and so could not test whether they could discriminate between
different directions of a predator's movement. It is easy to see how
this information could be useful; the same predator moving directly
towards an animal is more dangerous than one the same distance away but
moving in some other direction. In order to make this discrimination
between different directions of motion relative to the observer, an
animal must be able to obtain information about the way that a
predator's position changes over time. An experiment testing for the
ability to discriminate directions of movement is reported by Burger and
Gochfeld (1981), who compared the responses of nesting gulls to people
either walking directly towards the nest or walking on a tangential
route passing a minimum of one metre from the nest. Herring gulls
nesting in the open showed alarm responses when the directly approaching
person was at a greater distance than a person travelling tangentially.
Clearly, the gulls did not detect just the instantaneous distance of the
person, but unfortunately it cannot be firmly concluded that they
detected direction of travel, since the directly approaching person
looked straight at the gulls, but the tangentially walking person did
not. It is therefore possible that the gulls discriminated different
orientations or directions of gaze of the person relative to themselves
(we will discuss the perception of gaze later in this chapter). A second
way in which we might expect animals to use information about another
animal's direction of travel is to intercept it. As we have already
seen, a simple way for one animal to catch up with another is to move
directly towards it, as a housefly does in pursuing a potential mate.
This would result in the approach path shown in Figure 13.5a. In order
to move straight to an

13. PERCEPTION OF THE SOCIAL WORLD

373

(a) Animal B approaches animal A by keeping its bearing at zero. (b) B
    moves straight to a point where it intercepts A's path of movement.
    Adapted from Menzel (1978).

interception point (Figure 13.5b), more complex control of the pursuer's
movement is required, as when an infant reaches for a moving toy
(Chapter 12, p. 363). If both animals are moving on straight paths at
constant speeds, this can be achieved relatively easily. In these
conditions, a pursuer can intercept another animal by moving so as to
reduce its distance from the target, while keeping it at a constant
angle to its own direction of movement. This tactic could be useful to
some predators, although it is vulnerable to evasive manoeuvres made by
the prey. In other situations, it may be necessary to use more complex
means of extrapolating from a target animal's path to predict an
interception point. There are strong hints from the behaviour of some
animals that they can move to intercept the paths of targets in the way
shown in Figure 13.5b. For example, dogs are strikingly accurate in
jumping to intercept a ball and catch it in their jaws, and in running
to intercept or head off the path of movement of a dog or person. They
will also run on ahead of their owner, looking back to check the owner's
path of travel and to adjust their own path accordingly. Chimpanzees
behave similarly (Menzel, 1978). If a number of chimps are travelling
together and there is a clear leader of the group, who determines its
direction of travel, other chimps will run on ahead and look back from
time to time to adjust their direction to keep on the same route as the
leader. Why should the ability to intercept other animals' paths of
movement be useful to a dog in catching prey or in social interaction?
Dogs are closely related to wolves, which, like lions and

other large carnivorous mammals, capture large prey by hunting in packs.
An individual dog or wolf would have little success in catching large
animals such as deer or caribou because it would be outdistanced or
injured by a kick from the intended prey. A dog's abilities to move in
relation to other animals' paths of movement are therefore likely to
have evolved for more complex tasks than just running to intercept
fleeing prey; in particular, for the task of co-operative pack hunting.
What information about prey and conspecifics do dogs or wolves need to
perform this task? Observations of pack-hunting predators have
demonstrated a striking degree of co-ordination between individual
animals. Lions fan out as they stalk their quarry (Schaller, 1972, Ch.
8), while Mech (1970, Ch. 7) describes wolves surrounding a caribou
standing at bay or pursuing a running caribou in single file. Behaviour
of this kind suggests that each animal in a pack is able to detect the
positions and paths of movement of both the prey and the other members
of the pack relative to itself, and to use this information to plan its
own path. It is clearly difficult to determine the specific information
used by pack-hunting animals, but some features of the behaviour of
sheepdogs provide evidence for some of the processes involved. The ways
in which a shepherd and a sheepdog control a group of sheep draws on
behavioural predispositions of dogs that evolved as part of pack-hunting
behaviour, and Vines (1981) has described how the trainer builds on
these predispositions when training a dog to respond to whistled
commands. Our main interest is in the

374

VISUAL PERCEPTION

behaviour shown by a naive dog towards a group of sheep; the behaviour
on which either packhunting skills or co-ordination with a shepherd is
built. There are two particularly interesting features of this
behaviour. First, an untrained dog tends to "herd" sheep, by circling
around them, moving from side to side while keeping a roughly constant
distance from them. The sheep draw closer together when a dog is near
and move as a group, keeping beyond a minimum distance from the dog.
Second, a naive dog tends to position itself on the opposite side of a
group of sheep from its trainer. If the trainer moves either to his
right or to his left, the dog matches his move so as to keep the group
of sheep directly between them. A shepherd exploits this tendency in
training by giving the right or left turn whistle while the dog makes
the appropriate turn relative to the sheep. In these situations, the dog
is moving so as to maintain its position relative to both the group of
sheep and the trainer. On its own, it moves about a good deal but keeps
a roughly constant distance from the sheep, while they keep a constant
(and much smaller) distance from each other and a minimum distance from
the dog. With the trainer present, the dog keeps the centre of the group
of sheep on a line between itself and the trainer. Are these rules
regulating position relative to sheep and trainer part of a pack-hunting
strategy? Predators such as dogs or wolves stand little chance of taking
an animal such as a sheep from a group without risking injury from other
prey.

Two dogs (solid circles) circling a group of sheep (open circles) in an
uncoordinated way. There is always a consistent direction in which the
sheep can move to escape from both dogs.

They therefore face the problem of splitting off one sheep from its
group. Once this is done, they can move between it and the rest of the
group and then attack it. To achieve this, however, they must overcome
the sheep's strong tendency to keep close to other sheep. The dog's
tendency to keep a position opposite the trainer gives a clue as to how
two dogs might be able to break up a group of sheep and split one off.
The chances of this happening will be greater if they can make the sheep
mill about and increase their distances from neighbouring sheep. Two
dogs circling about a group of sheep in an uncoordinated way would not
achieve this to any extent, as most of the time the sheep would be able
to move as a group away from both dogs at once, maintaining close
contact as they do so (Figure 13.6). If the dogs maintain positions
opposite each other as they circle about, however, there will always be
two directions in which each sheep could move to escape from the dogs
(Figure 13.7). The chances of splitting the group of sheep in two, or of
splitting one off from the rest, will therefore be greater. All that
needs to be added is for the dogs to detect a large gap between one
sheep and the others and drive a wedge between them by running into the
gap. A shepherd "singling" one sheep off from a group works in this way,
whistling a command to his dog to run towards him into a gap between one
sheep and the rest. These abilities to detect the positions and

13. PERCEPTION OF THE SOCIAL WORLD

375

Two dogs circling a group of sheep and maintaining positions
diametrically opposite each other (a). As the dogs move, there are two
possible escape routes for the sheep. In (b), one sheep moves in the
opposite direction to the rest of the flock and is then pursued by the
dogs (c,d).

paths of movement of both prey and conspecifics provide one component of
dogs' pack-hunting strategy, and no doubt abilities to detect further
aspects of other animals' behaviour are also involved. Sheep are
certainly able to detect the orientation and posture of dogs, keeping
closer contact if a dog stares at them fixedly in a tense posture, and
it is likely that dogs and other predators are able to recognise similar
aspects of prey behaviour.

Perceiving other animals' gaze As we will see later in this chapter,
people are able to perceive accurately the direction of other people's
gaze, and use this information in a variety of ways during social
interactions. Interest in the evolutionary origins of this ability has
stimulated research into the perception of gaze in other species. In the
terms that we have been using here, detecting another individual's
direction of gaze is just one case of detecting the orientation of parts

of its body relative to the observer; in this case, the orientation of
the axes of the eyes. Many animals are able to detect when another
animal is fixating them, and respond fearfully or aggressively when this
happens. For example, when a group of jays feeds, the dominant bird eats
first while the others mill about near the food. If a subordinate
approaches closely, the dominant turns its head to fixate it either in
its frontal or its lateral visual field. The subordinate often hops
backwards when this happens, and is more likely to do so if the dominant
bird fixates it frontally (Bossema & Burgler, 1980). The same thing may
happen with potential predators as well as with conspecifics. Ristau
(1991) observed the alarm responses of plovers to human intruders
walking near their nests, and found that these were more prolonged when
the intruders looked in the direction of the nest than when they looked
away from it. There are two reasons why responses to being

376

VISUAL PERCEPTION

fixated by another animal's head and eyes are so widespread. First,
predators as diverse as preying mantids and cats orient their heads
towards prey before striking at it, presumably as part of a mechanism
for controlling the direction of the strike. Eyes and head pointing
towards an animal can therefore provide it with information that it is
in immediate danger. Second, if an animal with a specialised retinal
area such as a fovea fixates another, this signals to the second animal
that the first is better able to obtain information about its identity
or behaviour, which would be useful in an aggressive encounter. There
are various ways in which fixation by another animal could be specified
by optical information. If both head and eyes are oriented towards the
observer, the image of the face will be symmetrical, and this symmetry
will be invariant with size, distance, and direction. Hampton (1994)
tested this possibility in sparrows, observing their escape responses
when presented with model human heads. He found that the birds responded
more strongly to the heads when both eyes were visible to them than when
only one was, but that breaking the symmetry of the model face by moving
the nose had no effect. These results imply that sparrows are not
sensitive to the symmetry of a face, but respond simply to the presence
of a pair of eyes. This will sometimes lead to false alarms, when a
predator fixates an object to one side of the bird, but natural
selection has presumably favoured a fast, simple perceptual mechanism in
this vulnerable species. Primates are able to use gaze information in
much more sophisticated ways than just to avoid attacks by predators, or
to make and detect threats. Observations of monkeys and apes show that
they can make complex decisions about the best tactics to use in social
interactions, and that these often rely on being able to obtain subtle
information about other individuals' directions of gaze. For example, a
subordinate monkey or chimpanzee that is trying to obtain food, or an
opportunity to mate, may risk being detected by a dominant, who will
often intervene to attack and drive it off. In these circumstances, a
subordinate will carefully make brief glances towards the dominant and
wait until its gaze is averted from

the food or potential mate before making a move (for examples of
observations of this kind, see Whiten & Byrne, 1988). The more elaborate
uses that primates make of gaze information suggest that they are able
to perceive more complex properties of gaze than animals such as
sparrows, and experimental evidence confirms that this is so. Monkeys
are able to detect small deviations from fixation, or "sideways"
fixation with the eyes but not the head, and so clearly must be
sensitive to more complex optical information specifying direction of
gaze than are sparrows (Campbell, Heywood, Cowey, Regard, & Landis,
1990). The same is true of people, as we will see later in this chapter
(p. 390). Primates are also able to detect not only the orientation of
another individual's gaze relative to themselves, but also its
orientation relative to objects in the surroundings. In our own case, we
know from everyday experience that if someone is not looking straight at
us we can tell with reasonable accuracy where in the surroundings their
gaze is directed, and can often identify its target. Tomasello, Call,
and Hare (1998) tested for this ability in chimpanzees and in monkeys of
several different species. In their experiments, they found that animals
were more likely to shift their gaze to fixate an interesting object (a
juicy orange) if another individual nearby had just done the same than
if they were alone. On its own, Tomasello et al.'s (1998) result does
not prove that a monkey or chimpanzee can detect the location of the
target of another animal's gaze. It is possible that it simply detects
whether the other animal is looking to its own right or left, then
swings its gaze in that direction until it encounters some interesting
object, and then stops to fixate it. If so, then there is a situation
where we would not expect the two animals to end by fixating the same
object. This is where another attractive object lies in the path of the
second animal's sweep of its gaze and distracts it. Tomasello, Hare, and
Agnetta (1999) tested this prediction experimentally and found that
chimpanzees were generally not affected by a distracting object, but
shifted their gaze without interruption to the same target that another
chimp was fixating. The implication is that one

13. PERCEPTION OF THE SOCIAL WORLD

chimp can obtain enough information from another's gaze to plan ahead
the direction and size of a shift of gaze in order to arrive at a
target. The studies reviewed above do not distinguish the cues that
allow primates to respond appropriately to gaze cues---it seems most
likely to be shifts of the head and eyes that govern responding rather
than eye gaze alone. Kobayashi and Kohshima (1997) noted that human eyes
alone among the primates had a high contrast between the iris and sclera
which would make human eye gaze a very visible cue, but other primate
eye gaze difficult to see. Moreover, research on the use of human gaze
cues by primates suggests that capuchin monkeys cannot readily learn to
associate a human eye-gaze cue with food location, but can do the same
task when location is signalled by turns of head and eyes together
(Itakura & Anderson, 1996). In contrast, chimpanzees appear able to use
human eye gaze alone as a cue (Itakura & Tanaka, 1998). Something is
known of the physiological basis of gaze perception in primates. We
mentioned in Chapter 3 (p. 61) that cells in the temporal cortex of
monkeys respond differently to a face seen in different views
(full-face, three-quarters, etc.). Perrett, Hietanen, Oram, and Benson
(1992) have further tested cells of this kind, in the superior temporal
sulcus (STS), with stimuli in which both eye and head orientation vary,
and have shown that they are best described as selective for another
animal's direction of gaze relative to the perceiver. The activity of
such a cell may signal for example that another monkey is directing its
attention to the viewer's left, whatever combination of body, head, and
eye orientation is involved. Area STS of the monkey brain has a wider
role in social perception than just the detection of gaze direction.
Other experiments have discovered cells in this area that respond to
movements of a person's head as a monkey watches them, and are selective
for a particular direction of movement (e.g., ventral flexion, downwards
towards the chest). The response to this head movement may remain
constant despite changes in viewing direction, but movements in other
directions, or a static face, do not elicit a response

377

(Hasselmo, Rolls, Baylis, & Nalwa, 1989). Other cells may be selective
for particular facial expressions and movements. Parts of STS have
connections with areas in both the dorsal and ventral visual pathways
(Chapter 3, p. 58) and so may have a role in the integration of
information about form and motion needed to analyse other animals'
actions (Oram & Perrett, 1994). Output pathways from STS in turn connect
to other brain structures that are involved in the control of emotion
and social behaviour (Allison, Puce, & McCarthy, 2000). While it is
clear that at least some primates can use gaze cues from conspecifics
effectively, it is not clear what kind of meaning they derive from these
cues. When an adult human sees another looking at an object, the
observer may infer that the one looked at is attending to, thinking
about, longing for, or even intending to steal the object. Before we
discuss human gaze perception, however, we must first describe how
humans interpret the movements of much simpler displays since, as we
will see, there are connections between such abilities and other aspects
of social perception.

HUMAN PERCEPTION OF ANIMATE MOTION In Chapter 8, we described how the
perception of motion can be studied using displays made up of many small
moving dots. For example, experiments have determined the conditions in
which dots are seen to move coherently rather than in a random jumble
(p. 228). Displays made up of moving dots or other simple elements can
yield quite different perceptions, however. An observer may interpret
the display as an animate form, or may interpret individual moving
elements as having independent causal or even deliberate effects on
other elements. The implication seems to be that patterns of relative
motion between simple shapes can, in the absence of any detailed
information about form, give rise to perception of some properties of
the social world. In this part of the chapter, we describe these
perceptual and interpretative effects before returning to the topic

378

VISUAL PERCEPTION

of human gaze perception and other aspects of the social perception of
faces.

Biological motion The most dramatic demonstrations of the way in which
simple moving elements can be interpreted meaningfully were produced by
Johansson (1973) in his biological "point-light" motion displays.
Johansson produced films of people walking, running, and dancing in
which the only visible features were lights attached to the actors'
joints. Lights might be attached to the shoulders, elbows, wrists, hips,
knees, and ankles, to form a total of 12 moving lights in the dynamic
displays (see Figure 13.8). Such a display is easy to produce by
wrapping reflectant tape around the joints and filming with a video
camera set up to pick up only high contrast. In the film that results,
all information about the contour of the human figure has been removed.
If a still frame of such a film is shown, it looks like a meaningless
jumble of dots, or, at best, a Christmas tree. However, as soon as

Lights are attached to the joints of an actor who runs in the dark. The
changing pattern of lights is immediately interpreted as a human figure
in motion.

the actor is shown moving, the impression is immediately of a human
figure. The perception of a moving human can be achieved with as little
as 100 ms of film, or with as few as six lights shown. Not only can the
figure be clearly seen (with the invisible contours of arms and legs
present in a ghostly way), but the posture, gait, and activities of the
actor can be clearly described. It is clear from the displays whether
the person is walking, running, jumping, limping, doing push-ups, or
dancing with a partner likewise portrayed by a set of moving lights.
Mather and West (1993b) have more recently shown that adults can
recognise accurately which of a number of species of animal is shown in
a moving point-light display, but are quite unable to identify the
creatures from static frames of the same displays. Biological motion can
exert significant topdown influences on low-level perceptual processes.
For example, Bülthoff, Sinha, and Bülthoff (1996) showed that subjects'
stereoscopic depth perception was distorted by their perception of the
dynamic human figure. Subjects were presented stereoscopically with
biological point-light displays and with control displays that could not
be seen as biological forms. The depth positions of dots were
systematically varied, and subjects were asked to report if specified
dots lay in the same depth plane. Subjects were more accurate with the
control than the biological sequences, where they tended to report dots
as being in the same depth plane when they came from the same limb.
Sensitivity to the figural coherence of such displays of biological
motion appears either to be innate, or to develop very quickly.
Bertenthal, Proffitt, and Cutting (1984) found that 3- and 5month-old
infants were able to discriminate between upright and inverted walking
point-light figures, although not between static frames from the upright
and inverted conditions. Infants at this age were also able to
discriminate a "coherent" point-light display of a person walking a
treadmill from the same display with the dot motions scrambled to form
what adults judged to look like "a swarm of bees". Runeson and Frykholm
(1981, 1983) demonstrated the extraordinary subtlety of perception

13. PERCEPTION OF THE SOCIAL WORLD

possible by adults viewing moving point-light displays. In their
experiments, they filmed the movements of actors throwing sand-bags to
targets at different distances. Runeson and Frykolm (1981) showed that
observers can accurately judge differences in the weight carried by an
actor, where both the actors and load together are depicted by a total
of 21 points of light. In later experiments (Runeson & Frykholm, 1983),
point lights were placed on the actors' joints, but no lights were
placed on the bags, so that observers viewing the film had no
information from the bags themselves about their motions. Nevertheless,
observers were extremely accurate at judging how far the actors had been
trying to throw the bag on each attempt. Johansson considered the
perception of biological motion in displays like these to be consistent
with what he termed a "perceptual vector analysis", applied
hierarchically. Let us take the case of an actor walking across a screen
in front of the observer. The entire configuration of moving dots has a
common motion component in the horizontal direction in which the actor
is moving. Other motions can be described relative or subordinate to
this common horizontal motion. At the next level in the hierarchy, the
shoulders and hips make slight undulatory motions that provide a second
level baseline of common motion against which the knee describes a
pendular motion. Once the pendular motion of the knee has been
partialled out, the ankle can be seen to describe a further pendular
motion about this. Thus the dynamic configuration can be resolved into a
set of hierarchical, relative motions of rigid limb segments. These
walking figure displays show how the visual system apparently "prefers"
to interpret moving elements as representing the end-points of rigid
structures in motion, even if the resulting rigid structures may then
appear to have quite complex motions in depth. It should be stressed
that Johansson considers the perceptual decoding principles of the
vector-analytic type, and the preference for rigid motions, to be
"hardwired" rather than derived from experience with real moving
objects. We might note at this point a similarity with Ullman's theory
(described in Chapter 8, p. 254), who likewise made use of a

379

rigidity assumption to interpret structure from moving point
configurations. However, Mather, Radford, and West (1992) have provided
an alternative interpretation of the perception of these displays. They
examined systematically the effects of omitting different dots from
"synthesised" walking displays. These displays are obtained not by
filming real actors but by using a mathematical model of human movement,
based on biomechanical data, to generate dot motion (see Runeson, 1994,
for a discussion of possible limitations in the use of such displays).
According to the hierarchical "pendulum" approach, omitting dots at the
hips and shoulders should cause most problems since these are needed to
anchor the remaining residual motions. Alternatively, if the fixed
end-points of rigid limb segments play a crucial role, then omitting
points from the knees and elbows might create perceptual problems, since
then neither the lowerlimb or upper-limb segments have their endpoints
specified. In contrast, however, Mather et al.'s (1992) results showed
very clearly that omitting the shoulder/hip or elbow/knee points made
virtually no difference to the accuracy with which walkers' motions
could be detected, compared with full displays, while omitting the wrist
and ankle points dramatically reduced performance. Mather et al. (1992)
suggest that the perception of human biological motion in these displays
arises not from a complex hierarchical analysis but from the recognition
of characteristic patterns of motion in parts of the display. The
extremities (wrists and ankles) move the furthest and thus provide
better evidence than do other parts of the display. Given this, it would
be interesting to establish how displays comprising only wrist and ankle
points compare with other possible combinations of only four points.
According to Mather et al., wrist/ankle displays should be perceived as
very similar to full 12-point displays. Cutting and his co-workers have
investigated how observers may detect subtle differences in gait from
biological motion displays. In preliminary work, Cutting and Kozlowski
(1977) showed that observers performed well above chance at identifying
themselves and their room-mates from such dynamic displays. In a number
of

380

VISUAL PERCEPTION

subsequent experiments (Barclay, Cutting, & Kozlowski, 1978; Kozlowski &
Cutting, 1977, 1978) they have gone on to show that observers are
60--70% accurate on average at detecting the sex of a walker from a
display. In order to judge sex to this accuracy observers need to see
about 2 seconds of the display, which corresponds to about two step
cycles, suggesting that such judgements rely on some dynamic invariant
rather than on static configurational cues. The detection of the sex of
a walker does not seem to depend crucially on any particular elements in
the display. Judgements can still be made above chance level if only
points on the upper body or lower body are illuminated (but see
Kozlowski & Cutting, 1978, for a reinterpretation of the lowerbody
findings), although performance is best when joints on both upper and
lower halves of the body are shown. Thus the information on which such
judgements are made appears to be given by some global invariant, rather
than by particular elements in the display. Barclay et al. (1978) began
the search for such an invariant with the observation that male and
female bodies differ in the ratio of shoulder width to hip width. Men
have broad shoulders and narrow hips compared with women. However, in
the kinds of displays typically used, where the actor walks across the
line of sight, only a single shoulder light and hip light are visible,
so this ratio cannot be detected. Therefore the ratio of shoulder width
to hip width cannot provide the basis for judgements of sex. This ratio
does have consequences for other aspects of the relative motion in the
display, however (Cutting, Proffitt, & Kozlowski, 1978). During
locomotion, the hips and shoulders work in opposition to one another.
When the right leg is forward in the step cycle, the right hip is also
forward relative to the right shoulder which is back. Likewise, when the
left leg is forward, so too is the left hip, with the left shoulder
back. The relative widths of the shoulders and hips should thus affect
the relative side to side motion of the hip and shoulder joints when
viewed from the side. A measure based on this relative swing was found
to correlate reasonably well with the consistency with which different
walkers were rated as male or female.

However, Cutting et al. (1978) went on to derive a more general
invariant from their displays which correlated better with the ratings
given to different walkers. This measure was the relative height of the
centre of moment of the moving walkers. The centre of moment is the
reference point around which all movement in all parts of the body have
regular geometric relations: it corresponds to the point where the three
planes of symmetry for a walker's motion coincide. Its relative location
can be determined by knowing only the relative widths (or relative
swings) of the hips and shoulders (see Figure 13.9). The centre of
moment for male walkers is lower than that for females, and therefore
provides a possible basis for judgements of sex. Cutting (1978) offered
some support for the validity of the centre of moment as a determinant
of gait perception by using synthesised pointlight displays in which the
centre of moment could be varied while holding all other variables
constant. The synthetic "male" and "female" walkers produced were
correctly identified on 82% of trials, although if the lights
corresponding to the hips and shoulders were omitted performance dropped
to about 59%, still above chance. These results are compatible with
those obtained with real walkers, where above chance, but reduced,
performance was obtained when some of the lights were removed. However,
the range of variation in the location of the centre of moment that
Cutting incorporated into his synthetic displays was much greater than
would be found in real walkers, limiting the generality of these results
(see also Runeson, 1994). Nevertheless this study did lend some support
to the idea that a simple biomechanical invariant, the centre of moment,
might be recovered from a display such as this and could be used to
specify reasonably accurately the sex of the walking figure. Mather and
Murdoch (1994) re-examined how people judge the sex of figures shown in
moving point-light displays. They again used synthesised displays of
"walkers", but these now appeared to walk towards or away from the
camera, rather than across the line of sight as is more usual in such
research. In these "front" and "rear" views, dynamic cues from shoulder
and hip sways

13. PERCEPTION OF THE SOCIAL WORLD

381

The relative location of the centre of motion for male and female
walkers. Adapted from Cutting and Proffitt (1981).

appear to be used to determine whether walkers seem male or female. Male
upper bodies move more from side to side than female ones, while female
hips move more from side to side than male ones. Mather and Murdoch
varied both the amount of sway and the overall torso shape in their
experiments, and showed that information about relative body sway
dominated the structural cues arising from torso shape (centre of
moment) when these were set into conflict. For example, a "male" body
shape (wide shoulders, narrow hips) with "female" sway (hips moving from
side to side more than shoulders) was categorised as female almost as
often as a display in which the two sources of information were
consistent. Mather and Murdoch suggest that in the side-views generally
employed by other researchers, where lateral sway is not perceptible, it
might still be the local velocity of the swing of shoulder and hip
lights that underlies the perception of gender from such displays,
rather than the more global invariant of centre of moment. The fact that
people see moving human figures in point-light displays in such an
immediate and compelling way suggests that there is a specialised module
in the visual system that is specifically sensitive to characteristics
of biological motion. Psychophysical evidence provides support for this
hypothesis, showing that the spatial and temporal extent of integration
of the motions of individual elements in point-light displays is
different depending on whether biological motion or rigid

translation is depicted (Neri, Morrone, & Burr, 1998). Functional
neuroimaging methods have provided converging evidence for an
anatomically distinct region processing biological motion. An area in
the superior temporal sulcus (STS) is activated by point-light displays
of biological motion but not by displays of other patterns of motion
(Grossman et al., 2000). The STS is also activated by many other
depictions of human motion, such as films or animations of movements of
the head, face, mouth, or hands (for a review, see Allison et al.,
2000). This evidence indicates a wider role for this region of the
cortex in perception of socially important events than just the
perception of gaze direction that we described earlier (see p. 377).
When people observe sequences of human movement, other cortical areas
are activated in addition to STS. The particular pattern of activation
is influenced by the movement depicted, and also by the task that the
observer is set. Different areas become involved in processing
biological motion depending on whether the movement is observed
passively, or particular movements have to be recognised, or the
observer is required to imitate the movements that they see (for a
review, see Decety & Grèzes, 1999). One particularly interesting result
from experiments of this kind is that observation of human movement
often activates areas within, or closely linked to, primary motor
cortex, the region that controls movements of the body through
descending connections to the spinal cord (e.g.,

382

VISUAL PERCEPTION

Buccino et al., 2001). Furthermore, areas in motor or premotor cortex
that are activated when a particular movement is observed are also
activated when a person performs the same movement themselves (e.g.,
Nishitani & Hari, 2000). This evidence from neuroimaging studies
converges with physiological results from monkeys. Gallese, Fadiga,
Fogassi, and Rizzolatti (1996) recorded from neurons in area F5 of the
frontal cortex, which receives input from visual areas in the parietal
cortex and has reciprocal connections with primary motor cortex. They
found that some F5 neurons responded strongly both when the monkey
observed a goal-directed action, such as another monkey reaching for and
grasping a raisin, and when the monkey performed the same action itself.
Gallese et al. called these cells "mirror neurons". It therefore appears
that in both monkeys and humans the representation of visual biological
motion and the planning of motor actions are carried out in a single
cortical system. Such a system provides a possible physiological basis
for theories of "common coding", which hold that perceptual and motor
processes share a common representational system (e.g., Prinz, 1997).
The evidence for a common neural representation of other individuals'
actions and of a person's own actions (a "mirror system") has also
supported theoretical arguments that the perception of others'
intentions relies on an internal simulation of their action, which helps
to predict its future course (e.g., Gallese & Goldman, 1998). We will
return to the perception of intentions later in this chapter, as we
explore some further perceptual effects of viewing displays of moving
elements. In research on the perception of biological motion, observers
view displays and report the motions present, or identify or otherwise
describe the structures (e.g., human figures) that give rise to these
motions. In the following section, we turn to experiments in which
observers view displays and report the causal properties that they
attribute to elements within them.

Perception of causality When we watch a football match we are in no
doubt about why the ball suddenly speeds up and

changes direction---it was kicked. That is, the change in the movement
of the ball was caused by the action of one of the players' feet.
Moreover, because the causal event arises from an animate object (the
player) we may also judge that the player intended to kick the ball.
Near the goal, we would find it trivial to conclude further that the
player intended to score a goal with this kick. Likewise, in boxing or
judo, we see the action of one of the combatants as causing the other to
fall to the floor, and the action as intentional. In this section, we
describe a body of work that suggests that causal and intentional
perceptions, including quite subtle social intentions, may be specified
directly by dynamic patterns of visual information. Michotte (1946,
translated 1963) experimented with simple displays. In one situation
(Figure 13.10a), subjects viewed a display in which a black square (A)
moved towards a red square (B) at constant speed. At the moment when A
came into contact with B, A stopped and B moved off, either at the same
speed or at an appreciably slower one. After a short time B also came to
rest. Michotte reports that in this situation observers see the black
square bump into the red square and set it into motion. "The impression
is clear; it is the blow given by A which makes B go, which produces B's
movement" (p. 20). This has been termed the "launching effect". In
another demonstration (Figure 13.10b) A again moves towards B, but
continues its course without changing speed. When the two objects
contact, B in turn moves off at the same speed as A until both objects
finally come to rest together. In this situation, Michotte describes the
impression as of A carrying B along or pushing it ahead. This effect is
known as "entraining". Michotte backed up his claim that phenomenal
causality is directly apprehended by demonstrating that the impression
is critically dependent on the temporal, spatial, and structural
properties of the display. In the first experiment described, if an
interval is introduced between A contacting B, and B moving off, the
impression of causing, or launching, B's movement is eradicated. If B
moves off faster than A then the impression is of B being "triggered"
rather than launched, while if

13. PERCEPTION OF THE SOCIAL WORLD

383

Frames of a film of the kinds of display used by Michotte: black square
A, red square B (shown here in white). (a) Launching; (b) Entraining;
(c) Display where B moves off at 90° and launching is not perceived.

B moves off more slowly it appears to be launched rather than triggered.
However, whether launching or triggering is seen also depends on the
length of the path that B subsequently follows as well as on the ratio
of the objects' speeds, an observation that was confirmed by Boyle
(1960). According to Michotte, the launching effect depended not only on
these spatial and temporal aspects but also on the similarity between
the paths of the motion of the two objects. As object B's path was
shifted in angle away from the direction of A's path so the reported
perceptions of launching declined. If B went off at a direction at right
angles to A (Figure 13.10c), Michotte claimed that launching was almost
never observed. While the impression of launching is thus crucially
dependent on the temporal and spatial parameters, Michotte claimed that
it was unaffected by the nature of the items used. If A was a wooden
ball, and B a small, brightly coloured circle, Michotte reported that
the launching effect was unchanged. Michotte regards this as important
evidence for the "direct" perception of causality, since if previous
experiences of cause and effect were responsible there should be no

reason to see a causal relationship between two quite dissimilar items.
While few would doubt that such causal impressions can be gained from
displays of the types used by Michotte, there has subsequently been some
doubt about the universality of observers' impressions. Michotte himself
was often vague about the precise numbers of subjects he tested, or the
instructions he gave them, and in places based strong claims on the
results obtained with a very small number of highly practised subjects.
Boyle (1960) reported having to discard 50% of his subjects on the basis
of a pre-test in which these subjects failed to report "launching" or
"entraining" from standard displays. Beasley (1968) assessed formally
the extent of individual differences in the perception of these displays
and reported that only 65% and 45% of his subjects responded
respectively to "launching" and "entraining" displays in causal terms.
Contrary to Michotte's claim, 45% did report causal impressions when
object B departed at 90 degrees from A in the launching display. In
addition, and again contrary to claims made by Michotte, Beasley found
that the nature of the objects used---squares, discs, or cars---did have
had an effect on the nature

384

VISUAL PERCEPTION

of the responses elicited. Such variability appeared to cast doubt on
Michotte's claims of the universality of such causal impressions, and on
the idea that such effects are perceived "directly". More recent
research into the perception of causality has used more rigorous
experimental techniques to investigate individual, experiential, and
display factors affecting the perception of causality. Schlottman and
Shanks (1992) have shown clear dissociations between factors that affect
the perception of causality and those that affect judgements of
contingency. In their experiments, they showed that extensive experience
during the experimental session did not affect the causal impressions
gained from the impacts, which suggests that learning---at least within
the limits of this experiment---does not affect causal impressions in
adults. Moreover, in a second experiment they found that the
predictiveness of a colour change did affect judgements of the necessity
of one event for the occurrence of another, but that the causal
impressions gained by the impacts themselves remained unaffected by
their predictive validity within the experiment. For example, if on some
trials object B moved away following a change in object A's colour, and
on other trials object B moved away following an impact by object A and
a change in its colour, and if object B never moved unless there was a
change in colour, then clearly it is the colour change rather than the
impacts that predicts whether object B will move. Nevertheless, the
causal impression given by "launching" trials that do include impacts is
not affected by their reduced necessity for the production of B's
movement. Schlottman and Shanks use these results to support the idea of
a "causality module" which is impervious to subjects' beliefs or learned
expectations in a particular situation. As we argue in Chapter 14, the
notion of "cognitively impenetrable" computational modules is an
important advance on that of "direct perception". Leslie (1984; Leslie &
Keeble, 1987) investigated the "directness" of the perception of
causality using a developmental approach and showed that by 27 weeks
infants can perceive causal relations such as launching. Leslie suggests
that causal perception results from a visual mechan-

ism that takes its input from lower-level motion processors, analyses
the submovements to produce higher-level descriptions of the spatial and
temporal properties of the event, and finally describes its causal
structure. Thus according to Leslie the causality module can be reduced
to elementary information-processing operations, but remains entirely
impervious to higher-level beliefs or expectations. In contrast,
however, Oakes (1994) provides more evidence of an influence of general
information-processing skills than Leslie's analysis would predict.
Oakes found that 7-month-olds are able to perceive causality in simple
displays but not in ones containing complex objects, and that older (10
months) infants are unable to maintain causal impressions if the
specific objects used change from trial to trial. Thus there is some
conflict in the research evidence over the extent to which causality
perception is automatic, or may be influenced by other information
processing and by subjective beliefs or strategies. In an attempt to
resolve these issues, Schlottmann and Anderson (1993) presented
observers with displays in which one object, A, collided with another,
B, and varied the spatial gap between A and B, the temporal delay at the
point of impact between A and B, and the ratio of the speeds of the two
objects' motions. They examined individual differences in the resulting
causal impressions as well as impressions of the naturalness of the
displayed launching events. They explain their data using an information
integration model in which there are two distinct processing
stages---valuation and integration. "Valuation" determines the weights
that people attach to different stimulus dimensions, and these may vary
from one person to another. On the other hand, subjects' processes of
integration across the different dimensions are remarkably invariant and
agree with Michotte's original conclusions. Schlottmann and Anderson
suggest that individual experience may affect the values placed on the
different dimensions, and thus allow some reconciliation between
Michotte and his critics.

Perception of social intentions Observers who view displays of the type
used by Michotte may describe the actions of the objects

13. PERCEPTION OF THE SOCIAL WORLD

as animate and may even use intentional terms to describe their actions.
Thus A may be seen to "kick" or to "shove" B, or B may be seen to
"escape" from A who is "chasing" it. Just as causality perception seems
to depend on particular characteristics of a display, so further
characteristics may give rise to the perception that objects are animate
(self-propelled) rather than inanimate (see Scholl & Tremoulet, 2001,
for a review). Here we focus specifically on a further property that may
be accorded to animate objects---intentionality. In everyday life
humans, and perhaps some apes, interpret the actions of animate beings
in terms of what they are trying to do. Moreover, the momentary
intentions that we observe may lead us to attribute enduring
dispositional traits to the animate entities we observe. If person A
kicks B, chases him, then kicks him again, A may be seen as a "bully".
The classic study of such attributional processes in perception was
conducted by Heider and Simmel (1944). They showed observers a film in
which two triangles of different sizes (the larger and smaller hereafter
referred to as T and t respectively) and a circle (c) were seen to move
in

385

the vicinity of a rectangular frame (the house), with a moveable flap in
one side (the door) (see Figure 13.11). The first few frames of the film
sequence depicted the following movements (illustrated roughly in Figure
13.11, and described here, as in the original article, in
"anthropomorphic" terms for simplicity): T moved toward the house,
opened the door, moved inside and closed the door. t and c appeared and
moved around near the door. T moved out of the house towards t; the two
fight, with T the winner; during the fight, c moved into the house.
Heider and Simmel showed the entire film to 34 subjects who were simply
asked to "write down what happened in the picture". All but one of their
subjects described the film in terms of the movements of animate beings.
A typical subject's description of the first few frames (pp. 246--247)
was: A man has planned to meet a girl and the girl comes along with
another man. The

A sequence of events from the early part of Heider and Simmel's (1944)
film. The film shows a big triangle, a small triangle, and a small
circle moving around near a box with a moveable flap (the door). See
text for a description of the event sequence.

386

VISUAL PERCEPTION

first man tells the second to go; the second tells the first, and he
shakes his head. Then the two men have a fight, and the girl starts to
go into the room to get out of the way and hesitates and finally goes
in. She apparently does not want to be with the first man.

and figural aspects of the display. Thus when T is seen to "hit" t, the
stimulus parameters are very similar to those in the "launching"
experiment of Michotte. T approaches t until it comes into contact with
it. Then T stands still while t starts to move in the same direction as
T's earlier movement (Heider & Simmel, 1944, p.253).

In a second experiment, Heider and Simmel asked their subjects to
interpret the movements of the figures as actions of persons and then to
answer a series of questions which included such items as "what kind of
a person is the big triangle?". Such questions were answered with high
consistency. Both triangles were usually perceived as male, with the
larger one seen as "aggressive", a "bully", "bad-tempered", etc. The
circle was seen as "frightened", "meek", or "helpless". Even in the
first experiment, where no specific direction to see the objects as
people was given, subjects tended to describe the objects as being of
different sex and with differing dispositions. Heider and Simmel's
original film confounded dynamic with structural properties of the
displays. It is not clear, for example, how much of the perceptual
interpretation is given by the relative sizes and shapes of the
protagonists (small circles versus small and large triangles) as opposed
to their movements. Berry and Misovich (1994) used a quantisation
technique to degrade the structural forms shown in the original Heider
and Simmel film while preserving their dynamic aspects, and report that
adults produced similar numbers of anthropomorphic terms in the
structurally degraded as in the original condition. However, when static
frames were shown from either the original or quantised versions then
the number of anthropomorphic terms dropped substantially, suggesting
that it is the dynamic characteristics of the displays that are critical
for their perception. The same pattern of results was observed with
young children, though in all conditions of the experiment young
children reported fewer anthropomorphic terms than did adults. These
recent investigations lend some support to Heider and Simmel's
suggestion that causal impressions were given by the spatial, temporal,

This phenomenal relationship is obviously determined by temporal
succession and spatial proximity. The good continuation of the line --
the fact that the direction of t's movement continues the direction of
T's probably plays a role in the convincing appearance of this apparent
energetic movement. The movement of T and the door that results in the
impression of T "opening" or "closing" the door are similar to
Michotte's "entraining" displays, since the movement of T is imparted to
the door by prolonged contact rather than sudden impact. The question
arises as to why it is always T who appears to push the door (rather
than the door pushing T). Heider and Simmel suggest that here the
interpretation is influenced by the context in which such movements
occur. The door never moves without contact from T or one of the other
shapes, whereas each of the shapes is seen to move in isolation. The
shapes are therefore seen as "animate" the door as "inanimate", which
resolves the ambiguity in the pushing action. To resolve ambiguity in
the interpretation of the movement in these displays, subjects may use a
combination of the stimulus parameters and the personality
characteristics which have been attributed to the display members. For
example, if two objects move together at the same distance apart, the
first may be seen to lead, with the second following, or the second may
be seen to chase the first, which is fleeing. The interpretation given
depends on the element that is seen as initiating the movement, and also
on the power relationships that exist between the "people" who are
moving. If T is seen as timid or cowardly, then t may seem to chase him
(some such reports were obtained when the original film was shown in

13. PERCEPTION OF THE SOCIAL WORLD

reverse). If T is seen as dictatorial or aggressive, then t may seem to
follow him (Heider & Simmel, 1944, p. 254). If one sees two animals
running in file through high grass, one will interpret these movements
in accordance with other data. If the one in front is a rabbit and the
one behind a dog, he will perceive a dog chasing a rabbit. If the first
one is a big rabbit and the second a small one, he will not see
"chasing" but "leading" and "following." Thus while Heider and Simmel
see some aspects of the interpretation as given by the stimulus
parameters, other aspects, while constrained by these features, will
additionally be influenced by the total context in which the individual
action is embedded. Intention as well as action is involved in the
interpretation of their film. If T is hitting t, then T wants to hurt t.
If T chases t into the house, then t may be trying to hide from T. Such
intentional attributions themselves influence the dispositions that are
accorded to the individual elements, and these dispositions may in turn
influence how a new action sequence is interpreted. These examples
illustrate that even in the perception of the movements of simple shapes
in a relatively neutral context, we see the application of quite complex
and subtle social attributional processes by humans. What we see
happening will depend not only on the momentary motions in the display,
but also on expectations built up over a sequence of
actions---expectations that are derived from our broader social
experiences. It would seem to be stretching the Gibsonian line too far
to say that all the qualities imparted to these simple objects are
specified in the light. The pattern of activity present doubtless
constrains the range of possible interpretations, but cannot specify
which interpretation will be given. Nevertheless, it is still
interesting to explore the ways in which different stimulus parameters
influence the perception of such displays. Heider and Simmel's study was
limited to a single film. Unlike Michotte, they made no attempt to vary
spatial or temporal parameters systematically.

387

Indeed the film that they constructed probably reflected their own
intuitions about the phenomenological processes they wished to study.
However, Bassili (1976) has conducted a study in which aspects of a
Heider and Simmel type of display were systematically varied. In
Bassili's computer-generated displays, a black and a white circle were
filmed undergoing various movements. Five different films were produced
which ranged from a "chase", in which the temporal and spatial
characteristics of the following (black) circle were tightly linked to
those of the leading (white) one, to a film in which both elements moved
randomly and independently about the screen. Thus the temporal and
spatial linking of the movements were progressively relaxed. Subjects
who viewed the films were initially required to "Describe what you saw
in one concise sentence" and then asked more specific questions about
their perceptions of the film. The effect of temporal contingency was
assessed by comparing the responses to two films, in both of which the
directions of the motions were random. In one, however, a change in
direction of one element was quickly followed by a change in direction
of the other. In the other no such temporal contingency held. It was
found that temporal contingency between the changes in direction of the
two figures was critical for the perception of an interaction between
them, while the motion configuration (the spatial contingencies) were an
important factor in the kind of interaction and the intentionality
attributed to the figures. For example, subjects were much more likely
to report that the black circle was chasing or following the white
circle, and to ascribe intention to either or both of the circles, when
the direction of the changes in the black circle's path were tightly
linked to those of the white circle. When the directions were random,
but temporally linked, subjects saw the circles as interacting in some
unspecified way, but were less likely to describe this interaction in
intentional terms. Dittrich and Lea (1994) produced simulations of
biologically meaningful motion sequences using simple displays of moving
letters in which subjects were asked to try to spot one letter acting

388

VISUAL PERCEPTION

as a "wolf" in pursuit of one of the other "sheep" letters, or in other
conditions a letter was acting as a "lamb" trying to catch up with its
mother "sheep". They found clear influences of dynamic variables on the
perception of intentional motions of this kind, with intentionality
appearing stronger when the "wolf/lamb" letter's path was more directly
related to its target, and being more salient when its speed was
relatively fast (wolf) rather than relatively slow (lamb). In these
experiments, Dittrich and Lea did not find significant effects of
instruction---performance did not differ according to whether the task
was described in intentional (wolf/sheep) or neutral (letters) terms.
Scholl and Tremoulet (2001) describe further recent studies that have
addressed the question of the perceptual determinants of animacy in
simple displays. It seems we cannot prevent ourselves perceiving even
the simplest of moving shapes in social terms, which go beyond the
perception of simple causality to the attribution of agency and intent.
As we will see in the next section, there are links between such human
tendencies and other aspects of social perception, particularly in the
area of human face perception.

HUMAN FACE PERCEPTION So far in this chapter, we have separated our
discussion of the perception of what another animal or person is doing
from questions about how individual identity is recognised, which we
discussed in Chapter 9. To some extent, this distinction is a real one.
We may use posture and gait to inform us of another's actions or even
intentions, but recognise individual identity through other sources such
as clothing, the voice, and particularly the face. However, the
distinction often breaks down, because both voice and face inform us not
just about identity but also about a person's intentions, desires, and
emotional state. Voice and face perception mediate social perception in
a number of different ways, telling us about what a person is doing and
feeling as well as telling us who they are. In this final section we

consider a range of different uses made of facial information within a
broadly ecological framework. We will not, in this chapter, deal with
the very large body of literature on how adults recognise the identities
of familiar faces. This topic was covered in Chapter 9, in the context
of object recognition more generally. Moreover, the processes that
underlie the recognition of familiar faces dissociate both
neuropsychologically, and logically, from those involved in other
aspects of face perception (see Young, 1998). Rather, in this chapter we
will focus on the perception of other aspects of the face that play a
role in interpersonal behaviour, and/or which have been analysed within
the ecological perception framework that guides this part of our book.
For a general introduction to the fast-expanding topic of face
perception, see Bruce and Young (1998), or the collection edited by
Bruce et al. (1992a).

Development of face perception There is now good evidence that face
learning in the infant is guided by some innate specifications of what
faces look like. Goren et al. (1975) showed that newborn infants (with
an average age of 9 minutes) would track schematic face-like patterns
more than control patterns with the same features rearranged, a result
that has been replicated by Johnson, Dziurawiec, Ellis, and Morton
(1991). This result suggests that human infants may come equipped with
knowledge of roughly what heads and faces look like, and this innate
knowledge may allow them to attend selectively to such objects so that
they can subsequently learn more about the appearance of their own
caregivers. Some such learning appears to take place very rapidly within
days of birth (Bushnell, Sai, & Mullen, 1989). Young babies also appear
to have some sophisticated abilities to respond appropriately to
different facial expressions. Field, Woodson, Greenberg, and Cohen
(1982) showed that infants aged 1--2 days looked less at faces whose
expressions remained constant than at faces whose expressions changed,
suggesting that the infants must have been able to discriminate between
the different expressions. Meltzoff and Moore (1977) found that neonates
would imitate facial

13. PERCEPTION OF THE SOCIAL WORLD

expressions such as mouth opening and tongue poking. The facility to
imitate requires that the infant is not only able to tell the difference
between two different expressions, but is able to map a particular seen
expression onto a particular pattern of muscle activity. Such a mapping
involves a rather sophisticated kind of expression-recognition ability,
although these demonstrations are somewhat controversial as they have
not always been replicated (e.g., McKenzie & Over, 1983). In addition to
neonatal following of head-like patterns, slightly older infants will
shift their attention to a location to which an adult face gazes. Hood,
Willen, and Driver (1998) showed 3month-old babies a computer-presented
face at the centre of their field of vision, whose eyes were then seen
to glance to the right or left. Infants were both quicker, and more
likely, to look at a peripherally located patterned target presented
just after the face, when the target was shown in the direction the face
had looked (cued location) than when it appeared on the opposite side
(uncued location). This effect was found only when the face disappeared
before the target appeared. When the central face remained in view,
infants were more likely to remain fixated on the face than to look to
the target wherever it appeared. This study shows that young infants can
use eye gaze shifts---perhaps orienting reflexively to the direction of
gaze---but also that their tendency to use such shifts may be inhibited
by the stronger tendency to keep fixating on the face itself. This may
be a reason why, for infants as for monkeys, movements of eyes and head
together are a more potent cue than eyes alone. When an adult turns
their head, the infant must turn even if only to maintain fixation on
the head itself. Moreover, the motion cues provided by head and eyes
turning together may be much more powerful than those from eyes alone.
The human infant thus appears to have an innate knowledge of "faceness"
plus, possibly, some innate or rapidly acquired knowledge of certain
facial gestures such as expressions and gaze, and how to map these onto
its own action patterns. Other face-processing abilities appear either
to be more gradually learned during the

389

first few months of life, and/or to rely on postnatal maturational
processes (see Flin & Dziurawiec, 1989, for a review). A general theory
of the development of face perception and recognition has been proposed
by Johnson & Morton (1991; see also Morton & Johnson, 1991) who argue
that the underlying processes are similar to those in imprinting in
young birds. In both cases, initial attention to conspecifics (the
following of mother hen by a chick, or of a human parent by an infant's
head and eye movements) is mediated by mid-brain mechanisms. In the case
of humans, this initial attentional system then allows a cortical system
to learn about the characteristics of particular individuals in the
environment.

Adult face perception We now turn to consider what is known about the
processes of face perception when fully developed in the adult. The
dynamic configuration of the human face is endowed with a number of
different kinds of meaning, all of which need to be extracted in the
course of social interaction. The extraction of these different kinds of
meaning must rely on the abstraction of different invariant and variant
information. In addition to identifying familiar faces, we can derive
other information even from unknown faces, and it is the information
available from unfamiliar faces that forms the focus of this section. We
can decide that a face looks young or old, male or female, hostile or
friendly. We notice that someone looks happy, or angry, and their
direction of gaze tells us to whom or to what they are directing their
attention. This information is important in our social interaction---we
may decide to go and talk to someone at a party because they look
attractive or interesting to us, we take account of a child's apparent
age when choosing topics of conversation, and so forth. Because the face
and head are mobile, we may identify structural information from a face
that remains invariant despite these transformations in pose and
expression. Alternatively, or additionally, these dynamic patterns
themselves may provide information. In this section we will review what
is known about the dynamic and/or structural information that

390

VISUAL PERCEPTION

allows us to make such socially relevant judgements about faces.

Perception of gaze For mature adults, gaze patterns provide an extremely
important and rich set of social signals that help to regulate
conversation as well as expressing intimacy and social control (see
Kleinke, 1986). The timing as well as direction of such facial gestures
may be crucial for their interpretation. A prolonged stare means
something different from a brief glance, for example. Conversants signal
their "turns" within a conversation by gaze, and gaze patterns may also
reveal to an observer where another person is attending, and whether
they are attending to things in the world or to their own thoughts. Even
very young children are able to interpret a face with eyes rolled
upwards as a person who is "thinking", and are able to use direction of
gaze to understand what a person may be thinking about (BaronCohen,
1994). Humans are very accurate at detecting changes in gaze direction.
For example, Watt (1992) describes studies by Gibson and Pick (1963) and
Cline (1967) showing that people could reliably discriminate deviations
in gaze of approximately 1 minute of arc of visual angle. Detecting gaze
direction from a full-face image might seem relatively straightforward,
since it could be achieved by assessing the degree of deviation of the
pupils from the axis of facial symmetry, but such a task becomes much
more complex when head angle also varies, particularly as perceived gaze
direction must take account of head angle in a rather subtle way. There
is evidence that head direction influences the perceived direction of
eye gaze (Gibson & Pick, 1963; see also Langton, Watt, & Bruce, 2000).
Moreover, Vecera and Johnson (1995) showed that sensitivity to a
deviation in gaze (of 0.1° of visual angle) was much greater for an
upright schematic than for an inverted or scrambled schematic face
display, suggesting that even a task that could be performed by
examining the symmetry of the eyes alone seems to make use in some way
of the frame of reference provided by the whole face. Recent research
has indicated that for adults,

like infants, a shift in gaze appears to trigger a shift in visual
attention reflexively. Langton and Bruce (1999) asked observers to press
the space bar on a computer keyboard whenever a target appeared in one
of four peripheral locations. Prior to the target, a central head was
seen to look up, down, left, or right, which might or might not
correspond to the direction of the target. The target was no more likely
to appear in a direction that had been cued than in a different
direction. However, even though the cue was not predictive of the
eventual location of the target, participants were faster to detect
targets in cued than in uncued locations. The cueing effect was
strongest at the shortest cue--target intervals and disappeared at
longer ones, suggesting that the cue acted to trigger a reflex action at
short locations but had no effect on more deliberate intentional
attention shifts that would take longer. So, an observer pressed the
space bar more rapidly when a target appeared to the right very soon
(100 ms) after the head had been seen looking to the right, but this
effect was not found when the interval was increased (1 s). Similar
findings have been reported for eye-gaze cues alone by Driver, Davis,
Ricciardelli, Kidd, Maxwell, and Baron-Cohen (1999) using images of real
faces and by Friesen and Kingstone (1998) with schematic faces. Other
research has also shown that adults cannot easily ignore cues to
direction from eyes and head even when their task makes such visual cues
strictly irrelevant. For example, Langton (2000) found that head and eye
cues could influence participants' speed of responding to the direction
signalled by a spoken voice. If the voice said "up", for example,
participants were slower to respond with the correct key-press when the
head pointed downwards compared with upwards. Langton independently
varied the direction pointed by head and eyes (e.g., a head could point
down, but eyes point up), and was able to show separate and independent
influences of eye gaze direction and head direction on responses to the
voice (see Figure 13.12). Both head direction and eye gaze appear to be
processed and used, and usually they would provide consistent rather
than conflicting signals. What is going on when we perceive and react

13. PERCEPTION OF THE SOCIAL WORLD

391

Variations in head angle and eye gaze direction. Both eye gaze and head
direction appear important in determining the direction of another's
attention. Reproduced from Langton (2000).

to another person's gaze? At one level, we can think about this just
like detecting gaze direction in animals---a combination of eyes, head,
and body posture tells us where another is directing their attention.
However, in humans at least, the perception of gaze has been seen to
play a fundamental role in further cognitive activities. First, the
detection of another's direction of attention allows the establishment
of joint attention between two people, which may play a key role in
conversation (for example, by disambiguating the referent in an
utterance such as "what's that?"). Second, according to some theorists
(e.g., see Baron-Cohen, 1994), the perception of another's gaze plays a
key role in understanding other mental states. We understand what
another person may be wanting or thinking about from their patterns of
eye contact. The ability to understand that other people may have mental
states different

from one's own develops during the first few years of life. Children
develop such abilities at about the same age (3--4 years) that they
describe displays such as Heider and Simmel's in terms of social goals.
Baron-Cohen (1994) has theorised that these more sophisticated
"mind-reading" abilities are built up on the basis of more primitive
perceptual abilities that include simpler skills such as detecting
intentionality and perceiving eye gaze. Langton et al. (2000) provide a
recent review and further discussion of the perception of gaze in
humans.

Perceiving facial expressions Knowing what someone is wanting or
thinking about may depend on their facial expression as well as
direction of gaze. Expressive movements of the face, such as smiles and
frowns, are complex, nonrigid motions that include stretching

392

VISUAL PERCEPTION

and bulging of different parts of the face, produced by complex sets of
muscles. Bassili (1978, 1979) has used a technique like Johansson's (see
p. 378), in which small illuminated spots are scattered over a face that
is then filmed in the dark, to show that observers can identify a "face"
from a moving configuration of lights without seeing any structural
information about the facial features. Not only can a "face" be
identified, but observers also have some success in identifying
different emotions portrayed in such displays. Quite specific
information about faces can be gleaned simply from the pattern of
transformations present, without any need for information about the form
of the face, just as human walkers can be identified in Johansson's
displays, without any detail of the form of their limbs. Bruce and
Valentine (1988) used displays like those of Bassili and confirmed that
participants were highly accurate at deciphering which of a small number
of expressive movements were shown in such point-light displays. Despite
the dynamic nature of facial expressions, most work in this area has
used photographs of posed (and static) expressions to determine how
accurately human observers can perceive the different emotions portrayed
(for a review see Ekman, 1982; Russell, 1994). People are fairly
accurate at assigning posed emotional expressions to one of a few fairly
broad categories, such as happiness, surprise, anger, and disgust. There
is some degree of universality in such judgements, as people from a
variety of literate and some pre-literate cultures judge such displays
in similar ways (Ekman & Oster, 1982; although see Russell, 1994, and
Ekman, 1994, for debate). While less is known about the accuracy with
which observers can judge spontaneous expressive movements, there is
evidence that at the very least, positive and negative emotions can be
distinguished in natural situations (Ekman, Friesen, & Ellsworth, 1982;
Russell, 1994). What processes might underlie our ability to judge
emotional expressions? One possibility is that information about
different facial "postures" is encoded and compared to some kind of
stored catalogue. A particular emotion or group of related emotional
states might be characterised by

the relative dispositions and shapes of the component parts of the face,
perhaps with respect to the major axis of symmetry, in a manner
analogous to the part-based theories of object recognition that we
discussed in Chapter 9 (e.g., Biederman, 1987a; Marr & Nishihara, 1978).
However, it would be difficult to apply such a scheme in natural
situations where there is continuous movement in the face. A better way
to describe the information that underlies expressive judgements might
be to make use of dynamic rather than static cues. Ekman and Friesen
(1982b) have developed a Facial Action Coding System (FACS) to describe
in detail the movements made by different parts of a face. The FACS
consists of an inventory of all the perceptually distinct actions that
can be produced by the facial muscles. Using such an inventory, we are
in a position to ask whether unique combinations of actions (independent
of who the actor is) underlie the perception of different emotions. The
kind of analysis is illustrated here for the eyebrows alone (Ekman,
1979). Figure 13.13 shows the distinguishable action units for the brows
and forehead together, and the distinguishable combinations of these
units. These patterns have been "frozen" for the purposes of
illustration, and it is important to emphasise that Ekman and Friesen
are concerned to code actions rather than configurations. Ekman has
shown that different action units are indeed involved in different
emotions. For example, action unit 1 alone, or with 4, indicates
sadness, 4 alone yields anger or distress, 1 with 2 gives surprise, and
1 + 2 + 4 gives fear. Ekman has thus shown that distinct patterns of
activity are related to changes in emotional state, and these may be the
patterns of activity that observers detect in the face. While
momentarily frozen expressions might be compared with some stored
catalogue of facial postures, it may be more profitable to think of
observers matching transformations in expression over time to some kind
of representation of dynamic as well as static form (cf. Freyd, 1987).
Expressions are never all-or-none, but are graded and blended. A
person's momentary expression of faint surprise may represent a point of
increasing or decreasing

13. PERCEPTION OF THE SOCIAL WORLD

393

The different action units for the brow and forehead identified by the
Facial Action Coding System (Ekman & Friesen, 1978). Action units 1, 2,
and 4 may occur alone (top) or in combination (bottom). The drawings
were obtained by tracing photographs. From Ekman (1979), "About brows:
Emotional and conversational signals". In M. von Cranach, K. Foppa, W.
Lepenies, and D. Ploog (Eds.), Human ethology. Reproduced with
permission of the publishers, Cambridge University Press.

amazement, so we need to know its relationship both to prior and
subsequent expressive movements, and to concurrent events in the world,
in order to interpret it properly. Moreover, even when interpreting a
single emotional category, the timing as well as the final posture of
the face movements is important. Some expressions flicker rapidly across
the face and last as little as 200 ms. Kamachi, Bruce, Mukaida, Gyoba,
Yoshikawa, and Akamatsu (2001) used expression sequences created by
"morphing" between a neutral face and an expressive endpoint, which
allowed the timing of the sequences to be varied while their spatial
content remained constant. Timing was shown to affect both the accuracy
of expression perception and the perceived intensity of the emotions.
For example, sadness is a slowly unfolding emotion, while surprise is a
rapid one, and altering the temporal properties affects the emotional
impact of such displays. Ekman and Friesen (1982a) analysed differences
between spontaneous and deliberate, or "deceptive", smiles and showed
that deceptive smiles are more asymmetrical in terms of the muscle
movements involved and are also timed

differently. Deceptive smiles have shorter onset times, irregular offset
times, and are either relatively short or long in duration compared with
spontaneous smiles. A more extensive investigation by Hess and Kleck
(1990) suggested that differences in timing between spontaneous and
posed expressions are clearer when actors are explicitly required to
pose an expression that is deceptive about their underlying emotional
state; for example to smile while watching a disgusting video film. If
there are physical differences between genuine and "deceptive"
expressions, it is reasonable to enquire whether observers can
distinguish one from the other. Ekman (1992) suggested that observers
can distinguish genuine from posed smiles from a series of face images,
although it is not clear whether this judgement can be made when the
smile is embedded in the richer context of everyday communication.

Categorising faces: Age, sex and attractiveness In Chapter 9 we
discussed theories of the representations underlying our ability to
recognise individual faces, and here we will consider how we categorise
faces in other ways that are important

394

VISUAL PERCEPTION

The profile on the right looks younger than the one on the left, and the
central one appears intermediate in age.

for our social interactions with people whom we may not know. How do we
decide whether a face is old or young, male or female, attractive or
unattractive? These are categories that can be derived even from
unfamiliar faces, and which may influence our behaviour towards them. We
have placed this discussion of facial categorisation here, rather than
in Chapter 9, because some of the most interesting work on some of these
topics has been conducted within the ecological psychology framework.
The most extensively researched example of an ecological approach to
face perception has examined the information that underlies ageing. Shaw
and Pittenger (1977) examined the non-rigid transformation that the
profile of a human head undergoes while it ages, and have identified
information that remains invariant under this transformation. Shaw and
Pittenger showed that people are very consistent at rank ordering
profile outlines according to their apparent relative age, suggesting
that head shape provides at least one of the sources of information we
use when establishing a person's age. Consider the set of profiles shown
in Figure 13.14. You will probably agree that the one on the right looks
"young" and the one on the left looks "old". How can we describe the
nature of the transformation that relates the older to the younger
profiles? Shaw and Pittenger demonstrated that the growth process
transforms the human head in a similar way to that which occurs in
dicotyledonous plants. The profile of a human head is very similar in
shape to a dicotyledonous structure (see Figure 13.15). Ignoring facial
detail, the shape is like an inverted heart with a rounded top---a
cardioid. Shaw, McIntyre, and Mace (1974) demon-

strated that a single transformation, if applied to the outline of the
skulls of infant, child, and adult, could map one skull continuously
onto the other. They hypothesised that there might be a cardioidal shape
invariant for growth space, with ageing representing cardioidal strain.
Strain is imposed on the bones of the skull by stresses produced by
growth of softer, highly elastic tissues. Pittenger and Shaw (1975)
tested the extent to which perceived changes in relative age level are
captured by a strain transformation as opposed to

The small dots show a regular cardioidal shape lying above a profile of
a human skull. Appropriate transformation of this shape gives a good fit
to the shape of the skull, as shown by the large black dots. Reprinted
from Shaw et al. (1974). Copyright © 1974 by Cornell University. Used by
permission of the publisher, Cornell University Press.

13. PERCEPTION OF THE SOCIAL WORLD

a shear transformation (which modifies the angle of the facial profile).
Subjects were shown a series of profiles produced by modifying a single
outline profile over seven levels of strain and five levels of shear
(Figure 13.16). They found that 91% of the relative age judgements made
by their subjects were consistent with the hypothesis that a strain
transformation was responsible for these perceived age changes, while
only 65% of the judgements were consistent with a shear transformation,
which confirmed their intuition that it was strain that was the
important determinant. In further experimental work they demonstrated
that observers were consistent in perceiving a profile with larger
strain as "older" than a different one with smaller strain, and that
they showed a high sensitivity in these judgements even when the pairs
of profiles differed to a very small degree. Finally, they showed that
sufficient structural invariants are preserved during growth to permit
the identification of heads at different age levels, despite the
remodelling produced by ageing. They asked subjects to select the
age-transformed skull profile that matched a target profile, from a set
of two in which the "foil" was the profile of a differ-

395

ent head transformed to the same degree (see Figure 13.17). Subjects
performed this task considerably better than chance. It thus appears
that the ageing transformation preserves invariant information that
might specify individual identity. We are indeed able to match pictures
of people taken at different ages, provided the age spans are not too
great (Seamon, 1982), and Shaw and his colleagues have shown how one
source of information---skull profile shape---might contribute to these
judgements. We have omitted all the mathematical detail from this
account of Shaw and Pittenger's work, and you are referred to their
articles for a full discussion of this (see also Todd, Mark, Shaw, &
Pittenger, 1980). It is worth pointing out here that the invariant that
they claim accounts for age transformations is topological rather than
metrical. The former requires a different kind of geometry from the
familiar Euclidean geometry we learn at school. The concept of "shape"
that emerges from a weaker (nonmetrical) geometry is qualitative rather
than quantitative, but may provide the right way to handle the changes
in shape provided by nonrigid transformations.

The series of profiles used by Pittenger and Shaw (1975). The profiles
were all formed from the same original, which was modified by five
different levels of shear (vertical axis) and seven different levels of
strain (horizontal axis) to give this set of 35. Reproduced from Shaw
and Pittenger (1977). Copyright © 1977 Lawrence Erlbaum Associates
Inc. Reprinted with permission.

396

VISUAL PERCEPTION

The skull outline at the top is the same as B, but agetransformed to a
different extent. Skull A is a different individual, at the same
transformational level as B. Reproduced from Shaw and Pittenger (1977).
Copyright © Lawrence Erlbaum Associates Inc. Reprinted with permission.

The early work on cardioidal strain that we have just reviewed was
rather restricted in its application of the growth transformation to
simple, line-drawn cranio-facial profiles, and as such said rather
little about the perception of age from normal faces. Mark and Todd
(1983) published an interesting extension of this work in which the
cardioidal strain transformation was applied in three dimensions to a
representation of the head of a 15-year-old girl (see Figure 13.18). A
computer-sculpted bust of the girl is shown to the right of Figure
13.18, and to the left is a bust that was cut from the same database,
age-transformed in a direction that should render it younger in
appearance. The vast majority of subjects indeed saw the transformed
version as younger, which led Mark and Todd to conclude that the strain
transformation could be perceived from more realistic, three-dimensional
heads. Bruce, Burton, Doyle, and Dench (1989) examined this claim more
extensively, using computer-aided design techniques that allow a
three-dimensional model to be constructed in wire-frame form and
displayed as a smooth surface using standard lighting models (e.g., see
Figure 13.19). Bruce et al. obtained a database of 3-D head measurements
from a laser-scan of an

adult head and age-transformed this to different extents in a direction
that should have rendered the resulting head younger in appearance.
Subjects were asked to judge which member of each pair of heads looked
younger. Their ability to do this was assessed as a function of the
amount of difference in age levels between the two heads shown, and the
views in which these were shown (two profiles, two three-quarter views,
or one profile and one three-quarter view). Subjects made relative age
judgements equally accurately when shown two different views (a profile
and a threequarter view) and when shown two identical views (e.g., two
profiles) that allowed comparison between the shapes of the occluding
contours. These results were consistent with Mark and Todd's (1983)
conclusion that subjects can detect the strain transformation in three
dimensions. However, Bruce et al. (1989) also found that many subjects
did not spontaneously see the supposedly "younger" versions of the head
as younger, but needed to be given some information about the range of
ages that were to be anticipated. Without this guidance, some subjects
saw the "younger" heads, for example, as belonging to "little old ladies
who had lost their teeth". It seems, then, that richer data structures
such as

13. PERCEPTION OF THE SOCIAL WORLD

397

(Right) A computer-sculpted bust of a girl aged about 15 years. (Left)
The bust that was cut after age-transforming the database in a direction
that should make the girl appear younger in age. From Mark and Todd
(1983), reprinted by permission of Psychonomic Society Inc.

(Left) A threedimensional database of a head obtained from
laser-scanning. (Right) The same head, age-transformed in a direction
that should make it appear younger.

3-D models of heads present wider opportunities for interpretation.
Cardioidal strain level is only one possible route to judging age and
does not necessarily lead to the "direct" perception of age level. Our
argument here is similar to that used against the "direct" perception of
causality in the displays used by Michotte. Because naive subjects are
not consistent in the impressions formed of heads with varying levels of
cardioidal strain, we

would argue that this is only one of a number of cues that constrain the
judgement of age. Such an interpretation would be consistent with
experiments by Burt and Perrett (1995), who examined the separate
contributions of surface colouration and head shape to the perception of
the relative age of faces. By using a morphing technique, they were able
to average together faces in a way that did not result in blurred

398

VISUAL PERCEPTION

images. In this way, they produced composite colour images of faces of
different chronological age bands, and found that the resulting
composites appeared to be of similar, though somewhat younger, age range
to the component faces. An "older" composite could be made to look older
still if the differences in colouration between it and a "younger"
composite were exaggerated, confirming the contribution of superficial
colour information about skin and hair colour to the perceived age of
faces. Just as the perception of age seems to depend on the use of
multiple sources of information, so does our ability to recognise the
sex of faces.

Human performance at the task of deciding whether faces are male or
female is impressive--- people are over 95% accurate at this task even
when faces have their hairstyles concealed, when faces are clean-shaven
and when cues from cosmetics are absent (see Figure 13.20). In two
extensive investigations, Burton, Bruce, and Dench (1993) and Bruce et
al. (1993) investigated the information that underlies such judgements.
Burton et al. (1993) conducted an investigation in which a large number
of measurements were made on different face features and other aspects
of face shape, and the statistics of these measures were investigated to
see which ones distinguished

You can easily tell whether these faces are male or female, despite the
absence of cues from hairstyle and facial hair. From Bruce and Young
(1998).

13. PERCEPTION OF THE SOCIAL WORLD

male from female faces. Burton et al. (1993) found that in order to
approach the 95% success rate achieved by human vision, discrimination
requires a combination of a large set of local cues (e.g., width and
spacing of eyebrows), and 3-D cues (e.g., nose shape). A parallel set of
experiments conducted by Bruce et al. (1993) gave results consistent
with the multiple determination of face sex by a range of different
cues. For example, when faces were displayed as 3-D surfaces devoid of
their usual pigmented features and texture (such as Figure 13.19)
performance at a sex decision task was considerably above chance, at
least when angled viewpoints were shown. On the other hand,
discrimination of sex from these displays was much less accurate than
was possible with photographs in which hairstyle was concealed. These
results suggest that features such as the eyebrows give important clues
about face gender, and that other aspects of face shape also contribute
to performance. The conclusion therefore appears to be that perception
of both the sex and the age of faces does not rely on a single, or
simple, "invariant", but rather a very large and complex set of cues
derived from different aspects of a face's appearance. Such a conclusion
is reinforced by a study by Hill, Bruce, and Akamatsu (1995) in which it
was possible to examine the relative contributions of the underlying
shape of the face alongside the superficial surface colour and texture
(as is picked up by a colour camera taking a picture of the face). Using
computer image-processing techniques it was possible to combine the
surface colour information from one face with the underlying shape of
another. Volunteers were asked to make categorical decisions about the
sex of the faces (half were female) or the race of the faces (half were
Japanese, half Caucasian), from the shape information alone, from colour
information alone, or from shape--colour combinations. As in Bruce et
al.'s (1993) results, people were about 75% accurate at deciding the sex
of faces from shape alone, but much more accurate when given the colour
information that presented such local features as eyebrows and face
texture (e.g., visible stubble in some male faces). When face shape and
colour were combined, decisions were

399

dominated by colour, but there was some influence of shape as well; for
example, a female colour image mapped to a male face shape was more
likely to be seen as male than if it were mapped onto a female face
shape. For decisions about the race of faces, subjects were considerably
more accurate when presented with shape alone, and more influenced by
the shape---particularly in angled views---when the shape was combined
with colour from a different race. Thus these studies also confirm the
use of multiple information sources in the determination of simple
categories of facial appearance. However, the cues that determine the
relative masculinity of femininity of faces and those that determine
their relative age are not independent. Faces that appear more feminine
in appearance share many features of relatively young, immature
"baby"-faces. High testosterone levels cause forward growth of the brow
ridge and increase the size of the jaw and lower face---features that
make faces appear more masculine. So, as children reach puberty, the
male sex hormones trigger facial growth to the masculine form. Female
faces remain more like their sexually immature version, although
oestrogen may encourage fat deposition on the face (see Penton-Voak &
Perrett, 2001, for an extensive review). Within a set of male or female
faces, however, there are variations in the degree to which they exhibit
more masculine or more feminine facial characteristics. Feminine faces
have more "baby-faced" characteristics, and it is possible for male
faces to have more or less of these "baby-faced" characteristics too.
Within the ecological approach to face perception, some researchers
(e.g., Berry & McArthur, 1986; see also the collection edited by Alley,
1988) have argued that invariant structural information perceived from
faces leads to the "direct" perception of subtle social attributes. For
example "baby-faced" adults, whose facial features are relatively low
down the face, are seen as more weak and submissive, and/or to have more
feminine characteristics than adults whose features are placed
relatively higher up the face. Berry and Landry (1997) asked a large
group of people to keep diaries of their social encounters for a week.
Those men among the

400

VISUAL PERCEPTION

sample whose faces had more "baby-faced" characteristics reported more
intimacy in their interactions but less dominance in opposite-sex
interactions than men with more mature (masculine) faces. A final
example of the way in which adults categorise unfamiliar faces lies in
our judgements about facial attractiveness. There is reasonable
agreement between individuals, and even across different racial groups
(e.g., see Perrett, May, & Yoshikawa, 1994) about which faces look more
and which less attractive, but what visual information forms the basis
of such judgements? Since Galton's work on facial composites (Galton,
1907), it has been reported that blending faces produces faces that are
more attractive than the individuals contributing to the blend. Such
observations have led to the suggestion that attractiveness is
averageness. However, early studies used photographic composites that
suffer from the blurring that arises from averaging together different
shapes, and it may be the softening that results from blurring that
gives rise to the increased attractiveness. Modern digital blending
reduces these problems, and using this technique, Langlois and Roggman
(1990) showed that the average of a set of faces was indeed rated as
more attractive than the contributing indi-

vidual faces. However, in a clever follow-up study, Perrett et
al. (1994) showed that the average of a subset of female faces that were
rated as attractive, was seen as more attractive than the average of the
whole set of faces (including the attractive subset). Moreover, if the
"attractive" average was compared with the "overall" average and its
difference exaggerated using caricaturing techniques, it became more
attractive still. This study found consistent findings using both male
and female observers, from Japanese and Caucasian populations, and using
both Japanese and Caucasian faces. Thus the study shows that there is
considerable cross-cultural universality in attractiveness judgements,
but that these judgements cannot solely be based on how average a face
looks. The studies reviewed above used female faces, and analysis of the
differences between more and less attractive female faces shows clear
linkages with those features that are characteristic of femininity, and
this in turn linked to baby-faces. Female faces with relatively high
brows, large eyes, small chins etc. are seen as more attractive, and
more feminine, and younger. We can take a particular face and make it
more feminine or more masculine in appearance using morph techniques
(see Figure 13.21). A more feminised

Masculinisation and feminisation through morphing and caricature. The
second face from the left is an average of many young adult male faces.
The second from the right is the average of many young adult females. In
the centre is the average of all the faces---an androgenous face. By
comparing the male average with the central face and exaggerating the
differences, a "supermale" (caricatured male) is produced (far left). In
the same way a "superfemale" caricature is obtained (far right) by
exaggerating the differences between the female average and the central
face. Images courtesy of Peter Hancock, University of Stirling.

13. PERCEPTION OF THE SOCIAL WORLD

401

Feminised (left) and masculinised (right) versions of an average male
face. Courtesy of Ian Penton-Voak, University of Stirling. Women often
find the feminised (left) version more attractive, though less masculine
in appearance.

female face will appear younger, and more attractive. But what about
male faces? Here the story is not so simple, but perhaps a little more
interesting. We have already outlined that male sex hormones lead to the
exaggeration of sexually dimorphic male features, the heavy brow ridge,
and large jaw. Such faces are rated as more masculine in appearance (see
Figures 13.21 and 13.22) but often not more attractive (see Penton-Voak
& Perrett, 2001, for a review). Penton-Voak and Perrett review their own
and other studies that generally show that women prefer slightly
feminised male faces---and this has been found with both European and
Japanese faces and with both European and Japanese observers. This seems
a little puzzling since this suggests a preference for faces associated
with lower levels of testoterone, and hence potentially less fertile
mates. However,

a simple analysis in terms of "optimal" levels of sex hormones ignores
the other attributes that are associated with more feminised males. Such
faces may be seen as more dependable and trustworthy mates in comparison
with the more masculine. Intriguingly, Penton-Voak and colleagues have
shown that female preferences for male faces shift during their
menstrual cycles, with preferences for relatively masculine males around
ovulation, and relatively feminine ones during the rest of the cycle.
These provocative findings demonstrate intriguing interactions between
subtle physical characteristics of a face, the social impacts of such
variations, and variations in the underlying physiological and/or
hormonal state of the perceiver. This example seems to us to provide a
rather interesting example of "affordances" in the social domain.

402

VISUAL PERCEPTION

CONCLUSIONS This chapter has briefly surveyed the perceptual bases of
social perception, suggesting ways in which the ecological approach to
perception might be applied to problems as diverse as hunting in packs
and judging a person's age. We concluded with a more detailed survey of
some of the processes of face perception in humans, and examined how an
ecological approach has contributed to this research area. We have
deliberately used a broad definition of the term

"ecological" here, rather than confining our discussion to research by
those taking a "direct" perception stance. Much of the work that has
been conducted on the ways in which the different shapes (masculine,
feminine) and postures (angry, happy) of faces are perceived has been
conducted within a broadly social and evolutionary framework rather than
a strictly "ecological perception" one---whatever the guiding framework,
it is only recently that very carefully controlled yet realistic full
colour images have been used, and very little of the work has paid
attention to perception of dynamic rather than static stimuli.

Part IV

Conclusions

Page Intentionally Left Blank

14 Contrasting Theories of Visual Perception

the assumption that visual perception creates a representation of the
world that can be probed by psychophysical methods. In contrast, as we
saw in Chapter 10, the Gibsonian or "ecological" approach argues that
perception and action are directly linked and that each must be
understood in the context of the other. From this standpoint, Parts II
and III of this book survey two bodies of research and theory that have
been built on fundamentally different theoretical foundations. So far,
we have not compared the cognitive and ecological approaches directly,
and an analysis of the points at issue between them will be our topic in
the next section.

In Parts II and III of this book, we reviewed theories and evidence that
contribute to understanding two broad functions of visual perception; to
provide knowledge and awareness of the world around us and to guide our
activities within it. This distinction is more than just an arbitrary
and convenient one, and there are two main reasons why it might be
adopted. First, Milner and Goodale's (1995) theory suggests that it
reflects the way that the brain handles visual information. If awareness
relies exclusively on processing in a ventral pathway, and visuomotor
control on processing in a dorsal pathway, it would make sense to
consider these two aspects of visual perception separately. As we saw in
Chapter 3, however, there is evidence that the two pathways interact
closely and are themselves divided into multiple subsystems. It is
therefore premature to base a fundamental distinction between two
functions of vision on current neurological evidence. Second, apart from
any physiological considerations, there are theoretical reasons for
treating awareness and action as distinct functions of vision. The short
historical surveys that we gave in Chapter 4 described how the
"informationprocessing" or "cognitive" approach is based on

COGNITIVE AND ECOLOGICAL THEORIES OF PERCEPTION One strong contrast
between cognitive and ecological theories is in the starting point for
visual perception that each assumes. Since the establishment of
geometric optics in the 17th century, almost all theories of visual
perception have assumed that explanations in psychological terms 405

406

VISUAL PERCEPTION

should begin with a retinal image, and recent cognitive approaches have
maintained that assumption. Gibson stood apart from this approach,
treating the starting point for vision as the optic array; a geometric
abstraction independent of a perceiver. What issues are at stake between
these two theoretical positions? Obviously, there is no argument over
the facts that a single-chambered eye forms an image on the retina and
that the image has to be there for vision to occur. The image is a
projection of a segment of the optic array and the spatiotemporal
pattern of light is reproduced in it, within limits imposed by the eye's
acuity. For many practical purposes, the terminology of image and optic
array can be used interchangeably, as we saw in Chapter 11 when we
discussed the relationship between retinal and optic flow. So, what
difference does it make to introduce the concept of the optic array? One
consequence is that it leads to a quite different conception of the
information available in light to a perceiver. The traditional view took
the input to be nothing more than a mosaic of points of light on the
retina, each characterised by its intensity and spectral composition.
Gibson attacked this belief on several grounds, but particularly because
it makes the mistake of describing the input for a perceiver in the same
terms as the input for a single photoreceptor. The input for a receptor
is a stream of photons, but the input for a perceiver is a pattern of
light extended over space and time. Gibson therefore argued that ways of
describing this pattern should be devised, and examples of the terms he
introduced are gradients and rates of flow of optic texture. Although
descriptions of spatial structure can be applied equally easily to an
optic array or to a retinal image, it is important to avoid artificially
"freezing" the retinal image and losing sight of the temporal pattern of
light in the changing optic array. Once the possibility of describing
the input for vision in terms of structure is accepted, it becomes
possible to ask what relationships hold between a perceiver's
environment and the structure of the optic array. Gibson's "ecological
optics" sets out to explore these relationships, and

we have seen examples of them throughout the third part of this book. In
the traditional view, there was no obvious place for such questions;
input was made up of elements of point intensity, and the intensity of
light falling on a single receptor provides no information about the
environment. That said, we would note that the contrast between these
two positions has weakened considerably over the years, in part because
of technical advances. Forty years ago, psychophysical investigations of
human vision worked with points of light or simple line figures as
stimuli, and this perhaps encouraged theorists to treat such things as
the real "elements" of the visual world. Modern computer graphics
techniques allow the use of enormously more sophisticated
stimuli---consider for example the displays described in Chapter 7
(p. 202) used to analyse how disparity, shading, occlusion, and motion
contribute to our perception of depth. The use of displays such as these
rests easily with the view that the input for vision is the dynamic
structure of light reaching the eyes, and not elements such as dots and
lines. Gibson might therefore have had little quarrel with modern
psychophysics about the nature of the input for vision, and about the
ability of spatiotemporal structure in light to specify simple
properties of the environment such as the distances, slants, and
textures of surfaces. Where conflict would certainly still arise is over
the answer to another question; are all objects and events in the
environment fully specified by the structure of light in the optic
array? Gibson and other ecological theorists claim that they are, and
that perception is therefore direct. We will next examine this claim by
contrasting it with the senses in which cognitive theories argue that
perception is indirect, or mediated.

Is perception mediated by algorithms? As we saw in Chapter 4, most
theories have argued that perception requires processes of inference to
supplement the apparently impoverished nature of the flat, static
retinal image. Although their details vary from one theory to another,
these processes of inference were held to

14. CONTRASTING THEORIES OF VISUAL PERCEPTION

mediate between retinal image and perception. Further, they often bore
some resemblance to familiar, consciously accessible mental operations
such as measuring, or scaling retinal images, retrieving information
from memory, or making logical deductions from evidence. The fact that
we are rarely aware of carrying out these operations when we see was
explained by arguing that they take place too quickly to be consciously
accessible, as Helmholtz (1866) expressed in the phrase "unconscious
inference". In his early work, Gibson was concerned with the perception
of the layout and distances of surfaces surrounding an observer and his
analysis of the structure of the optic array and optic flow led him to
the conclusion that mediation by processes of these kinds is
unnecessary. The long-standing belief to the contrary, he argued, was
based on a mistakenly restricted description of the input for vision.
Later, Gibson extended this conclusion further, and went on to argue
against any role for mediating or inferential processes in perception
and to claim that invariant properties of the optic array specifying all
structures and events in the environment are detected directly. Marr's
theory of vision and those following this computational tradition
departed to some extent from the traditional conception of the processes
that mediate perception (see Chapter 4). While accepting the possibility
that "top-down" processes involving memory or reasoning may operate,
their main aim is to account for visual perception in terms of
algorithms: processing rules that are implemented in the hardware of the
brain. Although this still involves decomposing perception into
subprocesses, they are no longer the intuitively familiar (but very
fast) mental operations of traditional theories such as Helmholtz's or
Gregory's. Instead, they are algorithms applied to various kinds of
representation of the retinal image, such as convolution of a grey-scale
representation of the image with a difference-ofGaussians operator (see
Chapter 5, p. 94). Perception is still mediated, but by complex
processes that have to be expressed in mathematical form, and are very
unlike any that most of us use consciously and deliberately. The
contrast between the view that perception

407

is mediated by a series of algorithms and representations, and the
ecological view that it is direct, can be expressed in terms of the
levels at which perception can properly be explained. For a direct
theory of perception, there are two: the ecological and the
physiological. The ecological level is concerned with the information an
animal needs from the environment in order to organise its activities,
and with the ways in which the changing optic array can provide that
information. The physiological level is concerned with how networks of
nerve cells are organised so as to detect invariants in the optic array.
Gibson was concerned to work at the first of these levels, and had
little to say about the second, speaking simply of an animal being
"attuned" or "resonating" to invariants. Taking the case of the
parameter τ as an example, an explanation at the ecological level would
be concerned with how τ specifies time to contact with a surface, and
with the evidence that the timing of animals' and people's actions
relies on detection of τ. A direct theory of perception would say that
there is nothing more for psychology to ask about this situation, but
would accept as valid a physiological investigation of how nerve cells
are organised so as to "resonate" to τ. Ullman (1980) argues that the
"direct" theory is mistaken in believing that there are just two levels
of explanation, the ecological and the physiological, and concurs with
Marr in believing that an algorithmic account must come between the two
in order to organise physiological knowledge. We agree with Ullman's
position, and feel that "direct" theorists have not paid sufficient
attention to the problem of the relationship between ecological and
physiological levels of explanation. For example, consider what is
actually involved in determining how an animal's nervous system is built
to detect τ. The discovery that cells selective for values of τ exist is
important (see Chapter 12, p. 352), but still only a first step. A
physiologist's goal would be to establish what interactions between
neurons in the visual pathway yield a value of τ as output, given a
fluctuating pattern of light intensities as input. To achieve this,
models of the computation of τ from the responses of motion-sensitive
cells (see Chapter 8)

408

VISUAL PERCEPTION

must be tested against neurophysiological evidence. The modelling
required for this physiological analysis would constitute an algorithmic
level of explanation. Marr (1982) makes a similar argument, accepting
the value of ecological optics but asserting the need for algorithmic
explanations of how properties of the optic array are detected. Marr
suggested (1982, p. 29) that: In perception, perhaps the nearest anyone
came to the level of computational theory was Gibson (1966). This is not
at all a paradoxical claim, since Marr's notion of a "computational
theory" is an abstract understanding of the structure and constraints of
the visual information-processing problem and must not be confused with
the algorithmic level which spells out actual computations. It is
Gibson's denial of the algorithmic level with which Marr (1982, p. 29)
quarrels: Gibson's important contribution was to take the debate away
from the philosophical consideration of sense-data and the affective
qualities of sensation and to note instead that the important thing
about the senses is that they are channels for perception of the real
world outside, or, in the case of vision, of the visible surfaces. He
therefore asked the critically important question, How does one obtain
constant perceptions in everyday life on the basis of continually
changing sensations? This is exactly the right question, showing that
Gibson correctly regarded the problem of perception as that of
recovering from sensory information "valid properties of the external
world". Although one can criticise certain shortcomings in the quality
of Gibson's analysis, its major, and, in my view, fatal shortcoming lies
at a deeper level and results from a failure to realise two things.
First, the detection of physical invariants, like image surfaces, is
exactly and precisely an infor-

mation-processing problem, in modern terminology. And second, he vastly
underrated the sheer difficulty of such detection. (Marr, 1982, p. 30)
If Ullman's and Marr's arguments are accepted, they lead us to ask how
variables of optic structure and flow are computed from a fluctuating
pattern of light intensities in a timevarying image. We cannot agree
with the ecological argument that analyses of this kind are irrelevant
to the explanation of perception, and that matters must be left at the
assertion that any information available in light is "directly"
perceived. For the purposes of an ecological level of analysis, they can
be left at that point, but, if links to a physiological level are to be
made, they cannot.

Does perception require representations? There is a second sense in
which traditional theory regards perception as mediated, and this raises
issues that extend beyond problems of perception to questions of the
nature of our knowledge of the world. The argument is that in order to
perceive the world an animal or person must form an internal
representation of it. We have seen examples of representations of the
world which theories of perception have postulated; Marr's 2½D sketch
(see Chapter 7) represents the layout of the surfaces surrounding an
observer, while theories of object recognition (see Chapter 9) have
postulated catalogues of stored descriptions of objects held in memory.
The term "representation" is used in a wide variety of senses. It can
refer to any symbolic description of the world---whether this is the
world as it has been in the past (as in stored "memories"), as it is now
(the 2½D sketch, or structural descriptions), or as it might be in the
future (as in certain kinds of imagery). It is also used by
connectionist theorists (see Chapter 4) to refer to nonsymbolic patterns
of activation in simple networks, which, nevertheless represent some
object, feature, distance, or other property of the surroundings. Direct
theories of perception completely reject all such representations as
further examples of mediating processes standing between the world

14. CONTRASTING THEORIES OF VISUAL PERCEPTION

and the perceiver. The counterargument would run as follows. Just as
sizes, slants, and distances of surfaces surrounding an observer are
specified by invariant properties of optic structure, so any object we
can recognise is specified by as yet unidentified "high-level"
invariants. Therefore, just as we can, at an ecological level, regard
detection of distance specified by a texture gradient as direct, so we
can equally well regard perception of any object, person, or event as
direct. There is no need for any processes of constructing or matching
representations. The ecological argument against a role for
representation in explaining perception seems to carry some weight in
the context of relatively simple visually guided behaviour in animals.
We have already argued in similar terms (see Chapter 11, p. 321) that an
insect's vision does not work to build up a representation of its
surroundings, but instead to provide just the information required to
control its flight. A female fly (or anything resembling one) could be
said to afford pursuit, and the fly has to detect the information in the
optic array specifying this affordance. In these terms, where is the
need for a representation of a female fly to which input is matched? It
is clear that if an algorithmic level of explanation is allowed (as we
have argued it must be), then "representations" of some sort are
inevitably involved, for the purpose of an algorithm is to transform one
representation into another. Thus, Marr (1982, p. 34) states that the
algorithms involved in fly vision: . . . deliver a representation in
which at least three things are specified: 1) whether the visual field
is looming sufficiently that the fly should contemplate landing; 2)
whether there is a small patch -- it could be a black speck or, it turns
out, a textured figure in front of a textured ground -- having some kind
of motion relative to its background; . and if there is such a patch, 3)
ψ and ψ for this patch are delivered to the motor sys. tem. \[ψ is the
angular bearing and ψ the angular velocity of the patch; see Chapter 11,
p. 319.\]

409

Here, Marr analyses the link between the fly's perception and action
into a series of algorithms and representations, and we have already
argued that such an explanation is not only legitimate but necessary if
a physiological analysis is to be undertaken. It is not at all the same
as saying that a conceptual representation of the world is built up
which a "little fly in the head" consults before taking action, although
by using the word "contemplate" Marr could mislead us into thinking that
this is what he intends. The representations are embodied as patterns of
activity in networks of neurons. In this situation, it becomes difficult
to distinguish the ecological and representational positions. The first
states that the tuning of the fly to the invariances in the light
reaching it fits it to its environment, while the second states that
properties of the fly's environment are represented in the networks of
nerve cells that process signals from its photoreceptors. If the term
representation is used in this sense, it seems there is really nothing
to argue about. It is only if we wanted to claim that the fly had a
"concept" of something to pursue that there should be any dispute. By
choosing the example of a fly's visual perception, however, we have
avoided more difficult issues raised by the "representation" debate. In
this example, we are dealing with the direct control of activity by a
known invariant in the light currently reaching the fly's eyes. Let us
consider an example from animal perception not meeting these criteria.
If a chimpanzee is carried around a familiar field, shown the locations
of 20 pieces of concealed food, and then released, it will move around
the field gathering up all the pieces (Menzel, 1978). Control
experiments show that the locations of food cannot be detected by a
chimp that has not previously been shown them. What are we to make of
this observation? The first difference from the fly example is that
control of activity is not direct; information obtained from light
specifying where food is hidden is used later to guide travel around the
field. The second is that there is no information in the light reaching
the chimpanzee while it is gathering up the food to specify where food
is. In an indirect theory of perception, these facts are

410

VISUAL PERCEPTION

accommodated by saying that the chimp forms a representation of the
information specifying the locations of food, and this representation is
later used to guide travel around the field. An ecological theorist
would reply that the representational explanation makes the mistake of
putting over-narrow bounds on the sample of light in which information
can be detected. If we take the sample of light to be that stretching
over both sessions of the experiment, then there are invariants
specifying the locations of food and there is no need to invoke
representations. There is no difference in principle, to a "direct"
theorist, between this situation and one in which a chimpanzee's view of
a piece of food while approaching it is briefly interrupted by an
obstacle. In both cases, activities extended over time are guided by
directly detected invariants in a sample of light extended over time.
The difficulties many have in accepting this formulation are typified by
Menzel's comment (1978, p. 417) that: I am an ardent admirer of Gibson,
and I don't doubt that his theory could explain much of the data cited
in this chapter; but when he starts talking about how animals can "see"
things that are hidden from sight or even located in a different room he
loses me. The disagreements become sharper when we move to human
perception. The Gibsonian position is that anything we perceive must be
specified by invariant properties of stimulation, directly detected
without any need for representation of information. There are invariants
specifying a friend's face, a performance of Hamlet, or the sinking of
the Titanic, and no knowledge of the friend, of the play, or of maritime
history is required to perceive these things. Gibson (1979) also applied
the concept of affordance to human perception; a pen, he argued, affords
writing and a mailbox the posting of a letter. No knowledge of writing
or of the postal system needs to be represented in memory, as invariants
specifying these affordances are directly detected. An important
difference between these examples and those we used earlier is that we
are

now dealing with situations where no invariants specifying objects,
events, or affordances have been demonstrated or seem likely to be
demonstrated. The calm with which Gibsonian theorists contemplate this
difficulty is captured in Michaels and Carello's (1981, p. 178) reply to
the question: How can the ecological approach account for experiential
dimensions of hedonic tone (humor, pleasure, amusement) that appear to
have no physical stimulus referent? with the answer: The invariants must
be very higher-order indeed. Critics of the Gibsonian position do not
accept that this answer is adequate, and we will outline the objections
made to it by Fodor and Pylyshyn (1981). Their principal criticism is
that the terms "invariant" and "directly detected" are left so
unconstrained in their meanings in Gibsonian theory as to be
meaningless. They take the example of an art expert detecting the fact
that a painting was executed by da Vinci and not by an imitator. To
explain this by stating that the expert has directly detected the
invariant property "having been painted by da Vinci" is, they argue, to
use the term "invariant" in a trivial way. For any percept, if a
sufficiently large sample of available light is considered, there can in
principle be some invariant to specify a property of this sort: here, a
sample including the light reflected from all da Vincis, imitations of
da Vincis, books on da Vinci, and so on. Fodor and Pylyshyn argue that
the notions of invariant and direct detection can be constrained by what
they term a "sufficiency criterion". The pattern of light reflected from
a looming surface is sufficient to cause the perception of impending
collision, as shown by experiments with artificial simulations of such a
pattern. Therefore, it makes sense to speak of an invariant property of
this pattern being detected directly. In contrast, we have no reason to
believe that a sample of light reflected from a da Vinci is sufficient
to cause

14. CONTRASTING THEORIES OF VISUAL PERCEPTION

recognition of it. Rather, Fodor and Pylyshyn argue, further information
beyond that in the light is necessary, and this must be information
about the properties of da Vinci's paintings represented in memory. This
"sufficiency criterion" suggests a way in which two kinds of explanation
of visual perception can be distinguished. Some properties of the world
are perceived directly, in the sense that no explicit knowledge of the
world or mental operations on it are required (although, as we argued
above, such "psychologically" direct perception can be decomposed into
algorithmic components). However, such direct perception is restricted
to relatively simple properties such as looming objects, steep drops,
and so on. Fodor and Pylyshyn (1981) call this process "seeing", in
contrast to "seeing as", where there is inevitably a role for knowledge
of the world in achieving perception. They give the following example of
the difference (1981, p. 189): What you see when you see a thing depends
upon what the thing you see is. But what you see the thing as depends
upon what you know about what you are seeing . . . Here is Smith at sea
on a foggy evening, and as lost as ever he can be. Suddenly the skies
clear, and Smith sees the Pole Star. What happens next? In particular,
what are the consequences of what Smith perceives for what he comes to
believe and do? Patently, that depends upon what he sees the Pole Star
as. If, for example, he sees the Pole Star as the star that is at the
Celestial North Pole (plus or minus a degree or two), then Smith will
know, to that extent, where he is; and we may confidently expect that he
will utter "Saved!" and make for port. Whereas, if he sees the Pole Star
but takes it to be a firefly, or takes it to be Alpha Centauri, or --
knowing no astronomy at all -- takes it to be just some star or other,
then seeing the Pole Star may have no particular consequences for his
behaviour or his further cognitive states. Smith will be just as lost
after he sees it as he was before.

411

The same argument could be made for any of the other examples that we
have considered. One person might examine a painting closely and see it
as patches of pigment applied to a canvas, while another might see it as
particular kinds of pigment applied using particular brush strokes that
are characteristic of da Vinci's style. The two people see the same
patches of pigment, but see them as different things, depending on the
knowledge of 15th-century Italian art that they use to interpret what
they see. Consider Gibson's (1979) more everyday example of perceiving a
mailbox. Whatever knowledge they had, anyone confronted with a mailbox
would see a rigid object that would be painful to collide with, and that
had an opening in which small objects could be placed. To see it as a
mailbox, however, requires knowledge about the role that the object
plays in a wider pattern of human activity, and particularly knowledge
that stamped letters placed in boxes like it will be collected and taken
to the addresses written on them. This knowledge is possessed by people
in most societies, but obviously not in all. The kind of knowledge that
we bring to "seeing as", and the ways in which we acquire it, vary
greatly from one case to another. Seeing a star as the Pole Star and
using it to navigate, or seeing a painting as the work of a particular
artist, draws on explicit knowledge acquired in relatively formal ways.
We would be aware of possessing such knowledge, would be able to explain
it to someone else, and would perhaps be able to recall how we acquired
it. When we see things like mailboxes as having particular uses, we are
drawing on more implicit knowledge acquired early in our experience on
occasions that we cannot recall. "Seeing as" often draws on general
knowledge that we share with others about the properties, meanings, and
uses of objects around us. However, it can also be idiosyncratic and
creative, when we interpret what we see in terms of individual memories,
associations, or images. Writers have used "stream of consciousness"
techniques to try to capture this quality of perception. Take for
example this description by James Joyce (1934) in Ulysses of "seeing as"
while walking over clumps of seaweed on a rocky shore as the tide comes
in (p. 50):

412

VISUAL PERCEPTION

Under the upwelling tide he saw the writhing weeds lift languidly and
sway reluctant arms, hising up their petticoats, in whispering water
swaying and upturning coy silver fronds. Day by day: night by night:
lifted, flooded and let fall. Lord, they are weary: and, whispered to,
they sigh. Usually, "seeing" and "seeing as" occur together. We might
see a mailbox as a means of posting a birthday card that we have been
carrying around for days, while at the same time seeing a gap in the box
towards which we must extend a hand with the correct trajectory. Or if
we were strolling along a seashore and seeing clumps of seaweed as an
elaborate, metaphorical image, we would hopefully at the same time be
seeing slippery and potentially dangerous surfaces and adjusting our
gait to avoid them. Most of the time, visual perception involves "seeing
as" in some way, as well as "seeing", and so must draw on
representations of individual experiences and of culturally mediated
knowledge. Although ecological theorists have made valid criticisms of
current cognitive models of the representation and use of information
held in memory (e.g., Bransford, McCarrell, Franks & Nitsch, 1977), the
nature of much of our perceptual experience requires some explanation of
this kind. Cognitive theory may not yet have provided an adequate
explanation of "seeing as", but ecological theory has done little more
than side-step the problem.

Modularity Fodor and Pylyshyn's (1981) distinction between "seeing" and
"seeing as" is derived from a wider argument that cognitive processes in
general are organised in a modular way. Here, the term module is used to
refer to a more extensive set of cognitive operations than in Marr's
theory, where a module is a set of algorithms carried out on a
particular visual representation (see Chapter 4, p. 80). Fodor (1983)
argued that some cognitive processes, such as the understanding of
speech, are modular in the sense that a person's conscious mental states
such as knowledge, beliefs, and intentions, cannot influence them. For
example, when someone speaks a language foreign to us,

we hear an undifferentiated stream of sound, but we can only hear our
own language as a sequence of words and phrases no matter how hard we
may try not to. Fodor draws on evidence from visual illusions to make a
similar argument about vision. A person may know about a geometric
illusion such as Titchener's circles (Chapter 3, p. 67), or may remember
having just measured the sizes of the central circles with a ruler, but
will nonetheless persist in seeing two circles of different sizes.
Similarly, someone could inspect a hollow mask closely and appreciate
its actual shape, but after walking far enough away from would see it as
a normal, convex face just as clearly as someone entirely naive about
the "hollow face" illusion (Chapter 7, p. 184). In such cases it is
possible for an illusory perceptual experience to exist alongside an
accurate state of knowledge about the figure or object being perceived.
In Fodor's (1983) terms, a "vision module" that is sealed off from other
cognitive processes yields the perceptual experience. Pylyshyn (1999)
has developed a considerably more detailed theory of the modular nature
of visual perception, arguing for the existence of a distinct process
called "early vision" that is "cognitively impenetrable", meaning that
it cannot be influenced by knowledge, beliefs, expectations, or other
mental states. Early vision does however embody "natural constraints",
which can be thought of as knowledge about regularities in the world.
For example, the way that gradients of shading give rise to perceptions
of curved surfaces (see Chapter 7, p. 190) is an outcome of early vision
and embodies knowledge that natural scenes are almost always lit from
above. However, this kind of knowledge is specific to early vision and
inaccessible to other cognitive processes. Most of us would be quite
unaware of knowing anything about regularities in the direction of
illumination in our environment unless we had first been prompted to
think about it. The crucial feature of Pylyshyn's (1999) theory is the
distinction between an autonomous early vision process that yields a
representation of specific properties of the surroundings, and general
cognitive processes that operate on that representation in ways that are
accessible to conscious

14. CONTRASTING THEORIES OF VISUAL PERCEPTION

awareness. If this distinction is real, it should be possible to specify
the nature of the representation produced by early vision. Pylyshyn
draws on Nakayama, He, and Shimojo's (1995) argument that this
representation is of the positions, orientations, and shapes of surfaces
surrounding an observer, roughly similar to Marr's proposed 2½D sketch.
In order to answer the question definitively, however, it is necessary
in turn to have clearly defined criteria for cognitive impenetrability.
Much of Pylyshyn's (1999) paper is taken up with discussion of this
problem, and particularly with distinguishing between cognitive
penetrability and other ways in which cognitive states can influence
perception without directly affecting early vision. In the next section,
we will look closely at one of the ways in which Pylyshyn (1999) accepts
that cognitive processes influence early vision without intervening
directly in its internal operations, and will see that this leads to new
questions about the relationships between representational and direct
theories of visual perception.

ACTIVE VISION As a person's direction of gaze changes, the regions of a
scene fixated on the fovea and on the peripheral retina at any moment
are processed differently by the visual system, because of the different
physiological properties of the two retinal areas. However, visual
processing is also influenced by the spatial distribution of attention
across the visual field. The location of attention cannot be observed in
the way that gaze can, but must be inferred from indirect evidence. We
have seen one way that this can be done when we described the effects of
attention on the responses of cortical cells (see Chapter 3, p. 73). A
monkey can be trained to expect a visual cue, which will provide useful
information about how to obtain a reward, at a particular location in
the visual field. Cells with fields at this location show increased
sensitivity to stimuli, implying that the allocation of attention to it
affects visual processing. Effects of attention on human vision can be

413

demonstrated using a similar method, known as pre-cueing. If an observer
is given a cue of some kind to where a target is about to appear, a
variety of psychophysical detection or discrimination tasks are
performed more effectively in this location than in others that have not
been cued (e.g., Yeshurun & Carrasco, 1999). Divided attention tasks can
also be used to demonstrate an impairment of visual processing at one
location when a second task requires attention to another (e.g., Dobkins
& Bosworth, 2001). Although in some circumstances it is possible for
spatial attention to be allocated to a different part of the visual
field from that fixated in foveal vision, attention and the control of
gaze are closely interrelated. The evidence suggests that saccades are
preceded by shifts of attention to the region of the next fixation, so
that in normal circumstances gaze and attention are tightly linked
(e.g., McPeek, Maljkovic, & Nakayama, 1999). For the purposes of our
discussion, we will treat the two as equivalent, although in other
contexts this would be an oversimplification. The way that a person
allocates attention to different parts of a scene in order to carry out
some task does not only affect psychophysical thresholds, but may
determine whether they report seeing an object at all. A variety of
experiments have shown that an incongruous and unexpected object
appearing in a display may not be noticed by many observers when their
attention is directed elsewhere by a task (Mack & Rock, 1998; Simons,
2000b). This phenomenon is known as "inattentional blindness" and is
familiar in some everyday situations. An example would be failing to
notice a friend waving to you in a crowded station while you search
intently for information about your platform and departure time. In most
situations, we are unaware of such effects of attention on what we
perceive, because our attention and gaze usually move rapidly and
continually from one part of our surroundings to another. In
circumstances where attention remains fixed for a prolonged period it
can have surprising effects on our awareness. This is a characteristic
of perception that is exploited by conjurors, who use a variety of
skilled techniques to keep their audience's attention away from the

414

VISUAL PERCEPTION

actions that they need to perform to produce "magical" results. The
allocation of spatial attention has important effects on the processing
of visual information, but how is it controlled? Relatively simple
"exogenous" stimuli, such as sudden movements in the peripheral visual
field, cause shifts of attention and gaze, but these can also come about
through more complex "endogenous" processes. We have already seen
examples in Chapter 12, when table tennis or cricket players shift their
gaze to fixate a point where they have rapidly and unconsciously
predicted that the ball will bounce (see p. 363). Such "intelligent"
control of eye movements is not confined to specialised tasks such as
intercepting a ball. Studies of fixation patterns during everyday
activities such as making a cup of tea or a sandwich (Hayhoe, 2000;
Land, Mennie, & Rusted, 1999) show that gaze is tightly locked to body
and limb movements in ways that are highly consistent between people.
For example, gaze regularly shifts to fixate the next object to be
grasped by the hand as a series of actions proceeds. Ballard, Hayhoe,
and Pelz (1995) have obtained similar findings from a more artificial
task, in which observers move coloured blocks to copy a model
arrangement. These observations show that there is more to the control
of gaze than scanning the surroundings mechanically in order to build up
a representation that is in turn used to control actions. The close and
consistent relationship between shifts of gaze and actions suggests
instead that head and eye movements are made to obtain the specific
information that is needed at each stage of a task. This in turn implies
that the systems controlling the direction of gaze have access to
knowledge of the temporal structure of a complex task and of the spatial
layout of the objects used to complete it. Whether or not this implies
that early vision is "cognitively penetrable" is perhaps a matter of
definition, and further discussion of this question can be found in
Pylyshyn (1999). Here, we will consider some wider implications that
these "active" properties of visual perception have for cognitive
theories of visual perception. In theories such as Marr's (1982), the
move-

ment of gaze and spatial attention around a scene is regarded as a minor
problem. It is assumed that retinal physiology imposes constraints on
how a scene can be sampled, and so this must be done a piece at a time.
The image on the retina is processed during each fixation, and the
resulting output is added to a representation of the scene that endures
over many fixations. We saw in Chapter 7 how various kinds of short-term
store have been proposed to hold visual information in this way.
Evidence that attention and gaze are directed towards parts of a scene
that are relevant to current tasks is compatible with these assumptions.
It suggests a striking degree of cognitive complexity in the sampling of
visual information, but does not necessarily challenge the existence of
an enduring and stable representation of the visual environment of the
kind that Marr and other cognitive theorists have proposed. However,
this evidence has been used to support a more radical theoretical
argument that little or no information is held in a representation from
one fixation to the next. Instead, it is claimed, gaze is directed at
each saccade to the region of a scene where information needed for
current tasks is most likely to be found, and an entirely new
representation is created. This is not an intuitively easy idea, because
we experience a stable visual world that is equally detailed everywhere,
and not a series of brief restricted glimpses. We will next discuss
evidence from a phenomenon known as change blindness that has been used
to support this argument, before evaluating it further.

Change blindness In experiments demonstrating change blindness,
observers are presented first with a picture, and then a short time
later with a different version of it in which an object has disappeared
or moved. For example, a building or mountain in a landscape might have
been deleted or moved relative to other features, or a person's clothes
might have changed. The observer's task is to identify the change that
has occurred between the two pictures. If one picture immediately
follows the other, this task is trivially easy. This is because the
change gives rise to transient optical motion in the part of the scene
concerned, which is detected

14. CONTRASTING THEORIES OF VISUAL PERCEPTION

early in motion processing (see Chapter 8). In the real world, changes
caused by moving objects will create such transients at least during one
fixation, and usually over many successive ones. What would happen,
however, if a change occurred during a saccade? The eye movement will
cause optical motion throughout the retinal image (which will not affect
awareness; see Chapter 8, p. 261), and it is difficult to see how the
local transient motion caused by the external change could be separated
from these effects of the saccade. In the natural world, changes in the
layout of a scene are extremely unlikely to occur quickly enough to be
confined to a saccade, but these conditions can be created
experimentally by using video images. If this is done, detection of a
change cannot rely on transient optical motion, but must instead depend
on a comparison of the scene on the second fixation with some
representation of it held over from the first fixation. Can people
detect changes in this way? It turns out that there is no
straightforward answer to the question of whether changes in scenes
during saccades can be detected. Depending on the precise circumstances,
changes may or may not be detected, indicating that visual
representations do persist from one fixation to another but that they
contain only selective information about scenes. A change is more likely
to be detected if it occurs in the region that is the target of the
current saccade than if it occurs elsewhere (McConkie & Currie, 1996).
The history of an observer's attention to parts of a scene is also
important, as changes are detected more reliably in regions that have
been previously fixated than in ones that have not (Hollingworth &
Henderson, 2002). Results are also affected by the method of measuring
change detection. Observers may report seeing no change in a scene when
asked explicitly, but show evidence of detecting it when tested using
forced-choice recognition procedures (Hollingworth & Henderson, 2002),
or they may make longer fixations on a region of a scene that has just
changed (Hayhoe, Bensinger, & Ballard, 1998). Finally, the information
that a person needs from a scene at any particular time, in order to
carry out some task, may influence change detection. Hayhoe et
al. (1998) made

415

changes in the colours of blocks during saccades as observers carried
out a block-copying task. Implicit detection of changes, revealed by
prolonged fixations on blocks that had just changed colour, was more
likely to occur at those points in the task where the next block to be
fixated had to match the colour of one already picked up. This suggests
that the information held from one fixation to another depends on the
moment-bymoment demands of a task. More dramatic failures to see
changes, described as "change blindness", have been demonstrated in
experiments that have used other methods to prevent change detection by
motion transients. These include introducing a blank field during a
short interval between the two pictures (Rensink, O'Regan, & Clark,
1997); superimposing briefly a pattern of "mud-splashes" on the scene at
the time that the change is made (O'Regan, Rensink, & Clark, 1999); or
making the change at a "cut" in a film from one camera position to
another (Levin & Simons, 1997). Staged real-life experiments have also
been carried out, in which naive participants begin a conversation with
a stranger who is then replaced by a different person during some brief
interruption, such as a large object being moved between them (Simons &
Levin, 1998). All these methods have a common aim, which is to cause
widespread transients across the visual field at the same time as the
change occurs, and so prevent it being detected simply by processing
image motion. They all demonstrate a surprising degree of change
blindness, with a substantial proportion of participants failing to
identify the change or even to detect whether one has occurred or not.
All the changes involved are ones that people see immediately when they
are shown the two scenes simultaneously. This is particularly striking
when a participant has been unaware that they spoke to more than one
person during the staged encounters used by Simons and Levin (1998).
These "change blindness" effects have been used to argue that
representations of our visual surroundings are not maintained from one
fixation to another, but must be created anew on each one. If transient
optical motion is not available

416

VISUAL PERCEPTION

to specify the change, we therefore cannot detect it. In this view, our
experience of seeing our surroundings in equal detail everywhere must
arise from our ability almost always to find the information that we
expect each time we make a saccade to a new fixation. But is such a
strong conclusion actually necessary? An alternative interpretation is
that we do maintain a visual representation across fixations, and that
transient optical motion is a powerful signal to update particular parts
of it (Phillips & Singer, 1974). The extensive transients generated by
"mud splashes", film cuts, and other methods cause the representation to
be "overwritten", flushing out the information held in it from earlier
fixations and preventing any comparisons being made with the changed
scene. Or again, it could be argued that change blindness arises from
constraints on the capacity to make comparisons between two visual
representations. These and other alternatives are discussed further in
the volume edited by Simons (2000a). Given the difficulties involved in
interpreting change blindness effects, the strongest evidence is perhaps
that from experiments in which changes occur during saccades, when
widespread transient optical motion occurs naturally. As we have seen,
these yield less dramatic effects on change detection, which indicate
that some information is represented and held across fixations, but that
it is selected according to the way that attention is allocated to a
scene as a person seeks to obtain information from it. Neuroimaging
evidence has also demonstrated a role for the active control of
attention in the detection of changes in a scene. Beck, Rees, Frith, and
Lavie (2001) compared patterns of fMRI activation on trials where
observers did and did not detect a change between two pictures. On both
types of trial, areas in occipital cortex were activated. On those
trials when change was detected, additional areas, in parietal and
prefrontal cortex, were also activated. As we would expect on the basis
of Milner and Goodale's (1995) theory, processing of images for
recognition involved ventral areas in occipital cortex, but detection of
change also requires activity in dorsal areas that presumably control
attention. Thus awareness of changes in a

scene seems to arise from interactions between the ventral and dorsal
pathways of a kind that the theory did not originally envisage.

CONCLUSIONS Earlier in this chapter, we argued that cognitive theories
of visual perception attempt to deal with two important problems that
ecological theories do not address satisfactorily. These are the
explanation of perception at a physiological level, and the role of
knowledge in perception. For different reasons, each of these requires
the concept of representation in some form. We then went on describe
some recent developments in research and theory on the active properties
of vision, and we now conclude by considering their implications for the
general theoretical questions that we discussed earlier. We have seen
that one interpretation of change blindness and related effects is to
assert that no stable, enduring representation of the visual world is
built up over successive fixations. Instead, the retinal image is
processed anew on each fixation to obtain specific pieces of information
needed for a perceiver's current goals. This radical position resembles
ecological theory in proposing that information is obtained directly
from the world, as it is needed (cf. Chapter 10, p. 313). Even so, it
does not challenge the concept of representation in a fundamental way,
as it still requires represented knowledge of the world to be used to
predict where relevant information will be found and so to control the
movement of gaze and attention. Such a representation may be sparser
than previous theories of short-term visual storage assumed, and it may
draw on implicit knowledge of statistical regularities in the structure
of scenes, which a person is unaware of possessing (Chun, 2000).
Nevertheless, it still specifies a person's knowledge of the world. As
we have seen, other interpretations of change blindness effects are
possible, and these imply a rather richer, more stable visual
representation. The exact nature of the information that we hold across
fixations therefore remains a matter for

14. CONTRASTING THEORIES OF VISUAL PERCEPTION

research and debate, but within what is fundamentally a representational
framework. What does emerge more clearly from recent research is the
extent to which visual processing is influenced by task demands at early
stages and over short timescales. Whatever visual representations are
held across successive fixations, the processes that update them involve
complex, high-level control of spatial attention and gaze. Even within
fixations, we saw in Chapter 3 how effects of attention operate through
feedback pathways to influence neural processing in early parts of the
visual pathway. These findings suggest that older indirect theories of
visual perception such as Gregory's (1973) rather underestimated the
importance of "top-down" influences, not appreciating their power to
influence early visual processing within timescales of tenths of
seconds. At the same time, paradoxically, they are also consistent with
one strand in Gibson's (1966) thinking; his emphasis on perception as an

417

activity in which a perceiver moves about to obtain information from
light. However, ecological theory rejected concepts such as attention
because they imply that perception is mediated, and so it could not
consider the possibility that this active exploration involved not only
overt body movements but also faster changes in cognitive and neural
organisation. In this book, we have organised our discussions of visual
perception around a distinction between two functions that it serves;
its roles in the creation of awareness of a visual world, and in the
control of our activities. We suggest that the effect of new insights
into the active properties of visual processing will be to dissolve this
distinction. This will come about through a wider conception of the
activities that vision controls, which will include fast cognitive and
neural processes that continually reconfigure visual processing to seek
and gather new information from the world.

Page Intentionally Left Blank

References

Alderson, G.H.K., Sully, D.J., & Sully, H.G. (1974). An operational
analysis of a one-handed catching task using high speed photography.
Journal of Motor Behaviour, 6, 217--226. Alley, T.R. (Ed). (1988).
Social and applied aspects of perceiving faces. Hillsdale, NJ: Lawrence
Erlbaum Associates Inc. Allison, T., Puce, A., & McCarthy, G. (2000).
Social perception from visual cues: Role of the STS region. Trends in
Cognitive Sciences, 4, 267--278. Alonso, J.M., & Martinez, L.M. (1998).
Functional connectivity between simple cells and complex cells in cat
striate cortex. Nature Neuroscience, 1, 395--403. Andersen, R.A., &
Bradley, D.C. (1998). Perception of three-dimensional structure from
motion. Trends in Cognitive Sciences, 2, 222--228. Andersen, R.A.,
Essick, G.K., & Siegel, R.M. (1985). Encoding of spatial location by
posterior parietal neurons. Science, 230, 456--458. Andersen, R.A.,
Snyder, L.H., Bradley, D.C., & Xing, J. (1997). Multimodal
representation of space in the posterior parietal cortex and its use in
planning movements. Annual Review of Neuroscience, 20, 303--330.
Anderson, B.L. (1994). The role of partial occlusion in stereopsis.
Nature, 367, 365--368. Anderson, B.L., & Nakayama, K. (1994). Toward a
general theory of stereopsis: Binocular matching, occluding contours and
fusion. Psychological Review, 101, 414--445. Anderson, P.A., & Movshon,
J.A. (1989). Binocular

Abraham, F.D., Abraham, R.H., & Shaw, C.D. (1991). A visual introduction
to dynamical systems theory for psychology. Santa Cruz, CA: Aerial
Press. Adelson, E.H., & Bergen, J.R. (1985). Spatiotemporal energy
models for the perception of motion. Journal of the Optical Society of
America A, 2, 284--299. Adelson, E.H., & Bergen, J.R. (1986). The
extraction of spatio-temporal energy in human and machine vision.
Workshop on motion: representation and analysis, Charleston, SC.
Adelson, E.H., & Bergen, J.R. (1991). The plenoptic function and the
elements of early vision. In M.S. Landy, & J.A. Movshon (Eds.),
Computational models of visual processing. Cambridge MA: MIT Press.
Adelson, E.H., & Movshon, J.A. (1982). Phenomenal coherence of moving
visual patterns. Nature, 300, 523--525. Adini, Y., Sagi, D., & Tsodyks,
M. (1997). Excitatory-- inhibitory network in the visual cortex:
Psychophysical evidence. Proceedings of the National Academy of Sciences
of the USA, 94, 10426--10431. Alais, D., & Blake, R. (1999). Neural
strength of visual attention gauged by motion adaptation. Nature
Neuroscience, 2, 1015--1018. Albrecht, D.G., & Geisler, W.S. (1991).
Motion selectivity and the contrast-response function of simple cells in
the visual cortex. Visual Neuroscience, 7, 531--546. Albright, T.D.
(1992). Form-cue invariant motion processing in primate visual cortex.
Science, 255, 1141--1143. 419

420

REFERENCES

combination of contrast signals. Vision Research, 29, 1115--1132.
Anderson, S.J., & Burr, D.C. (1985). Spatial and temporal selectivity of
the human motion detection system. Vision Research, 25, 1147--1154.
Anderson, S.J., & Burr, D.C. (1991). Spatial summation properties of
directionally selective mechanisms in human vision. Journal of the
Optical Society of America A, 8, 1330--1339. Anstis, S.M. (1980). The
perception of apparent movement. Philosophical Transactions of the Royal
Society of London B, 290, 153--168. Anstis, S.M. (1990). Motion
aftereffects from a motionless stimulus. Perception, 19, 301--306.
Anstis, S.M., & Duncan, K. (1983). Separate motion aftereffects from
each eye and from both eyes. Vision Research, 23, 161--169. Anzai, A.,
Ohzawa, I., & Freeman, R.D. (1999). Neural mechanisms for processing
binocular information. I. Simple cells. Journal of Neurophysiology, 82,
891--908. Arditi, A.R., Anderson, P.A., & Movshon, J.A. (1981).
Monocular and binocular detection of moving sinusoidal gratings. Vision
Research, 21, 329--336. Attneave, F. (1971). Multistability in
perception. Scientific American, 225, December, 63--71. Backus, B.T.,
Fleet, D.J., Parker, A.J., & Heeger, D.J. (2001). Human cortical
activity correlates with stereoscopic depth perception. Journal of
Neurophysiology, 86, 2054--2068. Bahill, A.T., & LaRitz, T. (1984). Why
can't batters keep their eyes on the ball? American Scientist, 72,
249--253. Baizer, J.S., Ungerleider, L.G., & Desimone, R. (1991).
Organization of visual inputs to the inferior temporal and posterior
parietal cortex in macaques. Journal of Neuroscience, 11, 168--190.
Baker, C.L., & Braddick, O.J. (1985). Temporal properties of the short
range process in apparent motion. Perception, 14, 181--192. Bakin, J.S.,
Nakayama, K., & Gilbert, C.D. (2000). Visual responses in monkey areas
V1 and V2 to three-dimensional surface configurations. Journal of
Neuroscience, 20, 8188--8198. Ball, W., & Tronick, E. (1971). Infant
responses to impending collision: Optical and real. Science, 171,
818--820. Ballard, D.H., Hayhoe, M.M., & Pelz, J.B. (1995). Memory
representations in natural tasks. Journal of Cognitive Neuroscience, 7,
66--80. Ballard, D.H., Hinton, G.E., & Sejnowski, T.J. (1983). Parallel
visual computation. Nature, 306, 21--26.

Banks, M.S., Ehrlich, S.M., Backus, B.T., & Crowell, J.A. (1996).
Estimating heading during real and simulated eye movements. Vision
Research, 36, 431--443. Barclay, C.D., Cutting, J.E., & Kozlowski, L.T.
(1978). Temporal and spatial factors in gait perception that influence
gender recognition. Perception and Psychophysics, 23, 145--152. Bardy,
B.G., Warren, W.H., & Kay, B.A. (1996). Motion parallax is used to
control postural sway during walking. Experimental Brain Research, 111,
271-- 282. Barlow, H.B. (1961). The coding of sensory messages. In W.H.
Thorpe, & O.L. Zangwill (Eds.), Current problems in animal behaviour
(pp. 331--360). Cambridge: Cambridge University Press. Barlow, H.B.
(1972). Single units and sensation: A neuron doctrine for perceptual
psychology? Perception, 1, 371--394. Barlow, H.B., Blakemore, C., &
Pettigrew, J.D. (1967). The neural mechanism of binocular depth
discrimination. Journal of Physiology, 193, 327--342. Barlow, H.B., &
Hill, R.M. (1963). Selective sensitivity to direction of motion in
ganglion cells of the rabbit's retina. Science, 139, 412--414. Barlow,
H.B., & Levick, W.R. (1965). The mechanism of directionally selective
units in rabbit's retina. Journal of Physiology, 178, 477--504. Barlow,
H.B., & Mollon, J.D. (1982). The senses. Cambridge: Cambridge University
Press. Barnes, R.D. (1968). Invertebrate zoology (2nd Edn.).
Philadelphia, PA: W.B. Saunders. Barnes, R.D. (1974). Invertebrate
zoology (3rd Edn.). Philadelphia, PA: Saunders College Publishing/ Holt,
Rinehart, & Winston. Baron-Cohen, S. (1994). How to build a baby that
can read minds -- cognitive mechanisms in mindreading. Cahiers de
Psychologie Cognitive -- Current Psychology of Cognition, 13, 513--552.
Bartfeld, E., & Grinvald, A. (1992). Relationships between
orientation-preference pinwheels, cytochrome oxidase blobs, and ocular
dominance columns in primate striate cortex. Proceedings of the National
Academy of Sciences of the USA, 89, 11905--11909. Barth, E., Zetsche,
C., & Rentschler, I. (1998). Intrinsic two-dimensional features as
textons. Journal of the Optical Society of America A, 15, 1723--1732.
Bartlett, J.C., & Searcy, J. (1993). Inversion and configuration of
faces. Cognitive Psychology, 25, 281--316. Bassili, J.N. (1976).
Temporal and spatial contingencies

REFERENCES

in the perception of social events. Journal of Personality and Social
Psychology, 33, 680--685. Bassili, J.N. (1978). Facial motion in the
perception of faces and of emotional expression. Journal of Experimental
Psychology: Human Perception and Performance, 4, 373--379. Bassili, J.N.
(1979). Emotion recognition. The role of facial movement and the
relative importance of upper and lower areas of the face. Journal of
Personality and Social Psychology, 37, 2049--2058. Baylis, G.C., Rolls,
E.T., & Leonard, C.M. (1985). Selectivity between faces in the responses
of a population of neurons in the cortex in the superior temporal sulcus
of the monkey. Brain Research, 342, 91--102. Beall, A.C., & Loomis, J.M.
(1996). Visual control of steering without course information.
Perception, 25, 481--494. Beasley, N.A. (1968). The extent of individual
differences in the perception of causality. Canadian Journal of
Psychology, 122, 399--407. Beauchamp, M.S., Haxby, J.V., Jennings, J.E.,
& DeYoe, E.A. (1999). An fMRI version of the Farnsworth-- Munsell
100-hue test reveals multiple colourselective areas in human ventral
occipitotemporal cortex. Cerebral Cortex, 9, 257--263. Beck, D.M., Rees,
G., Frith, C.D., & Lavie, N. (2001). Neural correlates of change
detection and change blindness. Nature Neuroscience, 4, 645--650. Beck,
J. (1972). Similarity grouping and peripheral discriminability under
uncertainty. American Journal of Psychology, 85, 1--20. Beck, J., &
Gibson, J.J. (1955). The relation of apparent shape to apparent slant in
the perception of objects. Journal of Experimental Psychology, 50,
125--133. Beckers, G., & Zeki, S. (1995). The consequences of
inactivating areas V1 and V5 on visual motion perception. Brain, 118,
49--60. Bell, A.J., & Sejnowski, T.J. (1997). The "independent
components" of natural scenes are edge filters. Vision Research, 37,
3327--3338. Benardete, E.A., & Kaplan, E. (1999). The dynamics of
primate M retinal ganglion cells. Visual Neuroscience, 16, 355--368.
Bennett, A.T.D., & Cuthill, I.C. (1994). Ultraviolet vision in birds:
What is its function? Vision Research, 34, 1471--1478. Berg, W.P., Wade,
M.G., & Greer, N.L. (1994). Visual regulation of gait in bipedal
locomotion: Revisiting Lee, Lishman and Thomson (1982). Journal of
Experimental Psychology: Human Perception and Performance, 20, 854--863.

421

Bergen, J.R., & Adelson, E.H. (1988). Early vision and texture
perception. Nature, 333, 363--364. Bergen, J.R., & Landy, M.S. (1991).
Computational modelling of visual texture segregation. In M.S Landy, &
J.A. Movshon (Eds.), Computational models of visual processing.
Cambridge MA: MIT Press. Berkeley, G. (1709). An essay towards a new
theory of vision. In A New Theory of Vision and other writings, Intro.
A.D. Lindsay. London: J.M. Dent & Sons Ltd (1910). Berkley, M.A.,
DeBruyn, B., & Orban, G. (1994). Illusory, motion and luminance-defined
contours interact in the human visual system. Vision Research, 34,
209--216. Bernstein, N. (1967). The coordination and regulation of
movements. Oxford: Pergamon Press. Berry, D., & Landry, J.C. (1997).
Facial maturity and daily social interaction. Journal of Personality and
Social Psychology, 72, 570--580. Berry, D.S., & McArthur, L.Z. (1986).
Perceiving character in faces: The impact of age-related craniofacial
changes on social perception. Psychological Bulletin, 100, 3--18. Berry,
D.S., & Misovich, S.J. (1994). Methodological approaches to the study of
social event perception. Personality and Social Psychology Bulletin, 20,
139-- 152. Bertenthal, B.I., Proffitt, D.R., & Cutting, J.F. (1984).
Infant sensitivity to figural coherence in biomechanical motion. Journal
of Experimental Child Psychology, 37, 213--230. Beusmans, J.M.H. (1998).
Perceived object shape affects the perceived direction of self-movement.
Perception, 27, 1079--1085. Beverley, K.I., & Regan, D. (1979).
Separable aftereffects of changing-size and motion-in-depth: Different
neural mechanisms? Vision Research, 19, 727--732. Biederman, I. (1987a).
Recognition by components: A theory of human image understanding.
Psychological Review, 94, 115--145. Biederman, I. (1987b). Matching
image edges to object memory. Proceedings of the First International
Conference on Computer Vision. IEEE Computer Society, London. Biederman,
I. (1995). Some problems of visual shape recognition to which the
application of clustering mathematics might yield some potential
benefits. DIMACS Series in Discrete Mathematics, 19, 313--329.
Biederman, I., & Cooper, E.E. (1991). Priming contour-deleted images:
Evidence for intermediate

422

REFERENCES

representations in visual object recognition. Cognitive Psychology, 23,
393--419. Biederman, I., & Cooper, E.E. (1992). Size invariance in
visual object priming. Journal of Experimental Psychology: Human
Perception and Performance, 18, 121--133. Biederman, I., & Gerhardstein,
P.C. (1993). Recognizing depth-rotated objects: Evidence and conditions
for three-dimensional viewpoint invariance. Journal of Experimental
Psychology: Human Perception and Performance, 19, 1162--1182. Biederman,
I., & Ju, G. (1988). Surface versus edgebased determinants of visual
recognition. Cognitive Psychology, 20, 38--64. Biederman. I., &
Kalocsai, P. (1997). Neurocomputational bases of object and face
recognition. Philosophical Transactions of the Royal Society of London
Series B, 352, 1203--1219. Biederman, I., Subramaniam, S., Bar, M.,
Kalocsai, P., & Fiser, J. (1999). Subordinate-level object
classification reexamined. Psychological Research, 62, 131-- 153.
Binford, T.O. (1971). Visual perception by computer. Paper presented at
the IEEE Conference on Systems and Control, December, 1971, Miami, as
cited by Marr (1982). Bizzi, E., Mussa-Ivaldi, F.A., & Giszter, S.
(1991). Computations underlying the execution of movement: A biological
perspective. Science, 253, 287--291. Bjorklund, R.A., & Magnussen, S.
(1981). A study of interocular transfer of spatial adaptation.
Perception, 10, 511--518. Blake, R., & Hiris, E. (1993). Another means
for measuring the motion aftereffect. Vision Research, 33, 1589--1592.
Blake, R., & Overton, R. (1979). The site of binocular rivalry
suppression. Perception, 8, 143--152. Blakemore, C. (1970). The
representation of threedimensional visual space in the cat's striate
cortex. Journal of Physiology, 209, 155--178. Blakemore, C., Carpenter,
R.H.S., & Georgeson, M.A. (1970). Lateral inhibition between orientation
detectors in the human visual system. Nature, 228, 37--39. Blakemore,
C.B., & Campbell, F.W. (1969). On the existence of neurones in the human
visual system selectively sensitive to the size and orientation of
retinal images. Journal of Physiology, 203, 237--260. Blasdel, G.G.
(1992). Orientation selectivity, preference and continuity in monkey
striate cortex. Journal of Neuroscience, 12, 3139--3161. Blondeau, J., &
Heisenberg, M. (1982). The three-

dimensional optomotor torque system of Drosophila melanogaster. Journal
of Comparative Physiology, 145, 321--329. Boden, M. (1987). Artificial
intelligence and natural man. 2nd edition. Hassocks, UK: Harvester
Press. Bonds, A.B. (1991). Temporal dynamics of contrast gain in single
cells of the cat striate cortex. Visual Neuroscience, 6, 239--255.
Bonnar, L., Gosselin, F., & Schyns, P.G. (2002). Understanding Dali's
slave market with the disappearing bust of Voltaire: A case study in the
scale information driving perception. Perception, 31, 683--691. Bootsma,
R.J. (1989). Accuracy of perceptual processes subserving different
perception--action systems. Quarterly Journal of Experimental
Psychology, 41A, 489--500. Bootsma, R.J., & Oudejans, R.R.D. (1993).
Visual information about time to collision between two objects. Journal
of Experimental Psychology: Human Perception and Performance, 19,
1041--1052. Boring, E.G. (1942). Sensation and perception in the history
of experimental psychology. New York: Appleton-Century-Crofts. Born,
R.T., & Tootell, R.B.H. (1991). Spatial frequency tuning of single units
in macaque supragranular striate cortex. Proceedings of the National
Academy of Sciences of the USA, 88, 7066--7070. Born, R.T., & Tootell,
R.B.H. (1992). Segregation of global and local motion processing in
primate middle temporal area. Nature, 357, 497--499. Borst, A., & Bahde,
S. (1988). Spatio-temporal integration of motion. Naturwissenschaften,
75, 265--267. Bossema, I., & Burgler, R.R. (1980). Communication during
monocular and binocular looking in European jays (Garrulus garrulus
glandarius). Behaviour, 74, 274--283. Boulton, J.C., & Baker, C.L.
(1993). Dependence on stimulus onset asynchrony in apparent motion:
Evidence for two mechanisms. Vision Research, 33, 2013--2019. Bovik,
A.C., Clark, M., & Geisler, W.S. (1990). Multichannel texture analysis
using localized spatial filters. IEEE Trans. PAMI, 12, 55--73. Bower,
T.G.R. (1966). The visual world of infants. Scientific American, 215,
December, 80--92. Bower, T.G.R. (1971). The object in the world of the
infant. Scientific American, 225, October, 30--38. Bower, T.G.R.,
Broughton, J.M., & Moore, M.K. (1970). Infant responses to approaching
objects. An indicator of response to distal variables. Perception and
Psychophysics, 9, 193--196.

REFERENCES

Boyle, D.G. (1960). A contribution to the study of phenomenal causation.
Quarterly Journal of Experimental Psychology, 12, 171--179. Boynton,
G.M., Demb, J.B., Glover, G.H., & Heeger, D.J. (1999). Neuronal basis of
contrast discrimination. Vision Research, 39, 257--269. Braddick, O.J.
(1974). A short-range process in apparent motion. Vision Research, 14,
519--527. Braddick, O.J. (1980). Low-level and high-level processes in
apparent motion. Philosophical Transactions of the Royal Society of
London, Series B, 209, 137--151. Bradley, D.C., Chang, G., & Andersen,
R.A. (1998). Encoding of 3-D structure-from-motion by primate area MT
neurons. Nature, 392, 714--717. Bradshaw, M.F., & Rogers, B.J. (1996).
The interaction of binocular disparity and motion parallax in the
computation of depth. Vision Research, 36, 3457-- 3468. Braitenberg, V.,
& Ferretti, C.L. (1966). Landing reaction of Musca domestica induced by
visual stimuli. Naturwissenschaften, 53, 155. Bransford, J.D.,
McCarrell, N.S., Franks, J.J., & Nitsch, K.E. (1977). Toward
unexplaining memory. In R. Shaw, & J. Bransford (Eds.), Perceiving,
acting and knowing: Toward an ecological psychology. Hillsdale, NJ:
Lawrence Erlbaum Associates Inc. Braun, J. (2000). Targeting visual
motion. Nature Neuroscience, 3, 9--11. Braunstein, M.L. (1968). Motion
and texture as sources of slant information. Journal of Experimental
Psychology, 78, 247--253. Brefczynksi, J.A., & DeYoe, E.A. (1999). A
physiological correlate of the "spotlight" of visual attention. Nature
Neuroscience, 2, 370--374. Breitmeyer, B.G. (1984). Visual masking: An
integrative approach. Oxford: Oxford University Press. Bridgeman, B.
(1996). Extraretinal signals in visual orientation. Handbook of
perception and action: Volume 1 (pp. 191--223). New York: Academic
Press. Brindley, G.S., & Merton, P.A. (1960). The absence of position
sense in the human eye. Journal of Physiology, 153, 127--130. Britten,
K.H., Shadlen, M.N., Newsome, W.T., & Movshon, J.A. (1993). Responses of
neurons in macaque MT to stochastic motion signals. Visual Neuroscience,
10, 1157--1169. Broadbent, D.E. (1985). A question of levels: Comment on
McClelland and Rumelhart. Journal of Experimental Psychology: General,
114, 189--192. Bruce, V. (1988). Recognising faces. London: Lawrence
Erlbaum Associates Ltd.

423

Bruce, V., Burton, A.M., Carson, D., Hanna, E., & Mason, O. (1994).
Repetition priming of face recognition. In C. Umilta, & M. Moscovitch
(Eds.), Attention & performance XV. Cambridge, MA: MIT Press. Bruce, V.,
Burton, A.M., Doyle, A., & Dench, N. (1989). Further experiments on the
perception of growth in three dimensions. Perception & Psychophysics,
46, 528--536. Bruce, V., Burton, A.M., Hanna, E., Healey, P., Mason, O.,
Coombes, A. et al. (1993). Sex discrimination: How do we tell the
difference between male and female faces? Perception, 22, 131--152.
Bruce, V., Cowey, A., Ellis, A.W., & Perrett, D.I. (1992a). Processing
the facial image. Proceedings of a Discussion Meeting at the Royal
Society of London. Philosophical Transactions of the Royal Society,
Series B, 335, 1--28. Oxford: Clarendon Press, Oxford Science
Publications. Bruce, V., Hanna, E., Dench, N., Healy, P., & Burton, A.M.
(1992b). The importance of "mass" in line drawings of faces. Applied
Cognitive Psychology, 6, 619--628. Bruce, V., & Humphreys, G.W. (1994).
Recognising objects and faces. Visual Cognition, 1, 141--180. Bruce, V.,
& Langton, S. (1994). The use of pigmentation and shading information in
recognising the sex and identities of faces. Perception, 23, 803--822.
Bruce, V., & Morgan, M.J. (1975). Violations of symmetry and repetition
in visual patterns. Perception, 4, 239--249. Bruce, V., & Valentine, T.
(1988). When a nod's as good as a wink: The role of dynamic information
in facial recognition. In M.M. Gruneberg, P.E. Morris, & R.N. Sykes
(Eds.), Practical aspects of memory: Current research and issues,
Volume 1. Chichester, UK: Wiley. Bruce, V., & Young, A. (1998). In the
eye of the beholder: The science of face perception. Oxford University
Press. Bruner, J.S., & Goodman, C.C. (1947). Value and need as
organising factors in perception. Journal of Abnormal and Social
Psychology, 42, 33--44. Buccino, G., Binkofski, F., Fink, G.R., Fadiga,
L., Fogassi, L., Gallese, V., Seitz, R.J., Zilles, K., Rizzolatti, G., &
Freund, H.J. (2001). Action observation activates premotor and parietal
areas in a somatotopic manner: An fMRI study. European Journal of
Neuroscience, 13, 400--404. Bullier, J. (2001). Feedback connections and
conscious vision. Trends in Cognitive Sciences, 5, 369--370. Bülthoff,
H.H., & Edelman, S. (1992). Psychophysical support for a two-dimensional
view interpolation

424

REFERENCES

theory of object recognition. Proceedings of the National Academy of
Sciences, 89, 60--64. Bülthoff, H.H., & Mallot, H.A. (1988). Integration
of depth modules: Stereo and shading. Journal of the Optical Society of
America A, 5, 1749--1758. Bülthoff, I., Sinha, P., & Bülthoff, H.H.
(1996). Topdown influence of recognition on stereoscopic depth
perception. Investigative Ophthalmology and Visual Science, 37, 5168.
Burbeck, C.A., & Kelly, D.H. (1984). Role of local adaptation in the
fading of stabilized images. Journal of the Optical Society of America
A, 1, 216--220. Burger, J., & Gochfeld, M. (1981). Discrimination of the
threat of direct versus tangential approach to the nest by incubating
herring and great blackbacked gulls. Journal of Comparative and
Physiological Psychology, 95, 676--684. Burr, D.C, Holt, J., Johnstone,
J.R., & Ross, J. (1982). Selective depression of motion sensitivity
during saccades. Journal of Physiology, 333, 1--15. Burr, D.C., &
Morrone, M.C. (1994). The role of features in structuring visual images.
In G.R. Bock & J.A. Goode (Eds.), Higher-order processing in the visual
system (pp. 129--141. Ciba Symposium 184). Chichester, UK: Wiley & Sons.
Burr, D.C., Morrone, M.C., & Ross, J. (1994). Selective suppression of
the magnocellular visual pathway during saccadic eye movements. Nature,
371, 511--513. Burr, D.C., Morrone, M.C., & Ross, J. (2001). Separate
visual representations for perception and action revealed by saccadic
eye movements. Current Biology, 11, 798--802. Burr, D.C., Morrone, M.C.,
& Vaina, L.M. (1998). Large receptive fields for optic flow detection in
humans. Vision Research, 38, 1731--1743. Burr, D.C., Ross, J., &
Morrone, M.C. (1986). Seeing objects in motion. Proceedings of the Royal
Society of London B, 227, 249--265. Burr, D.C., & Santoro, L. (2001).
Temporal integration of optic flow, measured by contrast and coherence
thresholds. Vision Research, 41, 1891--1899. Burt, D.M., & Perrett, D.I.
(1995). Perception of age in adult caucasian male faces -- computer
graphic manipulation of shape and colour information. Proceedings of the
Royal Society of London, Series B -- Biological Sciences, 259, 137--143.
Burt, P., & Julesz, B. (1980). Modifications of the classical notion of
Panum's fusional area. Perception, 9, 671--682. Burton, A.M. (1994).
Learning new faces in an interactive activation and competition model.
Visual Cognition, 1, 313--348.

Burton, A.M., Bruce, V., & Dench, N. (1993). What's the difference
between men and women? Evidence from facial measurement. Perception, 22,
153--176. Bushnell, I.W.R., Sai, F., & Mullin, J.T. (1989). Neonatal
recognition of the mother's face. British Journal of Developmental
Psychology, 7, 3--15. Butterworth, G. (1983). Structure of the mind in
human infancy. In L.P. Lipsett (Ed.), Advances in infancy research. Vol
2. Norwood, NJ: Ablex. Butterworth, G., & Hicks, L. (1977). Visual
proprioception and postural stability in infancy. A developmental study.
Perception, 6, 255--262. Caine, N.G., & Mundy, N.I. (2000).
Demonstration of a foraging advantage for trichromatic marmosets
(Callithrix geoffroyi) dependent on food colour. Proceedings of the
Royal Society of London B, 267, 439--444. Callaway, E.M. (1998). Local
circuits in primary visual cortex of the macaque monkey. Annual Review
of Neuroscience, 21, 47--74. Cameron, D.A., & Pugh, E.N. (1991). Double
cones as a basis for a new type of polarization vision in vertebrates.
Nature, 353, 161--164. Camhi, J.M. (1970). Yaw-correcting postural
changes in locusts. Journal of Experimental Biology, 52, 519-- 531.
Campbell, F.W., & Robson, J.G. (1968). Application of Fourier analysis
to the visibility of gratings. Journal of Physiology, 197, 551--566.
Campbell, F.W., & Wurtz, R.H. (1978). Saccadic omission: Why do we not
see a grey-out during a saccadic eye movement? Vision Research, 18,
1297--1303. Campbell, R., Heywood, C.A., Cowey, A., Regard, M., &
Landis, T. (1990). Sensitivity to eye gaze in prosopagnosic patients and
monkeys with superior temporal sulcus ablations. Neuropsychologia, 28,
1123--1142. Cannon, M.W., & Fullenkamp, S.C. (1991). Spatial
interactions in apparent contrast: Inhibitory effects among grating
patterns of different spatial frequencies, spatial positions and
orientations. Vision Research, 31, 1985--1998. Canny, J. (1986). A
computational approach to edge detection. IEEE Transactions PAMI, 8,
679--698. Carey, D.P. (2001). Do action systems resist visual illusions?
Trends in Cognitive Sciences, 5, 109--113. Carey, S. (1992). Becoming a
face expert. Philosophical Transactions of the Royal Society of London
B, 335, 95--103. Carman, G.J., & Welch, L. (1992). Three-dimensional
illusory contours and surfaces. Nature, 360, 585-- 587.

REFERENCES

Carney, T. (1997). Evidence for an early motion system which integrates
information from the two eyes. Vision Research, 37, 2361--2368.
Carpenter, R.H.S. (1988). Movements of the eyes (2nd Edn). London: Pion
Press. Cavanagh, P. (1991). Short-range vs long-range motion: Not a
valid distinction. Spatial Vision, 5, 303--309. Cavanagh, P. (1992).
Attention-based motion perception. Science, 257, 1563--1565. Cavanagh,
P., & Mather, G. (1989). Motion: The long and short of it. Spatial
Vision, 4, 103--129. Chen, C.-C., & Tyler, C.W. (2001). Lateral
sensitivity modulation explains the flanker effect in contrast
discrimination. Proceedings of the Royal Society B, 268, 509--516.
Chodosh, L.A., Lifson, L.E., & Tabin, C. (1995). Letter. Science, 268,
1682--1683. Chomsky, N. (1959). Review of Verbal Behaviour by Skinner.
Language, 35, 26--58. Chubb, C., & Sperling, G. (1988). Drift-balanced
random stimuli: A general basis for studying nonFourier motion
perception. Journal of the Optical Society of America A, 5, 1986--2006.
Chubb, C., Sperling, G., & Solomon, J.A. (1989). Texture interactions
determine perceived contrast. Proceedings of the National Academy of
Sciences of the USA, 86, 9631--9635. Chun, M.M. (2000). Contextual
cueing of visual attention. Trends in Cognitive Sciences, 4, 170--178.
Cleland, B.G., & Levick, W.R. (1974). Properties of rarely encountered
types of ganglion cells in the cat's retina and an overall
classification. Journal of Physiology, 240, 457--492. Cline, M.G.
(1967). The perception of where a person is looking. American Journal of
Psychology, 80, 41--50. Clowes, M.B. (1971). On seeing things.
Artificial Intelligence, 2, 79--112. Colby, C.L., & Goldberg, M.E.
(1999). Space and attention in parietal cortex. Annual Review of
Neuroscience, 22, 319--349. Collett, T.S. (1977). Stereopsis in toads.
Nature, 267, 349--351. Cook, P.B., & McReynolds, J.S. (1998). Lateral
inhibition in the inner retina is important for spatial tuning of
ganglion cells. Nature Neuroscience, 1, 714--719. Cooper, E.E., &
Biederman, I. (1993). Metric versus viewpoint-invariant shape
differences in visual object recognition. Poster presented at the Annual
Meeting of the Association for Research in Vision and Ophthalmology,
Sarasota, Florida, May. Cooper, L.A., Schacter, D.L., Ballesteros, S., &
Moore,

425

C. (1992). Priming and recognition of transformed three-dimensional
objects: Effects of size and reflection. Journal of Experimental
Psychology: Learning, Memory and Cognition, 18, 43--57. Cornsweet, T.N.
(1970). Visual perception. New York: Academic Press. Cott, H.B. (1940).
Adaptive coloration in animals. London: Methuen. Cowey, A., & Stoerig,
P. (1995). Blindsight in monkeys. Nature, 373, 247--249. Croner, L.J., &
Kaplan, E. (1995). Receptive fields of P and M ganglion cells across the
primate retina. Vision Research, 35, 7--24. Croze, H. (1970). Searching
image in carrion crows. Zeitschrift für Tierpsychologie supplement 5,
cited in Edmunds, M. (1974). Defence in animals. New York: Longman.
Cumming, B. (1994). Motion-in-depth. In A.T. Smith, & R.J. Snowden
(Eds.), Visual detection of motion. London: Academic Press. Cumming,
B.G., Johnston, E.B., & Parker, A.J. (1993). Effects of different
texture cues on curved surfaces viewed stereoscopically. Vision
Research, 33, 827--838. Cumming, B.G., & Parker, A.J. (2000). Local
disparity not perceived depth is signaled by binocular neurons in
cortical area V1 of the macaque. Journal of Neuroscience, 20,
4758--4767. Cutting, J.E. (1978). Generation of synthetic male and
female walkers through manipulation of a biomechanical invariant.
Perception, 7, 393--405. Cutting, J.E. (1986). Perception with an eye
for motion. Cambridge, MA: MIT Press. Cutting, J.E., & Kozlowski, L.T.
(1977). Recognizing friends by their walk: Gait perception without
familiarity cues. Bulletin of the Psychonomic Society, 9, 353--356.
Cutting, J.E., & Proffitt, D.R. (1981). Gait perception as an example of
how we may perceive events. In R.D. Walk, & H.L. Pick, Jr (Eds.),
Intersensory perception and sensory integration. New York: Plenum Press.
Cutting, J.E., Proffitt, D.R., & Kozlowski, L.T. (1978). A biomechanical
invariant for gait perception. Journal of Experimental Psychology: Human
Perception and Performance, 4, 357--372. Dakin, S.C. (1997). Glass
patterns: Some contrast effects re-evaluated. Perception, 26, 253--268.
Dakin, S.C., & Bex, P.J. (2001). Local and global visual grouping:
tuning for spatial frequency and contrast. Journal of Vision
\[http://JournalOfVision.org/1/2/4/\], 1, 99--111.

426

REFERENCES

Dakin, S.C., & Hess, R.F. (1998). Spatial frequency tuning of visual
contour integration. Journal of the Optical Society of America A, 15,
1486--1499. Dannemiller, J.L., Babler, T.G., & Babler, B.L. (1996). On
catching fly balls. Science, 273, 256--257. Dartnall, H.J.A., Bowmaker,
J.K., & Mollon, J.D. (1983). Human visual pigments:
Microspectrophotometric results from the eyes of seven persons.
Proceedings of the Royal Society of London B, 220, 115--130. Das, A., &
Gilbert, C.D. (1999). Topography of contextual modulations mediated by
short-range interactions in primary visual cortex. Nature, 399,
656--661. David, C.T. (1985). Visual control of the partition of flight
force between lift and thrust in free-flying Drosophila. Nature, 313,
48--50. Davies, G.M., Ellis, H.D., & Shepherd, J.W. (1978). Face
recognition accuracy as a function of mode of representation. Journal of
Applied Psychology, 63, 180--187. Davies, M.N.O., & Green, P.R. (1988).
Head-bobbing during walking, running and flying: Relative motion
perception in the pigeon. Journal of Experimental Biology, 138, 71--91.
Davies, M.N.O., & Green, P.R. (1990). Flow-field variables trigger
landing in hawk but not in pigeons. Naturwissenschaften, 77, 142--144.
Davis, J.M. (1975). Socially induced flight reactions in pigeons. Animal
Behaviour, 23, 597--601. Dawkins, M.S., & Guilford, T. (1994). Design of
an intention signal in the bluehead wrasse (Thalassoma bifasciatum).
Proceedings of the Royal Society of London B, 257, 123--128. DeAngelis,
G.C. (2000). Seeing in three dimensions: The neurophysiology of
stereopsis. Trends in Cognitive Sciences, 4, 80--90. DeAngelis, G.C.,
Anzai, A., Ohzawa, I., & Freeman, R.D. (1995). Receptive field structure
in the visual cortex -- does selective stimulation induce plasticity?
Proceedings of the National Academy of Sciences of the United States of
America, 92, 9682--9686. DeAngelis, G.C., Cumming, B.C., & Newsome, W.T.
(1998). Cortical area MT and the perception of stereoscopic depth.
Nature, 394, 677--680. DeAngelis, G.C., Freeman, R.D., & Ohzawa, I.
(1994). Length and width tuning of neurons in the cat's primary visual
cortex. Journal of Neurophysiology, 71, 347--374. DeAngelis, G.C.,
Ohzawa, I., & Freeman, R.D. (1991). Depth is encoded in the visual
cortex by a specialized receptive field structure. Nature, 352,
156--159.

DeAngelis, G.C., Ohzawa, I., & Freeman, R.D. (1993). Spatiotemporal
organization of simple-cell receptive fields in the cat's striate
cortex. II. Linearity of temporal and spatial summation. Journal of
Neurophysiology, 69, 1118--1135. Decety, J., & Grèzes, J. (1999). Neural
mechanisms subserving the perception of human actions. Trends in
Cognitive Sciences, 3, 172--178. de Monasterio, F.M. (1978). Properties
of ganglion cells with atypical receptive-field organization in retina
of macaques. Journal of Neurophysiology, 41, 1435--1449. de Monasterio,
F.M., & Gouras, P. (1975). Functional properties of ganglion cells of
the rhesus monkey retina. Journal of Physiology, 251, 167--195. Denton,
G.G. (1976). The influence of adaptation on subjective velocity for an
observer in simulated rectilinear motion. Ergonomics, 19, 409--430.
Derrington, A., & Cox, M. (1998). Temporal resolution of dichoptic and
second-order motion mechanisms. Vision Research, 38, 3531--3539.
Derrington, A.M., Krauskopf, J., & Lennie, P. (1984). Chromatic
mechanisms in lateral geniculate nucleus of macaque. Journal of
Physiology, 357, 241--265. Derrington, A.M., & Lennie, P. (1984).
Spatial and temporal contrast sensitivities of neurones in lateral
geniculate nucleus of macaque. Journal of Physiology, 357, 219--240.
Desimone, R., & Schein, S.J. (1987). Visual properties of neurons in
area V4 of the macaque: Sensitivity to stimulus form. Journal of
Neurophysiology, 57, 835--868. Desmurget, M., & Grafton, S. (2000).
Forward modelling allows feedback control for fast reaching movements.
Trends in Cognitive Sciences, 4, 423--431. DeSouza, J.F.X., Dukelow,
S.P., Gati, J.S., Menon, R.S., Andersen, R.A., & Vilis, T. (2000). Eye
position signal modulates a human parietal pointing region during
memory-guided movements. Journal of Neuroscience, 20, 5835--5840. De
Valois, R.L., Abramov, I., & Jacobs, G.H. (1966). Analysis of response
patterns of LGN cells. Journal of the Optical Society of America, 56,
966--977. De Valois, R.L., Albrecht, D.G., & Thorell, L.G. (1982).
Spatial frequency selectivity of cells in macaque visual cortex. Vision
Research, 22, 545-- 559. De Valois, R.L., Cottaris, N.P., Mahon, L.E.,
Elfar, S.D., & Wilson, J.A. (2000). Spatial and temporal receptive
fields of geniculate and cortical cells and directional selectivity.
Vision Research, 40, 3685-- 3702.

REFERENCES

De Valois, R.L., & De Valois, K.K. (1990). Spatial vision. Oxford:
Oxford University Press. De Valois R.L., Thorell, L.G., & Albrecht, D.G.
(1985). Periodicity of striate cortex cell receptive fields. Journal of
the Optical Society of America A, 2, 1115--1123. DeYoe, E., Knierem, J.,
Sagi, D., Julesz, B., & Van Essen, D. (1986). Single unit responses to
static and dynamic texture patterns in macaque V2 and V1 cortex.
Investigative Ophthalmology and Visual Science, 27, 18. DeYoe, E.A., &
Van Essen, D.C. (1985). Segregation of efferent connections and
receptive field properties in visual area V2 of the macaque. Nature,
317, 58--61. Diamond, R., & Carey, S. (1986). Why faces are and are not
special: An effect of expertise. Journal of Experimental Psychology:
General, 115, 107--117. Dijkstra, T.M.H., Schöner, G., & Gielen,
C.C.A.M. (1994). Temporal stability of the action--perception cycle for
postural control in a moving visual environment. Experimental Brain
Research, 97, 477--486. Dittrich, W., Gilbert, F.S., Green, P.R.,
McGregor, P.K., & Grewcock, D. (1993). Imperfect mimicry: A pigeon's
perspective. Proceedings of the Royal Society of London B, 251,
195--200. Dittrich, W.H., & Lea, S.E.G. (1994). Visual perception of
intentional motion. Perception, 23, 253--268. Dobkins, K.R., & Bosworth,
R.G. (2001). Effects of set-size and selective spatial attention on
motion processing. Vision Research, 41, 1501--1517. Douglas, R.H.,
Collett, T.S., & Wagner, H.-J. (1986). Accommodation in anuran Amphibia
and its role in depth vision. Journal of Comparative Physiology, 158,
133--143. Dowling, J.E. (1968). Synaptic organization of the frog
retina: An electron microscopic analysis comparing the retinas of frogs
and primates. Proceedings of the Royal Society of London, Series B, 170,
205--228. Driver, J., Davis, G., Ricciardelli, P., Kidd, P., Maxwell,
E., & Baron-Cohen, S. (1999). Gaze perception triggers reflexive
visuospatial orienting. Visual Cognition, 6, 509--540. Duffy, C.J., &
Wurtz, R.H. (1991). Sensitivity of MST neurons to optic flow stimuli. I.
A continuum of response selectivity to large-field stimuli. Journal of
Neurophysiology, 65, 1329--1345. Eagle, R.A., & Rogers, B.J. (1996).
Motion detection is limited by element density not spatial frequency.
Vision Research, 36, 545--558. Edelman, S. (1995). Representation,
similarity and the Chorus of Prototypes. Minds and Machines, 5, 45--68.

427

Edelman, S., & Bülthoff, H.H. (1992). Orientation dependence in the
recognition of familiar and novel views of three-dimensional objects.
Vision Research, 32, 2385--2400. Edwards, A.S. (1946). Body sway and
vision. Journal of Experimental Psychology, 36, 526--535. Edwards, D.P.,
Purpura, K.P., & Kaplan, E. (1995). Contrast sensitivity and spatial
frequency response of primate cortical neurons in and around the
cytochrome oxidase blobs. Vision Research, 35, 1501--1523. Edwards, M.,
& Badcock, D.R. (1994). Global motion perception: Interaction of the ON
and OFF pathways. Vision Research, 34, 2849--2858. Edwards, M., Badcock,
D.R., & Smith, A.T. (1998). Independent speed-tuned global-motion
systems. Vision Research, 38, 1573--1580. Edwards, M., & Nishida, S.
(1999). Global motion detection with transparent motion signals. Vision
Research, 39, 2239--2249. Egelhaaf, M., Hausen, K., Reichardt, W., &
Wehrhahn, C. (1989). Visual course control in flies relies on neuronal
computation of object and background motion. Trends in Neuroscience, 11,
351--358. Egelhaaf, M., Kern, R., Krapp, H.G., Kretzberg, J., Kurtz, R.,
& Warzecha, A.-K. (2002). Neural encoding of behaviourally relevant
visual-motion information in the fly. Trends in Neurosciences, 25,
96--102. Eisner, T., Silberglied, R.E., Aneshansley, D., Carrel, J.E., &
Howland, H.C. (1969). Ultraviolet videoviewing: The television camera as
an insect eye. Science, 146, 1172--1174. Ejima, Y., & Takahashi, S.
(1984). Facilitatory and inhibitory after-effect of spatially localized
grating adaptation. Vision Research, 24, 979--985. Ejima, Y., &
Takahashi, S. (1985). Effect of localized grating adaptation as a
function of separation along the length axis between test and adaptation
areas. Vision Research, 25, 1701--1707. Ekman, P. (1979). About brows:
Emotional and conversational signals. In M. von Cranach, K. Foppa, W.
Lepenies, & D. Ploog (Eds.), Human ethology. Cambridge: Cambridge
University Press. Ekman, P. (1982). Emotion and the human face, Second
edition. Cambridge: Cambridge University Press. Ekman, P. (1992). Facial
expressions of emotion: An old controversy and new findings.
Philosophical Transactions of the Royal Society of London B, 335,
63--69. Ekman, P. (1994). Strong evidence for universals in

428

REFERENCES

facial expressions: A reply to Russell's mistaken critique.
Psychological Bulletin, 115, 268--287. Ekman, P., & Freisen, W.V.
(1978). Facial Action Coding System. Palo Alto, CA: Consulting
Psychologists Press. Ekman, P., & Friesen, W.V. (1982a). Felt, false and
miserable smiles. Journal of Nonverbal Behaviour, 6, 238--252. Ekman,
P., & Friesen, W. (1982b). Measuring facial movement with the Facial
Action Coding System. In P. Ekman (Ed.), Emotion in the human face:
Second edition. Cambridge: Cambridge University Press. Ekman, P.,
Friesen, W.V., & Ellsworth, P. (1982). Does the face provide accurate
information? In P. Ekman (Ed.), Emotion in the human face: Second
edition. Cambridge: Cambridge University Press. Ekman, P., & Oster, H.
(1982). Review of research, 1970--1980. In P. Ekman (Ed.), Emotion in
the human face: Second edition. Cambridge: Cambridge University Press.
Elder, J.H., & Zucker, S.W. (1998). Local scale control for edge
detection and blur estimation. IEEE Transactions on Pattern Analysis,
20, 699--716. Emerson, R.C., Bergen, J.R., & Adelson, E.H. (1992).
Directionally selective complex cells and the computation of motion
energy in cat visual cortex. Vision Research, 32, 203--218. Engel, A.K.,
& Singer, W. (2001). Temporal binding and the neural correlates of
sensory awareness. Trends in Cognitive Sciences, 5, 16--25.
Enroth-Cugell, C., & Robson, J.G. (1966). The contrast sensitivity of
retinal ganglion cells of the cat. Journal of Physiology, 187, 517--552.
Erkelens, C. (1988). Fusional limits for a large randomdot stereogram.
Vision Research, 28, 345--353. Ewert, J.P. (1974). The neural basis of
visually guided behaviour. Scientific American, 230, March, 34--49.
Fechner, G.T. (1860). Elemente der Psychophysik. Leipzig, Germany:
Brechtkopf & Härtel. Feldman, J.A. (1985). Four frames suffice: A
provisional model of vision and space. The Behavioural and Brain
Sciences, 8, 265--289. Fender, D.H., & Julesz, B. (1967). Extension of
Panum's fusional area in binocularly stabilized vision. Journal of the
Optical Society of America, 57, 819-- 830. Ferster, D. (1981). A
comparison of binocular depth mechanisms in areas 17 and 18 of the cat
visual cortex. Journal of Physiology, 311, 623--655. Ferster, D., &
Miller, K.D. (2000). Neural mechanisms

of orientation selectivity in the visual cortex. Annual Review of
Neuroscience, 23, 441--471. Fetz, E.E. (1992). Are movement parameters
recognizably coded in the activity of single neurons? Behavioural and
Brain Sciences, 15, 679--690. Field, D.J. (1987). Relations between the
statistics of natural images and the response properties of cortical
cells. Journal of the Optical Society of America, A4, 2379--2394. Field,
D.J., Hayes, A., & Hess, R.F. (1993). Contour integration by the human
visual system: Evidence for a local "association field". Vision
Research, 33, 173--193. Field, T.M., Woodson, R., Greenberg, R., &
Cohen, D. (1982). Discrimination and imitation of facial expressions by
neonates. Science, 281, 179--181. Fitch, H.L., Tuller, B., & Turvey,
M.T. (1982). The Bernstein Perspective III. Timing of coordinative
structures with special reference to perception. In J.A.S. Kelso (Ed.),
Human motor behaviour: An introduction. Hillsdale, NJ: Lawrence Erlbaum
Associates Inc. Flanagan, P., Cavanagh, P., & Favreau, O.E. (1990).
Independent orientation-selective mechanisms for the cardinal directions
of colour space. Vision Research, 30, 769--778. Fleet, D.J., Jepson,
A.D., & Jenkin, M.R.M. (1991). Phase-based disparity measurement.
Computer Vision, Graphics and Image Processing: Image Understanding, 53,
198--210. Flin, R., & Dziurawiec, S. (1989). Developmental factors in
face processing. In A.W. Young, & W.D. Ellis (Eds.), Handbook of
research on face processing. Amsterdam: North Holland. Fodor, J.A.
(1983). The modularity of mind. Cambridge, MA: MIT Press. Fodor, J.A., &
Pylyshyn, Z.W. (1981). How direct is visual perception? Some reflections
on Gibson's 'Ecological Approach'. Cognition, 9, 139--196. Fodor, J.A.,
& Pylyshyn, Z.W. (1988). Connectionism and cognitive architecture: A
critical analysis. Cognition, 28, 3--71. Foley, J.M. (1980). Binocular
distance perception. Psychological Review, 87, 411--435. Foley, J.M.
(1994). Human luminance pattern-vision mechanisms: Masking experiments
require a new model. Journal of the Optical Society of America A, 11,
1710--1719. Foley, J.M., & Boynton, G.M. (1993). Forward pattern masking
and adaptation: Effects of duration, interstimulus interval, contrast,
and spatial and temporal frequency. Vision Research, 33, 959--980.

REFERENCES

Fox, R., Lehmkuhle, S.W., & Westendorff, D.H. (1976). Falcon visual
acuity. Science, 192, 263--265. Freeman, R.D., & Ohzawa, I. (1990). On
the neurophysiological organization of binocular vision. Vision
Research, 30, 1661--1676. Freeman, T.C.A., & Banks, M.S. (1998).
Perceived head-centric speed is affected by both extra-retinal and
retinal errors. Vision Research, 38, 941--945. Freeman, T.C.A., Harris,
M.G., & Tyler, P.A. (1994). Human sensitivity to temporal proximity: The
role of spatial and temporal speed gradients. Perception and
Psychophysics, 55, 689--699. Freyd, J.J. (1987). Dynamic mental
representations. Psychological Review, 94, 427--438. Friedman, M.B.
(1975). Visual control of head movements during avian locomotion.
Nature, 255, 67--69. Friesen, C.K., & Kingstone, A. (1998). The eyes
have it! Reflexive orienting is triggered by nonpredictive gaze.
Psychonomic Bulletin and Review, 5, 490--495. Frisby, J.P. (1979).
Seeing: Illusion, brain and mind. Oxford: Oxford University Press.
Frost, B.J. (1978). The optokinetic basis of headbobbing in the pigeon.
Journal of Experimental Biology, 74, 187--195. Gallese, V., & Goldman,
A. (1998). Mirror neurons and the simulation theory of mind-reading.
Trends in Cognitive Sciences, 2, 493--501. Gallese, V., Fadiga, L.,
Fogassi, L., & Rizzolatti, G. (1996). Action recognition in the premotor
cortex. Brain, 119, 593--609. Galton, F. (1907). Inquiries into human
faculty and its development. London: J.M. Dent and Sons Ltd. Garnham, A.
(1987). Artificial intelligence: An introduction. London: Routledge &
Kegan Paul. Gauthier, I., Skudlarski, P., Gore J.C., & Anderson, A.W.
(2000). Expertise for cars and birds recruits brain areas involved in
face recognition. Nature Neuroscience, 3, 191--197. Gauthier, I., &
Tarr, M.J. (1997). Becoming a 'greeble' expert: Exploring mechanisms for
face recognition. Vision Research, 37, 1673--1682. Gauthier, I., Tarr,
M.J., Anderson, A.W., Skudlarski, P., & Gore, J.C. (1999). Activation of
the middle fusiform 'face area' increases with expertise in recognizing
novel objects. Nature Neuroscience, 2, 568--573 Gauthier, I., Williams,
P., Tarr, M.J., & Tanaka, J. (1998). Training 'greeble' experts: A
framework for studying expert object recognition processes. Vision
Research, 38, 2401--2428. Georgeson, M.A. (1980). The perceived spatial
frequency, contrast and orientation of illusory gratings. Perception, 9,
695--712.

429

Georgeson, M.A. (1992). Human vision combines oriented filters to
compute edges. Proceedings of the Royal Society of London B, 249,
235--245. Georgeson, M.A. (1994). From filters to features: Location,
orientation, contrast and blur. In Higher order processing in the visual
system (pp. 147--165), (Ciba Foundation Symposium 184). Chichester, UK:
Wiley. Georgeson, M.A., & Freeman, T.C.A. (1997). Perceived location of
bars and edges in one-dimensional images: Computational models and human
vision. Vision Research, 37, 127--142. Georgeson, M.A., & Georgeson,
J.M. (1987). Facilitation and masking of briefly presented gratings:
Time-course and contrast dependence. Vision Research, 27, 369--379.
Georgeson M.A., & Harris, M.G. (1984). Spatial selectivity of contrast
adaptation: Models and data. Vision Research, 24, 729--741. Georgeson,
M.A., & Harris, M.G. (1990). The temporal range of motion sensing and
motion perception. Vision Research, 30, 615--619. Georgeson, M.A., &
Scott-Samuel, N.E. (1999). Motion contrast: A new metric for direction
discrimination. Vision Research, 39, 4393--4402. Georgeson, M.A., &
Scott-Samuel, N.E. (2000). Spatial resolution and receptive field height
of motion sensors in human vision. Vision Research, 40, 745--758.
Georgeson, M.A., & Shackleton, T.M. (1989). Monocular motion sensing,
binocular motion perception. Vision Research, 29, 1511--1523.
Georgopoulos, A.P. (1991). Higher order motor control. Annual Review of
Neuroscience, 14, 361--378. Gibson, E.J., Gibson, J.J., Smith, O.W., &
Flock, H.R. (1959). Motion parallax as a determinant of perceived depth.
Journal of Experimental Psychology, 58, 40--51. Gibson, E.J., & Walk,
R.D. (1960). The 'visual cliff'. Scientific American, 202, April,
64--71. Gibson, J.J. (1947). Motion picture testing and research. AAF
Aviation Psychology Research Report No 7. Washington, DC: Government
Printing Office. Gibson, J.J. (1950a). The perception of the visual
world. Boston: Houghton Mifflin. Gibson, J.J. (1950b). The perception of
visual surfaces. American Journal of Psychology, 63, 367--384. Gibson,
J.J. (1961). Ecological optics. Vision Research, 1, 253--262. Gibson,
J.J. (1966). The senses considered as perceptual systems. Boston:
Houghton Mifflin. Gibson, J.J. (1975). The implications of experiments
on the perception of space and motion. Final Report to Office of Naval
research, Arlington, VA.

430

REFERENCES

Gibson, J.J. (1979). The ecological approach to visual perception.
Boston: Houghton Mifflin. Gibson, J.J., & Cornsweet, J. (1952). The
perceived slant of visual surfaces -- optical and geographical. Journal
of Experimental Psychology, 44, 11--15. Gibson, J.J., & Dibble, F.N.
(1952). Exploratory experiments on the stimulus conditions for the
perception of a visual surface. Journal of Experimental Psychology, 43,
414--419. Gibson, J.J., & Pick, A.D. (1963). Perception of another
person's looking behaviour. American Journal of Psychology, 76,
386--394. Gibson, J.J., & Radner, M. (1937). Adaptation, aftereffect and
contrast in the perception of tilted lines. I. Quantitative studies.
Journal of Experimental Psychology, 20, 453--467. Gibson, J.J., &
Waddell, D. (1952). Homogeneous retinal stimulation and visual
perception. American Journal of Psychology, 65, 263--270. Gilbert, C.,
Ito, M., Kapadia, M., & Westheimer, G. (2000). Interactions between
attention, context and learning in primary visual cortex. Vision
Research, 40, 1217--1226. Gilbert, C.D. (1995). Dynamic properties of
adult visual cortex. In M.S. Gazzaniga (Ed.), The cognitive
neurosciences. Cambridge, MA: MIT Press. Gilbert, C.D., & Wiesel, T.N.
(1992). Receptive field dynamics in adult primary visual cortex. Nature,
356, 150--152. Giurfa, M., & Capaldi, E.A. (1999). Vectors, routes and
maps: New discoveries about navigation in insects. Trends in
Neurosciences, 22, 237--242. Glass, L. (1969). Moire effects from random
dots. Nature, 243, 578--580. Glennerster, A., Hansard, M.E., &
Fitzgibbon, A.W. (2001). Fixation could simplify, not complicate, the
interpretation of retinal flow. Vision Research, 41, 815--834.
Gluhbegovic, N., & Williams, T.H. (1980). The human brain: A
photographic atlas. Hagerstown, MD: Harper & Row. Golani, I. (1976).
Homeostatic motor processes in mammalian interactions: A choreography of
display. In P.P.G. Bateson, & P.H. Klopfer (Eds.), Perspectives in
ethology Vol. II (pp. 69--134). New York: Plenum. Goodale, M.A., Ellard,
C.G., & Booth, L. (1990). The role of image size and retinal motion in
the computation of absolute distance by the Mongolian gerbil (Meriones
unguiculatus). Vision Research, 30, 399--413. Goodale, M.A., Milner,
A.D., Jakobson, L.S., & Carey,

D.P. (1991). A neurological dissociation between perceiving objects and
grasping them. Nature, 349, 154--156. Goodale, M.A., Pélisson, D., &
Prablanc, C. (1986). Large adjustments in visually guided reaching do
not depend on vision of the hand or perception of target displacement.
Nature, 320, 748--750. Goodman, L.J. (1965). The role of certain
optomotor reactions in regulating stability in the rolling plane during
flight in the desert locust Schistocerca gregaria. Journal of
Experimental Biology, 42, 385--407. Gorea, A., & Lorenceau, J. (1991).
Directional performances with moving plaids: Component-related and
plaid-related processing modes coexist. Spatial Vision, 5, 231--252.
Goren, C.C., Sarty, M., & Wu, R.W.K. (1975). Visual following and
pattern discrimination of face-like stimuli by newborn infants.
Pediatrics, 56, 544--549. Graham, N., & Nachmias, J. (1971). Detection
of grating patterns containing two spatial frequencies: A comparison of
single-channel and multiple-channel models. Vision Research, 11,
251--259. Graham, N.V.S. (1989). Visual pattern analyzers. Oxford
University Press. Gray, C.M., König, P., Engel, A.K., & Singer, W.
(1989). Oscillatory responses in cat visual cortex exhibit
inter-columnar synchronization which reflects global stimulus
properties. Nature, 338, 334--337. Gray, R., & Regan, D. (1998).
Accuracy of estimating time to collision using binocular and monocular
information. Vision Research, 38, 499--512. Gray, R., & Regan, D.
(2000). Estimating the time to collision with a rotating nonspherical
object. Vision Research, 40, 49--63. Graziano, M.S.A., Andersen, R.A., &
Snowden, R.J. (1994). Tuning of MST neurons to spiral motions. Journal
of Neuroscience, 14, 54--67. Green, P.R. (1998). Head orientation is
aligned with take-off trajectory as chicks jump. Experimental Brain
Research, 122, 295--300. Green, P.R., Davies, M.N.O., & Thorpe, P.H.
(1994). Head-bobbing and head orientation during landing flights of
pigeons. Journal of Comparative Physiology, 174, 249--256. Gregory, R.L.
(1972). Eye and brain (2nd Ed). London: Weidenfeld and Nicolson.
Gregory, R.L. (1973). The confounded eye. In R.L. Gregory, & E.H.
Gombrich (Eds.), Illusion in nature and art. London: Duckworth. Gregory,
R.L. (1980). Perceptions as hypotheses. Philo-

REFERENCES

sophical Transactions of the Royal Society of London, Series B, 290,
181--197. Griffin, D.R. (1958). Listening in the dark. New Haven, CT:
Yale University Press. Grimson, W.E.L. (1981). From images to surfaces.
Cambridge, MA: MIT Press. Grinvald, A., Lieke, E.E., Frostig, R.D., &
Hildesheim, R. (1994). Cortical point-spread function and longrange
lateral interactions revealed by real-time optical imaging of macaque
monkey primary visual cortex. Journal of Neuroscience, 14, 2545--2568.
Grossman, E., Donnelly, M., Price, R., Pickens, D., Morgan, V.,
Neighbor, G. et al. (2000). Brain areas involved in perception of
biological motion. Journal of Cognitive Neuroscience, 12, 711--720.
Grunewald, A., & Mingolla, E. (1998). Motion aftereffect due to
binocular sum of adaptation to linear motion. Vision Research, 38,
2963--2971. Grüsser, O.-J., Krizic, A., & Weiss, L.-R. (1987).
Afterimage movement during saccades in the dark. Vision Research, 27,
215--226. Gulick, W.L., & Lawson, R.B. (1976). Human stereopsis: A
psychophysical analysis. New York: Oxford University Press. Gurfinkel,
V.S., Kots, Ya.M., Krinsky, V.I., Pal'tsev, Ye.I., Feldman, A.G.,
Tsetlin, M.L., et al. (1971). Concerning tuning before movement. In I.M.
Gelfand, V.S. Gurfinkel, S.E. Fomin, & M.L. Tsetlin (Eds.), Models of
the structural--functional organisation of certain biological systems.
Cambridge, MA: MIT Press. Guzman, A. (1968). Decomposition of a visual
scene into three-dimensional bodies. AFIPS Proceedings of the Fall Joint
Computer Conference, 33, 291--304. Haarmeier, T., Thier, P., Repnow, M.,
& Petersen, D. (1997). False perception of motion in a patient who
cannot compensate for eye movements. Nature, 389, 849--852. Hadjikhani,
N., Liu, A.K., Dale, A.M., Cavanagh, P., & Tootell, R.B.H. (1998).
Retinotopy and colour sensitivity in human visual cortical area V8.
Nature Neuroscience, 1, 235--241. Hailman, J.P. (1977). Optical signals:
animal communication and light. Bloomington: Indiana University Press.
Hammett, S.T., & Smith, A.T. (1992). Two temporal channels or three? A
re-evaluation. Vision Research, 32, 285--291. Hammond, P., Mouat, G.SV.,
& Smith, A.T. (1986). Motion after-effects in cat striate cortex
elicited by moving texture. Vision Research, 26, 1055--1060. Hampton,
R.R. (1994). Sensitivity to information

431

specifying the line of gaze of humans in sparrows (Passer domesticus).
Behaviour, 130, 41--51. Hancock, P.J.B., Bruce, V., & Burton, M. (1998).
A comparison of two computer-based face identification systems with
human perceptions of faces. Vision Research, 38, 2277--2288 Harkness, L.
(1977). Chameleons use accommodation cues to judge distance. Nature,
267, 346--349. Harmon, L.D., & Julesz, B. (1973). Masking in visual
recognition: Effect of two-dimensional filtered noise. Science, 180,
1194--1197. Harris, J.M., & Bonas, W. (2002). Optic flow and scene
structure do not always contribute to the control of human walking.
Vision Research, 42, 1619--1626. Harris, J.P., & Gregory, R.L. (1973).
Fusion and rivalry of illusory contours. Perception, 2, 235--247.
Harris, M.G. (1980). Velocity sensitivity of the flicker to pattern
sensitivity ratio in human vision. Vision Research, 20, 687--691.
Harris, M.G. (1986). The perception of moving stimuli: A model of
spatiotemporal coding in human vision. Vision Research, 26, 1281--1287.
Harris, M.G. (1994). Optic and retinal flow. In A.T. Smith, & R.J.
Snowden (Eds.), Visual detection of motion. London: Academic Press.
Harris, M.G., & Carre, G. (2001). Is optic flow used to guide walking
while wearing a displacing prism? Perception, 30, 811--818. Hartline,
H.K., & Graham, C.H. (1932). Nerve impulses from single receptors in the
eye. Journal of Cellular and Comparative Physiology, 1, 227--295.
Hartline, H.K., Wagner, H.G., & Ratliff, F. (1956). Inhibition in the
eye of Limulus. Journal of General Physiology, 39, 651--673. Hasselmo,
M.E., Rolls, E.T., Baylis, G.C., & Nalwa, V. (1989). Object-centred
encoding by face-selective neurons in the cortex in the superior
temporal sulcus of the monkey. Experimental Brain Research, 75,
417--429. Hayhoe, M. (2000). Vision using routines: A functional account
of vision. Visual Cognition, 7, 43--64. Hayhoe, M.M., Bensinger, D.G., &
Ballard, D.H. (1998). Task constraints in visual working memory. Vision
Research, 38, 125--137. He, S., & MacLeod, D.I.A. (1998).
Contrastmodulation flicker: dynamics and spatial resolution of the
light-adaptation process. Vision Research, 38, 985--1000. He, S., &
MacLeod, D.I.A. (2000). Spatial and temporal properties of light
adaptation in the rod system. Vision Research, 40, 3073--3081. He, S., &
MacLeod, D.I.A. (2001). Orientation-

432

REFERENCES

selective adaptation and tilt after-effect from invisible patterns.
Nature, 411, 473--476. He, Z.J., & Nakayama, K. (1994). Apparent motion
determined by surface layout not by disparity or three-dimensional
distance. Nature, 367, 173--175. Heckenmuller, E.G. (1965).
Stabilization of the retinal image: A review of method, effects and
theory. Psychological Bulletin, 63, 157--169. Heeger, D., Boynton, G.,
Demb, J., Seidemann, E., & Newsome, W. (1999). Motion opponency in
visual cortex. Journal of Neuroscience, 19, 7162--7174. Heeger, D.J.
(1992a). Normalization of cell responses in cat striate cortex. Visual
Neuroscience, 9, 181--197. Heeger, D.J. (1992b). Half-squaring in
responses of cat striate cells. Visual Neuroscience, 9, 427--443.
Heider, F., & Simmel, M. (1944). An experimental study of apparent
behaviour. American Journal of Psychology, 57, 243--259. Heiligenberg,
W. (1973). Electrolocation of objects in the electric fish Eigenmannia.
Journal of Comparative Physiology, 87, 137--164. Heisenberg, M., & Wolf,
R. (1988). Reafferent control of optomotor yaw torque in Drosophila
melanogaster. Journal of Comparative Physiology, 163, 373--388. Heitger,
F., Rosenthaler, L., von der Heydt, R., Peterhans, E., & Kubler, O.
(1992). Simulation of neural contour mechanisms: From simple to
end-stopped cells. Vision Research, 32, 963--981. Helmholtz, H. von
(1866). Treatise on physiological optics, Vol III (trans. 1925 from the
third German edition, Ed. J.P.C. Southall) New York: Dover (1962).
Hendry, S.H.C., & Reid, R.C. (2000). The koniocellular pathway in
primate vision. Annual Review of Neuroscience, 23, 127--153. Hertz, M.
(1928). Figural perception in the jay bird. Zeitschrift für
vergleichende Physiologie, 7, 144--194. Trans. and abridged in Ellis,
W.D. (1955). A source book of gestalt psychology. London: Routledge &
Kegan Paul. Hertz, M. (1929). Figural perception in bees. Zeitschrift
für vergleichende Physiologie, 8, 693--748. Trans. and abridged in
Ellis, W.D. (1955). A source book of gestalt psychology. London:
Routledge & Kegan Paul. Hess, R.F., & Field, D. (1999). Integration of
contours: New insights. Trends in Cognitive Sciences, 3, 480--486. Hess,
R.F., & Holliday, I.E. (1992). The coding of spatial position by the
human visual system: Effects of spatial scale and contrast. Vision
Research, 32, 1085--1097.

Hess, R.F., Ledgeway, T., & Dakin, S. (2000). Impoverished second-order
input to global linking in human vision. Vision Research, 40,
3309--3318. Hess, R.F., & Snowden, R.J. (1992). Temporal properties of
human visual filters: Number, shapes and spatial covariation. Vision
Research, 32, 47--59. Hess, U., & Kleck, R.E. (1990). Differentiating
emotion elicited and deliberate facial expressions. European Journal of
Social Psychology, 20, 369--385. Heuer, H. (1993). Estimates of time to
contact based on changing size and changing target vergence. Perception,
22, 549--563. Heywood, C.A., & Cowey, A. (1987). On the role of cortical
area V4 in the discrimination of hue and pattern in macaque monkeys.
Journal of Neuroscience, 7, 2601--2617. Hildreth, E.C. (1984a).
Computations underlying the measurement of visual motion. Artificial
Intelligence, 23, 309--354. Hildreth, E.C. (1984b). The measurement of
visual motion. Cambridge, MA: MIT Press. Hildreth, E.C., Ando, H.,
Andersen, R.A., & Treue, S. (1995). Recovering three-dimensional
structure from motion with surface reconstruction. Vision Research, 35,
117--137. Hildreth, E.C., & Koch, C. (1987). The analysis of visual
motion: From computational theory to neuronal mechanisms. Annual Review
of Neuroscience, 10, 477--533. Hill, H., & Bruce, V. (1993). Independent
effects of lighting, orientation and stereopsis on the hollowface
illusion. Perception, 22, 887--897. Hill, H., & Bruce, V. (1994). A
comparison between the hollow-face and hollow-potato illusions.
Perception, 23, 1335--1337. Hill, H., Bruce, V., & Akamatsu, S. (1995).
Perceiving the sex and race of faces: Role of shape and colour.
Proceedings of the Royal Society of London B, 261, 367--373. Hinton,
G.E. (1984). Parallel computations for controlling an arm. Journal of
Motor Behaviour, 16, 171--194. Hochberg, J. (1950). Figure--ground
reversal as a function of visual satiation. Journal of Experimental
Psychology, 40, 682--686. Hochberg, J., & Brooks, V. (1960). The
psychophysics of form: Reversible perspective drawings of spatial
objects. American Journal of Psychology, 73, 337--354. Hoffman, D.D., &
Richards, W.A. (1984). Parts of recognition. Cognition, 18, 65--96.
Hollands, M.A., & Marple-Horvat, D.E. (1996). Visu-

REFERENCES

ally guided stepping under conditions of step cyclerelated denial of
visual information. Experimental Brain Research, 109, 343--356.
Hollands, M.A., Marple-Horvat, D.E., Henkes, S., & Rowan, A.K. (1995).
Human eye movements during visually guided stepping. Journal of Motor
Behaviour, 27, 155--163. Holliday, I.E., & Anderson, S.J. (1994).
Different processes underlie the detection of second-order motion at low
and high temporal frequencies. Proceedings of the Royal Society of
London B, 257, 165-- 173. Hollingworth, A., & Henderson, J.M. (2002).
Accurate visual memory for previously attended objects in natural
scenes. Journal of Experimental Psychology: Human Perception and
Performance, 28, 113--136. Honda, H. (1991). The time courses of visual
mislocalization and of extraretinal eye position signals at the time of
vertical saccades. Vision Research, 31, 1915--1921. Honda, H. (1993).
Saccade-contingent displacement of the apparent position of visual
stimuli flashed on a dimly illuminated structured background. Vision
Research, 33, 709--716. Hood, B.M., Willen, J.D., & Driver, J. (1998).
Adult's eyes trigger shifts of visual attention in human infants.
Psychological Science, 9, 131--134. Horn, B.K.P., & Brooks, M.J. (Eds.).
(1989). Shape from shading. Cambridge, MA: MIT Press. Horn, B.K.P., &
Schunck, B.G. (1981). Determining optical flow. Artificial Intelligence,
17, 185--203. Horton, J.C., & Hubel, D.H. (1981). Regular patchy
distribution of cytochrome oxidase staining in primary visual cortex of
macaque monkey. Nature, 292, 762--764. Howard, I.P., & Rogers, B.J.
(1995). Binocular vision and stereopsis. Oxford: Oxford University
Press. Hubel, D.H., & Wiesel, T.N. (1959). Receptive fields of single
neurons in the cat's striate cortex. Journal of Physiology, 148,
574--591. Hubel, D.H., & Wiesel, T.N. (1962). Receptive fields,
binocular interaction and functional architecture in the cat's visual
cortex. Journal of Physiology, 160, 106--154. Hubel, D.H., & Wiesel,
T.N. (1968). Receptive fields and functional architecture of monkey
striate cortex. Journal of Physiology, 195, 215--243. Hubel, D.H., &
Wiesel, T.N. (1970). Stereopsis vision in the macaque monkey. Nature,
225, 41--42. Hubel, D.H., & Wiesel, T.N. (1974). Sequence regularity and
geometry of orientation columns in the

433

monkey striate cortex. Journal of Comparative Neurology, 158, 267--294.
Hubel, D.H., & Wiesel, T.N. (1977). Functional architecture of macaque
monkey visual cortex. Proceedings of the Royal Society of London, Series
B, 198, 1--59. Huffman, D.A. (1971). Impossible objects as nonsense
sentences. In B. Meltzer, & D. Michie (Eds.), Machine intelligence 6.
Edinburgh: Edinburgh University Press. Huk, A.C., & Heeger, D.J. (2000).
Task-related modulation of visual cortex. Journal of Neurophysiology,
83, 3525--3536. Huk, A.C., & Heeger, D.J. (2002). Pattern-motion
responses in human visual cortex. Nature Neuroscience, 5, 72--75. Huk,
A.C., Ress, D., & Heeger, D.J. (2001). Neuronal basis of the motion
aftereffect reconsidered. Neuron, 32, 161--172. Hummel, J.E., &
Biederman, I. (1992). Dynamic binding in a neural network for shape
recognition. Psychological Review, 99, 480--517. Humphrey, G.K., &
Jolicoeur, P. (1993). An examination of the effects of axis
foreshortening, monocular depth cues, and visual field on object
identification. Quarterly Journal of Experimental Psychology, 46A,
137--159. Humphrey, G.K., & Khan, S.C. (1992). Recognizing novel views
of three-dimensional objects. Canadian Journal of Psychology, 46,
170--190. Humphreys, G.W. (1984). Shape constancy: The effects of
changing shape orientation and the effects of changing focal features.
Perception & Psychophysics, 35, 361--371. Hupé, J.-M., James, A.C.,
Girard, P., & Bullier, J. (2001). Response modulations by static texture
surround in area V1 of the macaque monkey do not depend on feedback
connections from V2. Journal of Neurophysiology, 85, 146--163. Hupé,
J.-M., James, A.C., Payne, B.R., Lomber, S.G., Girard, P., & Bullier, J.
(1998). Cortical feedback improves discrimination between figure and
background by V1, V2 and V3 neurons. Nature, 394, 784--787. Hyvärinen,
J., & Poranen, A. (1974). Function of the parietal associative area 7 as
revealed from cellular discharges in alert monkeys. Brain, 97, 673--692.
Itakura, S., & Anderson, J.R. (1996). Learning to use experimenter-given
cues during an object-choice task by a capuchin monkey. Current
Psychology of Cognition, 15, 103--112. Itakura, S., & Tanaka, M. (1998).
Use of experimenter-

434

REFERENCES

given cues during object-choice tasks by chimpanzees (Pan troglodytes),
an orangutan (Pongo pygmaeus), and human infants (Homo sapiens). Journal
of Comparative Psychology, 112, 119--126. Ito, M., & Gilbert, C.D.
(1999). Attention modulates contextual influences in the primary visual
cortex of alert monkeys. Neuron, 22, 593--604. Ittelson, W.H. (1952).
The Ames demonstrations in perception. Princeton, NJ: Princeton
University Press. Jackson, J.F., Ingram, W., & Campbell, H.W. (1976).
The dorsal pigmentation pattern of snakes as an antipredator strategy: A
multivariate approach. American Naturalist, 110, 1029--1053. Jacobs,
G.H. (1993). The distribution and nature of colour vision among the
mammals. Biological Reviews, 68, 413--471. Jain, R., Kasturi, R., &
Schunk, B.G. (1995). Machine vision. New York: McGraw-Hill. Janssen, P.,
Vogels, R., & Orban, G.A. (1999). Macaque inferior temporal neurons are
selective for disparitydefined three-dimensional shapes Proceedings of
the National Academy of Sciences of the USA, 96, 8217--8222. Jeannerod,
M. (1984). The timing of natural prehension movements. Journal of Motor
Behaviour, 16, 235--254. Johansson, G. (1973). Visual perception of
biological motion and a model for its analysis. Perception and
Psychophysics, 14, 201--211. Johnson, E.N., Hawken, M.J., & Shapley, R.
(2001). The spatial transformation of colour in the primary visual
cortex of the macaque monkey. Nature Neuroscience, 4, 409--416. Johnson,
M.H., Dziurawiec, S., Ellis, H., & Morton, J. (1991). Newborns
preferential tracking of face-like stimuli and its subsequent decline.
Cognition, 40, 1--19. Johnson, M.H., & Morton, J. (1991). Biology and
cognitive development: The case of face recognition. Oxford: Blackwell.
Johnston, A., & Clifford, C.W.G. (1995). A unified account of three
apparent motion illusions. Vision Research, 35, 1109--1123. Johnston,
A., McOwan, P.W., & Buxton, H. (1992). A computational model of the
analysis of some firstorder and second-order motion patterns by simple
and complex cells. Proceedings of the Royal Society of London B, 250,
297--306. Johnston, E.B., Cumming, B.G., & Landy, M.S. (1994).
Integration of stereopsis and motion shape cues. Vision Research, 34,
2259--2275. Johnston, E.B., Cumming, B.G., & Parker, A.J. (1993).

Integration of depth modules: Stereopsis and texture. Vision Research,
33, 813--826. Jones, J.P., & Palmer, L.A. (1987). An evaluation of the
two-dimensional Gabor filter model of simple receptive fields in cat
striate cortex. Journal of Neurophysiology, 58, 1233--1258. Jones, R.K.,
& Lee, D.N. (1981). Why two eyes are better than one: The two views of
binocular vision. Journal of Experimental Psychology: Human Perception
and Performance, 7, 30--40. Jones, R.M., & Tulunay-Keesey, U. (1980).
Phase selectivity of spatial frequency channels. Journal of the Optical
Society of America, 70, 66--70. Joyce, J. (1934). Ulysses. New York:
Random House. Judd, S.P.D., & Collett, T.S. (1998). Multiple stored
views and landmark guidance in ants. Nature, 392, 710--714. Judge, S.J.,
& Bradford, C.M. (1988). Adaptation to telestereoscopic viewing measured
by one-handed ballcatching performance. Perception, 17, 783--802. Judge,
S.J., & Rind, F.C. (1997). The locust DCMD: A movement-detecting neurone
tightly tuned to collision trajectories. Journal of Experimental
Biology, 200, 2209--2216. Julesz, B. (1965). Texture and visual
perception. Scientific American, 212, February, 38--48. Julesz, B.
(1971). Foundations of cyclopean perception. Chicago: University of
Chicago Press. Julesz, B. (1984). A brief outline of the texton theory
of human vision. Trends in Neurosciences, 7, 41--45. Julesz, B., &
Miller, J. (1975). Independent spatial-frequency-tuned channels in
binocular fusion and rivalry. Perception, 4, 125--143. Kaiser, M.K., &
Mowafy, L. (1993). Optical specification of time-to-passage: Observers'
sensitivity to global tau. Journal of Experimental Psychology: Human
Perception and Performance, 19, 1028--1040. Kaiser, M.K., & Phatak, A.V.
(1993). Things that go bump in the light: On the optical specification
of contact severity. Journal of Experimental Psychology: Human
Perception and Performance, 19, 194--202. Kalmus, H. (1949). Optomotor
responses in Drosophila and Musca. Physiologia Comparata et Oecologia,
1, 127--147. Kamachi, M., Bruce, V., Mukaida, S., Gyoba, J., Yoshikawa,
S., & Akamatsu, S. (2001). Dynamic properties influence the perception
of facial expressions. Perception, 30, 875--887. Kanade, T. (1981).
Recovery of the three-dimensional shape of an object from a single view.
Artificial Intelligence, 17, 409--460.

REFERENCES

Kanisza, G. (1976). Subjective contours. Scientific American, 234(4),
48--52. Kapadia, M.K., Ito, M., Gilbert, C.D., & Westheimer, G. (1995).
Improvement in visual sensitivity by changes in local context: Parallel
studies in human observers and in V1 of alert monkeys. Neuron, 15,
843--856. Kapadia, M.K., Westheimer, G., & Gilbert, C.D. (2000). Spatial
distribution of contextual interactions in primary visual cortex and in
visual perception. Journal of Neurophysiology, 84, 2048-- 2062. Kaplan,
E., & Shapley, R.M. (1986). The primate retina contains two types of
ganglion cells, with high and low contrast sensitivity. Proceedings of
the National Academy of Sciences of the USA, 83, 2755--2757. Karmeier,
K., Tabor, R., Egelhaaf, M., & Krapp, H.G. (2001). Early visual
experience and the receptivefield organisation of optic flow processing
interneurons in the fly motion pathway. Visual Neuroscience, 18, 1--8.
Katzir, G., Lotem, A., & Intrator, N. (1989). Stationary underwater prey
missed by reef herons, Egretta gularis: Head position and light
refraction at the moment of strike. Journal of Comparative Physiology,
165, 573--576. Kaufman, L. (1974). Sight and mind: An introduction to
visual perception. New York: Oxford University Press. Kay, B.A.,
Saltzman, E.L., & Kelso, J.A.S. (1991). Steady-state and perturbed
rhythmical movements: A dynamical analysis. Journal of Experimental
Psychology: Human Perception and Performance, 17, 183--197. Kayargadde,
V., & Martens, J.B. (1994a). Estimation of edge parameters and image
blur using polynomial transforms. CVGIP: Graphical models and image
processing, 56, 442--461. Kayargadde, V., & Martens, J.B. (1994b).
Estimation of perceived image blur. IPO Annual Progress Report, 29,
66--71. Eindhoven: Holland: Institute for Perception Research. Kelly,
D.H. (1979a). Motion and vision. I. Stabilized images of stationary
gratings. Journal of the Optical Society of America, 69, 1266--1274.
Kelly, D.H. (1979b). Motion and vision. II. Stabilized spatio-temporal
threshold surface. Journal of the Optical Society of America, 69,
1340--1349. Kelly, D.H., & Burbeck, C.A. (1980). Motion and vision. III.
Stabilized pattern adaptation. Journal of the Optical Society of
America, 70, 1283--1289. Kelso, J.A.S. (1995). Dynamic patterns: The
self-

435

organization of brain and behaviour. Cambridge, MA: MIT Press. Kelso,
J.A.S., Putnam, C.A., & Goodman, D. (1983). On the space-time structure
of human inter-limb coordination. Quarterly Journal of Experimental
Psychology, 35A, 347--375. Kemp, R., Pike, G., White, P., & Musselman,
A. (1996). Perception and recognition of normal and negative faces: The
role of shape from shading and pigmentation cues. Perception, 25, 37--52
Kennedy, J.M. (1978). Illusory contours and the ends of lines.
Perception, 7, 605--607. Kersten, D., Knill, D.C., Mamassian, P., &
Bülthoff, I. (1996). Illusory motion from shadows. Nature, 379, 31.
Kersten, D., Mamassian, P., & Knill, D.C. (1997). Moving cast shadows
induce apparent motion in depth. Perception, 26, 171--192. Kerzel, D.,
Hecht, H., & Kim, N.G. (1999). Image velocity, not tau, explains
arrival-time judgements from global optical flow. Journal of
Experimental Psychology: Human Perception and Performance, 25,
1540--1555. Kettlewell, B. (1973). The evolution of melanism. Oxford:
Oxford University Press. Kilpatrick, F.P. (Ed.). (1952). Human behaviour
from the transactionalist point of view. Princeton, NJ: Princeton
University Press. King, S.M., Dykeman, C., Redgrave, P., & Dean, P.
(1992). Use of a distracting task to obtain defensive head movements to
looming visual stimuli by human adults in a laboratory setting.
Perception, 21, 245--259. Kirchner, W.H., & Srinivasan, M.V. (1989).
Freely flying honeybees use image motion to estimate object distance.
Naturwissenschaften, 76, 281--282. Kirschfeld, K. (1976). The resolution
of lens and compound eyes. In F. Zettler, & R. Weiler (Eds.), Neural
principles in vision. Berlin: Springer. Kleinke, C.L. (1986). Gaze and
eye contact: A research review. Psychological Bulletin, 100, 78--100.
Klier, E.M., Wang, H.Y., & Crawford, J.D. (2001). The superior
colliculus encodes gaze commands in retinal co-ordinates. Nature
Neuroscience, 4, 627-- 632. Knierim, J.J., & Van Essen, D.C. (1992).
Neuronal responses to static texture patterns in area V1 of the alert
monkey. Journal of Neurophysiology, 67, 961--980. Knight, B., &
Johnston, A. (1997). The role of movement in face recognition. Visual
Cognition, 4, 265--273.

436

REFERENCES

Knill, D.C. (1992). Perception of surface contours and surface shape:
From computation to psychophysics. Journal of the Optical Society of
America, A9, 1449--1464. Knill, D.C., & Richards, W. (Eds.). (1996).
Perception as Bayesian inference. Cambridge: Cambridge University Press.
Kobatake, E., & Tanaka, K. (1994). Neuronal selectivities to complex
object features in the ventral visual pathway of the macaque cerebral
cortex. Journal of Neurophysiology, 71, 856--867. Kobayashi, H., &
Kohshima, S. (1997). Unique morphology of the human eye. Nature, 387,
767--768. Koenderink, J.J. (1986). Optic flow. Vision Research, 26,
161--180. Koenderink, J.J., & van Doorn, A.J. (1976). Local structure of
movement parallax of the plane. Journal of the Optical Society of
America, 66, 717--723. Koenderink, J.J., & van Doorn, A.J. (1987).
Representation of local geometry in the visual system. Biological
Cybernetics, 55, 367--375. Koffka, K. (1935). Principles of gestalt
psychology. New York: Harcourt Brace. Köhler, W. (1947). Gestalt
psychology: An introduction to new concepts in modern psychology. New
York: Liveright Publishing Corporation. Konczak, J. (1994). Effects of
optic flow on the kinematics of human gait -- a comparison of young and
older adults. Journal of Motor Behaviour, 26, 225--236. Kozlowski, L.T.,
& Cutting, J.E. (1977). Recognizing the sex of a walker from a dynamic
point-light display. Perception and Psychophysics, 21, 575--580.
Kozlowski, L.T., & Cutting, J.E. (1978). Recognizing the gender of
walkers from point-lights mounted on ankles: Some second thoughts.
Perception and Psychophysics, 23, 459. Krapp, H.G., Hengstenberg, B., &
Hengstenberg, R. (1998). Dendritic structure and receptive-field
organisation of optic flow processing interneurons in the fly. Journal
of Neurophysiology, 79, 1902-- 1917. Kuffler, S.W. (1953). Discharge
patterns and functional organization of mammalian retina. Journal of
Neurophysiology, 16, 37--68. Kulikowski, J.J., & King-Smith, P.E.
(1973). Spatial arrangement of line, edge and grating detectors revealed
by subthreshold summation. Vision Research, 13, 1455--1478. Lacquaniti,
F. (1989). Central representations of human limb movement as revealed by
studies of

drawing and handwriting. Trends in Neuroscience, 12, 287--291. Lamme,
V.A.F., Rodriguez-Rodriguez, V., & Spekreijse, H. (1999). Separate
processing dynamics for texture elements, boundaries and surfaces in
primary visual cortex of the macaque monkey. Cerebral Cortex, 9,
406--413. Lamme, V.A.F., & Roelfsema, P.R. (2000). The distinct modes of
vision offered by feedforward and recurrent processing. Trends in
Neuroscience, 23, 571--579. Lamme, V.A.F., Zipser, K., & Spekreijse, H.
(1998). Figure--ground activity in primary visual cortex is suppressed
by anesthesia. Proceedings of the National Academy of Sciences of the
USA, 95, 3263--3268. Land, E.H. (1977). The retinex theory of colour
vision. Scientific American, 237(6), 108--128. Land, M., Mennie, N., &
Rusted, J. (1999). The roles of vision and eye movements in the control
of activities of daily living. Perception, 28, 1311--1328. Land, M.F.
(1968). Functional aspects of the optical and retinal organization of
the mollusc eye. Symposia of the Zoological Society of London, 23,
75--96. Land, M.F. (1999). Compound eye structure: Matching eye to
environment. In S.N. Archer, M.B.A. Djamgoz, E.R. Loew, J.C. Partridge,
& S. Vallerga (Eds.), Adaptive mechanisms in the ecology of vision (pp.
51--71). Dordrecht: Kluwer. Land, M.F., & Collett, T.S. (1974). Chasing
behaviour of houseflies (Fannia canicularis). A description and
analysis. Journal of Comparative Physiology, 89, 331--357. Land, M.F., &
Furneaux, S. (1997). The knowledge base of the oculomotor system.
Philosophical Transactions of the Royal Society of London B, 352,
1231--1239. Land, M.F., & Lee, D.N. (1994). Where we look when we steer.
Nature, 369, 742--744. Land, M.F., & McLeod, P. (2000). From eye
movements to actions: How batsmen hit the ball. Nature Neuroscience, 3,
1340--1345. Land, M.F., & Nilsson, D.-E. (2002). Animal eyes. Oxford:
Oxford University Press. Lander, K., & Bruce, V. (2000). Recognizing
famous faces: Exploring the benefits of facial motion. Ecological
Psychology, 12, 259--272. Lander, K., Christie, F., & Bruce, V. (1999).
The role of movement in the recognition of famous faces. Memory &
Cognition, 27, 974--985. Langley, K., Atherton, T.J., Wilson, R.G., &
Larcombe,

REFERENCES

M.H.E. (1990). Vertical and horizontal disparities from phase. Proc. 1st
ECCV. Springer-Verlag Lecture Series in Computer Science, 427, 315--325.
Langlois, J.H., & Roggman, L.A. (1990). Attractive faces are only
average. Psychological Science, 1, 115--121. Langton, S.R.H. (2000). The
mutual influence of gaze and head orientation in the analysis of social
attention direction. Quarterly Journal of Experimental Psychology A:
Human Experimental Psychology, 53A, 825--845. Langton, S.R.H., & Bruce,
V. (1999). Reflexive visual orienting in response to the social
attention of others. Visual Cognition, 6, 541--568. Langton, S.R.H.,
Watt, R.J., & Bruce, V. (2000). Do the eyes have it? Cues to the
direction of social attention. Trends in Cognitive Sciences, 4, 50--59.
Lappe, M., & Rauschecker, J.P. (1994). Heading detection from optic
flow. Nature, 369, 712--713. Laurent, M., & Thomson, J.A. (1988). The
role of visual information in control of a constrained locomotion task.
Journal of Motor Behaviour, 20, 17--37. Lawson, R., & Humphreys, G.W.
(1996). Viewspecificity in object processing: Evidence from picture
matching. Journal of Experimental Psychology: Human Perception, &
Performance, 22, 395--416. Lawson, R., Humphreys, G.W., & Watson, D.
(1994). Object recognition under sequential viewing conditions: Evidence
for viewpoint-specific recognition procedures. Perception, 23, 595--614
Leder, H., & Bruce, V. (1998). Local and relational aspects of face
distinctiveness. Quarterly Journal of Experimental Psychology A: Human
Experimental Psychology, 51, 449--473. Leder, H., & Bruce, V. (2000).
When inverted faces are recognized: The role of configural information
in face recognition. Quarterly Journal of Experimental Psychology A:
Human Experimental Psychology, 53, 513--536. Ledgeway, T. (1994).
Adaptation to second-order motion results in a motion aftereffect for
directionally-ambiguous test stimuli. Vision Research, 34, 2879--2889.
Ledgeway, T., & Smith, A.T. (1994). Evidence for separate
motion-detecting mechanisms for first- and second-order motion in human
vision. Vision Research, 34, 2727--2740. Lee, D.N. (1976). A theory of
visual control of braking based on information about time-to-collision.
Perception, 5, 437--459. Lee, D.N. (1980a). Visuo-motor coordination in
spacetime. In G.E. Stelmach & J. Requin (Eds.), Tutorials

437

in motor behaviour. Amsterdam: North-Holland Publishing Company. Lee,
D.N. (1980b). The optic flow field: The foundation of vision.
Philosophical Transactions of the Royal Society of London, Series B,
290, 169--179. Lee, D.N., & Aronson, E. (1974). Visual proprioceptive
control of standing in human infants. Perception and Psychophysics, 15,
529--532. Lee, D.N., & Lishman, J.R. (1975). Visual proprioceptive
control of stance. Journal of Human Movement Studies, 1, 87--95. Lee,
D.N., Lishman, J.R., & Thomson, J.A. (1982). Regulation of gait in
long-jumping. Journal of Experimental Psychology: Human Perception and
Performance, 8, 448--459. Lee, D.N., & Reddish, P.E. (1981). Plummeting
gannets: A paradigm of ecological optics. Nature, 293, 293--294. Lee,
D.N., Reddish, P.E., & Rand, D.T. (1991). Aerial docking by
hummingbirds. Naturwissenschaften, 78, 526--527. Lee, D.N., & Young,
D.S. (1985). Visual timing of interceptive action. In D. Ingle, M.
Jeannerod, & D.N. Lee (Eds.), Brain mechanisms and spatial vision
(pp. 1--30). Dordrecht, The Netherlands: Martinus Nijhoff. Lee, D.N.,
Young, D.S., Reddish, P.E., Lough, S., & Clayton, T.M.H. (1983). Visual
timing in hitting an accelerating ball. Quarterly Journal of
Experimental Psychology, 35A, 333--346. Lee, D.N., Young, D.S., & Rewt,
D. (1992). How do somersaulters land on their feet? Journal of
Experimental Psychology: Human Perception and Performance, 18,
1195--1202. Lee, T.S., Mumford, D., Romero, R., & Lamme, V.A.F. (1998).
The role of the primary visual cortex in higher level vision. Vision
Research, 38, 2429--2454. Legge, G.E. (1978). Sustained and transient
mechanisms in human vision: Temporal and spatial properties. Vision
Research, 18, 69--82. Legge, G.E. (1981). A power law for contrast
discrimination. Vision Research, 21, 457--467. Legge, G.E., & Foley,
J.M. (1980). Contrast masking in human vision. Journal of the Optical
Society of America A, 70, 1458--1470. Legge, G.E., & Gu, Y. (1989).
Stereopsis and contrast. Vision Research, 29, 989--1004. Lehrer, M., &
Srinivasan, M.V. (1993). Object detection by honeybees: Why do they land
on edges? Journal of Comparative Physiology, 173, 23--32. Lennie, P.
(1998). Single units and visual cortical organisation. Perception, 27,
889--935.

438

REFERENCES

Leslie, A.M. (1984). Spatiotemporal continuity and the perception of
causality in infants. Perception, 13, 287--305. Leslie, A.M., & Keeble,
S. (1987). Do six-month-old infants perceive causality? Cognition, 25,
265--288. LeVay, S., Hubel, D.H., & Wiesel, T.N. (1975). The pattern of
ocular dominance columns in macaque visual cortex revealed by a reduced
silver stain. Journal of Comparative Neurology, 159, 559--576. LeVay,
S., & Voigt, T. (1988). Ocular dominance and disparity coding. Visual
Neuroscience, 1, 395--414. Leventhal, A.G., Rodieck, R.W., & Dreher, B.
(1981). Retinal ganglion cell classes in the Old World monkey:
Morphology and central projections. Science, 213, 1139--1142. Leventhal,
A.G., Wang, Y., Schmolesky, M.T., & Zhou, Y. (1998). Neural correlates
of boundary perception. Visual Neuroscience, 15, 1107--1118. Levin,
D.T., & Simons, D.J. (1997). Failure to detect changes to attended
objects in motion pictures. Psychonomic Bulletin and Review, 4,
501--506. Levinson, E., & Sekuler, R. (1975). The independence of
channels in human vision selective for direction of movement. Journal of
Physiology, 250, 347--366. Levinson, E., & Sekuler, R. (1976).
Adaptation alters perceived direction of motion. Vision Research, 16,
779--781. Levitt, J.B., & Lund, J.S. (1997). Contrast dependence of
contextual effects in primate visual cortex. Nature, 387, 73--76. Li,
A., & Zaidi, Q. (2000). Perception of threedimensional shape from
texture is based on patterns of oriented energy. Vision Research, 40,
217--242. Li, A., & Zaidi, Q. (2001). Information limitations in
perception of shape from texture. Vision Research, 41, 2927--2942. Li,
C., & Li, W. (1994). Extensive integration field beyond the classical
receptive field of cat's striate cortical neurons -- classification and
tuning properties. Vision Research, 34, 2337--2355. Li, Z. (2000).
Pre-attentive segmentation in primary visual cortex. Spatial Vision, 13,
25--50. Linden, D.E.J., Kallenbach, U., Heinecke, A., Singer, W., &
Goebel, R. (1999). The myth of upright vision: A psychophysical and
functional imaging study of adaptation to inverting spectacles.
Perception, 28, 469--481. Lindsay, P.H., & Norman, D.A. (1972). Human
information processing. New York: Academic Press. Lishman, J.R., & Lee,
D.N. (1973). The autonomy of visual kinaesthesis. Perception, 2,
287--294. Liu, L., Stevenson, S.B., & Schor, C.M. (1994). Quanti-

tative stereoscopic depth without binocular correspondence. Nature, 367,
66--69. Livingstone, M.S., & Hubel, D.H. (1983). Specificity of
cortico-cortical connections in monkey visual system. Nature, 304,
531--534. Locke, J. (1690). An essay concerning human understanding.
Edited from the Fourth (1700) and Fifth (1706) Editions by P.H.
Nidditch. Oxford: Oxford University Press (1975). Longuet-Higgins, H.C.,
& Prazdny, K. (1980). The interpretation of moving retinal images.
Proceedings of the Royal Society of London, Series B, 208, 385--397.
Lowe, D.G. (1987). Three-dimensional object recognition from single
two-dimensional images. Artificial Intelligence, 31, 355--395. Lu,
Z.-L., & Sperling, G. (1995). The functional architecture of human
visual motion perception. Vision Research, 35, 2697--2722. Lu, Z.-L., &
Sperling, G. (2001). Three-systems theory of human visual motion
perception: Review and update. Journal of the Optical Society of America
A, 18, 2331--2370. Luck, S.J., Chelazzi, L., Hillyard, S.A., & Desimone,
R. (1997). Neural mechanisms of spatial selective attention in areas V1,
V2 and V4 of macaque visual cortex. Journal of Neurophysiology, 77,
24--42. Mach, E. (1914). The analysis of sensations. Republished, 1959,
New York: Dover Publications. Mack, A., & Rock, I. (1998). Inattentional
blindness. Cambridge, MA: MIT Press. MacKay, D.M. (1970). Elevation of
visual threshold by displacement of retinal image. Nature, 225, 90--92.
Maffei, L., & Fiorentini, A. (1976). The unresponsive regions of visual
cortical receptive fields. Vision Research, 16, 1131--1139. Maier, E.J.,
& Bowmaker, J.K. (1993). Colour vision in the passeriform bird,
Leiothrix lutea: Correlation of visual pigment absorbance and oil
droplet transmission with spectral sensitivity. Journal of Comparative
Physiology, 172, 295--301. Malik, J., & Perona, P. (1990). Preattentive
texture discrimination with early vision mechanisms. Journal of the
Optical Society of America A, 7, 923--932. Malik, J., & Perona, P.
(1992). Finding boundaries in images. In H. Wechsler (Ed.), Neural
networks for perception Vol. I: Human and machine perception (Ch. II:
7). London: Academic Press. Mallot, H.A. (2000). Computational vision --
information processing in perception and visual behaviour. Cambridge,
MA: MIT Press. Malonek, D., Tootell, R.B.H., & Grinvald, A. (1994).
Optical imaging reveals the functional architecture

REFERENCES

of neurons processing shape and motion in owl monkey area MT.
Proceedings of the Royal Society of London B, 258, 109--119. Malpeli,
J.G., Schiller, P.H., & Colby, C.L. (1981). Response properties of
single cells in monkey striate cortex during reversible inactivation of
individual lateral geniculate laminae. Journal of Neurophysiology, 46,
1102--1119. Mamassian, P., Knill, D.C., & Kersten, D. (1998). The
perception of cast shadows. Trends in Cognitive Sciences, 2, 288--295.
Mandler, M.B., & Makous, W. (1984). A three channel model of temporal
frequency perception. Vision Research, 24, 1881--1887. Manning, A.
(1978). An introduction to animal behaviour (3rd Edn.). London: Edward
Arnold. Mark, L., & Todd, J.T. (1983). The perception of growth in three
dimensions. Perception and Psychophysics, 33, 193--196. Marotta, J.J., &
Goodale, M.A. (1998). The role of learned pictorial cues in the
programming and control of grasping. Experimental Brain Research, 121,
465--470. Marotta, J.J., Kruyer, A., & Goodale, M.A. (1998). The role of
head movements in the control of manual prehension. Experimental Brain
Research, 120, 134--138. Marr, D. (1976). Early processing of visual
information. Philosophical Transactions of the Royal Society of London,
Series B, 275, 483--524. Marr, D. (1977). Analysis of occluding contour.
Proceedings of the Royal Society of London, Series B, 197, 441--475.
Marr, D. (1982). Vision: A computational investigation into the human
representation and processing of visual information. San Francisco: W.H.
Freeman & Co. Marr, D., & Hildreth, E. (1980). Theory of edge detection.
Proceedings of the Royal Society of London, Series B, 207, 187--217.
Marr, D., & Nishihara, H.K. (1978). Representation and recognition of
the spatial organization of threedimensional shapes. Proceedings of the
Royal Society of London, Series B, 200, 269--294. Marr, D., & Poggio, T.
(1976). Cooperative computation of stereo disparity. Science, 194,
283--287. Marr, D., & Poggio, T. (1979). A computational theory of human
stereo vision. Proceedings of the Royal Society of London, Series B,
204, 301--328. Marr, D., & Ullman, S. (1981). Directional selectivity
and its use in early visual processing. Proceedings of the Royal Society
of London, Series B, 211, 151--180. Martin, G.R. (1985). Eye. In A.S.
King, & J. McClel-

439

land (Eds.), Form and function in birds. Vol. 3 (pp. 311--374). London:
Academic Press. Martin, G.R. (1994a). Visual fields in woodcocks
Scolopax rusticola (Scolopacidae; Charadriiformes). Journal of
Comparative Physiology, 174, 787--793. Martin, G.R. (1994b). Form and
function in the optical structure of bird eyes. In M.N.O. Davies & P.R.
Green (Eds.), Perception and motor control in birds (pp. 5--34). Berlin:
Springer. Martin, K.A.C. (1988). From enzymes to visual perception: A
bridge too far? Trends in Neuroscience, 11, 380--387. Martinez, A.,
Anllo-Vento, L., Sereno, M.I., Frank, L.R., Buxton, R.B., Dubowitz, D.J.
et al. (1999). Involvement of striate and extrastriate visual cortical
areas in spatial attention. Nature Neuroscience, 2, 364--369. Masland,
R.H., & Raviola, E. (2000). Confronting complexity: Strategies for
understanding the microcircuitry of the retina. Annual Review of
Neuroscience, 23, 249--284. Mather, G. (1994). Motion detector models:
Psychophysical evidence. In A.T. Smith & R.J. Snowden (Eds.), Visual
detection of motion. London: Academic Press. Mather, G., & Murdoch, L.
(1994). Gender discrimination in biological motion displays based on
dynamic cues. Proceedings of the Royal Society of London B, 258,
273--279. Mather, G., Radford, K., & West, S. (1992). Low-level visual
processing in biological motion. Proceedings of the Royal Society of
London B, 249, 149--155. Mather, G., Verstraten, F., & Anstis, S.
(1998). The motion aftereffect -- a modern perspective. Cambridge, MA:
MIT Press. Mather, G., & West, S. (1993a). Evidence for secondorder
motion detectors. Vision Research, 33, 1109-- 1112. Mather, G., & West,
S. (1993b). Recognition of animal locomotion from dynamic point-light
displays. Perception, 22, 759--766. Matin, L. (1976). Saccades and
extra-retinal signal for visual direction. In R.A. Monty, & J.W. Senders
(Eds.), Eye movements and psychological processes. Hillsdale, NJ:
Lawrence Erlbaum Associates Inc. Maunsell, J.H.R., & Newsome, W.T.
(1987). Visual processing in monkey extrastriate cortex. Annual Review
of Neuroscience, 10, 363--401. Mayhew, J.E.W., & Frisby, J.P. (1981).
Psychophysical and computational studies towards a theory of human
stereopsis. Artificial Intelligence, 17, 349--385.

440

REFERENCES

McBeath, M.K., Shaffer, D.M., & Kaiser, M.K. (1995). How baseball
outfielders determine where to run to catch fly balls. Science, 268,
569--573. McClelland, J.L., & Rumelhart, D.E. (1986). Parallel
distributed processing: Explorations in the microstructure of cognition.
Vol II: Applications. Cambridge, MA: Bradford Books. McCleod, R.W., &
Ross, H.E. (1983). Optic-flow and cognitive factors in time-to-collision
estimates. Perception, 12, 417--423. McConkie, G.W., & Currie, C.B.
(1996). Visual stability across saccades while viewing complex pictures.
Journal of Experimental Psychology: Human Perception and Performance,
22, 563--581. McIlwain, J.T. (1964). Receptive fields of optic tract
axons and lateral geniculate cells: Peripheral extent and barbiturate
sensitivity. Journal of Neurophysiology, 27, 1154--1173. McGraw, P.V.,
Levi, D.M., & Whitaker, D. (1999). Spatial characteristics of the
second-order visual pathway revealed by positional adaptation. Nature
Neuroscience, 2, 479--484. McKee, S.P., Silverman, G.H., & Nakayama, K.
(1986). Precise velocity discrimination despite random variations in
temporal frequency and contrast. Vision Research, 26, 609--619.
McKenzie, B., & Over, R. (1983). Young infants fail to imitate facial
and manual gestures. Infant Behaviour and Development, 6, 85--89.
McLean, J., & Palmer, L.A. (1989). Contribution of linear spatiotemporal
receptive field structure to velocity selectivity of simple cells in
area 17 of the cat. Vision Research, 29, 675--679. McLean, J., & Palmer,
L.A. (1994). Organization of simple cell responses in the
three-dimensional (3-D) frequency domain. Visual Neuroscience, 11,
295--306. McLean, J., Raab, S., & Palmer, L.A. (1994). Contribution of
linear mechanisms to the specification of local motion by simple cells
in areas 17 and 18 of the cat. Visual Neuroscience, 11, 271--294.
McLeod, P. (1987). Visual reaction time and high speed ball games.
Perception, 16, 49--59. McLeod, P., & Dienes, Z. (1996). Do fielders
know where to go to catch the ball or only how to get there? Journal of
Experimental Psychology: Human Perception and Performance, 22, 531--543.
McPeek, R.M., Maljkovic, V., & Nakayama, K. (1999). Saccades require
focal attention and are facilitated by a short-term memory system.
Vision Research, 39, 1555--1566. Mech, L.D. (1970). The wolf: the
ecology and behaviour

of an endangered species. Garden City, New York: The Natural History
Press. Meese, T.S., & Harris, M.G. (2001). Independent detectors for
expansion and rotation, and for orthogonal components of deformation.
Perception, 30, 1189--1202. Meltzoff, A.N., & Moore, M.K. (1977).
Imitation of facial and manual gestures by human neonates. Science, 198,
75--78. Menzel, E.W. (1978). Cognitive mapping in chimpanzees. In S.H.
Hulse, F. Fowler, & W.K. Honig (Eds.), Cognitive processes in animal
behaviour. Hillsdale, NJ: Lawrence Erlbaum Associates Inc. Merigan,
W.H., Byrne, C.E., & Maunsell, J.H.R. (1991). Does primate motion
perception depend on the magnocellular pathway? Journal of Neuroscience,
11, 3422--3429. Merigan, W.H., & Eskin, T.A. (1986). Spatio-temporal
vision of macaques with severe loss of Pβ retinal ganglion cells. Vision
Research, 26, 1751--1761. Merigan, W.H., Katz, L.M., & Maunsell, J.H.R.
(1991). The effects of parvocellular lateral geniculate lesions on the
acuity and contrast sensitivity of macaque monkeys. Journal of
Neuroscience, 11, 994--1001. Merigan, W.H., & Maunsell, J.H.R. (1993).
How parallel are the primate visual pathways? Annual Review of
Neuroscience, 16, 369--402. Metzger, W. (1930). Optische Untersuchungen
in Ganzfeld II. Psychologische Forschung, 13, 6--29. Michael, C.R.
(1978). Color vision mechanisms in monkey striate cortex: Simple cells
with dual opponent-color receptive fields. Journal of Neurophysiology,
41, 1233--1249. Michaels, C.F., & Carello, C. (1981). Direct perception.
Englewood Cliffs, NJ: Prentice Hall. Michaels, C.F., Zeinstra, E.B., &
Oudejans, R.R.D. (2001). Information and action in punching a falling
ball. Quarterly Journal of Experimental Psychology, 54A, 69--93.
Michotte, A. (1963). The perception of causality. Translated by T., & E.
Miles from French (1946) edition. London: Methuen. Miller, E.K., Li, L.,
& Desimone, R. (1993). Activity of neurons in anterior inferotemporal
cortex during a short-term memory task. Journal of Neuroscience, 13,
1460--1478. Millott, N. (1968). The dermal light sense. Symposia of the
Zoological Society of London, 23, 1--36. Milner, A.D., & Goodale,
M.A. (1995). The visual brain in action. Oxford: Oxford University
Press. Milner, A.D., Paulignan, Y., Dijkerman, H.C., Michel, F., &
Jeannerod, M. (1999). A paradoxical

REFERENCES

improvement of misreaching in optic ataxia: New evidence for two
separate neural systems for visual localisation. Proceedings of the
Royal Society of London, B, 266, 2225--2229. Möhl, B. (1989).
"Biological noise" and plasticity of sensorimotor pathways in the locust
flight system. Journal of Comparative Physiology, 166, 75--82. Mollon,
J.D. (1982). Colour vision and colour blindness. In H.B. Barlow & J.D.
Mollon (Eds.), The senses (pp. 165--191). Cambridge: Cambridge
University Press. Mollon, J.D. (1989). "Tho' she kneel'd in that place
where they grew . . .": The uses and origins of primate colour vision.
Journal of Experimental Biology, 146, 21--38. Mon-Williams, M., &
Dijkerman, H.C. (1999). The use of vergence information in the
programming of prehension. Experimental Brain Research, 128, 578--582.
Moran, G., Fentress, J.C., & Golani, I. (1981). A description of
relational patterns of movement during "ritualized fighting" in wolves.
Animal Behaviour, 29, 1146--1165. Moran, J., & Desimone, R. (1985).
Selective attention gates visual processing in the extrastriate cortex.
Science, 229, 782--784. Morgan, M.J. (1992). Spatial filtering precedes
motion detection. Nature, 355, 344--346. Morgan, M.J., & Watt, R.J.
(1997). The combination of filters in early spatial vision: A
retrospective analysis of the MIRAGE model. Perception, 26, 1073--1088.
Morris, R.G.M. (Ed.). (1989). Parallel distributed processing:
Implications for psychology and neuroscience. Oxford: Oxford University
Press. Morrone, M.C., & Burr, D.C. (1986). Evidence for the existence
and development of visual inhibition in humans. Nature, 321, 235--237.
Morrone, M.C., & Burr, D.C. (1988). Feature detection in human vision: A
phase-dependent energy model. Proceedings of the Royal Society of London
B, 235, 221--245. Morrone, M.C., Burr, D.C., & Maffei, L. (1982).
Functional significance of cross-orientational inhibition, Part I.
Neurophysiology. Proceedings of the Royal Society of London B, 216,
335--354. Morrone, M.C., Burr, D.C., & Vaina, L.M. (1995). Two stages of
visual processing for radial and circular motion. Nature, 376, 507--509.
Morrone, M.C., Navangione, A., & Burr, D.C. (1995). An adaptive approach
to scale selection for line and edge detection. Pattern Recognition
Letters, 16, 667-- 677. Morrone, M.C., & Owens, R.A. (1987). Feature
detec-

441

tion from local energy. Pattern Recognition Letters, 6, 303--313.
Morrone, M.C., Ross, J., Burr, D.C., & Owens, R.A. (1986). Mach bands
are phase-dependent. Nature, 324, 250--253. Morrone, M.C., Tosetti, M.,
Montanaro, D., Fiorentini, A., Cioni, G., & Burr, D.C. (2000). A
cortical area that responds specifically to optic flow, revealed by
fMRI. Nature Neuroscience, 3, 1322--1328. Morton, J., & Johnson, M.
(1991). Conspec and Conlern: A two-process theory of infant face
recognition. Psychological Review, 98, 164--181. Motter, B.C. (1993).
Focal attention produces spatially selective processing in visual
cortical areas V1, V2 and V4 in the presence of competing stimuli.
Journal of Neurophysiology, 70, 909--919. Motter, B.C. (1994). Neural
correlates of attentive selection for colour or luminance in
extrastriate area V4. Journal of Neuroscience, 14, 2178--2189. Moulden,
B. (1980). After-effects and the integration of patterns of neural
activity within a channel. Philosophical Transactions of the Royal
Society of London B, 290, 39--55. Movshon, J.A., Adelson, E.H., Gizzi,
M.S., & Newsome, W.T. (1985). The analysis of moving visual patterns. In
C. Chagas, R. Gattass, & C. Gross (Eds.), Pattern recognition mechanisms
(pp. 117-- 151). Vatican City: Vatican Press. Movshon, J.A., &
Blakemore, C.B. (1973). Orientation specificity and spatial selectivity
in human vision. Perception, 2, 53--60. Movshon, J.A., Thompson, I.D., &
Tolhurst, D.J. (1978). Receptive field organization of complex cells in
the cat's striate cortex. Journal of Physiology, 283, 79--99. Murphy,
K.J., Carey, D.P., & Goodale, M.A. (1998). The perception of spatial
relations in a patient with visual form agnosia. Cognitive
Neuropsychology, 15, 705--722. Murphy, P.C., Duckett, S.G., & Sillito,
A.M. (1999). Feedback connections to the lateral geniculate nucleus and
cortical response properties. Science, 286, 1552--1554. Murthy, A., &
Humphrey, A.L. (1999). Inhibitory contributions to spatiotemporal
receptive-field structure and direction selectivity in simple cells of
cat area 17. Journal of Neurophysiology, 81, 1212--1224. Murthy, A.,
Humphrey, A.L., Saul, A.B., & Feidler, J.C. (1998). Laminar differences
in the spatiotemporal structure of simple cell receptive fields in cat
area 17. Visual Neuroscience, 15, 239--256. Nakayama, K., He, Z.J., &
Shimojo, S. (1995). Visual surface representation: A critical link
between

442

REFERENCES

lower-level and higher level vision. In S.M. Kosslyn, & D.N. Osherson
(Eds.), An invitation to cognitive science, Vol. 2. Visual cognition
(pp. 1--70). Cambridge, MA: MIT Press. Nakayama, K., Shimojo, S., &
Silverman, G.H. (1989). Stereoscopic depth: Its relation to image
segmentation, grouping and the recognition of occluded objects.
Perception, 18, 55--68. Nakayama, K., & Silverman, G.H. (1984). Temporal
and spatial characteristics of the upper displacement limit for motion
in random dots. Vision Research, 24, 293--299. Nalbach, H.-O.,
Wolf-Oberhollenzer, F., & Remy, M. (1993). Exploring the image. In H.P.
Zeigler & H.-J. Bischof (Eds.), Vision, brain and behaviour in birds
(pp. 25--46). Cambridge, MA: MIT Press. Nawrot, M., & Blake, R. (1989).
Neural integration of information specifying structure from stereopsis
and motion. Science, 244, 716--718. Nayar, S.K., & Oren, M. (1995).
Visual appearance of matte surfaces. Science, 267, 1153--1156. Neisser,
U. (1967). Cognitive psychology. New York: Appleton-Century-Crofts.
Nelson, B. (1978). The gannet. Berkhamsted, UK: T. & A.D. Poyser.
Nelson, J.I., & Frost, B.J. (1978). Orientation-selective inhibition
from beyond the classical receptive field. Brain Research, 139,
359--365. Neri, P., Morrone, M.C., & Burr, D.C. (1998). Seeing
biological motion. Nature, 395, 894--896. Newsome, W.T., Britten, K.H.,
& Movshon, J.A. (1989). Neuronal correlates of a perceptual decision.
Nature, 341, 52--54. Newsome, W.T., & Paré, E.B. (1988). A selective
impairment of motion perception following lesions of the middle temporal
visual area (MT). Journal of Neuroscience, 8, 2201--2211. Nishida, S.,
Ashida, H., & Sato, T. (1994). Complete interocular transfer of motion
aftereffect with flickering test. Vision Research, 34, 2707--2716.
Nishida, S., Ledgeway, T., & Edwards, M. (1997). Dual multiple-scale
processing for motion in the human visual system. Vision Research, 37,
2685--2698. Nishida, S., & Sato, T. (1995). Motion aftereffect with
flickering test patterns reveals higher stages of motion processing.
Vision Research, 35, 477--490. Nishitani, N., & Hari, R. (2000).
Temporal dynamics of cortical representation for action. Proceedings of
the National Academy of Sciences of the USA, 97, 913--918. Normann,
R.A., & Werblin, F.S. (1974). Control of retinal sensitivity. I. Light
and dark adaptation

of vertebrate rods and cones. Journal of General Physiology, 63, 37--61.
Nothdurft, H.C. (1985). Sensitivity for structure gradient in texture
discrimination tasks. Vision Research, 25, 1957--1968. Nothdurft, H.C.
(1991). Texture segmentation and pop-out from orientation contrast.
Vision Research, 31, 1073--1078. Nothdurft, H.C. (1993). The role of
features in preattentive vision: Comparison of orientation, motion and
colour cues. Vision Research, 33, 1937-- 1958. Nothdurft, H.-C.,
Gallant, J.L., & van Essen, D.C. (1999). Response modulation by texture
surround in primate area V1: Correlates of 'popout' under anesthesia.
Visual Neuroscience, 16, 15--34. Oakes, L.M. (1994). Development of
infants' use of continuity cues in their perception of causality.
Developmental Psychology, 30, 869--879. Obermayer, K., & Blasdel, G.G.
(1993). Geometry of orientation and ocular dominance columns in monkey
striate cortex. Journal of Neuroscience, 13, 4114--4129. Ogle, K.N.
(1964). Researches in binocular vision. New York: Hafner. Ohzawa, I.,
DeAngelis, G.C., & Freeman, R.D. (1990). Stereoscopic depth
discrimination in the visual cortex: Neurons ideally suited as disparity
detectors. Science, 249, 1037--1041. Ohzawa, I., DeAngelis, G.C., &
Freeman, R.D. (1996). Encoding of binocular disparity by simple cells in
the cat's visual cortex. Journal of Neurophysiology, 75, 1779--1805.
Olson, R.K., & Attneave, F. (1970). What variables produce similarity
grouping? American Journal of Psychology, 83, 1--21. Oram, M.W., &
Perrett, D.I. (1994). Responses of anterior superior temporal
polysensory (STPA) neurons to biological motion stimuli. Journal of
Cognitive Neuroscience, 6, 99--116. O'Regan, J.K., Rensink, R.A., &
Clark, J.J. (1999). Change-blindness as a result of 'mudsplashes'.
Nature, 398, 34. Osorio, D., & Vorobyev, M. (1996). Colour vision as an
adaptation to frugivory in primates. Proceedings of the Royal Society of
London B, 263, 593--599. Palmer, S.E. (1992). Modern theories of Gestalt
perception. In G.W. Humphreys (Ed.), Understanding vision. Oxford:
Blackwell. Palmer, S.E., Rosch, E., & Chase, P. (1981). Canonical
perspective and the perception of objects. In J. Long, & A.D. Baddeley
(Eds.), Attention and per-

REFERENCES

formance IX. Hillsdale, NJ: Lawrence Erlbaum Associates Inc. Paradiso,
M.A., Shimojo, S., & Nakayama, K. (1989). Subjective contours, tilt
aftereffects, and visual cortical organization. Vision Research, 29,
1205--1213. Parker, A.J., Cumming, B.G., & Dodd, J.V. (2000). Binocular
neurons and the perception of depth. In M.S. Gazzaniga (Ed.), The new
cognitive neurosciences (pp. 263--277). Cambridge, MA: MIT Press.
Pasupathy, A., & Connor, C.E. (2001). Shape representation in area V4:
Position-specific tuning for boundary conformation. Journal of
Neurophysiology, 86, 2505--2519. Patla, A.E., & Goodale, M.A. (1996).
Obstacle avoidance during locomotion is unaffected in a patient with
visual form agnosia. Neuroreport, 8, 165--168. Patla, A.E., Prentice,
S.D., Rietdyk, S., Allard, E., & Martin, C. (1999). What guides the
selection of alternate foot placement during locomotion in humans?
Experimental Brain Research, 128, 441-- 450. Patla, A.E., Prentice,
S.D., Robinson, C., & Neufeld, J. (1991). Visual control of locomotion:
Strategies for changing direction and for going over obstacles. Journal
of Experimental Psychology: Human Perception and Performance, 17,
603--634. Pearlman, A.L., Birch, J., & Meadows, J.C. (1979). Cerebral
colour blindness: An acquired deficit in hue discrimination. Annals of
Neurology, 5, 253--261. Pearson, D.E., & Robinson, J.A. (1985). Visual
communication at very low data rates. Proceedings of the IEEE, 73,
795--812. Pelah, A., & Barlow, H.B. (1996). Visual illusion from
running. Nature, 381, 283. Pelz, J.B., & Hayhoe, M.M. (1995). The role
of exocentric reference frames in the perception of visual direction.
Vision Research, 35, 2267--2275. Pentland, A.P. (1987). A new sense for
depth of field. IEEE Trans. PAMI, 9, 523--531. Pentland, A.P. (1989).
Shape information from shading: A theory about human perception. Spatial
Vision, 4, 165--182. Penton-Voak, I.S., & Perrett, D.I. (2001). Male
facial attractiveness: Perceived personality and shifting female
preferences for male traits across the menstrual cycle. Advances in the
Study of Behaviour, 30, 219--259. Perrett, D.I., Hietanen, J.K., Oram,
M.W., & Benson, P.J. (1992). Organization and function of cells
responsive to faces in the temporal cortex. Philosophical Transactions
of the Royal Society of London B, 335, 23--30.

443

Perrett, D.I., May, K.A., & Yoshikawa, S. (1994). Facial shape and
judgements of female attractiveness: Preferences for non-average
characteristics. Nature, 368, 239--242. Perrett, D.I., Mistlin, A.J.,
Potter, D.D., Smith, P.A.J., Head, A.S., Chitty, A.J. et al. (1986).
Functional organisation of visual neurones processing face identity. In
H.D. Ellis, M.A. Jeeves, F. Newcombe, & A. Young (Eds.), Aspects of face
processing. Dordrecht: Martinus Nijhoff. Perrett, D.I., Rolls, E.T., &
Caan, W. (1982). Visual neurones responsive to faces in the monkey
temporal cortex. Experimental Brain Research, 47, 329--342. Perrone,
J.A. (2001). A closer look at the visual input to self-motion
estimation. In J.M. Zanker & J. Zeil (Eds.), Motion vision --
computational, neural and ecological constraints (pp. 169--179). Berlin:
Springer. Perrone, J.A., & Stone, L.S. (1994). A model of selfmotion
estimation within primate extrastriate cortex. Vision Research, 34,
2917--2938. Perrone, J.A., & Thiele, A. (2001). Speed skills: Measuring
the visual speed analyzing properties of primate MT neurons. Nature
Neuroscience, 4, 526--532. Perry, V.H., Oehler, R., & Cowey, A. (1984).
Retinal ganglion-cells that project to the dorsal lateral
geniculate-nucleus in the macaque monkey. Neuroscience, 12, 1101--1123.
Pessoa, L., & Neumann, H. (1998). Why does the brain fill in? Trends in
Cognitive Sciences, 2, 422--424. Peterhans, E., & von der Heydt, R.
(1989). Mechanisms of contour perception in monkey visual cortex. II.
Contours bridging gaps. Journal of Neuroscience, 9, 1749--1763.
Pettigrew, J.D., Collin, S.P., & Ott, M. (1999). Convergence of
specialised behaviour, eye movements and visual optics in the sandlance
(Teleostei) and the chameleon (Reptilia). Current Biology, 9, 421--424.
Pheiffer, C.H., Eure, S.B., & Hamilton, C.B. (1956). Reversible figures
and eye movements. American Journal of Psychology, 69, 452--455.
Phillips, R.J. (1972). Why are faces hard to recognise in photographic
negative? Perception and Psychophysics, 12, 425--426. Phillips, W.A., &
Singer, W. (1974). Function and interaction of on and off transients in
vision. I. Psychophysics. Experimental Brain Research, 19, 493--506.
Pietrewicz, A.T., & Kamil, A.C. (1977). Visual detection of cryptic prey
by blue jays. Science, 195, 580-- 582. Pittenger, J.B., & Shaw, R.E.
(1975). Aging faces as

444

REFERENCES

viscal-elastic events: Implications for a theory of non-rigid shape
perception. Journal of Experimental Psychology: Human Perception and
Performance, 1, 374--382. Poggio, G., & Poggio, T. (1984). The analysis
of stereopsis. Annual Review of Neuroscience, 7, 379-- 412. Poggio,
G.F., & Fischer, B. (1977). Binocular interaction and depth sensitivity
in striate and prestriate cortex of behaving rhesus monkeys. Journal of
Neurophysiology, 40, 1392--1407. Polat, U., & Sagi, D. (1993). Lateral
interactions between spatial channels: Suppression and facilitation
revealed by lateral masking experiments. Vision Research, 33, 993--999.
Polat, U., & Sagi, D. (1994). The architecture of perceptual spatial
interactions. Vision Research, 34, 73-- 78. Pollard, S.B., Mayhew,
J.E.W., & Frisby, J.P. (1985). PMF: A stereo correspondence algorithm
using a disparity gradient limit. Perception, 14, 449--470. Pollen,
D.A., Gaska, J.P., & Jacobson, L.D. (1988). Responses of simple and
complex cells to compound gratings. Vision Research, 28, 25--39. Pollen,
D.A., & Ronner, S.F. (1981). Phase relationships between adjacent simple
cells in the visual cortex. Science, 212, 1409--1411. Potts, W.K.
(1984). The chorus-line hypothesis of manoeuvre co-ordination in avian
flocks. Nature, 309, 344--345. Price, C.J., & Humphreys, G.W. (1989).
The effects of surface detail on object categorization and naming.
Quarterly Journal of Experimental Psychology, 41A, 797--828. Pringle,
J.W.S. (1974). Locomotion: flight. In M. Rockstein (Ed.), The physiology
of insecta Vol. III (2nd Edn., pp. 433--476). London: Academic Press.
Prinz, W. (1997). Perception and action planning. European Journal of
Cognitive Psychology, 9, 129--154. Pritchard, R.M. (1961). Stabilized
images on the retina. Scientific American, 204, June, 72--78. Pugh,
E.N., & Cobbs, W.H. (1986). Visual transduction in vertebrate rods and
cones: A tale of two transmitters, calcium and cyclic GMP. Vision
Research, 26, 1613--1643. Purple, R.L., & Dodge, F.A. (1965).
Interaction of excitation and inhibition in the eccentric cell in the
eye of Limulus. Cold Spring Harbor Symposia on Quantitative Biology Vol
30. Cold Spring Harbor, NY: Cold Spring Harbor Laboratory. Pylyshyn,
Z.W. (1999). Is vision continuous with cognition? The case for cognitive
impenetrability of visual

perception. Behavioural and Brain Sciences, 22, 341-- 365. Pylyshyn,
Z.W., & Storm, R.W. (1988). Tracking multiple independent targets:
Evidence for a parallel tracking mechanism. Spatial Vision, 3, 179--
197. Qian, N., & Andersen, R. (1994). Transparent motion perception as
detection of unbalanced motion signals. II. Physiology. Journal of
Neuroscience, 14, 7367--7380. Qian, N., Andersen, R.A., & Adelson, E.H.
(1994a). Transparent motion perception as detection of unbalanced motion
signals. I. Psychophysics. Journal of Neuroscience, 14, 7357--7366.
Qian, N., Andersen, R.A., & Adelson, E.H. (1994b). Transparent motion
perception as detection of unbalanced motion signals. III. Modeling.
Journal of Neuroscience, 14, 7381--7392. Quinlan, P.T., & Humphreys,
G.W. (1993). Perceptual frames of reference and 2-dimensional shape
recognition -- further examination of internal axes. Perception, 22,
1343--1364. Ramachandran, V.S. (1988). Perception of shape from shading.
Nature, 331, 163--166. Ramachandran, V.S., & Cavanagh, P. (1985).
Subjective contours capture stereopsis. Nature, 317, 527--531. Rao,
R.P.N., & Ballard, D.H. (1999). Predictive coding in the visual cortex:
A functional interpretation of some extra-classical receptive-field
effects. Nature Neuroscience, 2, 79--87. Redfern, M.S., & Furman, J.M.
(1994). Postural sway of patients with vestibular disorders during optic
flow. Journal of Vestibular Research: Equilibrium and Orientation, 4,
221--230. Reed, E., & Jones, R. (Eds.) (1982). Reasons for realism.
Selected essays of J.J. Gibson. Hillsdale, NJ: Lawrence Erlbaum
Associates Inc. Rees, G., Friston, K., & Koch, C. (2000). A direct
quantitative relationship between the functional properties of human and
macaque V5. Nature Neuroscience, 3, 716--723. Regan, D. (1992). Visual
judgements and misjudgements in cricket, and the art of flight.
Perception, 21, 91--115. Regan, D., & Beverley, K.I. (1978). Looming
detectors in the human visual pathway. Vision Research, 18, 415--421.
Regan, D., & Beverley, K.I. (1979). Binocular and monocular stimuli for
motion in depth: Changing disparity and changing size feed the same
motionin-depth stage. Vision Research, 19, 1331--1342. Regan, D., &
Gray, R. (2000). Visually guided collision

REFERENCES

avoidance and collision achievement. Trends in Cognitive Sciences, 4,
99--107. Regan, D., & Hamstra, S.J. (1992). Dissociation of orientation
discrimination from form detection for motion-defined bars and
luminance-defined bars: Effects of dot lifetime and presentation
duration. Vision Research, 32, 1655--1666. Reichardt, W. (1969).
Movement perception in insects. In W. Reichardt (Ed.), Processing of
optical data by organisms and by machines. New York: Academic Press.
Reichardt, W., & Poggio, T. (1976). Visual control of orientation
behaviour in the fly. Part I. A quantitative analysis. Quarterly Review
of Biophysics, 9, 311--375. Reichardt, W., & Poggio, T. (1979).
Figure--ground discrimination by relative movement in the visual system
of the fly. Biological Cybernetics, 35, 81--100. Reichardt, W., Poggio,
T., & Hausen, K. (1983). Figure-- ground discrimination by relative
movement in the visual system of the fly. Part II: Towards the neural
circuitry. Biological Cybernetics, 46, 1--30. Rensink, R.A., O'Regan,
J.K., & Clark, J.J. (1997). To see or not to see: The need for attention
to perceive changes in scenes. Psychological Science, 8, 368--373. Ress,
D., Backus, B.T., & Heeger, D.J. (2000). Activity in primary visual
cortex predicts performance in a visual detection task. Nature
Neuroscience, 3, 940--945. Richards, W. (1971). Anomalous stereoscopic
depth perception. Journal of the Optical Society of America, 61,
410--414. Rieser, J.J., Pick, H.L., Ashmead, D.H., & Garing, A.E.
(1995). Calibration of human locomotion and models of perceptual-motor
organisation. Journal of Experimental Psychology: Human Perception and
Performance, 21, 480--497. Riggs, L.A., Merton, P.A., & Morton, H.B.
(1974). Suppression of visual phosphenes during saccadic eye movements.
Vision Research, 14, 997--1010. Riggs, L.A., Volkmann, F.C., & Moore,
R.K. (1981). Suppression of the black-out due to blinks. Vision
Research, 21, 1075--1079. Rind, F.C., & Simmons, P.J. (1999). Seeing
what is coming: Building collision-sensitive neurons. Trends in
Neurosciences, 22, 215--220. Ristau, C.A. (1991). Before mindreading:
Attention, purposes and deception in birds? In A. Whiten (Ed.), Natural
theories of mind: Evolution, development and simulation of everyday
mindreading (pp. 209--222). Oxford: Basil Blackwell.

445

Roberts, L.G. (1965). Machine perception of threedimensional solids. In
J.T. Tippett, D.A. Berkowitz, L.C. Clapp, C.J. Koester, & A. Vanderburgh
(Eds.), Optical and electro-optical information processing. Cambridge,
MA: MIT Press. Robertson, R.M., & Johnson, A.G. (1993). Collision
avoidance of flying locusts: Steering torques and behaviour. Journal of
Experimental Biology, 183, 35--60. Robson, J.G. (1966). Spatial and
temporal contrast sensitivity functions of the visual system. Journal of
the Optical Society of America, 8, 1141--1142. Robson, J.G. (1983).
Frequency domain visual processing. In O.J. Braddick & A.C. Sleigh
(Eds.), Physical and biological processing of images (pp. 73--87).
Berlin: Springer. Robson, J.G., Tolhurst, D.J., Freeman, R.D., & Ohzawa,
I. (1988). Simple cells in the visual cortex of the cat can be narrowly
tuned for spatial frequency. Visual Neuroscience, 1, 415--419. Rock, I.
(1973). Orientation and form. New York: Academic Press. Rodman, H.R., &
Albright, T.D. (1987). Coding of visual stimulus velocity in area MT of
the macaque. Vision Research, 27, 2035--2048. Rodman, H.R., & Albright,
T.D. (1989). Single-unit analysis of pattern-motion selective properties
in the middle temporal visual area (MT). Experimental Brain Research,
75, 53--64. Rogers, B., & Graham, M. (1979). Motion parallax as an
independent cue for depth perception. Perception, 8, 125--134. Rogers,
B., & Graham, M. (1982). Similarities between motion parallax and
stereopsis in human depth perception. Vision Research, 22, 261--270.
Rolls, E.T., & Deco, G. (2002). Computational neuroscience of vision.
Oxford: Oxford University Press. Rose, D. (1978). Monocular versus
binocular contrast thresholds for movement and pattern. Perception, 7,
195--200. Ross, J. (1976). The resources of binocular perception.
Scientific American, 234, March, 80--86. Ross, J., Morrone, M.C., &
Burr, D.C. (1989). The conditions under which Mach bands are visible.
Vision Research, 29, 699--715. Rossel, S. (1983). Binocular stereopsis
in an insect. Nature, 302, 821--822. Roth, G., & Wiggers, W. (1983).
Responses of the toad Bufo bufo to stationary prey stimuli. Zeitschrift
für Tierpsychologie, 61, 225--234. Rowland, W.J., Grindle, N., MacLaren,
R.D., & Granquist, R. (2002). Male preference for a subtle posture

446

REFERENCES

cue that signals spawning readiness in female sticklebacks. Animal
Behaviour, 63, 743--748. Royden C.S., Crowell, J.A., & Banks, M.S.
(1994). Estimating heading during eye movements. Vision Research, 34,
3197--3214. Rumelhart, D.E., & McClelland, J.L. (1985). Levels indeed! A
response to Broadbent. Journal of Experimental Psychology: General, 114,
193--197. Rumelhart, D.E., & McClelland, J.L. (1986). Parallel
distrubuted processing: Explorations in the microstructure of
cognition, 1. Foundations. Cambridge, MA: MIT Press. Runeson, S. (1994).
Perception of biological motion: The KSD-Principle and the implications
of a distal versus proximal approach. In G. Jansson, S.S. Bergstrom, &
W. Epstein (Eds.), Perceiving events and objects (pp. 383--405).
Hillsdale, NJ: Lawrence Erlbaum Associates Inc. Runeson, S., & Frykolm,
G. (1981). Visual perception of lifted weights. Journal of Experimental
Psychology: Human Perception and Performance, 7, 733-- 740. Runeson, S.,
& Frykolm, G. (1983). Kinematic specifications of dynamics as an
informational basis for person-and-action perception: Expectation,
genderrecognition and deceptive intention. Journal of Experimental
Psychology: General, 112, 585--615. Rushton, S.K., Harris, J.M., Lloyd,
M.R., & Wann, J.P. (1998). Guidance of locomotion on foot uses perceived
target location rather than optic flow. Current Biology, 8, 1191--1194.
Rushton, S.K., & Wann, J.P. (1999). Weighted combination of size and
disparity: A computational model for timing a ball catch. Nature
Neuroscience, 2, 186--190. Rushton, W.A.H. (1965). Visual adaptation.
The Ferrier lecture. Proceedings of the Royal Society of London, B, 162,
20--46. Russell, J.A. (1994). Is there universal expression of emotion
from facial expression? A review of crosscultural studies. Psychological
Bulletin, 115, 102-- 141. Sakai, H.M., Wang, J.L., & Naka, K.I. (1995).
Contrast gain-control in the lower vertebrate retinas. Journal of
General Physiology, 105, 815--835. Salzman, C.D., Britten, K.H., &
Newsome, W.T. (1990). Cortical microstimulation influences perceptual
judgements of motion direction. Nature, 346, 174--177. Sanger, T.D.
(1988). Stereo disparity computation using Gabor filters. Biological
Cybernetics, 59, 405--418. Sary, G., Vogels, R., & Orban, G.A. (1994).
Orientation

discrimination of motion-defined gratings. Vision Research, 34,
1331--1334. Savelsbergh, G.J.P., Whiting, H.T.A., & Bootsma, R.J.
(1991). Grasping tau. Journal of Experimental Psychology: Human
Perception and Performance, 17, 315--322. Schaeffel, F., & Howland, H.C.
(1987). Corneal accommodation in chick and pigeon. Journal of
Comparative Physiology, 160, 375--384. Schalkoff, R.J. (1989). Digital
image processing and computer vision. New York: Wiley. Schaller, G.B.
(1972). The Serengeti lion: A study of predator--prey relations.
Chicago: Chicago University Press. Schenk, T., & Zihl, J. (1997). Visual
motion perception after brain damage. I. Deficits in global motion
perception. Neuropsychologia, 35, 1289--1297. Schiff, W., Caviness,
J.A., & Gibson, J.J. (1962). Persistent fear responses in rhesus monkeys
to the optical stimulus of "looming". Science, 136, 982--983. Schiff,
W., & Detwiler, M.L. (1979). Information used in judging impending
collision. Perception, 8, 647--658. Schiller, P.H., & Lee, K. (1991).
The role of primate extrastriate area V4 in vision. Science, 251, 1251--
1253. Schiller, P.H., Logothetis, N.K., & Charles, E.R. (1990).
Functions of the colour-opponent and broad-band channels of the visual
system. Nature, 343, 68--70. Schlottman, A., & Anderson, N.H. (1993). An
information integration approach to phenomenal causality. Memory &
Cognition, 21, 785--801. Schlottman, A., & Shanks, D.R. (1992). Evidence
for a distinction between judged and perceived causality. Quarterly
Journal of Experimental Psychology, 44A, 321--342. Schmidt, F., &
Tiffin, J. (1969). Distortion of drivers' estimates of automobile speed
as a function of speed adaptation. Journal of Applied Psychology, 53,
536--539. Schmidt, R.C., Carello, C., & Turvey, M.T. (1990). Phase
transitions and critical fluctuations in the visual co-ordination of
rhythmic movements between people. Journal of Experimental Psychology:
Human Perception and Performance, 16, 227--247. Schofield, A.J., &
Georgeson, M.A. (2000). The temporal properties of first- and
second-order vision. Vision Research, 40, 2475--2487. Scholl, B.J., &
Tremoulet, P.D. (2001). Perceptual causality and animacy. Trends in
Cognitive Sciences, 4, 299--309. Schöner, G., Dijkstra, T.M.H., & Jeka,
J.J. (1998).

REFERENCES

Action--perception patterns emerge from coupling and adaptation.
Ecological Psychology, 10, 323--346. Schor, C.M., & Tyler, C.W. (1981).
Spatio-temporal properties of Panum's fusional area. Vision Research,
21, 683--692. Schor, C.M., & Wood, I. (1983). Disparity range for local
stereopsis as a function of luminance spatial frequency. Vision
Research, 23, 1649--1654. Schor, C.M., Wood, I., & Ogawa, J. (1984).
Binocular sensory fusion is limited by spatial resolution. Vision
Research, 24, 661--665. Schrater, P.R., Knill, D.C., & Simoncelli, E.P.
(2000). Mechanisms of visual motion detection. Nature Neuroscience, 3,
64--68. Schwartz, O., & Simoncelli, E.P. (2001). Natural signal
statistics and sensory gain control. Nature Neuroscience, 4, 819--825.
Schyns, P.G. (1998). Diagnostic recognition: Task constraints, object
recognition, and their interactions. Cognition, 67, 147--179. Schyns,
P.G., & Oliva, A. (1997). Flexible, diagnosticitydriven, rather than
fixed, perceptually determined scale selection in scene and face
recognition. Perception, 26, 1027--1038. Sclar, G., Maunsell, J.H.R., &
Lennie, P. (1990). Coding of image contrast in central visual pathways
of the macaque monkey. Vision Research, 30, 1--10. Scott-Samuel, N.E., &
Georgeson, M.A. (1999). Does early non-linearity account for
second-order motion? Vision Research, 39, 2853--2865. Seamon, J.G.
(1982). Dynamic facial recognition: Examination of a natural phenomenon.
American Journal of Psychology, 95, 363--381. Seiffert, A.E., &
Cavanagh, P. (1998). Position displacement, not velocity, is the cue to
motion detection of second-order stimuli. Vision Research, 38,
3569--3582. Sekuler, R.W., & Ganz, L. (1963). After-effect of seen
motion with a stabilized retinal image. Science, 139, 419--420.
Selfridge, O.G. (1959). Pandemonium: A paradigm for learning. In The
mechanisation of thought processes. London: HMSO. Servos, P., Goodale,
M.A., & Jakobson, L.S. (1992). The role of binocular vision in
prehension: a kinematic analysis. Vision Research, 32, 1513--1521.
Shadlen, M., & Carney, T. (1986). Mechanisms of motion perception
revealed by a new cyclopean illusion. Science, 232, 95--97. Shadlen,
M.N., & Movshon, A. (1999). Synchrony unbound: A critical evaluation of
the temporal binding hypothesis. Neuron, 24, 67--77.

447

Shapley, R., & Perry, V.H. (1986). Cat and monkey retinal ganglion cells
and their visual functional roles. Trends in Neuroscience, 9, 229--235.
Sharpe, C.R., & Tolhurst, D.J. (1973). Orientation and spatial frequency
channels in peripheral vision. Vision Research, 13, 2103--2112. Sharpe,
L.T., & Stockman, A. (1999). Rod pathways: The importance of seeing
nothing. Trends in Neuroscience, 22, 497--504. Shaw, R.E., McIntyre, M.,
& Mace, W. (1974). The role of symmetry in event perception. In R.B.
MacCleod & H.L. Pick (Eds.), Perception: Essays in honour of James J.
Gibson. Ithaca, New York: Cornell University Press. Shaw, R.E., &
Pittenger, J.B. (1977). Perceiving the face of change in changing faces:
Implications for a theory of object recognition. In R.E. Shaw & J.
Bransford (Eds.), Perceiving, acting and knowing: Toward an ecological
psychology. Hillsdale, NJ: Lawrence Erlbaum Associates Inc. Sherrington,
C.S. (1906). Integrative action of the nervous system. New Haven, CT:
Yale University Press (reset edition, 1947). Shimojo, S., & Nakayama, K.
(1990). Real world occlusion constraints and binocular rivalry. Vision
Research, 30, 69--80. Shirai, Y. (1973). A context sensitive line finder
for recognition of polyhedra. Artificial Intelligence, 4, 95--120.
Sillito, A.M., Jones, H.E., Gerstein, G.L., & West, D.C. (1994).
Feature-linked synchronization of thalamic relay cell firing induced by
feedback from the visual cortex. Nature, 369, 479--482. Silverman, M.S.,
Grosof, D.H., DeValois, R.L., & Elfar, S.D. (1989). Spatial frequency
organization in primate striate cortex. Proceedings of the National
Academy of Sciences of the USA, 86, 711--715. Simmers, A.J., & Bex, P.J.
(2001). Deficit of visual contour integration in dyslexia. Investigative
Ophthalmology, & Visual Science, 42, 2737--2742. Simoncelli, E.P., &
Heeger, D.J. (1998). A model of neuronal responses in visual area MT.
Vision Research, 38, 743--761. Simons, D.J. (Ed.). (2000a). Change
blindness and visual memory. Hove, UK: Psychology Press. Simons, D.J.
(2000b). Attentional capture and inattentional blindness. Trends in
Cognitive Sciences, 4, 147--155. Simons, D.J., & Levin, J.T. (1998).
Failure to detect changes to people during a real-world interaction.
Psychonomic Bulletin and Review, 5, 644--649. Simpson, M.J.A. (1968).
The display of the Siamese

448

REFERENCES

fighting fish Betta splendens. Animal Behaviour Monographs, 1, 1--73.
Sinai, M.J., Ooi, T.L., & He, Z.J. (1998). Terrain influences the
accurate judgement of distance. Nature, 395, 497--500. Sivak, J.G.
(1978). A survey of vertebrate strategies for vision in air and water.
In M.A. Ali (Ed.), Sensory ecology: Review and perspectives. New York:
Plenum Press. Sivak, J.G., Hildebrand, T., & Lebert, C. (1985).
Magnitude and rate of accommodation in diving and nondiving birds.
Vision Research, 25, 925--933. Skottun, B.C., De Valois, R.L., Grosof,
D.H., Movshon, J.A., Albrecht, D.G., & Bonds, A.B. (1991). Classifying
simple and complex cells on the basis of response modulation. Vision
Research, 31, 1079-- 1086. Smallman, H.S., & MacLeod, D.I.A. (1994).
Sizedisparity correlation in stereopsis at contrast threshold. Journal
of the Optical Society of America A, 11, 2169--2183. Smallman, H.S.,
MacLeod, D.I.A., He, S., & Kentridge, R.W. (1996). Fine grain of the
neural representation of human spatial vision. Journal of Neuroscience,
16, 1852--1859. Smirnakis, S.M., Berry, M.J., Warland, D.K., Bialek, W.,
& Meister, M. (1997). Adaptation of retinal processing to image contrast
and spatial scale. Nature, 386, 69--73. Smith, A.T. (1994).
Correspondence-based and energybased detection of second-order motion in
human vision. Journal of the Optical Society of America, A11,
1940--1948. Smith, A.T., & Edgar, G.K. (1994). Antagonistic comparison
of temporal frequency filter outputs as a basis for speed perception.
Vision Research, 34, 253-- 265. Smith, A.T., & Ledgeway, T. (1998).
Sensitivity to second-order motion as a function of temporal frequency
and eccentricity. Vision Research, 38, 403--410. Smith, A.T., & Over, R.
(1975). Tilt aftereffects with subjective contours. Nature, 257,
581--582. Smith, A.T., & Over, R. (1977). Orientation masking and the
tilt illusion with subjective contours. Perception, 6, 441--447. Smith,
E.L., Chino, Y., Ni, J., & Cheng, H. (1997). Binocular combination of
contrast signals by striate cortical neurons in the monkey. Journal of
Neurophysiology, 78, 366--382. Smolensky, P. (1987). Connectionist AI,
symbolic AI

and the brain. Artificial Intelligence Review, 1, 95--110. Smolensky, P.
(1988). On the proper treatment of connectionism. The Behavioural and
Brain Sciences, 11, 1--74. Snowden, R.J. (1992). Orientation bandwidth:
The effect of spatial and temporal frequency. Vision Research, 32,
1965--1974. Snowden, R.J., & Milne, A.B. (1997). Phantom motion
aftereffects -- evidence of detectors for the analysis of optic flow.
Current Biology, 7, 717--722. Snowden, R.J., Stimpson, N., & Ruddle,
R.A. (1998). Speed perception fogs up as visibility drops. Nature, 392,
450. Snyder, L.H., Batista, A.P., & Andersen, R.A. (2000).
Intention-related activity in the posterior parietal cortex: A review.
Vision Research, 40, 1433--1441. Sobel, E.C. (1990). Depth perception by
motion parallax and paradoxical parallax in the locust.
Naturwissenschaften, 77, 241--243. Solomon, J.A., & Morgan, M.J. (2000).
Facilitation from collinear flanks is cancelled by non-collinear flanks.
Vision Research, 40, 279--286. Spurr, R.T. (1969). Subjective aspects of
braking. Automobile Engineer, 59, 58--61. Srinivasan, M.V., Lehrer, M.,
& Horridge, G.A. (1990). Visual figure--ground discrimination in the
honeybee: The role of motion parallax at boundaries. Proceedings of the
Royal Society of London B, 238, 331--350. Srinivasan, M.V., Lehrer, M.,
Kirchner, W.H., & Zhang, S.W. (1991). Range perception through apparent
image speed in freely flying honeybees. Visual Neuroscience, 6,
519--535. Srinivasan, M.V., Poteser, M., & Kral, K. (1999). Motion
detection in insect orientation and navigation. Vision Research, 39,
2749--2766. Srinivasan, M., & Zhang, S.W. (1998). Probing perception in
a miniature brain: Pattern recognition and maze navigation in honeybees.
Zoology: Analysis of Complex Systems, 101, 246--259. Srinivasan, M.V.,
Zhang, S.W., Chahl, J.S., Barth, E., & Venkatesh, S. (2000). How
honeybees make grazing landings on flat surfaces. Biological
Cybernetics, 83, 171--183. Steiner, V., Blake, R., & Rose, D. (1994).
Interocular transfer of expansion, rotation and translation motion
aftereffects. Perception, 23, 1197--1202. Stevens, J.K., Emerson, R.C.,
Gerstein, G.L., Kallos, T., Neufield, G.R., Nichols, C.W. et al. (1976).
Paralysis of the awake human: Visual perceptions. Vision Research, 16,
93--98.

REFERENCES

Stevens, K., & Brookes, A. (1988). Integrating stereopsis with monocular
interpretations of planar surfaces. Vision Research, 28, 371--386.
Stone, J., & Fukuda, Y. (1974). Properties of cat retinal ganglion
cells: A comparison of W cells with X and Y cells. Journal of
Neurophysiology, 37, 722--748. Stone, J.V. (1998). Object recognition
using spatiotemporal signatures. Vision Research, 38, 947--951. Stoner,
G.R., & Albright, T.D. (1994). Visual motion integration: A
neurophysiological and psychophysical perspective. In A.T. Smith & R.J.
Snowden (Eds.), Visual detection of motion. London: Academic Press.
Stratton, G.M. (1897). Vision without inversion of the retinal image.
Psychological Reviews, 4, 341--360. Sugase, Y., Yamane, S., Ueno, S., &
Kawano, K. (1999). Global and fine information coded by single neurons
in the temporal visual cortex. Nature, 400, 869--873. Sugita, Y. (1999).
Grouping of image fragments in primary visual cortex. Nature, 401,
269--272. Sumner, F.B. (1934). Does "protective coloration" protect?
Results of some experiments with fishes and birds. Proceedings of the
National Academy of Science, 10, 559--564. Sun, H., & Frost, B.J.
(1998). Computation of different optical variables of looming objects in
pigeon nucleus rotundus neurons. Nature Neuroscience, 1, 296--303. Sun,
H.-J., Carey, D.P., & Goodale, M.A. (1992). A mammalian model of
optic-flow utilization in the control of locomotion. Experimental Brain
Research, 91, 171--175. Sutherland, N.S. (1973). Object recognition. In
E.C. Carterette & M.P. Friedman (Eds.), Handbook of perception Volume
III: Biology of perceptual systems. London: Academic Press. Sutherland,
N.S., & Williams, C. (1969). Discrimination of checkerboard patterns by
rats. Quarterly Journal of Experimental Psychology, 21, 77--84. Tailor,
D.R., Finkel, L.H., & Buchsbaum, G. (2000). Colour-opponent receptive
fields derived from independent component analysis of natural images.
Vision Research, 40, 2671--2676. Tanaka, H., & Saito, H. (1989).
Analysis of motion of the visual field by direction,
expansion/contraction and rotation cells clustered in the dorsal part of
the medial superior temporal area of the macaque monkey. Journal of
Neurophysiology, 62, 626--641. Tanaka, J., & Farah, M.J. (1993). Parts
and wholes in face recognition. Quarterly Journal of Experimental
Psychology, 46A, 225--246.

449

Tanaka, K. (1993). Neuronal mechanisms of object recognition. Science,
262, 685--688. Tanaka, K. (1998). Representation of visual motion in the
extrastriate visual cortex. In T. Watanabe (Ed.), High level motion
processing (pp. 295--313). Cambridge, MA: MIT Press. Tarr, M.J. (1995).
Rotating objects to recognize them: A case study on the role of
viewpoint dependency in the recognition of three-dimensional objects.
Psychonomic Bulletin, & Review, 2, 55--82. Tarr, M.J., & Bülthoff, H.H.
(1995). Is human object recognition better described by
geon-structuraldescriptions or by multiple views -- comment on Biederman
and Gerhardstein (1993). Journal of Experimental Psychology: Human
Perception and Performance, 21, 1494--1505 Tarr, M.J., & Bülthoff, H.H.
(1998). Image-based object recognition in man, monkey and machine.
Cognition, 67, 1--20 Tarr, M.J., & Pinker, S. (1989). Mental rotation
and orientation-dependence in shape recognition. Cognitive Psychology,
21, 233--282. Thayer, G.H. (1918). Concealing coloration in the animal
kingdom. New York: Macmillan. Thompson, P.G. (1981). Velocity
aftereffects: The effects of adaptation to moving stimuli on the
perception of subsequently seen moving stimuli. Vision Research, 21,
337--345. Thompson, P.G. (1984). The coding of velocity of movement in
the human visual system. Vision Research, 24, 41--45. Thomson, J.A.
(1983). Is continuous visual monitoring necessary in visually guided
locomotion? Journal of Experimental Psychology: Human Perception and
Performance, 9, 427--443. Thorpe, S., Fize, D., & Marlot, C. (1996).
Speed of processing in the human visual system. Nature, 381, 520--522.
Tinbergen, N. (1951). The study of instinct. Oxford: Clarendon Press.
Todd, J.T., Mark, L.S., Shaw, R.E., & Pittenger, J.B. (1980). The
perception of human growth. Scientific American, 242, February,
106--114. Tolhurst, D.J. (1973). Separate channels for the analysis of
the shape and the movement of a moving visual stimulus. Journal of
Physiology, 231, 385--402. Tomasello, M., Call, J., & Hare, B. (1998).
Five primate species follow the visual gaze of conspecifics. Animal
Behaviour, 55, 1063--1069. Tomasello, M., Hare, B., & Agnetta, B.
(1999). Chimpanzees, Pan troglodytes, follow gaze direction
geometrically. Animal Behaviour, 58, 769--777.

450

REFERENCES

Tootell, R.B.H., Hadjikhani, N.K., Mendola, J.D., Marrett, S., & Dale,
A.M. (1998). From retinotopy to recognition: fMRI in human visual
cortex. Trends in Cognitive Sciences, 2, 174--183. Tootell, R.B.H.,
Reppas, J.B., Dale, A.M., Look, R.B., Sereno, M.I., Malach, R. et
al. (1995). Visual motion aftereffect in human cortical area MT revealed
by functional magnetic resonance imaging. Nature, 375, 139--141.
Tootell, R.B.H., Reppas, J.B., Kwong, K.K., Malach, R., Born, R.T.,
Brady, T.J. et al. (1995). Functional analysis of human MT and related
visual cortical areas using magnetic resonance imaging. Journal of
Neuroscience, 15, 3215--3230. Tootell, R.B.H., Silverman, M.S.,
Hamilton, S.L., Switkes, E., & DeValois, R.L. (1988). Functional anatomy
of macaque striate cortex. V. Spatial frequency. Journal of
Neuroscience, 8, 1610--1624. Tresilian, J.R. (1991). Empirical and
theoretical issues in the perception of time to contact. Journal of
Experimental Psychology: Human Perception and Performance, 17, 865--876.
Tresilian, J.R. (1995). Perceptual and cognitive processes in
time-to-contact estimation: Analysis of prediction-motion and relative
judgement tasks. Perception and Psychophysics, 57, 231--245. Treue, S.
(2001). Neural correlates of attention in primate visual cortex. Trends
in Neurosciences, 24, 295-- 300. Treue, S., Andersen, R.A., Ando, H., &
Hildreth, E.C. (1995). Structure-from-motion: Perceptual evidence for
surface interpolation. Vision Research, 35, 139-- 148. Treue, S., &
Trujillo, J.C.M. (1999). Feature-based attention influences motion
processing gain in macaque visual cortex. Nature, 399, 575--579. Tucker,
V.A., Tucker, A.E., Akers, K., & Enderson, H.J. (2000). Curved flight
paths and sideways vision in peregrine falcons (Falco peregrinus).
Journal of Experimental Biology, 203, 3755--3763. Tuller, B., Turvey,
M.T., & Fitch, H.L. (1982). The Bernstein perspective II. The concept of
muscle linkage or coordinative structure. In J.A.S. Kelso (Ed.), Human
motor behaviour: An introduction. Hillsdale, NJ, Lawrence Erlbaum
Associates Inc. Tulunay-Keesey, U. (1982). Fading of stabilized retinal
images. Journal of the Optical Society of America, 72, 440--447.
Tulunay-Keesey, U., & Jones, R.M. (1980). Contrast sensitivity measures
and accuracy of image stabilization systems. Journal of the Optical
Society of America, 70, 1306--1310.

Turner, E.R.A. (1964). Social feeding in birds. Behaviour, 24, 1--46.
Turvey, M.T. (1977). Preliminaries to a theory of action with reference
to seeing. In R. Shaw & J. Bransford (Eds.), Perceiving, acting and
knowing: Toward an ecological psychology. Hillsdale, NJ: Lawrence
Erlbaum Associates Inc. Turvey, M.T., Fitch, H.L., & Tuller, B. (1982).
The Bernstein perspective I. The problems of degrees of freedom and
context-conditioned variability. In J.A.S. Kelso (Ed.), Human motor
behaviour: An introduction. Hillsdale, NJ. Lawrence Erlbaum Associates
Inc. Turvey, M.T., Shaw, R.E., & Mace, W. (1978). Issues in the theory
of action: Degrees of freedom, coordinative structures and coalitions.
In J. Requin (Ed.), Attention and performance VII. Hillsdale, NJ:
Lawrence Erlbaum Associates Inc. Tynan, P., & Sekuler, R. (1975). Moving
visual phantoms: A new contour completion effect. Science, 188,
951--952. Ullman, S. (1979). The interpretation of visual motion.
Cambridge, MA: MIT Press. Ullman, S. (1980). Against direct perception.
Behavioral and Brain Sciences, 3, 373--415. Ullman, S. (1989). Aligning
pictorial descriptions: An approach to object recognition. Cognition,
32, 193--254. Ullman, S. (1998). Three-dimensional object recognition
based on the combination of views. Cognition, 67, 21--44. Ungerleider,
L.G., & Mishkin, M. (1982). Two cortical visual systems. In D.J. Ingle,
M.A. Goodale, & R.J.W. Mansfield (Eds.), Analysis of visual behaviour
(pp. 549--586). Cambridge, MA: MIT Press. Uttal, W.R. (1981). Taxonomy
of visual processes. Hillsdale, NJ: Lawrence Erlbaum Associates Inc. van
den Berg, A.V., & Brenner, E. (1994). Why two eyes are better than one
for judgements of heading. Nature, 371, 700--702. van der Meer, A.L.H.,
van der Weel, F.R., & Lee, D.N. (1994). Prospective control in catching
by infants. Perception, 23, 287--302. van der Zwan, R., & Wenderoth, P.
(1994). Psychophysical evidence for area V2 involvement in the reduction
of subjective contour tilt aftereffects by binocular rivalry. Visual
Neuroscience, 11, 823--830. van Essen, D.C. (1985). Functional
organization of primate visual cortex. In A. Peters & E.G. Jones (Eds.),
Cerebral cortex. Vol. 3: Visual cortex (pp. 259--329). New York: Plenum
Press. van Essen, D.C., Anderson, C.H., & Felleman, D.J.

REFERENCES

(1992). Information processing in the primate visual system: An
integrated systems perspective. Science, 255, 419--423. van Hateren,
J.H., & Ruderman, D.L. (1998). Independent component analysis of natural
image sequences yields spatio-temporal filters similar to simple cells
in primary visual cortex. Proceedings of the Royal Society of London, B,
265, 2315--2320. van Santen, J.P.H., & Sperling, G. (1984). Temporal
covariance model of human motion perception. Journal of the Optical
Society of America A, 1, 451-- 473. Vautin, R.G., & Berkley,
M.A. (1977). Responses of single cells in cat visual cortex to prolonged
stimulus movement: Neural correlates of visual aftereffects. Journal of
Neurophysiology, 40, 1051--1065. Vecera, S.P., & Johnson, M.H. (1995).
Gaze detection and the cortical processing of faces. Evidence from
infants and adults. Visual Cognition, 2, 59--87. Victor, J.D. (1987).
The dynamics of the cat retinal X cell centre. Journal of Physiology,
386, 219--246. Victor, J.D., & Shapley, R.M. (1979). The nonlinear
pathway of Y ganglion cells in the cat retina. Journal of General
Physiology, 74, 671--689. Vines, G. (1981). Wolves in dogs' clothing.
New Scientist, 10th September. Vogels, R., Biederman, I., Bar, M., &
Lorincz, A. (2001). Inferior temporal neurons show greater sensitivity
to nonaccidental than to metric shape differences. Journal of Cognitive
Neuroscience, 13, 444--453. Volkmann, F.C., Riggs, L.A., Moore, R.K., &
White, K.D. (1978). Contrast sensitivity during saccadic eye movements.
Vision Research, 18, 1193--1199. von der Heydt, R., & Peterhans, E.
(1989). Mechanisms of contour perception in monkey visual cortex. I.
Lines of pattern discontinuity. Journal of Neuroscience, 9, 1731--1748.
von der Heydt, R., Peterhans, E., & Dursteler, M.R. (1992). Periodic
pattern-selective cells in monkey visual cortex. Journal of
Neuroscience, 12, 1416-- 1434. von Hofsten, C. (1980). Predictive
reaching for moving objects by human infants. Journal of Experimental
Child Psychology, 30, 369--382. von Hofsten, C. (1983). Catching skills
in infancy. Journal of Experimental Psychology: Human Perception and
Performance, 9, 75--85. von Hofsten, C., & Lindhagen, K. (1979).
Observations of the development of reaching for moving objects. Journal
of Experimental Child Psychology, 28, 158-- 173. Von Holst, E. (1954).
Relation between the central

451

nervous system and the peripheral organs. British Journal of Animal
Behaviour, 2, 89--94. Walker, G.A., Ohzawa, I., & Freeman, R.D. (1999).
Asymmetric suppression outside the classical receptive field of the
visual cortex. Journal of Neuroscience, 19, 10536--10553. Walker, G.A.,
Ohzawa, I., & Freeman, R.D. (2000). Suppression outside the classical
cortical receptive field. Visual Neuroscience, 17, 369--379. Wallach,
H., & O'Connell, D.N. (1953). The kinetic depth effect. Journal of
Experimental Psychology, 45, 205--217. Walls, G.L. (1942). The
vertebrate eye and its adaptive radiation. New York: Hafner. Waltz, D.L.
(1975). Generating semantic descriptions from scenes with shadows. In
P.H. Winston (Ed.), The psychology of computer vision. New York:
McGraw-Hill. Wandell, B.A. (1999). Computational neuroimaging of human
visual cortex. Annual Review of Neuroscience, 22, 145--173. Wang, Y., &
Frost, B.J. (1992). Time to collision is signalled by neurons in the
nucleus rotundus of pigeons. Nature, 356, 236--238. Wann, J., & Land, M.
(2000). Steering with or without the flow: Is the retrieval of heading
necessary? Trends in Cognitive Sciences, 4, 319--324. Wann, J.P. (1996).
Anticipating arrival: Is the taumargin a specious theory? Journal of
Experimental Psychology: Human Perception and Performance, 22,
1031--1048. Wann, J.P., Edgar, P., & Blair, D. (1993). Time-tocontact
judgement in the locomotion of adults and preschool children. Journal of
Experimental Psychology: Human Perception and Performance, 19,
1053--1065. Wann, J.P., Mon-Williams, M., & Rushton, K. (1998). Postural
control and co-ordination disorders: The swinging room revisited. Human
Movement Science, 17, 491--513. Warren, W.H. (1984). Perceiving
affordances: Visual guidance of stair climbing. Journal of Experimental
Psychology: Human Perception and Performance, 10, 683--703. Warren,
W.H., & Hannon, D.J. (1988). Direction of self-motion is perceived from
optical flow. Nature, 336, 162--163. Warzecha, A.K., & Egelhaaf, M.
(1996). Intrinsic properties of biological motion detectors prevent the
optomotor control system from getting unstable. Philosophical
Transactions of the Royal Society of London, B, 351, 1579--1591.

452

REFERENCES

Watamaniuk, S.N.J., Sekuler, R., & Williams, D.W. (1989). Direction
perception in complex dynamic displays: The integration of direction
information. Vision Research, 29, 47--59. Watson, A.B., & Ahumada, A.J.
(1985). Model of human visual-motion sensing. Journal of the Optical
Society of America A, 2, 322--341. Watson, A.B., Ahumada, A.J., &
Farrell, J.E. (1986). Window of visibility: A psychophysical theory of
fidelity in time-sampled visual motion displays. Journal of the Optical
Society of America A, 3, 300-- 307. Watson, A.B., & Solomon, J.A.
(1997). Model of visual contrast gain control and pattern masking.
Journal of the Optical Society of America A, 14, 2379--2391. Watson,
A.B., Thompson, P.G., Murphy, B.J., & Nachmias, J. (1980). Summation and
discrimination of gratings moving in opposite directions. Vision
Research, 20, 341--347. Watson, J.B. (1913). Psychology as the
behaviourist views it. Psychological Review, 20, 158--177. Watson, J.B.
(1924). Psychology from the standpoint of a behaviourist. Philadelphia:
Lippincott. Watt, R.J. (1988). Visual processing: Computational,
psychophysical and cognitive research. London: Lawrence Erlbaum
Associates Ltd. Watt, R.J. (1992). Faces and vision. In V. Bruce & M.
Burton (Eds.), Processing images of faces. Norwood, NJ: Ablex. Watt,
R.J., & Morgan, M.J. (1983). The recognition and representation of edge
blur: Evidence for spatial primitives in human vision. Vision Research,
23, 1457--1477. Watt, R.J., & Morgan, M.J. (1985). A theory of the
primitive spatial code in human vision. Vision Research, 25, 1661--1674.
Watt, R.J., & Phillips, W.A. (2000). The function of dynamic vision in
grouping. Trends in Cognitive Sciences, 4, 447--454. Wehner, R. (2001).
Polarisation vision -- a uniform sensory capacity? Journal of
Experimental Biology, 204, 2589--2596. Weiskrantz, L. (1986).
Blindsight. Oxford: Clarendon Press. Weiskrantz, L., Warrington, E.K.,
Sanders, M.D., & Marshall, J. (1974). Visual capacity in the hemianopic
field following a restricted occipital ablation. Brain, 97, 709--728.
Welch, L. (1989). The perception of moving plaids reveals two motion
processing stages. Nature, 337, 734--736. Werblin, F.S., & Dowling, J.E.
(1969). Organization of

the retina of the mudpuppy Necturus maculosus. II. Intracellular
recording. Journal of Neurophysiology, 32, 339--355. Wertheimer, M.
(1923). Untersuchungen zur Lehre von der Gestalt, II. Psychologische
Forschung, 4, 301-- 350. Translated as 'Laws of organisation in
perceptual forms' in Ellis, W.D. (1955), A source book of Gestalt
psychology. London: Routledge & Kegan Paul. Wheatstone, C. (1838).
Contributions to the physiology of vision. Part I: On some remarkable
and hitherto unobserved phenomena of binocular vision. Philosophical
Transactions of the Royal Society of London, 128, 371--394. Whiten, A.,
& Byrne, R.W. (1988). Tactical deception in primates. Behavioural and
Brain Sciences, 11, 233-- 273. Whiting, H.T.A., & Sharp, R.H. (1974).
Visual occlusion factors in a discrete ball-catching task. Journal of
Motor Behaviour, 6, 11--16. Wigglesworth, V.B. (1964). The life of
insects. London: Weidenfeld, & Nicolson. Wilson, H.R. (1983).
Psychophysical evidence for spatial channels. In O.J. Braddick & A.C.
Sleigh (Eds.), Physical and biological processing of images (pp. 88--
99). Berlin: Springer. Wilson, H.R. (1994a). The role of second-order
motion signals in coherence and transparency. In Higher order processing
in the visual system (Ciba Foundation Symposium 184) (pp. 227--244).
Chichester, UK: Wiley. Wilson, H.R. (1994b). Models of two-dimensional
motion perception. In A.T. Smith & R.J. Snowden (Eds.), Visual detection
of motion. London: Academic Press. Wilson, H.R., Ferrera, V.P., & Yo, C.
(1992). A psychophysically motivated model for two-dimensional motion
perception. Visual Neuroscience, 9, 79--97. Wilson, H.R., & Kim, J.
(1994a). A model for motion coherence and transparency. Visual
Neuroscience, 11, 1205--1220. Wilson, H.R., & Kim, J. (1994b). Perceived
motion in the vector sum direction. Vision Research, 34, 1835-- 1842.
Wilson, H.R., McFarlane, D.K., & Phillips, G.C. (1983). Spatial
frequency tuning of orientation selective units estimated by oblique
masking. Vision Research, 23, 873--882. Wilson, H.R., & Wilkinson, F.
(1998). Detection of global structure in Glass patterns: implications
for form vision. Vision Research, 38, 2933--2947. Winston, P.H. (1973).
Learning to identify toy block

REFERENCES

structures. In R.L. Solso (Ed.), Contemporary issues in cognitive
psychology: The Loyola symposium. Washington, DC: Hemisphere Publishing
Corp. Winston, P.H. (1975). Learning structural descriptions from
examples. In P.H. Winston (Ed.), The psychology of computer vision. New
York: McGraw-Hill. Winston, P.H. (1984). Artificial intelligence.
London: Addison-Wesley. Wittreich, W.J. (1959). Visual perception and
personality. Scientific American, 200, April, 56--75. Wohlgemuth, A.
(1911). On the after-effect of seen movement. British Journal of
Psychology, Monograph Supplement 1. Wong-Riley, M.T.T. (1979). Changes
in the visual system of monocularly sutured or enucleated cats
demonstrable with cytochrome oxidase histochemistry. Brain Research,
171, 11--28. Wood, D.C. (1976). Action spectrum and electrophysiological
responses correlated with the photophobic response of Stentor coeruleus.
Photochemistry and Photobiology, 24, 261--266. Woodbury, P.B. (1986).
The geometry of predator avoidance by the blue crab Callinectes sapidus
Rathbun. Animal Behaviour, 34, 28--37. Wundt, W. (1896). Grundriss der
Psychologie. Translated by C.H. Judd (1907) as Outlines of Psychology.
New York: G.E. Stechart, & Co. Wurtz, R.H., & Albano, J.E. (1980).
Visual-motor function of the primate superior colliculus. Annual Review
of Neuroscience, 3, 189--226. Wylie, D.R.W., Bischof, W.F., & Frost,
B.J. (1998). Common reference frame for neural coding of translational
and rotational optic flow. Nature, 392, 278--282. Xu, X., Ichida, J.M.,
Allison, J.D., Boyd, J.D., Bonds, A.B., & Casagrande, V.A. (2001). A
comparison of koniocellular, magnocellular and parvocellular receptive
field properties in the lateral geniculate nucleus of the owl monkey
(Aotus trivirgatus). Journal of Physiology, 531, 203--218. Yabuta, N.H.,
Sawatari, A., & Callaway, E.M. (2001). Two functional channels from
primary visual cortex to dorsal visual cortical areas. Science, 292,
297-- 300. Yeshurun, Y., & Carrasco, M. (1999). Spatial attention
improves performance in spatial resolution tasks. Vision Research, 39,
293--306. Yilmaz, E.H., & Warren, W.H. (1995). Visual control of .
braking: a test of the τ hypothesis. Journal of Experimental Psychology:
Human Perception and Performance, 21, 996--1014. Yo, C., & Wilson, H.R.
(1992). Perceived direction of

453

moving two-dimensional patterns depends on duration, contrast and
eccentricity. Vision Research, 32, 135--147. Young, A.W. (1998). Face
and mind. Oxford: Oxford University Press. Young, A.W., Hellawell, D.J.,
& Hay, D.C. (1987). Configural information in face perception.
Perception, 16, 747--759. Young, M.J., Landy, M.S., & Maloney, L.T.
(1993). A perturbation analysis of depth perception from combinations of
texture and motion cues. Vision Research, 33, 2685--2696. Young, M.P.
(1992). Objective analysis of the topological organization of the
primate cortical visual system. Nature, 358, 152--155. Young, M.P., &
Yamane, S. (1992). Sparse population coding of faces in the
inferotemporal cortex. Science, 256, 1327--1331. Young, R.A. (1985). The
Gaussian derivative theory of spatial vision: Analysis of cortical cell
receptive field line-weighting profiles. Technical Report GMR 4920.
General Motors Research Labs. Warren, MI, USA. Young, R.A. (1987). The
Gaussian derivative model for spatial vision: I. Retinal mechanisms.
Spatial Vision, 2, 273--293. Young, R.A., & Lesperance, R.M. (2001). The
Gaussian derivative model for spatial-temporal vision: II. Cortical
data. Spatial Vision, 14, 321--389. Yu, C., Klein, S.A., & Levi, D.M.
(2001). Surround modulation of perceived contrast and the role of
brightness induction. Journal of Vision
\[http://JournalOfVision.org/1/1/3\], 1, 18--31. Yu, C., & Levi, D.M.
(1997). End stopping and length tuning in psychophysical spatial
filters. Journal of the Optical Society of America A, 14, 2346--2354.
Yu, C., & Levi, D.M. (1998). Spatial frequency and orientation tuning in
psychophysical end stopping. Visual Neuroscience, 15, 585--595. Yu, C.,
& Levi, D.M. (2000). Surround modulation in human vision unmasked by
masking experiments. Nature Neuroscience, 3, 724--728. Yuille, A.L., &
Grzywacz, N.M. (1988). A computational theory for the perception of
coherent visual motion. Nature, 333, 71--74. Zaal, F.T.J.M., & Bootsma,
R.J. (1995). The topology of limb deceleration in prehension tasks.
Journal of Motor Behaviour, 27, 193--207. Zanker, J.M., Egelhaaf, M., &
Warzecha, A.-K. (1991). On the co-ordination of motor output during
visual flight control of flies. Journal of Comparative Physiology, 169,
127--134.

454

REFERENCES

Zeki, S.M. (1978). Uniformity and diversity of structure and function in
rhesus monkey prestriate visual cortex. Journal of Physiology, 277,
273--290. Zeki, S. (1980). The representation of colours in the cerebral
cortex. Nature, 284, 412--418. Zeki, S. (1983). Colour coding in the
cerebral cortex: The reaction of cells in monkey visual cortex to
wavelength and colours. Neuroscience, 9, 741--765. Zeki, S., Watson,
J.D.G., Lueck, C.J., Friston, K.J., Kennard, C., & Frackowiak, R.S.J.
(1991). A direct demonstration of functional specialization in human
visual cortex. Journal of Neuroscience, 11, 641--649.

Zhou, Y.-X., & Baker, C.L. (1993). A processing stream in mammalian
visual cortex neurons for nonFourier responses. Science, 261, 98--101.
Zihl, J., Von Cramon, D., & Mai, N. (1983). Selective disturbance of
movement vision after bilateral brain damage. Brain, 106, 313--340.
Zipser, D., & Andersen, R.A. (1988). A backpropagation programmed
network that simulates response properties of a subset of posterior
parietal neurons. Nature, 331, 679--684. Zipser, K., Lamme, V.A.F., &
Schiller, P.H. (1996). Contextual modulation in primary visual cortex.
Journal of Neuroscience, 16, 7376--7389.

Appendix

http://psylux.psych.tu-dresden.de/i1/kaw/
diverses%20Material/www.illusionworks.com/ index.html

ON-LINE RESOURCES FOR PERCEPTION AND VISION SCIENCE

Chapter 3: Visual pathways and visual cortex Tutorials and animations
about receptive fields: http://www.bpe.es.osaka-u.ac.jp/ohzawa-lab/
teaching/ Contrast sensitivity testing in the form of a (very simple)
video game ("Gabori attack"):
http://www.bpe.es.osaka-u.ac.jp/ohzawa-lab/ vsoc/index.html

The study of perception is brought to life, and made more exciting, by
observing and playing with unusual or compelling visual phenomena for
oneself. The worldwide web has many sites devoted to vision-related
topics, often giving animated demonstrations that we cannot show on
these pages. We list here a few current sites of interest that also have
links to many others. They are grouped by relevance to the present
chapters. Web sites come and go, and their addresses change, so we
cannot guarantee they will continue to exist, but all were working at
the time of writing.

Chapters 5 & 6: Filters, features, and perceptual organisation Recent
computational models for multi-scale feature analysis
http://www.nada.kth.se/∼tony/ \[papers by Tony Lindeberg\] Chapter 7:
Depth perception For some striking demonstrations of the role of
shadows, and several other topics in 3-D vision, go to Dan Kersten's
site: http://vision.psych.umn.edu/www/kersten-lab/ demos.html Binocular
vision, rivalry and other demos by Randolph Blake:
http://WWW.PSY.VANDERBILT.EDU/ faculty/blake/demos.html

General sites A reliable gateway to many other sites is the NASA vision
lab site: http://www.visionscience.com Specific links to sites with
vision demonstrations (not all good) are given here:
http://www.visionscience.com/vsDemos.html The IllusionWorks site has
masses of good material, accessible through NASA or directly at: 455

456

APPENDIX

Receptive fields of binocular cells for stereo vision:
http://www.bpe.es.osaka-u.ac.jp/ohzawa-lab/ izumi/stereopsis/ Chapter 8:
Motion perception An excellent source of animated motion demonstrations,
with notes and explanations: http://www.biols.susx.ac.uk/home/George\_
Mather/Motion/ Download and run the Heeger--Simoncelli model of motion
coding in MT for yourself (Mac only):
http://white.stanford.edu/∼heeger/v1-mtmodel.html

Doing your own research Free software for setting up a research quality
display system, using a standard PC or Mac, in conjunction with the
Matlab application (not free): http://psychtoolbox.org/ . . . and for
analysing your psychophysical data, again using Matlab:
http://www.bootstrap-software.com/psignifit/ Natural image database (see
also NASA web site): http://hlab.phys.rug.nl/archive.html World's first
electronic journal of vision research: http://journalofvision.org/1/1/

Glossary

This glossary gives definitions of technical terms, particularly
physiological and mathematical ones, not defined in the text, and also
of terms used in sections of the book distant from those in which they
are explained. Terms italicised are defined elsewhere in the glossary.

light) occurs rapidly, while dark adaptation (e.g., when you walk from
daylight into a dark cinema) is a slower process. Affordance A term
introduced by Gibson, which refers to a possibility for action afforded
to a perceiver by an object. The affordances of an object depend on the
perceiver as well as on the characteristics of the object. For example,
a stream affords such actions as jumping and paddling to a person, but
to a frog it affords swimming. Algorithm A specified procedure for
solving a problem. Amacrine cell A type of cell in the vertebrate retina
(see Figure 2.4). Ambient optic array See optic array. Area centralis
Area in a vertebrate retina rich in cones, with little pooling of
receptor outputs. In the human eye, the area centralis corresponds to
the fovea, but this is not so in all species. Axon The long, slender
process of a nerve cell leading away from the cell body and ending at
synapses with other cells.

Absorption spectrum The relationship between the wavelength of light
striking a pigment and how strongly the light is absorbed. Accommodation
Adjustment of the optics of an eye to keep an object in focus on the
retina as its distance from the eye varies. In the human eye this is
achieved by varying the thickness of the lens. Action potential The
interior of a nerve cell has a negative electrical charge relative to
the exterior. If the axon of a nerve cell is stimulated electrically,
the membrane allows current to cross it and the charge is momentarily
reversed. This change in membrane behaviour spreads rapidly down the
axon and the wave of change in voltage across the membrane that it
causes is called an action potential. Acuity See visual acuity.
Adaptation A change in the sensitivity to light of either a
photoreceptor or of the visual system as a whole, so as to match the
current average light intensity. Adaptation to bright light (e.g., when
you wake up and switch on a 457

458

GLOSSARY

Bandwidth A measure of the frequency tuning of the response of a cell or
other device. The smaller a cell's bandwidth, the narrower the range of
frequencies to which it responds (see Figure 3.11). Bipolar cell A type
of cell in the vertebrate retina (see Figure 2.4). Bottom-up process See
data-driven process. Centre-off response A cell with a concentric
receptive field that responds to a reduction of light intensity in the
centre of its field relative to that in the surround is said to have a
centreoff response. Centre-on response A cell with a concentric
receptive field that responds to an increase in light intensity in the
centre of its field relative to that in the surround is said to have a
centreon response. Closed-loop control A control system in which the
output is continuously modified by feedback from the environment. An
example is a person maintaining balance on a surfboard; their posture
(the output) is continually adjusted in response to movement of the
board (feedback). Colour-opponency Cells with this property are excited
by light in one region of the spectrum and inhibited by light in another
region (see Figure 2.8). Complex cell Cell in the visual cortex
responding either to an edge, a bar or a slit stimulus of a particular
orientation falling anywhere within its receptive field. Computational
theory A term introduced by Marr. Computational theories of vision are
concerned with how, in principle, particular kinds of information such
as the shapes of objects or distances of surfaces can be extracted from
images. Solutions to such problems involve consideration of the
constraints that apply to the structures of natural objects and surfaces
and the ways in which they reflect light. An example is the
demonstration that the shape of an object can be recovered from its
silhouette if the shape approximates to a generalised cone. Concentric
field A receptive field divided into an inner circular region and an
outer ring-shaped

region. Light falling in each of the two regions has opposite effects on
the response of the cell. Conceptually driven process A process of
extraction of information from sensory input which relies for its
operation on prior knowledge of the properties of objects or events to
be detected. For example, a conceptually driven process for recovering
the structures of solid objects represented in an image would require
prior information about the geometrical properties of objects (such as
the nature of cubes and cylinders) that could be present. Cone
Vertebrate photoreceptor with short outer segment, which does not
respond to light of low intensity. Connectionist model Model of the
operation of the nervous system or some part of it made up of a large
number of units, each taking input from many of the others as well as
from external sources. Inputs and outputs of units are numerical
values---not symbols---and the operations performed by units on their
inputs are relatively simple. Contrast Difference between maximum and
minimum intensities in a pattern of light expressed as a proportion of
the mean intensity. Cue invariance A cell shows cue invariance if its
selectivity for, say, orientation is the same when the pattern
orientation is defined by different spatial cues such as variation in
luminance, colour, texture, or motion. Cytochrome oxidase (CO) An
enzyme. The level of CO in a cell reflects its metabolic activity.
Data-driven process A process of extraction of information from sensory
input which relies only on information available in the input. For
example, a data-driven process for recovering the structures of solid
objects represented in an image would require no knowledge of the
geometrical properties of particular kinds of object. Dendrites The
processes of nerve cells that carry slow potentials from synapses to the
cell body. Depolarisation A change in the membrane potential of a nerve
cell such that the interior becomes less negatively charged relative to
the

GLOSSARY

exterior. If the membrane of an axon is depolarised, action potentials
are generated with increased frequency. Derivative The result of
differentiating a function. A time derivative is obtained if a function
relating some quantity to time is differentiated with respect to time.
For example, the time derivative of a function relating the volume of
water in a bath to time expresses how the rate of emptying or filling of
the bath varies with time. See also partial derivative. Diffraction The
scattering of rays of light by collision with particles of matter as
they pass through a medium such as air or water. Direction preference An
alternative term to direction selectivity. Direction selectivity A
difference in the response of a cell to a pattern of light moving
through its receptive field according to the direction of movement.
Directional sensitivity A single photoreceptor is stimulated by light
arriving through a segment of the optic array. In this book, we have
used the term directional sensitivity to refer to the size of this
segment. The smaller it is, the greater the directional sensitivity of
the photoreceptor. In order to achieve directional sensitivity, some
means of forming an image is required. The term is used by some authors
as a synonym of directional selectivity. Eccentricity Angular distance
of a point on the retina from the centre of the fovea. Edge segment In
Marr's theory of early visual processing, a token in the raw primal
sketch formed where zero-crossing segments from ∇2G filters (see filter,
∇2G) of adjacent sizes coincide. Electrotonic spread The spread of a
slow potential over the membrane of a dendrite or nerve cell body.
End-inhibition A property of some cells in the visual cortex, which
respond strongly to an edge, a bar, or a slit that ends within the
receptive field, but which are inhibited by longer contours. Endothermic
Term describing an animal that can maintain its body temperature at a
fixed level above that of its surroundings.

459

Exothermic Term describing an animal that cannot maintain its body
temperature above that of its surroundings. Extrastriate cortex Region
of primate cerebral cortex anterior to striate cortex (see Figure 3.13).
Fibre See axon. Filter, ∇2G An algorithm that smooths an array of light
intensity values in an image with a Gaussian filter and then applies a
Laplacian operator to each region of the smoothed image. The wider the
filter, the greater the degree of smoothing of the image by the Gaussian
part of the filter. Firing rate The frequency at which action potentials
pass down the axon of a nerve cell. First-order Term used to describe
the structure of an image formed by local variation (modulation) in the
intensity (or wavelength) of a uniform field of light. Fixation In the
case of an animal with mobile eyes, alignment of the eyes so that the
image of the fixated target falls on the area centralis. Otherwise,
alignment of the head to point towards the fixated target. Fourier
analysis Fourier's theorem proves that any one-dimensional pattern can
be fully described as the sum of a number of sinewaves of different
frequencies, amplitudes, and phases. The same is true of a
two-dimensional pattern provided that horizontal and vertical components
of each sinusoid are analysed. Fourier analysis is a procedure for
breaking down a pattern into its sinusoidal components. Fourier
transform The description of a pattern in terms of the frequencies,
amplitudes, and phases of its sinusoidal components that is obtained by
applying Fourier analysis. Fovea Pit-shaped depression in a vertebrate
retina, usually in an area centralis. Gabor function A sinusoidal
wave-packet, i.e., a smoothly tapered segment of a sinusoidal wave,
formed by multiplying together a sinewave and a Gaussian function. Named
after scientist Denis Gabor. A Gabor patch is the 2-D image formed in
this way, as shown in Figure A.1 (p. 464). The phase of the sinusoid,
relative to the peak of the Gaussian,

460

GLOSSARY

determines the symmetry of the Gabor patch, which may be odd or even
(Figure 3.5) or asymmetric. Widely used in vision, because it has a
fairly well-defined spatial frequency and orientation, and a fairly
well-defined location in space. Ganglion cell A type of cell in the
vertebrate retina (see Figure 2.4). The axons of ganglion cells are
packed together in the optic nerve and carry information from retina to
brain. Gaussian filter Algorithm smoothing spatial or temporal variation
in an image by averaging neighbouring values of light intensity, the
contribution of values to the average being weighted according to a
Gaussian (normal) function. Generalised cone The surface created by
moving a cross-section of constant shape but variable size along an axis
(see Figure 9.15). Gradient A vector that defines the slope of a
function at a given point, and the direction in which the function
slopes. An image I(x,y) has partial derivatives ∂I / ∂x and ∂I / ∂y, and
these are the x- and y-components of the gradient vector: Grad(I) = (∂I
/ ∂x, ∂I / ∂y). Grad(I) is also written as ∇I. The slope at any point is
thus given by the magnitude of the gradient vector, √\[ (∂I / ∂x)2 + (∂I
/ ∂y)2\]. Grating A pattern of parallel dark and bright bars. In a
sinusoidal (or sine-wave) grating, brightness varies sinusoidally across
the pattern, so that the stripes are blurred (see Figure 2.6).
Horizontal cell A type of cell in the vertebrate retina (see Figure
2.4). Hypercomplex cell A simple or complex cell with the property of
end-inhibition. Hyperpolarisation A change in the membrane potential of
a nerve cell such that the interior becomes more negatively charged
relative to the exterior. If the membrane of an axon is hyperpolarised,
action potentials are generated with lower frequency. Impulse See action
potential. Interneuron Nerve cell in the central nervous system that is
neither a receptor nor a motor neuron. Intracellular recording Recording
the membrane

potential of a nerve cell by means of an electrode penetrating the
membrane. Invariant In Gibson's use of the term, some measure of the
pattern of light reflected from an object, event or scene that remains
constant as other measures of the pattern vary. For example, the size of
elements of optic texture in the light reflected from a surface varies
with the nature of the surface and with its distance from the perceiver,
and therefore does not provide information to specify the slant of the
surface relative to the perceiver. For a particular slant, however, the
rate of change of the size of texture elements is invariant for
different surfaces and different distances. IT (Inferotemporal area)
Visual area in extrastriate cortex (see Figure 3.13). K cell Cell in a
koniocellular layer of the LGN, or a retinal ganglion cell making
synaptic contact in one of these layers. LGN (lateral geniculate
nucleus) The part of the mammalian brain where the axons of retinal
ganglion cells terminate, and from which axons run to the visual cortex
(see Figure 3.1). Laplacian (∇2) If a quantity such as light intensity
varies along one dimension, then the second derivative of that quantity
describes the rate at which its gradient is changing at any point. For
example, a positive value of the second derivative would indicate that
the gradient of light intensity is becoming more positive. Where light
intensity varies along two dimensions (as it does in an image), the
Laplacian is the sum of the second derivatives of intensity taken in two
orthogonal directions (i.e., at right angles). Thus ∇2 = (∂2 / ∂x2 + ∂2
/ ∂y2). Linearity A device with a single input and output is said to
operate in a linear fashion if the relationship between the value x of
the input and and the value y of the output has the form y = mx + c (and
therefore can be represented graphically as a straight line). For a
linear device with multiple inputs, the relationship between output and
the values x1, x2,. . . of the inputs has the form y = m1x1 + m2x2 . .
. + c. Examples of nonlinearities are where the output is a function of
a higher

GLOSSARY

power of x (e.g., x2), log x, or the absolute value of x. Luminance
Intensity of light emitted or reflected from a surface. M cell Cell in a
magnocellular layer of the LGN, or a retinal ganglion cell making
synaptic contact in one of these layers. Masking General term for the
loss of visibility of a test pattern caused by the presence of another
pattern (the masker) that is either superimposed on the test pattern or
adjacent to it in space and/or time. There are many varieties of
masking, and many different visual processes contribute to it. Membrane
potential The difference in electrical potential between the interior
and the exterior of a nerve cell. In the resting state, the interior is
always negative relative to the exterior. Modulation One signal
modulates another if the values of the first signal directly control
some aspect or quantity in the second signal. Thus radio signals are
amplitude-modulated (AM) or frequency-modulated (FM) if the audio signal
imposes a corresponding variation in the local amplitude or frequency of
a highfrequency carrier wave. In vision, contrast modulation is produced
when a low-frequency wave (the envelope) is used to vary the local
contrast of a high-frequency carrier grating (Figure 8.16) or noise
pattern (Figure 8.17). The task of the radio receiver, and the visual
system, is to recover the modulating signal from the modulated
carrier---a process known as "demodulation". Motion parallax Movement of
the image of an object over the retina as the position of the eye
changes. The rate of movement depends on the velocity of the eye
relative to the object, and the distance of the object from the eye.
Motor neuron A nerve cell that synapses with a muscle cell. Action
potentials passing down the axon of the motor neuron cause the muscle to
contract. MST (Medial superior temporal area) Visual area in
extrastriate cortex (see Figure 3.13). MT (Middle temporal area) Visual
area in extrastriate cortex (see Figure 3.13). The term MT+ is used to
describe an area of human

461

cortex equivalent to MT and some adjoining areas in monkey cortex. MT+
is also known as V5. Neuron Nerve cell. Nonlinearity See linearity.
Ocular dominance Cells in the visual cortex that respond more strongly
to a stimulus presented to one eye than to the other are said to show
ocular dominance. Off-centre cell Cell with a centre-off response.
Ommatidium Unit of the compound eye containing a light-sensitive rhabdom
(see Figure 1.7). On-centre cell Cell with a centre-on response.
Open-loop control Control system in which the output is not continuously
modified by feedback from the environment. An example is a person
swatting a fly: once the swing of the arm begins, it is completed
whatever the fly does. Operant conditioning A term introduced by
Skinner. In an operant conditioning procedure, an animal's behaviour is
changed by pairing a piece of behaviour (the operant) with
reinforcement. For example, if a rat receives food each time it presses
a bar, it will come to press the bar more frequently. Such methods can
be used to determine whether an animal can discriminate two stimuli. For
example, if a rat can learn to press a bar when it hears a tone of one
pitch and not to press when it hears one of another pitch, then it must
be able to discriminate the tones. Opponent-colour response See
colouropponency. Opsins A group of pigments found in photoreceptor
cells. Optic array Term introduced by Gibson, to refer to the
instantaneous pattern of light reaching a point in space from all
directions. In different regions of the optic array, the spatial pattern
of light will differ, according to the nature of the surface from which
it has been reflected. Optic flow field The fluctuating patterns of
light intensity reaching an observer caused by any relative movement
between observer and environment.

462

GLOSSARY

Optic nerve Nerve running from retina to brain. Optic texture The
spatial pattern of light reflected from a textured surface. Optomotor
response The turning response of an animal presented with uniform flow
of optic texture, in the direction that minimises rate of flow relative
to the animal. Orientation preference (or selectivity) Variation in the
response of a cell in the visual cortex with the orientation of an edge,
bar, or slit. The preferred orientation is that giving the greatest
response. P cell Cell in a parvocellular layer of the LGN, or a retinal
ganglion cell making synaptic contact in one of these layers. Partial
derivative Given a quantity (e.g., image intensity, I) that is a
function of several variables (x, y, t . . .) the slope of the function
in the x-direction is given by the partial derivative with respect to x,
denoted ∂I / ∂x; and similarly for the other variables (y, t, . . .).
Using calculus, it is obtained by holding the other variables constant.
Thus if I = x2 + y2, then ∂I / ∂x = 2x + y2, and ∂I / ∂y = x2 + 2y. See
also gradient. Peak spectral sensitivity The wavelength of light to
which a photoreceptor responds most strongly. Perpendicular component
The component of the velocity of motion of an edge that is perpendicular
to the edge. Phase The position of a sine-wave relative to a given fixed
point (the origin). Expressed either in degrees or radians. Thus a 90°
phase shift (π/2 radians) displaces the sine-wave through one-quarter of
a cycle, equal to half a bar width. A 180° phase shift (π radians)
shifts the wave through half a cycle. This is equivalent to reversing
the contrast of the dark and light bars. Photon Unit of energy in
electromagnetic radiation. Photopic vision Vision in light sufficiently
bright to excite cones. Photoreceptor cell A receptor cell sensitive to
light. Pigment A chemical substance that absorbs light. Pigment
molecules change in shape as

they absorb light and, in a photoreceptor, this change begins a series
of biochemical processes that lead to the receptor potential. Pitch
Rotation of a flying insect or bird in a "head-up" or "head-down" manner
(see Figure 11.3). Plexiform layer Layer of nerve cell processes and
synapses in the vertebrate retina (see Figure 2.4). Prestriate cortex
See extrastriate cortex. Projection Light rays are said to be projected
to the image plane when an image is formed. The word also has a quite
different meaning in neurophysiology, to refer to the region where the
axons of a group of nerve cells in the brain terminate and make synaptic
contact. Proprioceptor Receptor cell that signals the extent of stretch
in a muscle or of rotation around a joint. Such cells provide
proprioceptive information about the positions of parts of the body
relative to one another. Psychophysics The analysis of perceptual
processes by studying the effect on a subject's experience or behaviour
of systematically varying the properties of a stimulus along one or more
physical dimensions. Quadrature Two signals or images are "in
quadrature" if the frequency components of one signal are shifted in
phase by 90° from the other signal. Thus sin(x) and cos(x) are in
quadrature, since sin(x) = cos(x − 90). Raw primal sketch In Marr's
theory of vision, a rich representation of the intensity changes present
in the original image. Receptive field The area of the retina in which
light causes a response in a particular nerve cell. Receptor cell A
nerve cell sensitive to external energy. Receptor potential The change
in membrane potential of a receptor cell caused by external energy
impinging on it. Refraction The bending of rays of light as they cross a
boundary between two transparent media of different optical densities.
Retinal ganglion cell See ganglion cell. Retinotopic map An array of
nerve cells that

GLOSSARY

have the same positions relative to one another as their receptive
fields have on the surface of the retina. Retinula cell A photoreceptor
in the eye of an insect or other arthropod (see Figure 1.7). Rod
Vertebrate photoreceptor with long outer segment, sensitive to light of
low intensity. Roll Rotation of a flying insect or bird around the long
axis of the body (see Figure 11.3). Saccade Rapid movement of the eye to
fixate a target. Scalar A quantity that has magnitude only. Scotopic
vision Vision in dim light, sufficiently bright to excite rods but not
cones. Second-order Term used to describe the structure of an image
formed by local variation (modulation) in some higher-order property of
the image, such as its local contrast, texture density, element
orientation, flicker rate, etc. Sensitivity spectrum The relationship
between the wavelength of light striking a photoreceptor and the size of
the receptor potential. Simple cell Cell in the visual cortex showing
linear spatial summation of light intensities in parts of its receptive
field separated by straight line boundaries. Sinusoidal grating See
grating. Slow potential A small change in the membrane potential of a
nerve cell, such as a receptor potential, which decays as it spreads
passively over the membrane. In contrast, an action potential is
propagated by an active change in membrane properties and does not decay
in amplitude as it is transmitted. Spatial frequency The frequency,
expressed as cycles per unit of visual angle, of a periodic pattern of
light such as a grating. At a particular viewing distance, the spatial
frequency of a grating depends on the width of its bars; the narrower
these are, the higher the frequency. Spatial frequency tuning A nerve
cell in a visual pathway that responds more strongly to sinusoidal
gratings which have spatial frequencies in a particular range than to

463

gratings of other frequencies is said to show spatial frequency tuning.
Spatial summation If the response of a cell to a pattern of light is a
linear function of the difference in the amounts of light falling in
different regions of its receptive field, then it is said to show linear
spatial summation. Stereopsis Perception of depth dependent on disparity
in the images projected on the retinas of the two eyes. Stereoscopic
fusion The process whereby the two disparate retinal images are combined
to yield a single percept in depth. Striate cortex See visual cortex.
Superior colliculus Structure in mammalian midbrain where some retinal
ganglion cell axons terminate. Synapse A point where the membranes of
two nerve cells nearly touch and where electrical activity in one cell
influences the membrane potential of the other cell. 3-D model
representation An object-centred representation of shape, organised
hierarchically (see Figure 9.16). Top-down process See conceptually
driven process. Topology A branch of geometry describing the properties
of forms that are unaffected by continuous distortion such as
stretching. For example, a doughnut and a record have the same topology.
Torque Turning force, equal to force applied multiplied by the distance
of its point of application from the centre of rotation. Transduction
The process by which external energy impinging on a receptor cell causes
a change in its membrane potential. 2½D sketch A viewer-centred
representation of the depths and orientations of visible surfaces. V1
Primary visual cortex of a human or other primate. V2, V3, V4 Visual
areas in extrastriate cortex (see Figure 3.13). V5 See MT. Vector A
quantity that has both magnitude and direction. Velocity field A
representation of the velocity of

464

GLOSSARY

image motion at any instant in each of many small regions of a
time-varying image. Vergence movements Binocular movements of the eyes
making their directions of gaze either more or less convergent.
Vestibular system The organ in the inner ear involved in the
transduction of angular acceleration of the head into nerve impulses.
Visual acuity An observer's visual acuity is measured by the angle
between adjacent bars in the highest frequency grating that they can
distinguish from a plain field of the same average brightness as the
grating. Visual angle The angle that an object subtends at the eye (see
Figure 1.12). Visual cortex, primary Region of the mammalian cortex in
the occipital lobe

receiving input from the LGN. Cells in the primary visual cortex respond
to light falling on the retina and are arranged in a retinotopic map. In
primates, this region is also known as the striate cortex or V1. W cells
Ganglion cells that do not have concentric fields. X cells Ganglion
cells that show linear spatial summation of light intensities in centre
and surround areas of the receptive field. Y cells Ganglion cells with
concentric fields, which show nonlinear responses to changes in light
intensity. Yaw Rotation of a flying insect or bird around the vertical
axis (see Figure 11.3). Zero-crossing A point where values of a function
change.

Author Index

Abraham, F.D. 312 Abraham, R.H. 312 Abramov, I. 46 Adelson, E.H. 110,
141, 143, 211, 218, 219, 221, 224, 225, 227, 229, 230, 232, 233, 250
Adini, Y. 162 Agnetta, B. 376 Ahumada, A.J. 218, 241 Akamatsu, S. 393,
399 Akers, K. 372 Alais, D. 239 Albano, J.E. 43 Albrecht, D.G. 48, 55,
99, 102, 103, 110, 220 Albright, T.D. 232, 247, 250, 251, 253 Alderson,
G.H.K. 356 Allard, E. 333, 334, 363 Alley, T.R. 399 Allison, J.D. 45, 47
Allison, T. 377, 381 Alonso, J.M. 50, 146 Andersen, R.A. 69, 83, 200,
203, 205, 206, 229, 230, 233, 236, 263 Anderson, A.W. 295, 296 Anderson,
B.L. 185 Anderson, C.H. 54 Anderson, J.R. 377 Anderson, N.H. 384
Anderson, P.A. 231, 232

Anderson, S.J. 215, 223, 234, 242, 245 Ando, H. 200, 203 Aneshansley, D.
133 Anllo-Vento, L. 74 Anstis, S.M. 214, 215, 220, 241, 246 Anzai, A.
70, 231 Arditi, A.R. 231 Aronson, E. 329, 330, 331 Ashida, H. 247
Ashmead, D.H. 335, 336 Atherton, T.J. 182 Attneave, F. 121, 122, 128,
129 Austin, A.L. 176 Babler, B.L. 362 Babler, T.J. 362 Backus, B.T. 206,
229, 339 Badcock, D.R. 234, 235 Bahde, S. 355, 356 Bahill, A.T. 364
Baizer, J.S. 58, 60 Baker, C.L. 111, 241, 245, 247 Bakin, J.S. 197, 198
Ball, W. 349 Ballard, D.H. 82, 148, 414, 415 Ballesteros, S. 284 Banks,
M.S. 263, 339 Bar, M. 285, 289 Barclay, C.D. 380 465

Bardy, B.G. 332 Barlow, H.B. 54, 57, 62, 74, 151, 173, 215, 216, 218,
219, 271, 336 Barnes, R.D. 9, 13 Baron-Cohen, S. 390, 391 Bartfeld, E.
54 Barth, E. 143, 323 Bartlett, J.C. 295 Bassili, J.N. 387, 392 Batista,
A.P. 69 Baylis, G.C. 62, 377 Beall, A.C. 341 Beasley, N.A. 383
Beauchamp, M.S. 63 Beck, D.M. 416 Beck, J. 128, 304 Beckers, G. 228
Bell, A.J. 57 Benardete, E.A. 36 Bennett, A.T.D. 23 Bensinger, D.G. 415
Benson, P.J. 377 Berg, W.P. 334 Bergen, J.R. 110, 141, 143, 144, 211,
218, 219, 221, 224, 225, 227, 232, 242 Berkeley, G. 78, 169, 200
Berkley, M.A. 117, 216 Bernstein, N. 311, 332 Berry, D.S. 386, 399
Berry, M.J. 41, 42

466

AUTHOR INDEX

Bertenthal, B.I. 378 Beusmans, J.M.H. 339, 341 Beverley, K.I. 258 Bex,
P.J. 161, 162 Bialek, W. 41, 42 Biederman, I. 282, 283, 284, 285, 286,
287, 288, 289, 290, 291, 293, 294, 296, 392 Binford, T.O. 276 Binkofski,
F. 382 Birch, J. 63 Bischof, W.F. 324 Bizzi, E. 314 Bjorklund, R.A. 101,
231 Blair, D. 360 Blake, R. 117, 204, 205, 237, 238, 239 Blakemore, C.B.
101, 102, 164, 173, 321, 246 Blasdell, G.G. 52, 53, 54 Blondeau, J. 318,
319 Boden, M. 80 Bonas, W. 341 Bonds, A.B. 45, 47, 110, 143 Bonnar, L.
111 Booth, L. 346, 356 Bootsma, R.J. 313, 352, 356, 360 Boring, E.G. 78,
174 Born, R.T. 56, 63, 237, 238, 240 Borst, A. 355, 356 Bossema, I. 375
Bosworth, R.G. 413 Boulton, J.C. 111, 245 Bovik, A.C. 141 Bower, T.G.R.
328, 349 Bowmaker, J.K. 22 Boyd, J.D. 45, 47 Boyle, D.G. 383 Boynton,
G.M. 101, 155, 233 Braddick, O.J. 124, 240, 241 Bradford, C.M. 356, 357
Bradley, D.C. 205, 206, 263 Bradshaw, M.F. 204 Brady, T.J. 63, 238
Braitenberg, V. 322 Bransford, J.D. 412 Braun, J. 252 Braunstein, M.L.
201 Brefczynksi, J.A. 74 Breitmeyer, B.G. 261 Brenner, E. 339 Bridgeman,
B. 264

Brindley, G.S. 262 Britten, K.H. 206, 228, 235 Broadbent, D.E. 83
Brookes, A. 202 Brooks, M.J. 191 Brooks, V. 127, 128 Broughton, J.M. 349
Bruce, V. 104, 125, 184, 289, 291, 294, 295, 297, 298, 388, 390, 391,
392, 393, 396, 398, 399 Bruner, J.S. 79 Buccino, G. 382 Buchsbaum, G. 57
Bullier, J. 150, 197, 240 Bülthoff, H.H. 202, 288, 378 Bülthoff, I. 193,
194, 378 Burbeck, C.A. 101, 259 Burger, J. 372 Burgler, R.R. 375 Burr,
D.C. 67, 92, 97, 108, 109, 111, 143, 215, 223, 234, 235, 236, 237, 261,
381 Burt, D.M. 397 Burt, P. 180 Burton, A.M. 287, 289, 291, 396, 398,
399 Burton, M. 297 Busby, J. 354 Bushnell, I.W.R. 388 Butterworth, G.
330 Buxton, H. 221, 223 Buxton, R.B. 74 Byrne, C.E. 47, 217, 227 Byrne,
R.W. 376 Caan, W. 61 Caine, N.G. 22 Call, J. 376 Callaway, E.M. 47, 59
Cameron, D.A. 23 Camhi, J.M. 317 Campbell, F.W. 99, 101, 102, 321, 246,
261 Campbell, H.W. 132 Campbell, R. 376 Cannon, M.W. 152, 153, 157
Canny, J. 92, 103, 110 Capaldi, E.A. 327 Carello, C. 313, 410 Carey,
D.P. 65, 66, 68, 356 Carey, S. 295 Carman, G.J. 204

Carney, T. 231 Carpenter, R.H.S. 21, 164 Carrasco, M. 413 Carre, G. 341
Carrel, J.E. 133 Carson, D. 289, 291 Casagrande, V.A. 45, 47 Cavanagh,
P. 63, 64, 117, 184, 185, 241, 245, 253 Caviness, J.A. 349 Chahl, J.S.
323 Chang, G. 205, 206 Charles, E.R. 46 Chase, P. 287 Chelazzi, L. 73
Chen, C.-C. 156, 163 Cheng, H. 231 Chino, Y. 231 Chitty, A.J. 61
Chodosh, L.A. 362 Chomsky, N. 83 Christie, F. 298 Chubb, C. 152, 241
Chun, M.M. 416 Cioni, G. 236 Clark, J.J. 415 Clark, M. 141 Clayton,
T.M.H. 354, 355, 358 Cleland, B.G. 34 Clifford, C.W.G. 221, 223 Cline,
M.G. 390 Clowes, M.B. 135, 136, 275 Cobbs, W.H. 13 Cohen, D. 388 Colby,
C.L. 50, 69 Collett, T.S. 320, 321, 322, 327, 346 Collin, S.P. 20
Connor, C.E. 61 Cook, P.B. 36 Coombes, A. 398, 399 Cooper, E.E. 283,
284, 289 Cooper, L.A. 284 Cornsweet, J. 304 Cornsweet, T.N. 259 Cott,
H.B. 132 Cottaris, N.P. 219, 220 Cowey, A. 34, 58, 59, 65, 289, 376, 388
Cox, M. 231, 245 Crawford, J.D. 43 Croner, L.J. 34, 41 Crowell, J.A. 339

AUTHOR INDEX

Croze, H. 133 Cumming, B. 258 Cumming, B.G. 197, 202, 203, 206 Currie,
C.B. 415 Cuthill, I.C. 23 Cutting, J.E. 305, 343, 365, 379, 380, 381
Cutting, J.F. 378 da Vinci, L. 410, 411 Dakin, S.C. 161, 162 Dale, A.M.
63, 64, 229, 238 Dannemiller, J.L. 362 Dartnall, H.J.A. 22 Darwin, C.
176 Das, A. 70 David, C.T. 322 Davies, G.M. 289 Davies, M.N.O. 342, 347,
353 Davis, G. 390 Davis, J.M. 369 Dawkins, M.S. 370 de Monasterio, F.M.
33, 34 De Valois, K.K. 26, 39, 56 Dean, P. 349 DeAngelis, G.C. 50, 70,
113, 149, 150, 173, 182, 183, 206, 220, 229, 230, 231 DeBruyn, B. 117
Decety, J. 381 Deco, G. 62 Demb, J.B. 155, 233 Dench, N. 396, 398
Denton, G.G. 336, 337 Derrington, A.M. 45, 47, 231, 245 Desimone, R. 58,
60, 73, 74 Desmurget, M. 348 DeSouza, J.F.X. 69 Detwiler, M.L. 357
DeValois, R.L. 26, 39, 46, 48, 55, 56, 99, 102, 103, 110, 219, 220
DeYoe, E.A. 59, 60, 63, 74, 128 Diamond, R. 295 Dibble, F.N. 302 Dienes,
Z. 361, 362 Dijkerman, H.C. 66, 69, 348 Dijkstra, T.M.H. 331, 332
Dittrich, W.H. 133, 387, 388 Dobkins, K.R. 413 Dodd, J.V. 206 Dodge,
F.A. 27 Donnelly, M. 381

Douglas, R.H. 346 Dowling, J.E. 28, 29, 35, 36 Doyle, A. 396 Dreher, B.
34, 45 Driver, J. 389, 390 Dubowitz, D.J. 74 Duckett, S.G. 71 Duffy,
C.J. 236 Dukelow, S.P. 69 Duncan, K. 215 Durstler, M.R. 103 Dykeman, C.
349 Dziurawiec, S. 388, 389 Eagle, R.A. 241 Edelman, S. 288 Edgar, G.K.
227 Edgar, P. 360 Edwards, A.S. 331 Edwards, D.P. 51, 56 Edwards, M.
234, 235, 236, 245, 246 Egelhaaf, M. 320, 322, 324, 325 Ehrlich, S.M.
339 Eisner, T. 133 Ejima, Y. 153, 154 Ekman, P. 392, 393 Elder, J.H. 97,
103, 110, 111 Elfar, S.D. 56, 219, 220 Ellard, C.G. 346, 356 Ellis, A.W.
289, 388 Ellis, H. 388 Ellis, H.D. 289 Ellsworth, P. 392 Emerson, R.C.
110, 227, 262 Enderson, H.J. 372 Engel, A.K. 71 Enroth-Cugell, C. 30,
31, 32, 34, 35 Erkelens, C. 176 Escher, M.C. 121 Eskin, T.A. 227 Essick,
G.K, 69 Eure, S.B. 121 Ewert, J.P. 267 Fadiga, L. 382 Farah, M.J. 295,
296 Farrell, J.E. 241 Favreau, O.E. 117 Fechner, G.T. 78 Feidler, J.C.
229 Feldman, A.G. 312

467

Feldman, J.A. 83 Felleman, D.J. 54 Fender, D.H. 175 Fentress, J.C. 370
Ferrera, V.P. 247, 248, 251 Ferretti, C.L. 322 Ferster, D. 48, 175, 182
Fetz, E.E. 314 Field, D.J. 56, 159, 160, 163 Field, T.M. 388 Fink, G.R.
382 Finkel, L.H. 57 Fiorentini, A. 149, 236 Fischer, B. 173 Fiser, J.
289 Fitch, H.L. 311, 313 Fitzgibbon, A.W. 339 Fize, D. 72 Flanagan, P.
117 Fleet, D.J. 182, 206 Flin, R. 389 Flock, H.R. 124 Fodor, J.A. 83,
410, 411, 412 Fogassi, L. 382 Foley, J.M. 101, 154, 155, 171 Foppa, K.
393 Fox, R. 17 Frackowiak, R.S.J. 63, 64 Frank, L.R. 74 Franks, J.J. 412
Freeman, R.D. 50, 70, 103, 113, 149, 150, 152, 173, 175, 182, 183, 220,
229, 230, 231 Freeman, T.C.A. 90, 92, 103, 110, 263, 357 Freund, H.J.
382 Freyd, J.J. 392 Friedman, M.B. 347 Friesen, C.K. 390 Friesen, W.V.
392, 393 Frisby, J.P. 174, 180 Friston, K.J. 51, 63, 64, 235 Frith, C.D.
416 Frost, B.J. 149, 324, 347, 352 Frostig, R.D. 70 Frykolm, G. 378, 379
Fukuda, Y. 34 Fullenkamp, S.C. 152, 153, 157 Furman, J.M. 331 Furneaux,
S. 363 Gallant, J.L. 150, 151

468

AUTHOR INDEX

Gallese, V. 382 Galton, F. 176, 400 Ganz, L. 214 Garing, A.E. 335, 336
Garnham, A. 80 Gaska, J.P. 110 Gati, J.S. 69 Gauthier, I. 295, 296
Geisler, W.S. 141, 220 Georgeson, J.M. 101, 170 Georgeson, M.A. 90, 92,
94, 101, 102, 103, 110, 164, 189, 231, 233, 240, 242, 245, 253
Georgopoulos, A.P. 314 Gerhardstein, P.C. 288 Gerstein, G.L. 71, 262
Gibson, E.J. 124, 328, 329 Gibson, J.J. 6, 80, 81, 116, 124, 259, 263,
264, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 314, 315, 327,
337, 340, 343, 349, 365, 387, 390, 405, 406, 407, 408, 410, 411, 417
Gielen, C.C.A.M. 331, 332 Gilbert, C.D. 70, 71, 74, 163, 164, 197, 198
Gilbert, F.S. 133 Girard, P. 150, 240 Giszter, S. 314 Giurfa, M. 327
Gizzi, M.S. 250 Glass, L. 161 Glennerster, A. 339 Glover, G.H. 155
Gluhbegovic, N. 44 Gochfeld, M. 372 Goebel, R. 68 Golani, I. 370, 371
Goldberg, M.E. 69 Goldman, A. 382 Goodale, M.A. 64, 65, 66, 67, 68, 69,
73, 346, 348, 356, 405, 416 Goodman, C.C. 79 Goodman, D. 312, 313
Goodman, L.J. 318 Gore, J.C. 295, 296 Gorea, A. 253 Goren, C.C. 268, 388
Gosselin, F. 111 Gouras, P. 33, 34 Grafton, S. 348 Graham, C.H. 25, 26

Graham, M. 199 Graham, N. 99 Graham, N.V.S. 101, 214, 216 Granquist, R.
368 Gray, C.M. 71 Gray, R. 349, 352, 357 Graziano, M.S.A. 236 Green, P.
266 Green, P.R. 133, 342, 343, 347, 353 Greenberg, R. 388 Greer, N.L.
334 Gregory, R.L. 78, 80, 81, 140, 148, 184, 259, 407, 417 Grewcock, D.
133 Grèzes, J. 381 Griffin, D.R. 4 Grimson, W.E.L. 180, 200 Grindle, N.
368 Grinvald, A. 54, 60, 70 Grosof, D.H. 56, 110 Grossman, E. 381
Grunewald, A. 215 Grusser, O.-J. 260 Grzywacz, N.M. 256 Gu, Y. 182
Guilford, T. 370 Gulick, W.L. 187 Gurfinkel, V.S. 312 Guzman, A. 135,
164, 275 Gyoba, J. 393 Haarmeier, T. 263 Hadjikhani, N.K. 63, 64, 229
Hailman, J.P. 130, 131, 132 Hamilton, C.B. 121 Hamilton, S.L. 56
Hammett, S.T. 223 Hammond, P. 216 Hampton, R.R. 376 Hamstra, S.J. 113
Hancock, P. 400 Hancock, P.J.B. 297 Hanna, E. 289, 291, 398, 399 Hannon,
D.J. 338, 339 Hansard, M.E. 339 Hare, B. 376 Hari, R. 382 Harkness, L.
346 Harmon, L.D. 111 Harris, J.M. 340, 341 Harris, J.P. 184

Harris, M.G. 101, 212, 213, 223, 236, 245, 253, 257, 341, 357 Hartline,
H.K. 25, 26, 30, 42 Hasselmo, M.E. 377 Hausen, K. 321, 322, 324, 325
Hawken, M.J. 50, 56 Haxby, J.V. 294 Hay, D.C. 294 Hayes, A. 159, 160,
163 Hayhoe, M.M. 263, 414, 415 He, S. 36, 37, 116, 102 He, Z.J. 166,
304, 413 Head, A.S. 61 Healey, P. 398, 399 Hecht, H. 357 Heckenmuller,
E.G. 259 Heeger, D.J. 110, 143, 155, 206, 220, 229, 232, 233, 238, 239,
250, 251, 252 Heider, F. 385, 386, 387, 391 Heiligenberg, W. 4 Heinecke,
A. 68 Heisenberg, M. 318, 319, 325, 327 Heitger, F. 113 Hellawell, D.J.
294 Helmholtz, H. von 78, 81, 148, 201, 259, 261, 262, 407 Henderson,
J.M. 415 Hendry, S.H.C. 34, 45 Hengstenberg, B. 324 Hengstenberg, R. 324
Henkes, S. 335 Hertz, M. 133 Hess, R.F. 111, 159, 160, 162, 163, 223
Hess, U. 393 Heuer, H. 352 Heywood, C.A. 59, 376 Hicks, L. 330 Hietanan,
J.K. 377 Hildebrand, T. 16 Hildesheim, R. 70 Hildreth, E.C. 87, 89, 90,
91, 92, 93, 94, 95, 97, 98, 99, 103, 104, 107, 110, 111, 200, 203, 212,
248, 256, 257 Hill, H. 184, 399 Hill, R.M. 215, 216 Hillyard, S.A. 74
Hinton, G.E. 82, 314 Hiris, E. 237 Hochberg, J.E. 121, 127, 128 Hoffman,
D.D. 164, 165, 281, 283

AUTHOR INDEX

Hollands, M.A. 335 Holliday, I.E. 111, 242, 245 Hollingworth, A. 415
Holt, J. 261 Honda, H. 260, 262 Hood, B.M. 389 Horn, B.K.P. 191, 256
Horridge, G.A. 321 Horton, J.C. 51 Howard, I.P. 187 Howland, H.C. 16,
133 Hubel, D.H. 47, 48, 50, 51, 52, 54, 58, 71, 74, 107, 150, 173, 270
Huffman, D.A. 135 Huk, A.C. 229, 238, 239, 250 Hummel, J.E. 285, 286,
287 Humphrey, A.L. 229, 230 Humphrey, G.K. 278, 284 Humphreys, G.W. 278,
288, 292, 295 Hupe, J.-M. 150, 240 Hyvärinen, J. 69 Ichida, J.M. 45, 47
Ingram, W. 132 Intrator, N. 346 Itakura, S. 377 Ito, M. 70, 71, 74
Ittelson, W.H. 79 Jackson, J.F. 132 Jacobs, G.H. 21, 46 Jacobson, L.D.
110 Jain, R. 89 Jakobson, L.S. 65, 348 James, A.C. 150, 240 Janssen, P.
207 Jastrow, J. 122 Jeannerod, M. 66, 69, 347, 348 Jeka, J.J. 332
Jenkin, M.R.M. 182 Jennings, J.E. 63 Jepson, A.D. 182 Johansson, G. 124,
378, 379, 392 Johnson, A.G. 350 Johnson, E.N. 50, 56 Johnson, M.H. 388,
389, 390 Johnston, A. 221, 223, 297 Johnston, E.B. 202, 203 Johnstone,
J.R. 261 Jolicoeur, P. 278 Jones, H.E. 71

Jones, J.P. 56 Jones, R. 302 Jones, R.K. 20 Jones, R.M. 101, 259 Joyce,
J. 411 Ju, G. 283 Judd, S.P.D. 327 Judge, S.J. 350, 356, 357 Julesz, B.
111, 128, 143, 146, 175, 177, 178, 179, 180, 187, 200, 253 Kahn, S.C.
284 Kaiser, M.K. 351, 360, 362 Kallenbach, U. 68 Kallos, T. 262 Kalmus,
H. 318 Kalocsai, P. 289, 290, 291, 296 Kamachi, M. 393 Kamil, A.C. 133
Kanade, T. 282 Kanisza, G. 115 Kapadia, M.K. 70, 71, 74, 163, 164
Kaplan, E. 34, 36, 41, 51, 56 Karmeier, K. 324 Kasturi, R. 89 Katz, L.M.
46, 217, 227 Katzir, G. 346 Kaufman, L. 187 Kawano, K. 72 Kay, B.A. 312,
332 Kayargadde, V. 103 Keeble, S. 384 Kelly, D.H. 101, 213, 216, 259
Kelso, J.A.S. 312, 313, 314 Kemp, R. 292 Kennard, C. 63, 64 Kennedy,
J.M. 113 Kentridge, R.W. 102 Kern, R. 324 Kersten, D. 193, 194, 195
Kerzel, D. 357 Kettlewell, B. 133 Kidd, P. 390 Kilpatrick, F.P. 79 Kim,
J. 251 Kim, N.G. 357 King, S.M. 349 King-Smith, P.E. 107 Kingstone, A.
390 Kirchner, W.H. 323 Kirschfeld, K. 13 Kleck, R.E. 393

469

Klein, S.A. 152 Kleinke, C.L. 390 Klier, E.M. 43 Knierim, J.J. 128, 149,
151 Knight, B. 297 Knill, D.C. 192, 193, 194, 195, 207, 252 Kobatake, E.
61 Kobayashi, H. 377 Koch, C. 51, 235, 248 Koenderink, J.J. 99, 256, 257
Koffka, K. 119, 127, 309 Köhler, W. 119 Kohshima, S. 377 Konczak, J. 335
König, P. 71 Kots, Ya.M. 312 Kozlowski, L.T. 379, 380 Kral, K. 323
Krapp, H.G. 324 Krauskopf, J. 45, 47 Kretzberg, J. 324 Krinsky, V.I. 312
Krizic, A. 260 Kruyer, A. 348 Kubler, O. 113 Kuffler, S.W. 30, 42, 74
Kulikowski, J.J. 107 Kurtz, R. 324 Kwong, K.K. 63, 238 Lacquaniti, F.
314 Lamme, V.A.F. 71, 72, 73, 146, 147, 148, 150 Land, E.H. 61 Land,
M.F. 9, 10, 11, 13, 23, 320, 321, 322, 341, 342, 363, 364, 414 Lander,
K. 297, 298 Landis, T. 376 Landry, J.C. 399 Landy, M.S. 141, 143, 144,
202, 203, 242 Langley, K. 182 Langlois, J.H. 400 Langton, S.R.H. 291,
294, 390, 391 Lappe, M. 338 Larcombe, M.H.E. 182 LaRitz, T. 364 Laurent,
M. 334, 335 Lavie, N. 416 Lawson, R.B. 187, 278, 288

470

AUTHOR INDEX

Lea, S.E.G. 387, 388 Lebert, C. 16 Leder, H. 295, 297 Ledgeway, T. 162,
243, 245, 246 Lee, D.N. 20, 329, 330, 331, 332, 333, 334, 342, 350, 351,
353, 354, 355, 358, 359, 360, 363, 364, 365 Lee, K. 59 Lee, T.S. 72, 147
Legge, G.E. 154, 182, 236 Lehmkuhle, S.W. 17 Lehrer, M. 321, 322, 323
Lennie, P. 45, 47, 54, 60, 232 Leonard, C.M. 62 Lepenies, W. 393 Leslie,
A.M. 384 Lesperance, R.M. 99 LeVay, S. 51, 182 Leventhal, A.G. 34, 45,
116 Levi, D.M. 111, 152, 156, 157 Levick, W.R. 34, 218, 219 Levin, D.T.
415 Levin, J.T. 415 Levison, E. 216 Levitt, J.B. 149 Li, A. 187, 188,
189, 190, 192 Li, C. 149 Li, L. 73 Li, W. 149 Li, Z. 164 Lieke, E.E. 70
Lifson, L.E. 362 Linden, D.E.J. 68 Lindhagen, K. 363 Lindsay, P.H. 271
Lishman, J.R. 329, 330, 331, 332, 334, 364 Liu, A.K. 63, 64 Liu, L. 185
Livingstone, M.S. 58 Lloyd, M.R. 340, 341 Locke, J. 78 Logothetis, N.K.
46 Lomber, S.G. 240 Longuet-Higgins, H.C. 338 Look, R.B. 63, 238 Loomis,
J.M. 341 Lorenceau, J. 253 Lorincz, A. 285 Lotem, A. 346 Lough, S. 354,
355, 358 Lowe, D.G. 282

Lu, Z.-L. 231, 245, 253 Luck, S.J. 73 Lueck, C.J. 63, 64 Lund, J.S. 149
Mace, W. 311, 312, 394 Mach, E. 262 Mack, A. 413 MacKay, D.M. 261
MacLaren, R.D. 368 MacLeod, D.I.A. 36, 37, 102, 116, 175 Maffei, L. 143,
149 Magnussen, S. 101, 231 Mahon, L.E. 219, 220 Mai, N. 63 Maier, E.J.
22 Makous, W. 223 Malach, R. 63, 238 Malik, J. 108, 110, 141, 143
Maljkovic, V. 413 Mallot, H.A. 89, 202 Malonek, D. 60 Maloney, L.T. 202
Malpeli, J.G. 50 Mamassian, P. 193, 194, 195 Mandler, M.B. 223 Manning,
A. 368 Mark, L.S. 395, 396, 397 Marlot, C. 72 Marotta, J.J. 348
Marple-Horvat, D.E. 335 Marr, D. 80, 81, 82, 83, 86, 87, 89, 90, 91, 92,
93, 94, 95, 97, 98, 99, 103, 104, 107, 110, 111, 117, 118, 133, 136,
137, 138, 139, 140, 141, 146, 159, 164, 165, 166, 167, 178, 179, 183,
187, 200, 201, 202, 204, 212, 218, 219, 220, 248, 253, 254, 258, 274,
276, 277, 278, 279, 280, 281, 282, 285, 287, 288, 293, 392, 407, 408,
409, 412, 413, 414 Marrett, S. 63, 229 Marshall, J. 65 Martens, J.B. 103
Martin, C. 333, 334, 363 Martin, G.R. 18, 20, 23 Martin, K.A.C. 58
Martinez, A. 74 Martinez, L.M. 50, 146 Masland, R.H. 36 Mason, O. 289,
291, 398, 399

Mather, G. 214, 241, 243, 246, 378, 379, 380, 381 Matin, L. 260
Maunsell, J.H.R. 46, 47, 58, 59, 217, 227 Maxwell, E. 390 May, K.A. 400
Mayhew, J.E.W. 180 McArthur, L.Z. 399 McBeath, M.K. 362 McCarrell, N.S.
412 McCarthy, G. 377, 381 McClelland, J.L. 82, 83 McConkie, G.W. 415
McFarlane, D.K. 101, 102 McGraw, P.V. 111 McGregor, P.K. 133 McIlwain,
J.T. 35 McIntyre, M. 394 McKee, S.P. 221 McKenzie, B. 389 McLean, J.
219, 220 McLeod, P. 361, 362, 364 McLeod, R.W. 357 McOwan, P.W. 221, 223
McPeek, R.M. 413 McReynolds, J.S. 36 Meadows, J.C. 64 Mech, L.D. 373
Meese, T.S. 236 Meister, M. 41, 42 Meltzoff, A.N. 388 Mendola, J.D. 63,
229 Mennie, N. 414 Menon, R.S. 69 Menzel, E.W. 373, 409, 410 Merigan,
W.H. 46, 47, 217, 227 Merton, P.A. 261, 262 Metzger, W. 302 Michael,
C.R. 240 Michaels, C.F. 354, 410 Michel, F. 66, 69 Michotte, A. 382,
383, 384, 386, 387, 397 Miller, E.K. 73 Miller, J. 179 Miller, K.D. 48
Millott, N. 8 Milne, A.B. 238, 239 Milner, A.D. 64, 65, 66, 67, 68, 69,
73, 405, 416 Mingolla, E. 215

AUTHOR INDEX

Mishkin, M. 64, 66 Misovich, S.J. 386 Mistlin, A.J. 61 Möhl, B. 325,
326, 327, 332 Mollon, J.D. 21, 22, 23, 61 Mon-Williams, M. 331, 348
Montanaro, D. 236 Moore, C. 284 Moore, M.K. 349, 388 Moore, R.K. 261
Moran, G. 370 Moran, J. 73 Morgan, M.J. 91, 103, 104, 107, 125, 162, 241
Morgan, V. 381 Morris, R.G.M. 83 Morrone, M.C. 67, 92, 97, 108, 109,
111, 143, 215, 235, 236, 237, 261, 381 Morton, H.B. 261 Morton, J. 388,
389 Motter, B.C. 73, 74 Mouat, G.S.V. 216 Moulden, B. 215 Mounsell,
J.H.R. 232 Movshon, A. 71 Movshon, J.A. 101, 110, 228, 231, 232, 235,
250 Mowafy, L. 351 Mukaida, S. 393 Mullen, J.T. 388 Mumford, D. 72, 147
Mundy, N.I. 22 Murdoch, L. 380, 381 Murphy, B.J. 216, 217 Murphy, K.J.
66 Murphy, P.C. 71 Murthy, A. 229, 230 Mussa-Ivaldi, F.A. 314 Musselman,
A. 292 Nachmias, J. 99, 216, 217 Naka, K.I. 36 Nakayama, K. 117, 166,
167, 185, 186, 196, 197, 198, 221, 240, 413 Nalbach, H.-O. 19 Nalwa, V.
377 Nawrot, M. 204, 205 Nayar, S.K. 86 Neighbor, G. 381 Neisser, U. 271
Nelson, B. 354

Nelson, J.I. 149 Neri, P. 381 Neufeld, J. 337 Neufield, G.R. 262
Neumann, H. 148 Newsome, W.T. 58, 59, 63, 206, 228, 233, 235, 250
Nichols, C.W. 262 Nilsson, D.-E. 10, 11, 13, 23 Nishida, S. 236, 245,
246, 247 Nishihara, H.K. 276, 277, 278, 280, 281, 282, 287, 288, 293,
392 Nishitana, N. 382 Nitsch, K.E. 412 Norman, D.A. 271 Normann, R.A. 37
Northdurft, H.C. 141, 143, 150, 151 Novangione, A. 97 O'Connell, D.N.
254, 255 O'Regan, J.K. 415 Oakes, L.M. 384 Obermeyer, K. 52, 53, 54
Oehler, R. 34 Ogawa, J. 175, 182 Ogle, K.N. 175, 177 Ohzawa, I. 50, 70,
103, 113, 149, 150, 152, 173, 175, 182, 183, 220, 229, 230, 231 Oliva,
A. 111 Olson, R.K. 128, 129 Ooi, T.L. 304 Oram, M.W. 377 Orban, G.A.
113, 117, 207 Oren, M. 86 Osorio, D. 22 Oster, H. 392 Ott, M. 20
Oudejans, R.R.D. 352, 354 Over, R. 117, 389 Overton, R. 117 Owens, R.A.
108 Pal'tsev, Ye.I. 312 Palmer, L.A. 56, 219, 220 Palmer, S.E. 121, 287
Paradiso, M.A. 117 Paré, E.B. 59, 63, 228 Parker, A.J. 197, 202, 206
Pasupathy, A. 61 Patla, A.E. 66, 333, 334, 337, 363

471

Paulignan, Y. 66, 69 Payne, B.R. 240 Pearlman, A.L. 63 Pearson, D.E.
103, 104, 105, 106, 289 Pelah, A. 336 Pélisson, D. 66 Pelz, J.B. 263,
414 Pentland, A.P. 171, 191 Penton-Voak, I.S. 399, 401 Perona, P. 108,
110, 141, 143 Perrett, D.I. 61, 289, 377, 388, 397, 399, 400, 401
Perrone, J.A. 236, 251, 252, 339 Perry, V.H. 34 Pessoa, L. 148
Peterhans, E. 103, 113, 114, 115 Petersen, D. 263 Pettigrew, J.D. 20,
173 Phatak, A.V. 360 Pheiffer, C.H. 121 Phillips, G.C. 101, 102
Phillips, R.J. 291 Phillips, W.A. 71, 416 Pick, A.D. 390 Pick, H.L. 335,
336 Pickens, D. 381 Pietrewicz, A.T. 133 Pike, G. 292 Pinker, S. 288
Pittenger, J.B. 394, 395, 396 Ploog, D. 393 Poggio, G. 173, 182, 187
Poggio, T. 173, 178, 179, 187, 253, 319, 321, 322, 325, 326 Polat, U.
162, 163 Pollard, S.B. 180 Pollen, D.A. 108, 110, 182 Poranen, A. 69
Poteser, M. 323 Potter, D.D. 61 Potts, W.K. 369 Prablanc, C. 66 Prazdny,
K. 338 Prentice, S.D. 333, 334, 337, 363 Price, C.J. 292 Price, R. 381
Pringle, J.W.S. 316 Prinz, W. 382 Pritchard, R.M. 259 Proffitt, D.R.
378, 380, 381 Puce, A. 377, 381

472

AUTHOR INDEX

Pugh, E.N. 13, 23 Purple, R.L. 27 Purpura, K.P. 51, 56 Putnam, C.A. 312,
313 Pylyshyn, Z.W. 83, 207, 245, 410, 411, 412, 413, 414 Qian, N. 229,
230, 233 Quinlan, P.T. 278 Raab, S. 220 Radford, K. 379 Radner, M. 116
Ramachandran, V.S. 184, 185, 191, 192 Rand, D.T. 360 Rao, R.P.N. 148
Ratliff, F. 26 Rauschecker, J.P. 338 Raviola, E. 36 Reddish, P.E. 353,
354, 355, 358, 360 Redfern, M.S. 331 Redgrave, P. 349 Reed, E. 302 Rees,
G. 51, 235, 416 Regan, D. 113, 258, 349, 352, 357, 364 Regard, M. 376
Reichardt, W. 227, 318, 319, 320, 321, 322, 323, 324, 325, 326 Reid,
R.C. 34, 45 Remy, M. 19 Rensink, R.A. 415 Rentschler, I. 143 Repnow, M.
263 Reppas, J.B. 63, 238 Ress, D. 229, 238, 239, 359 Ricciardelli, P.
390 Richards, W.A. 164, 165, 172, 207, 281, 283 Rieser, J.J. 335, 336
Rietdyk, S. 333, 334, 363 Riggs, L.A. 261 Rind, F.C. 350, 353 Ristau,
C.A. 375 Rizzolatti, G. 382 Roberts, L.G. 140 Robertson, R.M. 350
Robinson, C. 337 Robinson, J.A. 103, 104, 105, 106, 289

Robson, J.G. 30, 31, 32, 34, 35, 41, 56, 99, 103, 213 Rock, I. 122, 413
Rodieck, R.W. 34, 45 Rodman, H.R. 232, 250, 253 Rodriguez-Rodriguez, V.
147, 148 Roelfsema, P.R. 71, 72, 73 Rogers, B. 187, 199, 204, 241
Roggman, L.A. 400 Rolls, E.T. 61, 62, 377 Romero, R. 72, 147 Ronner,
S.F. 108, 182 Rosch, E. 287 Rose, D. 231, 238 Rosenthaler, L. 113 Ross,
H.E. 357 Ross, J. 67, 108, 176, 215, 261 Rossel, S. 346 Roth, G. 267
Rowan, A.K. 335 Rowland, W.J. 368 Royden, C.S. 339 Rubin, E. 120 Ruddle,
R.A. 337 Ruderman, D.L. 57 Rumelhart, D.E. 83 Runeson, S. 378, 379, 380
Rushton, K. 331 Rushton, S.K. 340, 341, 357, 358 Rushton, W.A.H. 37
Russell, J.A. 392 Rusted, J. 414 Sagi, D. 128, 162, 163 Sai, F. 388
Saito, H. 236 Sakai, H.M. 36 Saltzman, E.L. 312 Salzman, C.D. 206, 228
Sanders, M.D. 65 Sanger, T.D. 181, 182 Santoro, L. 236, 237 Sarty, M.
268, 388 Sary, G. 113 Sato, T. 246, 247 Saul, A.B. 229 Savelsbergh,
G.J.P. 356 Sawatari, A. 59 Schacter, D.L. 284 Schaeffel, F. 16
Schalkoff, R.J. 89

Schaller, G.B. 373 Schein, S.J. 60 Schenk, T. 63 Schiff, W. 349, 357
Schiller, P.H. 46, 50, 59, 146, 147 Schlottman, A. 384 Schmidt, F. 336
Schmidt, R.C. 313 Schmolensky, M.T. 116 Schofield, A.J. 245 Scholl, B.J.
385, 388 Schöner, G. 331, 332 Schor, C.M. 175, 176, 182, 186 Schrater,
P.R. 252 Schunck, B.G. 256 Schunk, B.G. 89 Schwartz, O. 57, 151 Schyns,
P.G. 111, 288 Sclar, G. 232 Scott-Samuel, N.E. 233, 240, 242, 245
Seamon, J.G. 395 Searcy, J. 295 Seidemann, E. 233 Seiffert, A.E. 245
Seitz, R.J. 382 Sejnowski, T.J. 57 Sejnowski, T.K. 82 Sekuler, R. 115,
214, 216, 234 Selfridge, O.G. 270, 271, 272 Sereno, M.I. 63, 74, 238
Servos, P. 348 Shackleton, T.M. 231 Shadlen, M. 71, 231, 235 Shaffer,
D.M. 362 Shanks, D.R. 384 Shapley, R. 34, 35, 50, 56 Sharp, R.H. 356
Sharpe, C.R. 214 Sharpe, L.T. 37 Shaw, C.D. 312 Shaw, R.E. 311, 312,
394, 395, 396 Shepherd, J.W. 289 Sherrington, C.S. 176, 261 Shimojo, S.
117, 185, 186, 196, 197, 413 Shirai, Y. 136 Siegel, R.M. 69 Silberglied,
R.E. 133 Sillito, A.M. 71 Silverman, G.H. 196, 197, 221, 240 Silverman,
M.S. 56

AUTHOR INDEX

Simmel, M. 385, 386, 387, 391 Simmers, A.J. 162 Simmons, P.J. 353
Simoncelli, E.P. 57, 151, 232, 233, 251, 252 Simons, D.J. 413, 415, 416
Simpson, M.J.A. 368 Sinai, M.J. 304 Singer, W. 68, 71, 416 Sinha, P. 378
Sivak, J.G. 16 Skottun, B.C. 110 Skudlarksi, P. 295, 296 Smallman, H.S.
102, 175 Smirnakis, S.M. 41, 42 Smith, A.T. 117, 216, 223, 227, 235,
243, 245, 253 Smith, E.L. 231 Smith, O.W. 124 Smith, P.A.J. 61
Smolensky, P. 83 Snowden, R.J. 101, 223, 236, 238, 239, 337 Snyder, L.H.
69, 263 Sobel, E.C. 346 Solomon, J.A. 152, 155, 162 Spekreijse, H. 147,
148, 150 Sperling, G. 152, 227, 231, 241, 242, 245, 253 Spurr, R.T. 360
Srinivasan, M.V. 321, 322, 323, 327 Steiner, V. 238 Stevens, J.K. 262
Stevens, K. 202 Stevenson, S.B. 185 Stimpson, N. 337 Stockman, A. 37
Stoerig, P. 65 Stone, J. 34, 297 Stone, L.S. 251, 339 Stoner, G.R. 251
Storm, R.W. 245 Stratton, G.M. 68 Subramaniam, S. 289 Sugase, Y. 72
Sugita, Y. 197, 198 Sully, D.J. 356 Sully, H.G. 356 Sumner, F.B. 133
Sun, H. 352 Sun, H.-J. 356

473

Sutherland, N.S. 269, 270, 273, 274, 276 Switkes, E. 56

Ungerleider, L.G. 58, 60, 64, 66 Uttal, W.R. 13

Tabin, C. 362 Tabor, R. 324 Tailor, D.R. 57 Takahashi, S. 153, 154
Tanaka, H. 236 Tanaka, J. 295, 296 Tanaka, K. 61, 236 Tanaka, M. 377
Tarr, M.J. 288, 295, 296, 297 Thayer, G.H. 132 Thiele, A. 252 Thier, P.
263 Thompson, I.D. 110 Thompson, P.G. 216, 217, 227 Thomson, J.A. 332,
334, 335, 336, 364 Thorell, L.G. 48, 55, 99, 102, 103 Thorpe, P.H. 342,
347 Thorpe, S. 72 Tiffin, J. 336 Tinbergen, N. 266, 267, 368 Titchener,
E.B. 67, 68, 412 Todd, J.T. 395, 396, 397 Tolhurst, D.J. 103, 110, 214
Tomasello, M. 376 Tootell, R.B.H. 56, 60, 63, 64, 229, 237, 238, 240
Tosetti, M. 236 Tremoulet, P.D. 385, 388 Tresilian, J.R. 351, 357 Treue,
S. 200, 203, 229, 232 Tronick, E. 349 Trujillo, J.C.M. 229 Tsetlin, K.L.
312 Tsodykes, M. 162 Tucker, A.E. 372 Tucker, V.A. 372 Tuller, B. 311,
313 Tulunay-Keesey, U. 101, 259 Turner, E.R.A. 369 Turvey, M.T. 311,
312, 313 Tyler, C.W. 156, 163, 176 Tyler, P.A. 357 Tynan, P. 115

Vaina, L.M. 235, 236, 237 Valentine, T. 392 van den Bergh, A.V. 339 van
der Meer, A.L.H. 363 van der Weel, F.R. 363 van der Zwan, R. 117 van
Doorn, A.J. 99, 256 Van Essen, D. 54, 57, 59, 60, 128, 149, 150, 151 van
Hateren, J.H. 57 van Santen, J.P.H. 227 Vasarely, V. 122 Vatan, P. 280
Vautin, R.G. 216 Vecera, S.P. 390 Venkatesh, S. 323 Verstraten, F. 214,
246 Victor, J.D. 35, 36 Vilis, T. 69 Vines, G. 373 Vogels, R. 113, 207,
285 Voigt, T. 182 Volkmann, F.C. 261 Von Cramon, D. 63 von Cranach, M.
393 von der Heydt, R. 103, 113, 114, 115 von Hofsten, C. 363 Von Holst,
E. 325, 327 Vorobyev, M. 22

Ueno, S. 72 Ullman, S. 212, 218, 219, 220, 248, 253, 254, 255, 288, 379,
407, 408

Waddell, D. 302 Wade, M.G. 334 Wagner, H.-J. 346 Wagner, H.G. 26 Walk,
R.D. 328, 329 Walker, G.A. 149, 150, 152 Wallach, H. 254, 255 Walls,
G.L. 13, 23 Waltz, D.L. 135, 136, 275 Wandell, B.A. 50, 51, 63 Wang,
H.Y. 43 Wang, J.L. 36 Wang, Y. 116, 352 Wann, J. 331, 340, 341, 354,
357, 358, 360 Ward, P. 131 Warland, D.K. 41, 42

474

AUTHOR INDEX

Warren, W.H. 309, 332, 338, 339, 360 Warrington, E.K. 65 Warzecha, A.-K.
320, 324 Watamaniuk, S.N.J. 234 Watson, A.B. 155, 216, 217, 218, 241
Watson, D. 288 Watson, J.B. 78 Watson, J.D.G. 63, 64 Watt, R.J. 71, 86,
91, 103, 104, 107, 390, 391 Weberlin, F.S. 37 Wehner, R. 11 Wehrhahn, C.
322, 324, 325 Weiskrantz, L. 65 Weiss, L.-R. 260 Welch, L. 204, 250
Wenderoth, P. 117 Werblin, F.S. 35, 36 Wertheimer, M. 119, 127 West,
D.C. 71 West, S. 243, 378, 379 Westendorff, D.H. 17 Westheimer, G. 70,
71, 74, 163, 164 Wheatstone, C. 173, 174 Whitaker, D. 111 White, K.D.
261 White, P. 292 Whiten, A. 376 Whiting, H.T.A. 356

Wiesel, T.N. 47, 48, 50, 51, 52, 54, 70, 71, 74, 107, 150, 173, 270
Wiggers, W. 267 Wigglesworth, V.B. 10 Wilkinson, F. 161 Willen, J.D. 389
Williams, C. 269, 270 Williams, D.W. 234 Williams, P. 295 Williams, T.H.
44 Wilson, H.R. 101, 102, 161, 247, 248, 250, 251 Wilson, J.A. 219, 220
Wilson, R.G. 182 Winston, P.H. 89, 274, 275, 276 Wittreich, W.J. 79
Wohlgemuth, A. 214 Wolf, R. 325, 327 Wolf-Oberhollenzer, F. 19
Wong-Riley, M.T.T. 51 Wood, D.C. 7 Wood, I. 175, 182 Woodbury, P.B. 372
Woodson, R. 388 Wu, R.W.K. 268, 388 Wundt, W. 119 Wurtz, R.H. 43, 236,
261 Wylie, D.R.W. 324 Xing, J. 263 Xu, X. 45, 47

Yabuta, N.H. 59 Yamane, S. 62, 72 Yeshurun, Y. 413 Yilmaz, E.H. 360 Yo,
C. 247, 248, 251 Yoshikawa, S. 393, 400 Young, A. 294, 388, 398 Young,
D.S. 351, 354, 355, 358, 359 Young, M.J. 202 Young, M.P. 58, 62 Young,
R.A. 99, 103, 295 Yu, C. 152, 156, 157 Yuille, A.L. 256 Zaal, F.T.J.M.
360 Zaidi, Q. 187, 188, 189, 190, 192 Zanker, J.M. 324 Zecki, S. 61
Zeinstra, E.B. 354 Zeki, S. 60, 63, 64, 228 Zetsche, C. 143 Zhang, S.W.
323, 327 Zhou, Y. 116 Zhou, Y.-X. 247 Zihl, J. 63 Zilles, K. 382 Zipser,
D. 83 Zipser, K. 146, 147, 150 Zucker, S.W. 97, 103, 110, 111

Subject Index

Absorption 5 spectrum 11 Accommodation 16, 169--71 Achromatopsia 63--4
Actions coding of 392--3 scaling with distance 345--8 timing of from
optic flow 348--58 vision and 345--65 Active vision 413--16 Adaptation
19 Adapting effect 258 Adaptive radiation 14 Adaptive scale filtering 97
Adelson--Bergen model 225, 227 Advertisement 129--32 Affordance 309--10
concept of 410 Agnosia 80 Air, theory of perception 304 Algorithms
81--2, 85, 406--8 cartoon 289 edge detection 89--98 face recognition 289
intersection of constraints 250 MIRAGE 107 multi-scale 182 Pentland's
191 shape from shading 191 stereo 179--80, 183

valley detecting 104 velocity 221, 255 Amacrine cells 28--9, 34 Ambient
optic array see optic array Ambiguous pictures 120--2 Ames chair 283
Ames room 79, 201 Amodal completion 196--8 Amoeba 7 Amphibians 35
Anableps 16 Anaglyphs 174 Analytic introspection 78 Angular velocity 316
Animals behaviour, perception of 367--77 camouflage 129--32 and
immobility 132 discrimination in 269--70 diurnal 17--18 nocturnal 17--18
swimming 327 Animate motion, human perception of 377--88 Antagonism 30
Anthropomorphic terms, use of 386 Ants 327 Aperture problem 247--50 Apes
21, 376 Apparent motion 240--1 475

Apposition eye type 10 Arca 9 Arctic fox 130 Art of flight 364
Artificial intelligence (AI) 80, 82, 134 and groupings 134--40
Association field 159 Associationism 119 Astigmatism 17 Atmospheric
perspective 195 Attention 175, 238--9, 413 divided 413 effects of 72--4
spatial 414 Aubert--Fleischl effect 263 Axis of shape representation
276--7 Babies see infants Background picturing 130 Balance maintenance
328--32 Ball-punching task 354--5 Bandwidth 55 Bar detectors 107 rule
104 Barlow--Levick mechanism 218 Bayesian perception 201 Bees 134,
321--3, 327 Behaviourism 78--80

476

SUBJECT INDEX

Bicuculline 229--30 Binocular disparity 199 Binocular fields 20, 51
Binocular matching 176--80 Binocular parallax 199 Binocular processing
230 Binocular rivalry 117, 231 Binocular summation 230 and motion 230--2
Binocular vision 172 see also stereopsis Biological motion 378--82
Bioluminescence 6 Bipolar cells 28--9, 34--5 Birds cone sensitivity of
23 flight of 324, 327--8, 342--3, 370 merging and contrasting in 131
panoramic vision of 187 perception in 372 refraction, correction for 346
Blackbirds 267 Blindness and balance 331 Blindsight 65 Blur 103, 110, 71
Bobolinks 132 Brain, visual pathways in 43--74 Braking 358--60
Branchiomma 9 Brightness 291 Broad-band cells 34 Broken-wheel display
254 Butterflies 130--1, 319 grayling 267--8 Canonical views 287
Cardioidal strain 394--7 Catching 313, 356--7, 358, 360--2 Caterpillars
132 Cats binocular simple cells in 182 cortex of 47, 103, 108, 149--50,
173, 175, 219--20, 227, 229, 247, 270 cue-invariant cells in 116 head
orientation in 376 motion sensitivity of 216 retinal response in 30,
32--5 filtering 41 Causality, perception of 382--4 Cell population,
pattern of activity in 62

Centre of moment 380--1 Cephalopods 12 Cerebral colour blindness 63--4
Cerebral cortex, visual areas of 57 Chameleons 20, 346 Change blindness
414--16 Chickens/chicks 328, 347, 369--70 Children see infants
Chimpanzees 373, 376--7, 409--10 food location by 409--10 Chromatic
aberration 17 Ciliary muscles 16 Ciliary receptors 12 Closed-loop
control 325, 345, 347, 358, 362--3 Closure 125 Co-ordinative structures
312 Coalition 312 organisation 370 Cognitive psychology 79--80, 82
Coherent motion 235 Collinear facilitation 162--4 Collisions, avoidance
of 348--50 Colour blindness 22, 133 constancy 60--1 deficiency 22
detection of 21--3 opponency 33 Colour-opponent cells 34 Common coding
382 Common fate 124 Complex cells 48, 50, 110, 182 Compound eyes 9--11,
25 Computation of image motion 209--64 Computational theory 81
Concealment 129--32 Cones 12, 18, 21, 28--9, 36--7 generalised 276--7,
279 Connectionist models of visual perception 82--3 Context-conditioned
variability 311 Contextual modulation 147, 149 Contour detection model
115 Contour generator 279--80 Contour integration 159--62 Contours
159--67 anomalous 113--17 illusory 113--16

Contrast 31 adaptation 101--2, 153--4 discrimination 153, 155--6, 163
energy 107 envelope 245 gain control model 157--8 invariance model 232
sensitivity 34 for movement and flicker 216--18 Contrast-modulated (CM)
patterns 242--6 Contrasting 130--1 Convergence 169--71 movements 20
Convex lens 11 Cornea 10, 12, 16 Corolla 8 Correspondence problem 176
Cortex 227, 229, 247 Cortical damage 63--4 Countershading 131--2 Crab
spiders 133 Crabs blue 372 horseshoe 25--8 Crows 130 Crustacea 10--11
Cuckoos 131 Cue integration model 202 and modules 200 in MT 204 and
surface description 201--6 Curved surfaces 202 Curvilinear aggregation
138--9, 159 Cuticle, transparent 10 Cyclic guanosine monophosphate
(CGMP) 13 Cypselurus 16 Cytochrome oxidase 51--2, 54, 56, 58 Dark
adaptation 26 Deafness and balance 331 Deceleration, controlling 358--60
Decussation 43 Deformation 257 Degrees of freedom problem 311 Demand
characters 309 Demons 271--2

SUBJECT INDEX

Depth cues 169--70, 358 integrating 200--6 encoding 172--3 of field 16
from motion 197--200 pictorial cues to 187--97 Derivative operators, as
spatial filters 98--9 Dichoptic motion 231 Dichromatic vision 21--2
Difference-of-Gaussian operator 407 Diffraction 5 Diplopia 175 Direction
control of 337--43 selectivity 218--21, 230 Directional aftereffect
(DAE) 216 Discrimination, animal 269--70 Disparity 173, 185 binocular
180--1 crossed 171 and depth, encoding 172--3 gradient 180 from spatial
phase 180--4 uncrossed 171 Disruptive coloration 130 Diving 353--5, 358
Dogs 368, 370 sheepdogs 373--5 Double opponency 240 Doves 347 Driving
and braking 358--60 and road fixation 342 and speed judgement 336--7 and
steering 341 Drosophila 325 melanogaster 318 Ducks 16, 20, 131 Dunlin
369--70 Ecological approach to visual perception 301--14 Ecological
theory 407, 409 Edge detection 146 theory of 212 Edge detectors 107 Edge
rule 104 Egocentric direction hypothesis 340--1

Ehrenstein illusion 114 Electromagnetic information 4 Empiricism 78, 169
Encoding local velocity 221--7 End-stopped cells 113--17 Energy models
227 for feature detection 107--11 for texture segmentation 140--58 peak
detection 110 stage 221 Entraining 382--3, 386 Environment 302 Euglena 7
Events 308 Expansion 237--9, 257 Extraretinal eye movement signal 263
Extrastriate cortex 57 functions of 59--60 hierarchical processing in
60--2 Eye-cup 8--9, 11, 19 Eye-spots 8 Eyes insect 324 movements of
258--64 motion perception during 263--4 position signals of 262--3
nature of 260--1 source of 261--3 true 8 Faces categorising of 393--401
by age 394--8 by attractiveness 400--1 by sex 398--9 composite 295
detection of 62 perception of 388--401 adult 389--90 development of
388--9 expressions on 391--3 preference for 61 recognition of 61,
289--97 Facial Action Coding System (FACS) 392--3 Facial expressions,
perception of 391--3 Facilitation 162--3

477

Falcon, peregrine 372 Fannia canicularis 320 Far cells 182 Fast Enabling
Link (FEL) 287 Features 85--118 analysis of 270--2 Feedback in visual
pathways 71--2 Figure detection cells 324 Filehne illusion 263
Filtering, changes in 72 Filters 85--118 bandwidths of 102 local energy
from 107--10 retinas as 37--42 First-order motion 241--2, 245 Fish
130--2, 171 flying 16 mosquito 133 Siamese fighting 368--9 Flicker 213,
216--18, 231, 233 rate of 241 Flies 355--6, 409 fruit 318--19, 322 house
319--22, 325, 372 hover 133 tethered 355 Flight 316--27 control,
physiological mechanisms of 324 speed, control of 322--3 towards objects
319--22 Focal length 15 Focal plane 15 Focusing the image 14--17
Foreshortening 187 Forms, static vs moving 297--8 Forward model 363
Fourier amplitude/phase 191 Fourier analysis 38--9, 56--7 Fourier
components 189, 241 Fourier spectrum 242--4 Fovea 19, 29, 101--2, 171,
258, 372, 376, 413 central field of 21 Frogs 346--7 tree 130--2
Fruitflies 318--19, 322 Full-wave rectification 141 Functional magnetic
resonance imaging (fMRI) 50--1, 63, 155, 207, 214, 228--9, 233, 235--6,
238--9, 250, 416

478

SUBJECT INDEX

Fusion 176 GABA 229 Gabor filters 182 Gabor functions 56, 107--8, 110,
149, 183 Gabor patches 159 Gait control of 332--5 and vision 333--4
Ganglion cells 28--9, 32--4, 43 axons of 43--5 Gannets 353--5, 358
Ganzfeld 211 Gaussian envelope 108 functions 99 operators 94, 99, 102
Gaze 21, 340--1, 389 control of 413--14 cues 377 perception of 375--7,
390--1 Geometric ions 282 Geons 282--5, 287--9, 294 Gerbils 346--7, 356
Gestalt psychologists 119--20, 127, 164, 234, 309 Gestaltism 79--80,
124--5, 127--9, 132, 159, 195, 310 isomorphism, doctrine of 127 laws of
organisation 123--7, 134, 138, 146 proximity 146 Gibson's theory of
perception 302--11, 315 Gigantocypris 11 Global motion 234--6 and MAE
237--9 perception of 228--9, 233 spatial patterns of 236--7 Global tau
(τ) 351--2 see also tau ratio Good continuation 124--5, 159 Gradients,
texture/velocity of 202 Grasping 347 Gratings 14, 30--2, 47, 113,
152--4, 156, 188--9 carrier 243, 245 CM 247 flickering 181--2, 212--14,
216--17, 231 illusory 102

LM 247 masking of 215 moving 110, 227, 231, 233 parameters of 31 phantom
114--15 resolution of 14, 36--7 second-order 242, 251 sinusoidal 30--2,
90, 101, 189, 232 spatial frequency of 40 stationary 231, 246 tilted 116
Greebles 295--7 Grey-level representation 87 Ground, theory of
perception 302, 304 Gulls 368 herring 372 Half-occlusions 186, 195
Half-wave rectification 141 Hand velocity 348 Harmonics 37--8 Haunted
swing 328 Hawks, Harris 353 Head movement, stereotyped 347 Head-bobbing
346--7, 353 Heading 337--41 Height, control of 322--3 Herding 373--5
Heron, reef 346 Heterarchy 312 Hollow-face illusion 412 Horizontal cells
28--9, 34, 324 Horizontal disparity 171 Horses 171 Houseflies 319--22,
325, 372 Hoverflies 133 Human action, control of 311--14 flexibility 311
Human face, perception of 388--401 adult 389--90 development of 388--9
Hummingbirds 327, 360 Hunting, pack 373 Hypercomplex cells 50 Illusory
contours 184--5, 197 Illusory surfaces 204 Image-space processor 280

Images 85--118 computation of 88--9 formation of 8 luminance of 86
motion, computation of 209--64 plane of 15 size of 175 stabilisation of
259 Imitation 389 Immobility 132 Inattentional blindness 413 Infants
(babies; children; toddlers) and approaching objects 248--9 and
causality 384 facial perception by 388--90 and figural coherence 378
locomotion of 328--9, 331 and optic flow 330--1 predictive control of
363 skull shape of 394 Inference 406 Inflow theory 261--3 Infra-red (IR)
radiation 18 Inhibitory end-zone model 150 Insects 355 diurnal 11 flight
of adaptive properties of 324--7 model of 327 visual control of 316--27
nocturnal 10 optomotor response of 217 Integrating depth cues 200--6
Intentional terms, use of 385--8 Intentions, perception of 382
Interception points 373 Interocular viewing 231 Interposition 195
Intersection of constraints 250--3 Invariants 305 Invertebrates 8 Iris
12 Isomorphism, doctrine of 127 Jays 134, 375 blue 133 Joint attention
391 JPEG files 118 Judgements of contingency 384 of necessity 384
Jumping 334, 364

SUBJECT INDEX

Junctions 135--6 K cells 45, 51 K pathway 34, 46 Kanisza figure 114 Key
stimulus 266--8 Kinematograms 131 Kinetic depth effect 255 Kingfishers
327 Koniocellular pathway 34, 58 L cones 45 Lambertian surface 85 Lambs
328 Lamina 324 Laplacian operator 93--4 Laplacian-of-Gaussian operator
94 Large field 322, 324 Lateral geniculate nucleus (LGN) 34, 43--8, 54,
65, 172, 215--16 blocking of 70--1 koniocellular layer of 58 M cells of
220 P cells of 50--1 Lateral inhibition 27--8 Launching effect 382--4
Lens 9, 12, 16 focal plane 15 power 15 Leonardo's window 188 Light 1--7,
85--6 adaptation 26 and eyes 3--24 intensity variations in 17
monochromatic 4 patterns 6, 406 source 190--1 Light-sensitive structures
evolution of 7--13 Limit-cycle oscillators 312 Limulus 25--30, 33, 36
Linear optical trajectory (LOT) hypothesis 362 Lions 373 LIP area 263
Lobula 324, 350 plate 324 Local disturbances 309 Local energy 107 Local
motion components and role of MT 251--3

Local tau (τ) 351--2, 357 see also tau ratio Local velocity, encoding
221--7 Localised opponency 233 Locomotion and optic flow 315--43 and
posture, visual control of 327--43 Locusts avoidance manoeuvres by 353
flight of 317--18, 326, 350 jumping by 346 Logarithmic coding 25
Long-range motions 240--7 Looming patterns 349, 352, 355, 409 Luminance
85--6, 117--18 profile 86 Luminance-modulated (LM) grating 242--6 M
cells 34, 45, 47--8, 50--1, 220 M cones 45 M pathway 46--7, 50 Mac-a-Mug
295 Mach bands 107--8 Macula lutea 19 Magnetic resonance imaging (MRI)
see functional magnetic resonance imaging (fMRI) Magnocellular lesions
46, 227 Mammals 43 Mantids 346--7, 376 Marr, D. visual perception theory
of 80--2 visual processing program of 136--40 Marr--Hildreth model 90,
97, 107 Marr--Ullman model 218, 220 Masking 101--2, 155--7, 162 forward
and backward 261 Mating behaviour 370 Medulla 324 MEG technique 228
Merging 130--1 Mind reading 391 MIRAGE model 104, 106--8, 111 Mirror
neurons 382 Modal completion 197 Models Adelson--Bergen 225, 227

479

contour detection 115 contrast gain control 157--8 contrast invariance
232 cue integration 202 energy 227 first stage of motion 318 forward 363
inhibitory end-zone 150 insect flight 327 MIRAGE 104, 106--8, 111 motion
analysis 258 motion coding 218, 247--8, 250--1 motion detectors 218--21
motion energy 221, 224 multi-channel gradient 143, 221, 223, 225
multi-scale 175 multiple filter 175 neural network 286 optical
acceleration cancellation (OAC) 361--2 optomotor response 325 predictive
coding 148 Reichardt 227, 318 segmentation by orientation 143--5 stereo
vision 180 stereopsis 179, 182--3 texture segmentation 140--58, 242 3-D
80, 277--81 transduction 325--6 velocity template 252 Modularity 412--13
Modules and cue integration 200--1 Molluscs 12 Monkeys attention
responses of 73--4 blindsight 65 capuchin 377 cone type of 21 cortex of
53, 69, 128, 173, 219, 247, 270 striate 47, 103 gaze information 376--7,
413 IT cells 62 judgement of 206 macaque 57--9 motion perception by
228--9 MT cells 63, 233 neurological studies of 382

480

SUBJECT INDEX

New World 21 Old World 21, 45 retina of 33--4 ganglion 40--1 rhesus 349
V1/V2 cells of 113--14, 116, 146, 150, 198, 216--17 Monocularity 230--1
Morphing 397, 400 Mosquito fish 133 Moths 132--3 peppered 130, 133
Motion 238--9, 258--64 aftereffect (MAE) 116, 213--16, 231, 237--9,
246--7 analysis model 258 biological 378--82 blindness 63 coding model
218, 247--8, 250--1 coherence task 238 detection, theory of 212
detectors 213--21 model 218--21 energy 221, 224--5 model 221, 224
system, hierarchy of processing 227--40 from feature tracking 253--5
first-stage model 318 measurement, integration of 247--53 opponency 230,
232--3 as orientation in space-time 210--13 parallax 131--2, 197, 199,
202--4, 258, 316, 337 perception 263--4 perspective 197 sensor 218
Motion-specific mechanisms 246 Movement 216--18 eyes versus world
259--60 MPEG files 118 MST 214, 227, 230, 236--9, 251, 263--4 MT 204,
206, 214, 227--40, 247, 264 integration role of 251--3 MT+ (V5) 227--9,
233, 235, 240, 250, 264 Mud splashes 415--16 Multi-channel gradient
model 143, 221, 223, 225

Multi-scale model 175 Multi-scale vision 111--12 Multiple-filter model
175 Musca domestica 319 Nativism 78--9 Nautilus 11 Near cells 182
Necturus 35--6 Neural network model 286 Neuroimaging 382 Neurons 350
Non-Fourier motions 240--7 Non-Fourier stimuli 242 Nonlinearities, role
of 242--5 Null rule 104 Object categories, discrimination within 289--97
Object recognition 265--98 Marr & Nishihara's theory of 276--81
Object-centred coordinate 276 Objects, flight towards 319--22 OBSCENE
program 135 Occipital cortex 57 Occluding contours 184 Occlusion 195
cues 113--17 and stereo depth 184--7 Ocellus 8 Ocular dominance 51--3
Ommatidia 10, 25--8 Open-loop control 325, 345--8, 358 Opponency 143
stage 221 Opponent energy 221, 224, 226 Optic array 6, 315, 406--7 flow
in 305 sampling 19--21 Optic ataxia 66 Optic chiasma 44 Optic flow 305,
335--8, 341, 347, 349, 407 contact information in 358 field 305,
315--16, 337, 351 centre of expansion 305 pole 305 hypothesis 337 local
analysis of 256--7 and locomotion 315--43 motion-in-depth from 258

patterns 333 and retinal flow 315--16 time to contact from 350--8 timing
actions from 348--58 Optic nerve 25, 44 Optic radiation 44 Optic tectum
43 Optical acceleration cancellation (OAC) model 361--2 Optomotor
response 318--19, 322--5 model of 325 Organs, activation of 307
Orientation 125 bodily 368 contrast 143 selectivity 47 Outflow theory
261--3 Outlining 130 P cells 34, 45, 47, 50--1, 220 P pathway 46--7, 50
Pandemonium system 271--2 Panum's fusional space 175 Panum's limit
175--6 Parabuteo uncinctus 353 Parallel distributed processing 83
Paralysed eye experiment 262 Parietal cortex 57, 60, 69, 382 Part-based
theory 392 Parvocellular lesions 227 Parvocellular systems 261 Paths of
travel, perception of 372--5 Pattern movement 250 Peacocks 131 Pecten 11
Pedestal 231 Penguins 16 Galapagos 133 Perception 3 cognitive theory of
405--13 direct 406, 408 ecological theory of 405--13 Gibson's theory of
302--11, 315 human 410 implications for role of 313--14 mediated 406--7
by algorithms 406--8 of social world 367--402 animal behaviour 367--77
animate motion 377--88

SUBJECT INDEX

causality 382--4 facial expressions 391--3 gaze 375--7, 390--1 human
face 388--401 intentions 382 paths of travel 372--5 Perceptual
completion 195--7 Perceptual groupings 166--7 Perceptual organisation
119--67, 132--4 experimental approaches to 127--9 Perceptual system 307
Perceptual vector analysis 379 Peripheral field 21 Periphery effect 35,
69 Perpetual systems 307 Perspective 187--90 Phase 31 invariance 48
Phasic response 34 Photons 4 Photopic vision 19, 21 Photoreceptor cells
8, 23, 36, 406 Physiological theory 407 Physiology, energy computations
in 110--11 Pictures, perception of 310 Pigeons 133, 324, 342, 347,
352--3, 369--70 Pit vipers 18 Pixels 87, 89, 110 Place tokens 138 Plane
surfaces 202 Play fighting 370 Plexiform layers 28--9 Plovers 375
Point-light motion displays 378--81, 392 Polarisation, plane of 11, 23
Position 258--64 Positron emission tomography (PET) 63 Posture 368 and
locomotion, visual control of 327--43 Prägnanz, Law of 127 Pre-cueing
413 Predictive coding model 148 Predictive control 363--4 Prestriate
cortex 57

Primal sketch 80, 85--119, 137--8, 201, 253, 274, 280, 282 other routes
to 103--7 Primary motor cortex 381--2 Primates 347, 376--7 retina of 33
Probability summation 101 Protozoans 7 Proximity 123 Pseudoscopic vision
201 Psychology ecological 310, 394 of visual perception 77--83
Psychophysics, energy computations in 110--11 Pulvinar nucleus 352 Pupil
19 Pursuit 258--9 movements 20 Rabbits 41, 171, 218 Race, perception of
399 Random dot patterns 233 Random-dot stereograms 123, 177, 182, 184
Ratio stage 221 Rats 269--70, 328 Reaching 347--8 Receptive fields,
concentric 30--3 Receptor potential 11 stimulation of 307 Recognition
alphanumeric 268, 271 in animals and people 265 complex processes of 268
by components 281--7 mechanisms of 266--8 of objects 265--98, 276--81
pattern 268 segmenting parts for 164--6 viewpoint-dependent 287--9
Rectification 35 Reflectance 86 Reflection 5 Refraction 5 Refractive
indices 15--16 Regularity 131 Reichardt's model 227, 318 Relative motion
239--40 Relative size 125 Repetition priming 283

481

Representation, and perception 408--12 Representations 80 Retina 12,
15--19, 21, 101, 173, 259, 413 direction of motion and 215--16, 218, 240
as filter 37--42 ganglion cells 36 image on 41, 406 neurophysiology of
the 25--42 receptive field of 30, 35--6, 43 vertebrate 28--37 Retinal
eccentricity 175 Retinal flow field 337--9, 343 and optic flow 315--16
Retinal mechanisms 35--7 Retino-geniculate pathway 227 Retinomotor
responses 19 Retinoptopic mapping 43, 50--1 Retinula 25 cells 11 Reverse
countershading 131--2 Reversible occlusion 308 Reversing figures 164
Rhabdom 10--11 Rhabdomere 11 Rhodopsin 12 pigment 11 Rivalry 185 Rods
12, 18, 21, 28--9, 36--7 Rotating cylinders 255 Rotation 237--9, 257
Running 360--2 control of 332--5 S cones 45 Saccades 20, 66, 258--62,
335, 363--4, 413--16 suppression of 261 Salamanders 36, 41 Scene
analysis programs 134--6 Schröder staircase 164--5 Scotopic vision 19,
21 Seals 16 Second-order motions 240--7 SEE program 135--6 "Seeing" and
"seeing as" 411--12 Segmentation by orientation model 143--5 Shading
190--2

482

SUBJECT INDEX

Shadows 192--5 depth and motion 193--4 Shape descriptions, modular
organisation of 276 properties of 285 recognition 286 Sheep 373--5
Sheepdogs 373--5 Short-range motion 241 Sign stimulus 266 Silhouettes
279--80 Similarity 123--4 conceptual 128 Simple cells 47--50, 56--7, 110
Single vision 173--6 Single-chambered eye 11--13 Sinusoidal gratings 31,
54--5 Size--disparity correlation 175 Small field 322, 324 Smiles,
deceptive 393 Snakes 131--2 Social intentions, perception of 367, 384--8
Social world, perception of 367--402 Space--time filter 220 Space--time
gradients velocity from 211--13 Space--time image 211 Space--time
receptive field 218 Space-variant filtering 97 Sparrows 131, 376 Spatial
coincidence assumption 97 Spatial derivatives 102 Spatial filters 40,
54--7, 96, 182 and derivative operators 98--9 multiple 99--103 Spatial
frequency 31, 47 Spatial patterns of motion 239 transformation of 26--8
Spatial pooling 230 Spatial resolution 240 Spatial smoothing 255--6
Spatial variations in velocity field 255--8 Speed, control of 335--7 and
vision 335 Squaring 141 Stabilised retinal images 258--9 fading of 259

Static energy 226 Steering a straight course 316--19 Stentor coeruleus 7
Stereo depth and occlusion cues 184--7 Stereo disparity 200
Stereo-motion integration 204--5 Stereo vision model 180 Stereograms 131
Stereopsis 131, 171, 176--7, 184, 253 binocular 171--87, 199 global
178--9 model 179, 182--3 and single vision 173--6 Stereoscope 173--4,
176 Sticklebacks 266--8, 368 Stimulus equivalence 265--6 key 266--8 sign
266 Striate cortex 47--57, 102 extrastriate regions 57--62 functional
architecture 50--4 spatial filtering 54--7 Striking at prey 345--6
Structural descriptions 272--6 Structuralism 78 Sub-threshold summation
99--101 Sufficiency criterion 410--11 Sula bassana 353 Superior
colliculi 43--4 Superior temporal sulcus 377, 381 Superposition eye type
10 Suppressive effects 70 Suppressive surrounds physiology 149--51
psychophysics 151--8 Surfaces 85--6, 159--67, 302, 304, 315 description
and cue integration 201--6 representation of 166--7 texture of 5
Surroundedness 125 Sustained response 34 Swinging room 329--33 virtual
331--2 Swinging tunnel 335 Symmetry 125, 131 Synchronisation 71, 287

Tapetum 18 Tasmanian devils 370--1 Tau (τ) ratio 350--60, 407
Telestereoscope 356--7 Template matching 268--9, 288 Temporal cortex 57,
60, 377 Temporal frequency 47 Temporal integration 236--7 Temporal
pattern, transformation of 26 Texture gradient 303--5 segmentation
models for 140--58, 242 neurophysiology of 146--8 Thalamus 43 Theta
aggregation 138--9 Third dimension, perception of 169 3-D model
description 277--81 representations 80 3-D world seeing a 169--207
Thrushes 267 Tilt aftereffect (TAE) 116--17, 164 Titchener illusion
(circles) 67--8, 412 Toads 267, 346--7 Toddlers see infants Tonic
response 34 Transactional functionalism 79 Transcranial magnetic
stimulation (TMS) technique 228 Transduction 7, 12, 25 model 325--6
Transfer function 39 Transformation 26--9, 395--6 Transient response 34
Translation 237--9 Transparent motion 233, 251 Transversality regularity
164--5 Treadmills 335--6 Tremor 20 Trichromatic vision 21--2 Trout 327
Tuned cells 182 Tuning 47 of segmental apparatus 312--13 2-D key points
113 2-D sketch 282 2½-D sketch 80, 180, 201, 202, 204, 258, 274, 408,
413

SUBJECT INDEX

Ultraviolet light 23 Unconscious inference 81, 148, 407 Unconscious
interference 201 Uniqueness constraint 178 V1 113, 115--18, 216, 247,
253 dorsal pathway 214 encoding 161, 172--3 filtering 297 motion energy
system 227--40, 250--1 perception 197--8 response 155 spatial
interactions 159 and suppressive surrounds 149--50, 152 and texture
segmentation 146--8 V2 113--18, 207, 247 dorsal pathway 214 encoding 173
motion energy system 227, 230 perception 197--8 and suppressive
surrounds 149--50, 152 and texture segmentation 146--8 V3 207 V3A 207 V4
148, 150, 161 Valleys 104 detecting scheme 105 Valuation 384 Vection 328
Vector summation 250--1

Velocity 320 code 221--2, 226 constraint line 248--50 field 247 spatial
variations in 255--8 of motion 321 from space--time gradients 211--13
template model 252 vectors 249--50 Vergence 258 Vertebrates 12--13, 23
aquatic 16 eyes, adaptive radiation of 14--23 Viewer-centred coordinate
276 Virtual reality techniques 357--8 Vision 85--6 for action 299--402
active 413--16 for awareness 75--298 in bright and dim light 17--19
module 412 and timing of actions 345--65 Visual acuity 14, 17 Visual
angle 14 Visual cliff 328--9 Visual control continuous 358--62 of insect
flight 316--27 Visual cortex 44, 47--8, 50, 52--4, 116, 227, 233
analysis of 219--20 cell assemblies in 70--1, 101, 287 feature detectors
in 270--1 spacial filters in 182

483

Visual denial 334 Visual form agnosia 65 Visual pathways 62--9 dynamics
and feedback in 69--74 Visual perception connectionist models of 82--3
contrasting theories of 405--17 the ecological approach to 301--14
Marr's theory of 80--2 physiological basis of 1--74 psychology of 77--83
Visual processing modularity of 80 VS cells 324 Walking, control of
332--6, 341 Waltz program 136 Wasps 133, 327 Wavelength 86 Weber's Law
153 Weber fraction 153 Wolves 370, 373--4 Wrasse, bluehead 370 X cells
32--5, 47, 218, 220 Y cells 32--5, 218, 220 Zebras 132 Zero-crossings
93--4, 97--9, 103--4, 180, 189 and energy model 110--11 location of 136
maps 104


