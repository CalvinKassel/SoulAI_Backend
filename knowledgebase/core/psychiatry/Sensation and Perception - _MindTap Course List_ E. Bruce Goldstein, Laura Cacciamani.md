Eleventh Edition

Sensation and Perception E. Bruce Goldstein University of Pittsburgh
University of Arizona

and

Laura Cacciamani California Polytechnic State University

Australia ● Brazil ● Canada ● Mexico ● Singapore ● United Kingdom ●
United States

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

This is an electronic version of the print textbook. Due to electronic
rights restrictions, some third party content may be suppressed.
Editorial review has deemed that any suppressed content does not
materially affect the overall learning experience. The publisher
reserves the right to remove content from this title at any time if
subsequent rights restrictions require it. For valuable information on
pricing, previous editions, changes to current editions, and alternate
formats, please visit www.cengage.com/highered to search by ISBN#,
author, title, or keyword for materials in your areas of interest.
Important Notice: Media content referenced within the product
description or the product text may not be available in the eBook
version.

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Sensation and Perception, Eleventh Edition E. Bruce Goldstein and Laura
Cacciamani SVP, Higher Education & Skills Product: Erin Joyner VP,
Higher Education & Skills Product: Thais Alencar Product Director: Laura
Ross Associate Product Manager: Cazzie Reyes

© 2022, 2017, 2013 Cengage Learning, Inc.

WCN: 02-300 Unless otherwise noted, all content is © Cengage. ALL RIGHTS
RESERVED. No part of this work covered by the copyright herein may be
reproduced or distributed in any form or by any means, except as
permitted by U.S. copyright law, without the prior written permission of
the copyright owner. For product information and technology assistance,
contact us at Cengage Customer & Sales Support, 1-800-354-9706 or
support.cengage.com.

Product Assistant: Jessica Witczak Learning Designer: Natasha Allen
Content Manager: Jacqueline Czel

For permission to use material from this text or product, submit all
requests online at www.cengage.com/permissions.

Digital Delivery Lead: Scott Diggins Director, Marketing: Neena Bali

Library of Congress Control Number: 2021900075

Marketing Manager: Tricia Salata

Student Edition: ISBN: 978-0-357-44647-8

IP Analyst: Deanna Ettinger IP Project Manager: Carly Belcher Production
Service: Lori Hazzard, MPS Limited Art Director: Bethany Bourgeois Cover
Designer: Bethany Bourgeois Cover Image Source: iStockphoto.com/ Chris
LaBasco

Loose-leaf Edition: ISBN: 978-0-357-44648-5 Cengage 200 Pier 4 Boulevard
Boston, MA 02210 USA Cengage is a leading provider of customized
learning solutions with employees residing in nearly 40 different
countries and sales in more than 125 countries around the world. Find
your local representative at www.cengage.com. To learn more about
Cengage platforms and services, register or access your online learning
solution, or purchase materials for your course, visit www.cengage.com.

Printed in the United States of America Print Number: 01 Print Year:
2021

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Bruce Goldstein

To Barbara: It's been a long and winding road, but we made it all the
way to the 11th edition! Thank you for your unwavering love and support
through all of the editions of this book. I also dedicate this book to
the editors I have had along the way, especially Ken King, who convinced
me to write the book in 1977, and also those that followed: Marianne
Taflinger, Jaime Perkins, and Tim Matray. Thank you all for believing in
my book and supporting its creation. Bruce Goldstein

Sarah Williams

To Zack, for supporting me through the winding roads of academia and
listening to me ramble about research on many, many occasions. And to my
mother, Debbie, for being my life-long role model and demonstrating what
it means to be a compassionate, persevering, independent woman. Laura
Cacciamani

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Nesrine Majzoub

Barbara Goldstein

About the Authors E. BRUCE GOLDSTEIN is Associate Professor Emeritus of
Psychology at the University of Pittsburgh and is affiliated with the
Department of Psychology at the University of Arizona. He received the
Chancellor's Distinguished Teaching Award from the University of
Pittsburgh for his classroom teaching and textbook writing. He received
his bachelor's degree in chemical engineering from Tufts University and
his PhD in experimental psychology from Brown University; he was a
postdoctoral fellow in the Biology Department at Harvard University
before joining the faculty at the University of Pittsburgh. Bruce has
published papers on a wide variety of topics, including retinal and
cortical physiology, visual attention, and the perception of pictures.
He is the author of Cognitive Psychology: Connecting Mind, Research, and
Everyday Experience, 5th Edition (Cengage, 2019), The Mind:
Consciousness, Prediction and the Brain (MIT Press, 2020), and edited
the Blackwell Handbook of Perception (Blackwell, 2001) and the
two-volume Sage Encyclopedia of Perception (Sage, 2010). He is currently
teaching the following courses at the Osher Lifelong Learning Institute,
for learners over 50, at the University of Pittsburgh, Carnegie-Mellon
University, and the University of Arizona: Your Amazing Mind, Cognition
and Aging, The Social and Emotional Mind, and The Mystery and Science of
Shadows. In 2016 he won "The Flame Challenge" competition, sponsored by
the Alan Alda Center for Communicating Science, for his essay, written
for 11-year-olds, on What Is Sound? (see page 286). LAURA CACCIAMANI is
Assistant Professor of Cognitive Neuroscience in the Department of
Psychology and Child Development at California Polytechnic State
University, San Luis Obispo. She received her bachelor's degree in
psychology and biological sciences from Carnegie Mellon University and
her MA and PhD in psychology with a minor in neuroscience from the
University of Arizona. She completed a two-year postdoctoral fellowship
at the Smith-Kettlewell Eye Research Institute while also lecturing at
California State University, East Bay before joining the faculty at Cal
Poly. Laura's research focuses on the neural underpinnings of object
perception and memory, as well as the interactions between the senses.
She has published papers that have used behavioral, neuroimaging, and
neurostimulation techniques to investigate these topics in young adults,
older adults, and people who are blind. Laura is also passionate about
teaching, mentoring, and involving students in research.

iv

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Brief Contents 1

Introduction to Perception

2

Basic Principles of Sensory Physiology

3

The Eye and Retina

4

The Visual Cortex and Beyond

67

5

Perceiving Objects and Scenes

89

6

Visual Attention

7

Taking Action

8

Perceiving Motion

9

Perceiving Color

10

Perceiving Depth and Size

11

Hearing

12

Hearing in the Environment

13

Perceiving Music

14

Perceiving Speech

15

The Cutaneous Senses

16

The Chemical Senses

3 21

39

123

149 175 197 229

263 291

311 335 357 389

Appendix

A The Difference Threshold 417 B Magnitude Estimation and the Power
Function C The Signal Detection Approach 420

418

Glossary 426 References 445 Name Index 472 Subject Index 483

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

v

Contents Chapter 1

Chapter 2

Introduction to Perception

3

1.1 Why Read This Book? 5 1.2 Why Is This Book Titled Sensation and
Perception? 5 1.3 The Perceptual Process 6 Distal and Proximal Stimuli
(Steps 1 and 2) Receptor Processes (Step 3) 7 Neural Processing (Step 4)
8 Behavioral Responses (Steps 5--7) 9 Knowledge 10 DEMONSTRATION \|
Perceiving a Picture 10

1.4 Studying the Perceptual Process

7

11

13

1.5 Measuring Perception

13

Measuring Thresholds 14 METHOD \| Determining the Threshold 14 Measuring
Perception Above Threshold 15 METHOD \| Magnitude Estimation 16

Something to Consider: Why Is the Difference Between Physical and
Perceptual Important? 18 TEST YOuRSELF 1.2 19 THINK ABOUT IT  19 KEY
TERMS 19

2.1 Electrical Signals in Neurons

21

Recording Electrical Signals in Neurons 22 METHOD \| The Setup for
Recording From a Single Neuron Basic Properties of Action Potentials 23
Chemical Basis of Action Potentials 24 Transmitting Information Across a
Gap 25

22

2.2 Sensory Coding: How Neurons Represent Information 27

The Stimulus--Behavior Relationship (A) 11 The Stimulus--Physiology
Relationship (B) 12 The Physiology--Behavior Relationship (C) 13 TEST
YOuRSELF 1.1

Basic Principles of Sensory Physiology 21

Specificity Coding 27 Sparse Coding 29 Population Coding 29 TEST
YOuRSELF 2.1

30

2.3 Zooming Out: Representation in the Brain

30

Mapping Function to Structure 30 METHOD \| Brain Imaging 31 Distributed
Representation 33 Connections Between Brain Areas 33 METHOD \| The
Resting State Method of Measuring Functional Connectivity 34

Something to Consider: The Mind--Body Problem

35

TEST YOuRSELF 2.2 36 THINK ABOUT IT  37 KEY TERMS 37

vi

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Chapter 3

Chapter 4

The Eye and Retina

The Visual Cortex and Beyond

39

3.1 Light, the Eye, and the Visual Receptors

3.2 Focusing Light Onto the Retina Accommodation

4.1 From Retina to Visual Cortex

40

Light: The Stimulus for Vision 40 The Eye 40 DEMONSTRATION \| Becoming
Aware of the Blind Spot DEMONSTRATION \| Filling in the Blind Spot 43

3.3 Photoreceptor Processes

43

4.2 The Role of Feature Detectors in Perception

43 44

45

Transforming Light Energy Into Electrical Energy 45 Adapting to the Dark
46 METHOD \| Measuring the Dark Adaptation Curve 46 Spectral Sensitivity
49 METHOD \| Measuring a Spectral Sensitivity Curve 49 TEST YOURSELF 3.1

51

Something to Consider: Early Events Are Powerful 59 Developmental
Dimension: Infant Visual Acuity 60 METHOD \| Preferential Looking TEST
YOuRSELF 3.2 62 THINK ABOUT IT  63 KEY TERMS 64

60

4.3 Spatial Organization in the Visual Cortex

75

The Neural Map in the Striate Cortex (V1) 75 DEMONSTRATION \| Cortical
Magnification of Your Finger The Cortex Is Organized in Columns 77 How
V1 Neurons and Columns Underlie Perception of a Scene 78

76

79

4.4 Beyond the Visual Cortex 54

72

Selective Adaptation 72 METHOD \| Psychophysical Measurement of the
Effect of Selective Adaptation to Orientation 72 Selective Rearing 74

TEST YOuRSELF 4.1

3.4 What Happens as Signals Travel Through the Retina 51 Rod and Cone
Convergence 51 DEMONSTRATION \| Foveal Versus Peripheral Acuity Ganglion
Cell Receptive Fields 55

67

Pathway to the Brain 68 Receptive Fields of Neurons in the Visual Cortex
69 METHOD \| Presenting Stimuli to Determine Receptive Fields 69

43

DEMONSTRATION \| Becoming Aware of What Is in Focus Refractive Errors 44

67

79

Streams for Information About What and Where 80 METHOD \| Brain Ablation
80 Streams for Information About What and How 81 METHOD \| Double
Dissociations in Neuropsychology 81

4.5 Higher-Level Neurons

83

Responses of Neurons in Inferotemporal Cortex Where Perception Meets
Memory 85

83

Something to Consider: "Flexible" Receptive Fields

86

TEST YOuRSELF 4.2 87 THINK ABOUT IT  87 KEY TERMS 87

Contents

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

vii

Chapter 5

Chapter 6

Perceiving Objects and Scenes

DEMONSTRATION \| Perceptual Puzzles in a Scene

89

5.1 Why Is It So Difficult to Design a Perceiving Machine? 91 The
Stimulus on the Receptors Is Ambiguous 91 Objects Can Be Hidden or
Blurred 93 Objects Look Different From Different Viewpoints

5.2 Perceptual Organization

94

94

The Gestalt Approach to Perceptual Grouping 94 Gestalt Principles of
Perceptual Organization 96 Perceptual Segregation 99 TEST YOuRSELF 5.1

103

Perceiving the Gist of a Scene 103 METHOD \| Using a Mask to Achieve
Brief Stimulus Presentations 104 Regularities in the Environment:
Information for Perceiving 105 DEMONSTRATION \| Visualizing Scenes and
Objects 106 The Role of Inference in Perception 107 TEST YOuRSELF 5.2

Visual Attention

123

6.1 What Is Attention? 124 6.2 The Diversity of Attention Research

109

Scanning a Scene with Eye Movements 127 How Does the Brain Deal with
What Happens When the Eyes Move? 128

6.4 Things That Influence Visual Scanning Visual Salience

DEMONSTRATION \| Attentional Capture 130 The Observer's Interests and
Goals 131 Scene Schemas 131 Task Demands 132 TEST YOuRSELF 6.1

Brain Responses to Objects and Faces 110 Brain Responses to Scenes 113
The Relationship Between Perception and Brain Activity 113 Neural Mind
Reading 114 METHOD \| Neural Mind Reading 114

Attention Speeds Responding 133 Attention Influences Appearance 134

TEST YOuRSELF 5.3 120 THINK ABOUT IT  120 KEY TERMS 121

133

6.5 The Benefits of Attention

133

6.6 The Physiology of Attention

116

130

130

5.5 Connecting Neural Activity and Object/Scene Perception 110

Something to Consider: The Puzzle of Faces Developmental Dimension:
Infant Face Perception 118

124

Attention to an Auditory Message: Cherry and Broadbent's Selective
Listening Experiments 124 Attention to a Location in Space: Michael
Posner's Precueing Experiment 125 METHOD \| Precueing 125 Attention as a
Mechanism for Binding Together an Object's Features: Anne Treisman's
Feature Integration Theory 126 DEMONSTRATION \| Visual Search 126

6.3 What Happens When We Scan a Scene by Moving Our Eyes? 127

102

5.3 Recognition by Components 102 5.4 Perceiving Scenes and Objects in
Scenes

89

135

Attention to Objects Increases Activity in Specific Areas of the Brain
135 Attention to Locations Increases Activity in Specific Areas of the
Brain 135 Attention Shifts Receptive Fields 136

6.7 What Happens When We Don't Attend? DEMONSTRATION \| Change Detection

136

137

6.8 Distraction by Smartphones

138

Smartphone Distractions While Driving Distractions Beyond Driving 139

138

6.9 Disorders of Attention: Spatial Neglect and Extinction 141 Something
to Consider: Focusing Attention by Meditating 142 Developmental
Dimension: Infant Attention and Learning Object Names 143 METHOD \|
Head-Mounted Eye Tracking

viii

144

Contents

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

TEST YOuRSELF 6.2 145 THINK ABOUT IT  145 KEY TERMS 146

Perceiving Objects 176 Perceiving Events 176 Social Perception 177
Taking Action 178

Chapter 7

8.2 Studying Motion Perception

Taking Action

When Do We Perceive Motion? 179 Comparing Real and Apparent Motion 180
Two Real-Life Situations We Want to Explain

149

179

180

8.3 The Ecological Approach to Motion Perception 181 8.4 The Corollary
Discharge and Motion Perception 181 TEST YOuRSELF 8.1

7.1 The Ecological Approach to Perception

8.5 The Reichardt Detector 182 8.6 Single-Neuron Responses to Motion

150

The Moving Observer Creates Information in the Environment 150 Reacting
to Information Created by Movement 151 The Senses Work Together 152
DEMONSTRATION \| Keeping Your Balance 152 Affordances: What Objects Are
Used for 152

7.2 Staying on Course: Walking and Driving

154

155

The Importance of Landmarks 156 Cognitive Maps: The Brain's "GPS" 157
Individual Differences in Wayfinding 158 TEST YOuRSELF 7.1

159

7.4 Interacting with Objects: Reaching, Grasping, and Lifting 160
Reaching and Grasping 160 Lifting the Bottle 162 Adjusting the Grip 163
Mirroring Others' Actions in the Brain Predicting People's Intentions
165

164

164

7.6 Action-Based Accounts of Perception 167 Something to Consider:
Prediction is Everywhere 168 Developmental Dimension: Infant Affordances
169 TEST YOuRSELF 7.2 171 THINK ABOUT IT  171 KEY TERMS 172

185

8.7 Beyond Single-Neuron Responses to Motion

186

The Aperture Problem 187 DEMONSTRATION \| Movement of a Bar Across an
Aperture Solutions to the Aperture Problem 187

187

8.8 Motion and the Human Body

7.5 Observing Other People's Actions

183

Experiments Using Moving Dot Displays 184 Lesioning the MT Cortex 185
Deactivating the MT Cortex 185 METHOD \| Transcranial Magnetic
Stimulation (TMS) Stimulating the MT Cortex 185 METHOD \|
Microstimulation 185

Walking 154 Driving a Car 155

7.3 Finding Your Way Through the Environment

182

188

Apparent Motion of the Body 188 Biological Motion Studied by Point-Light
Walkers

188

8.9 Motion Responses to Still Pictures 190 Something to Consider:
Motion, Motion, and More Motion 192 Developmental Dimension: Infants
Perceive Biological Motion 192 TEST YOuRSELF 8.2 194 THINK ABOUT IT  194
KEY TERMS 194

Chapter 9

Perceiving Color

197

Chapter 8

Perceiving Motion

175

9.1 Functions of Color Perception 9.2 Color and Light 199 Reflectance
and Transmission Color Mixing 201

200

9.3 Perceptual Dimensions of Color 8.1 Functions of Motion Perception
Detecting Things

176

176

TEST YOuRSELF 9.1

198

203

204 Contents

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

ix

9.4 The Trichromacy of Color Vision

204

A Little History 204 Color-Matching Evidence for Trichromacy 205 METHOD
\| Color Matching 205 Measuring the Characteristics of the Cone
Receptors 205 The Cones and Trichromatic Color Matching 206 Color Vision
with Only One Pigment: Monochromacy 207 Color Vision with Two Pigments:
Dichromacy 208 TEST YOuRSELF 9.2

210

9.5 The Opponency of Color Vision

210

Behavioral Evidence for Opponent-Process Theory 210 METHOD \| Hue
Cancellation 211 Physiological Evidence for Opponent-Process Theory 211
Questioning the Idea of Unique Hues 213

9.6 Color Areas in the Cortex TEST YOuRSELF 9.3

213

214

9.7 Color in the World: Beyond Wavelength Color Constancy

216

244

246

10.7 Perceiving Size

247

The Holway and Boring Experiment 247 Size Constancy 250 DEMONSTRATION \|
Perceiving Size at a Distance 250 DEMONSTRATION \| Size--Distance
Scaling and Emmert's Law 250

10.8 Illusions of Depth and Size

252

The Müller-Lyer Illusion 252 DEMONSTRATION \| The Müller-Lyer Illusion
with Books The Ponzo Illusion 254 The Ames Room 254

Binocular Disparity 257 Pictorial Cues 257 METHOD \| Preferential
Reaching

DEMONSTRATION \| The Penumbra and Lightness Perception 222 DEMONSTRATION
\| Perceiving Lightness at a Corner

TEST YOuRSELF 10.1

Something to Consider: The Changing Moon Developmental Dimension: Infant
Depth Perception 257

215

215

DEMONSTRATION \| Adapting to Red Lightness Constancy 220

10.5 The Physiology of Binocular Depth Perception 243 10.6 Depth
Information Across Species

222

Something to Consider: We Perceive Color from Colorless Wavelengths 223
Developmental Dimension: Infant Color Vision 225 TEST YOuRSELF 9.4 226
Think About It 226 KEY TERMS 227

253

255

258

TEST YOuRSELF 10.2 259 Think About It 259 Key Terms  259

Chap ter 11

Hearing

263

Chapter 10

Perceiving Depth and Size

229

11.1 Physical Aspects of Sound

10.1 Perceiving Depth 10.2 Oculomotor Cues

229

10.3 Monocular Cues

11.2 Perceptual Aspects of Sound

231

DEMONSTRATION \| Feelings in Your Eyes

Thresholds and Loudness Pitch 270 Timbre 271

231

231

Pictorial Cues 231 Motion-Produced Cues 234 DEMONSTRATION \| Deletion
and Accretion

10.4 Binocular Depth Information

TEST YOuRSELF 11.1

268

268

271

11.3 From Pressure Changes to Electrical Signals

234 236

Disparity (Geometrical) Creates Stereopsis (Perceptual) The
Correspondence Problem 242

272

The Outer Ear 272 The Middle Ear 272 The Inner Ear 273

236

DEMONSTRATION \| Two Eyes: Two Viewpoints Seeing Depth with Two Eyes 236
Binocular Disparity 238

x

264

Sound as Pressure Changes 264 Pure Tones 265 METHOD \| Using Decibels to
Shrink Large Ranges of Pressures 266 Complex Tones and Frequency Spectra
267

240

11.4 How Frequency Is Represented in the Auditory Nerve 276 Békésy
Discovers How the Basilar Membrane Vibrates

276

Contents

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

The Cochlea Functions as a Filter 277 METHOD \| Neural Frequency Tuning
Curves 278 The Outer Hair Cells Function as Cochlear Amplifiers TEST
YOuRSELF 11.2

Interactions in the Brain 307 Echolocation in Blind People 307 Listening
to or Reading a Story 308

278

279

11.5 The Physiology of Pitch Perception: The Cochlea

280

Place and Pitch 280 Temporal Information and Pitch 281 Problems
Remaining to Be Solved 281

Chapter 13

11.6 The Physiology of Pitch Perception: The Brain 282 The Pathway to
the Brain Pitch and the Brain 282

11.7 Hearing Loss

TEST YOuRSELF 12.2 309 Think About It 309 Key Terms  309

Perceiving Music

282

284

Presbycusis 284 Noise-Induced Hearing Loss Hidden Hearing Loss 285

284

Something to Consider: Explaining Sound to an 11-Year Old 286
Developmental Dimension: Infant Hearing 286 Thresholds and the
Audibility Curve 286 Recognizing Their Mother's Voice 287

Chapter 12

Hearing in the Environment

13.1 What Is Music? 311 13.2 Does Music Have an Adaptive Function? 13.3
Outcomes of Music 313

291

13.4 Musical Timing

314

The Beat 315 Meter 315 Rhythm 316 Syncopation 316 The Power of the Mind

317

13.5 Hearing Melodies

12.1 Sound Source Localization

293

12.2 The Physiology of Auditory Localization

296

The Jeffress Neural Coincidence Model 296 Broad ITD Tuning Curves in
Mammals 297 Cortical Mechanisms of Localization 298

12.3 Hearing Inside Rooms

299

Perceiving Two Sounds That Reach the Ears at Different Times 300
Architectural Acoustics 301 302

12.4 Auditory Scene Analysis

302

Simultaneous Grouping 303 Sequential Grouping 303

Something to Consider: Interactions Between Hearing and Vision 306 The
Ventriloquism Effect 306 The Two-Flash Illusion 306 Understanding Speech
306

Organized Notes Intervals 319 Trajectories 320 Tonality 320

319

TEST YOuRSELF 13.1

321

13.6 Creating Emotions

292

Binaural Cues for Sound Localization Spectral Cues for Localization 294

312

Musical Training Improves Performance in Other Areas Music Elicits
Positive Feelings 313 Music Evokes Memories 313

TEST YOuRSELF 11.3 288 THINK ABOUT IT  288 KEY TERMS 288

TEST YOuRSELF 12.1

311

313

319

321

Structural Features Linking Music and Emotion 322 Expectancy and Emotion
in Music 323 METHOD \| Studying Syntax in Language Using the
Event-Related Potential 323 Physiological Mechanisms of Musical Emotions
325

Something to Consider: Comparing Music and Language Mechanisms in the
Brain 327 Evidence for Shared Mechanisms 327 Evidence for Separate
Mechanisms 327

Developmental Dimension: How Infants Respond to the Beat 329 Newborns'
Response to the Beat 329 Older Infants' Movement to the Beat 329
Infants' Response to Bouncing to the Beat 329 METHOD \| Head-Turning
Preference Procedure 330

13.7 Coda: Music Is "Special"

330

TEST YOuRSELF 13.2 331 THINK ABOUT IT  331 KEY TERMS 331 Contents

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

xi

Mechanoreceptors 358 Pathways From Skin to Cortex and Within the Cortex
Somatosensory Areas in the Cortex 361

Chapter 14

Perceiving Speech

335

15.2 Perceiving Details

362

METHOD \| Measuring Tactile Acuity

363

Receptor Mechanisms for Tactile Acuity 363 DEMONSTRATION \| Comparing
Two-Point Thresholds Cortical Mechanisms for Tactile Acuity 364

15.3 Perceiving Vibration and Texture 14.1 The Speech Stimulus The
Acoustic Signal 336 Basic Units of Speech 337

TEST YOuRSELF 15.1

14.2 Variability of the Acoustic Signal

338

Variability From Context 338 Variability in Pronunciation 339

14.3 Some History: The Motor Theory of Speech Perception 340 The
Proposed Connection Between Production and Perception 340 The Proposal
That "Speech Is Special" 340 TEST YOuRSELF 14.1

342

14.4 Information for Speech Perception

342

Motor Processes 342 The Face and Lip Movements 343 Knowledge of Language
344 The Meaning of Words in Sentences 345 DEMONSTRATION \| Perceiving
Degraded Sentences 345 DEMONSTRATION \| Organizing Strings of Sounds 346
Learning About Words in a Language 346 TEST YOuRSELF 14.2

347

14.5 Speech Perception in Difficult Circumstances 347 14.6 Speech
Perception and the Brain 349 Something to Consider: Cochlear Implants
351 Developmental Dimension: Infant-Directed Speech 353 TEST YOuRSELF
14.3 354 THINK ABOUT IT  355 KEY TERMS 355

367

368

15.4 Perceiving Objects

368

DEMONSTRATION \| Identifying Objects 368 Identifying Objects by Haptic
Exploration 368 The Cortical Physiology of Tactile Object Perception

15.5 Social Touch

369

371

Sensing Social Touch 371 The Social Touch Hypothesis 371 Social Touch
and the Brain 372 Top-Down Influences on Social Touch

372

Pain Perception 15.6 The Gate Control Model of Pain 15.7 Top-Down
Processes 374

373

Expectation 375 Attention 375 Emotions 376 TEST YOuRSELF 15.2

376

15.8 The Brain and Pain Brain Areas 376 Chemicals and the Brain

376 377

15.9 Social Aspects of Pain

378

Pain Reduction by Social Touch 379 The Effect of Observing Someone
Else's Pain The "Pain" of Social Rejection 380

379

Something to Consider: Plasticity and the Brain 382 Developmental
Dimension: Social Touch in Infants 383 TEST YOuRSELF 15.3 385 THINK
ABOUT IT  385 KEY TERMS 386

Chapter 15

The Cutaneous Senses

364

365

Vibration of the Skin 365 Surface Texture 366 DEMONSTRATION \|
Perceiving Texture with a Pen

336

359

357

Chapter 16

The Chemical Senses

389

Perception by the Skin and Hands 15.1 Overview of the Cutaneous System
The Skin xii

358

358

16.1 Some Properties of the Chemical Senses 16.2 Taste Quality 390

390

Contents

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Something to Consider: The Community of the Senses 411

Basic Taste Qualities 391 Connections Between Taste Quality and a
Substance's Effect 391

16.3 The Neural Code for Taste Quality Structure of the Taste System
Population Coding 393 Specificity Coding 394

TEST YOuRSELF 16.3 415 THINK ABOUT IT  415 KEY TERMS 415

396

397

16.5 The Importance of Olfaction 16.6 Olfactory Abilities 398

397

appendix

Detecting Odors 398 Identifying Odors 398 DEMONSTRATION \| Naming and
Odor Identification 398 Individual Differences in Olfaction 398 Loss of
Smell in COVID-19 and Alzheimer's Disease 399

A The Difference Threshold 417 B Magnitude Estimation and the Power
Function 418 C The Signal Detection Approach 420

16.7 Analyzing Odorants: The Mucosa and Olfactory Bulb 400 The Puzzle of
Olfactory Quality 400 The Olfactory Mucosa 401 How Olfactory Receptor
Neurons Respond to Odorants METHOD \| Calcium Imaging 402 The Search for
Order in the Olfactory Bulb 403 TEST YOuRSELF 16.2

401

404

16.8 Representing Odors in the Cortex

A Signal Detection Experiment

420

The Basic Experiment 421 Payoffs 421 What Does the ROC Curve Tell Us?

422

Signal Detection Theory

405

How Odorants Are Represented in the Piriform Cortex How Odor Objects Are
Represented in the Piriform Cortex 406 How Odors Trigger Memories 407

16.9 The Perception of Flavor

412

Developmental Dimension: Infant Chemical Sensitivity 413

391

16.4 Individual Differences in Taste TEST YOuRSELF 16.1

Correspondences Influences 412

391

405

408

DEMONSTRATION \| Tasting With and Without the Nose 408 Taste and
Olfaction Meet in the Mouth and Nose 408 Taste and Olfaction Meet in the
Nervous System 408 Flavor Is Influenced by Cognitive Factors 410

423

Signal and Noise 423 Probability Distributions 423 The Criterion 423 The
Effect of Sensitivity on the ROC Curve Glossary

424

426

References

445

Name Index

472

Subject Index

483

Flavor Is Influenced by Food Intake: Sensory-Specific Satiety 410

Contents

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

xiii

Methods Determining the Threshold 14 Magnitude Estimation 16 The Setup
for Recording From a Single Neuron 22 Brain Imaging 31 The Resting State
Method of Measuring Functional Connectivity 34 Measuring the Dark
Adaptation Curve 46 Measuring a Spectral Sensitivity Curve 49
Preferential Looking 60 Presenting Stimuli to Determine Receptive Fields
69 Psychophysical Measurement of the Effect of Selective Adaptation to
Orientation 72 Brain Ablation 80 Double Dissociations in Neuropsychology
81 Using a Mask to Achieve Brief Stimulus Presentations 104

Neural Mind Reading 114 Precueing 125 Head-Mounted Eye Tracking 144
Transcranial Magnetic Stimulation (TMS) 185 Microstimulation 185 Color
Matching 205 Hue Cancellation 211 Preferential Reaching 258 Using
Decibels to Shrink Large Ranges of Pressures 266 Neural Frequency Tuning
Curves 278 Studying Syntax in Language Using the Event-Related Potential
323 Head-Turning Preference Procedure 330 Measuring Tactile Acuity 363
Calcium Imaging 402

xiv

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Demonstrations Perceiving a Picture 10 Becoming Aware of the Blind Spot
43 Filling in the Blind Spot 43 Becoming Aware of What Is in Focus 44
Foveal Versus Peripheral Acuity 54 Cortical Magnification of Your Finger
76 Perceptual Puzzles in a Scene 89 Visualizing Scenes and Objects 106
Visual Search 126 Attentional Capture 130 Change Detection 137 Keeping
Your Balance 152 Movement of a Bar Across an Aperture 187 Adapting to
Red 216 The Penumbra and Lightness Perception 222

Perceiving Lightness at a Corner 222 Feelings in Your Eyes 231 Deletion
and Accretion 234 Two Eyes: Two Viewpoints 236 Perceiving Size at a
Distance 250 Size--Distance Scaling and Emmert's Law 250 The Müller-Lyer
Illusion with Books 253 Perceiving Degraded Sentences 345 Organizing
Strings of Sounds 346 Comparing Two-Point Thresholds 364 Perceiving
Texture with a Pen 367 Identifying Objects 368 Naming and Odor
Identification 398 Tasting With and Without the Nose 408

xv

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Preface by Bruce Goldstein

A

long, long time ago, Ken King, the psychology editor of Wadsworth
Publishing Co., knocked on the door to my office at the University of
Pittsburgh, came in, and proposed that I write a textbook titled
Sensation and Perception. This led me to begin writing the first edition
of Sensation and Perception in 1977, the year Star Wars made its debut
in theaters and when the first mass-market personal computer was
introduced. While Luke Skywalker was dealing with Darth Vader and was
working to master the Force, I was dealing with understanding the
perception literature and was working to present the results in this
literature as a story that would be both interesting to students and
would help them understand how perception works. How do you tell a story
in a textbook? This is a problem I grappled with when writing the first
edition, because while the textbooks available at that time presented
"the facts," they did so in a way that wasn't very interesting or
inviting to students. I decided, therefore, that I would create a story
about perception that was a narrative in which one idea followed from
another and that related the results of research to everyday
experience---a story describing both the historical background behind
scientific discoveries and the reasoning behind scientific conclusions.
The result was the first edition of Sensation and Perception, which was
published in 1980 (Figure P.1). The

book was popular, largely because of my decision to present not just
facts, but also to present the story and reasoning behind the facts. The
producers of Star Wars had no idea, when they released their first
movie, that it would give birth to a franchise that is still alive
today. Similarly, I had no idea, when the first edition of Sensation and
Perception was published, that it would be the first of 11 editions. The
book you are reading was, in a sense, born as the first edition was
being written in 1977. But a lot has happened since then. One indication
of this is the graph in Figure P.2, which plots the number of references
in this edition by decade. Most of the references to the left of the
dashed line appeared in the first edition. The ones to the right were
published after the first edition. Another measure of the evolution of
this book is provided by the illustrations. The first edition had 313
illustrations. Of these, 116 have made it all the way to this edition
(but transformed from black and white into color). This edition has 440
illustrations that weren't in the first edition, for a total of 556. But
enough history. Most users of this book are probably more interested in
"what have you done for the book lately?" Returning to illustrations, 90
of the illustrations in this edition

406

Number of references

400

300 198

200 117 100 26 0 Before 1940

Figure P.1 The cover of the first edition of Sensation and Perception
(1980), which featured a reproduction of the painting Vega-Nor 1960, by
Victor Vasarely, from the Albright-Knox Art Gallery, Buffalo, New York.

410

13 1940

34

57

80 17

1950

1960

1970 1980 Decades

1990

2000

2010

2020

Figure P.2 The number of reference citations in this edition, by decade.
For example, 1970 includes all references dated from 1970 to 1979. This
means that all of the references to the right of the dashed vertical
line appeared 1980 or after, and so were in editions after the first
edition. The line on the right is dashed because it connects to 2020,
which includes references only from 2020 and the beginning of 2021, not
a whole decade.

xvi

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

are new since the 10th edition. There's much more that's new since the
10th edition when it comes to content, which I'll get to shortly. But
first, one of the most important things about this edition is that it
still contains the popular content and teaching features that have been
standbys for many editions. These features are as follows:

Features

expanded in this edition. This feature, which appears at the end of
chapters, focuses on perception in infants and young children. The
following feature provides digital learning opportunities that support
the material in the text: ■■

The following features focus on student engagement and learning: ■■

■■

■■

Learning Objectives. Learning Objectives, which provide a preview of
what students can expect to learn from each chapter, appear at the
beginning of each chapter. Test Yourself. Test Yourself questions appear
in the middle and at the end of each chapter. These questions are broad
enough that students have to unpack the questions themselves, thereby
making students more active participants in their studying. Think About
It. The Think About It section at the end of each chapter poses
questions that require students to apply what they have learned and that
take them beyond the material in the chapter.

For students: ■■

■■

■■

The following feature enables students to participate in perceptual
activities related to what they are reading: ■■

Demonstrations. Demonstrations have been a popular feature of this book
for many editions. They are integrated into the flow of the text and are
easy enough to be carried out with little trouble, thereby maximizing
the probability that students will do them. See list on page xv.

The following features highlight different categories of material: ■■

■■

■■

Methods. It is important not only to present the facts of perception,
but also to make students aware of how these facts were obtained.
Highlighted Methods sections, which are integrated into the ongoing
discussion, emphasize the importance of methods, and the highlighting
makes it easier to refer back to them when referenced later in the book.
See list on page xiv. Something to Consider. This end-of-chapter feature
offers the opportunity to consider especially interesting phenomena and
new findings. A few examples include The Puzzle of Faces (Chapter 5),
Focusing Attention by Meditating (Chapter 6), The Changing Moon (Chapter
10), and Community of the Senses (Chapter 16). Developmental Dimensions.
The Developmental Dimension feature, which was introduced in the ninth
edition, has proven to be popular and so has been continued and

MindTap for Sensation and Perception engages and empowers students to
produce their best work--- consistently. For those courses that include
MindTap, the textbook is supplemented with videos, activities, apps, and
much more. MindTap creates a unique learning path that fosters increased
comprehension and efficiency. MindTap delivers real-world relevance with
activities and assignments that help students build critical thinking
and analytic skills that will transfer to other courses and their
professional lives. MindTap helps students stay organized and efficient
with a single destination that reflects what's important to the
instructor, along with the tools students need to master the content.
MindTap empowers and motivates students with information that shows
where they stand at all times---both individually and compared to the
highest performers in class. Additionally, for instructors, MindTap
allows you to:

■■

■■

■■

■■

Control what content students see and when they see it with a learning
path that can be used as is, or matched to your syllabus exactly. Create
a unique learning path of relevant readings, multimedia, and activities
that move students up the learning taxonomy from basic knowledge and
comprehension to analysis, application, and critical thinking. Integrate
your own content into the MindTap Reader, using your own documents or
pulling from sources like RSS feeds, YouTube videos, websites, Google
Docs, and more. Use powerful analytics and reports that provide a
snapshot of class progress, time in course, engagement, and completion.

In addition to the benefits of the platform, MindTap for Sensation and
Perception includes: ■■

Exploration. The MindTap Exploration feature enables students to view
experimental stimuli, perceptual demonstrations, and short film clips
about the research being discussed. These features have been updated in
this edition, and new items have been added to the labs carried over
from the ninth edition. Preface

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

xvii

Most of these items have been generously provided by researchers in
vision, hearing, and perceptual development.

New to This Edition This edition offers many improvements in
organization, designed to make the text read more smoothly and flow more
logically. In addition, each chapter has been updated to highlight new
advances in the field, supported by many new references. Here are a few
examples of changes in this edition.

Key Terms New to This Edition The following key terms represent methods,
concepts, and topics that are new to this edition: Aberration Action
affordance Adaptive optical imaging Adult-directed speech Affective
function of touch Alzheimer's disease Arch trajectory Automatic speech
recognition (ASR) Cloze probability task COVID-19 Dopamine Duple meter
Early right anterior negativity (ERAN) Experience sampling Figural cues
Functional connectivity Hand dystonia Head-mounted eye tracking
Interpersonal touching Meditation Metrical structure Microneurography

Mild cognitive impairment Mind wandering Multimodal interactions Munsell
color system Music-evoked autobiographical memory (MEAM) Musical phrases
Novelty-preference procedure Odor-evoked autobiographical memory
Predictive coding Predictive remapping of attention Seed location
Semitone Social pain Social touch Social touch hypothesis Sustentacular
cell Syncopation Syntax, musical Task-related fMRI Temporal structure
Triple meter

Revisions and New Material Each chapter has been revised in two ways:
(1) Organization: Chapters and sections within chapters have been
reorganized to achieve smoother f low from one idea to

xviii

the next; (2) Updating: Material describing new experimental results and
new approaches in the field has been added. New "Developmental
Dimensions" topics are indicated by DD and new "Something to Consider"
topics by STC.

Perceptual Principles (Chapters 1--4) The initial chapters, which
introduce basic concepts and research approaches, have been completely
reorganized to make the opening of the book more inviting to students,
to create a more logical and smooth flow, and to include all of the
senses up-front. Discussing a number of senses in Chapter 2 corrects a
problem perceived by some teachers, who felt that the opening of the
10th edition was too "visioncentric." Chapter 2 also contains a new
section discussing structural and functional connectivity.

Perceiving Objects and Scenes (Chapter 5) Updated section on computer
vision Predictive coding ■■ Pre-wiring of functional connectivity for
faces in human infants ■■ ■■

Visual Attention (Chapter 6) Predictive remapping of attention Mere
presence of smartphones can negatively impact performance. ■■
Head-mounted tracking devices to measure infant attention ■■ STC:
Focusing Attention by Meditating ■■ DD: Infant Attention and Learning
Object Names ■■ ■■

Taking Action (Chapter 7) New material on proprioception
Hippocampus-related navigation differences in non-taxi drivers ■■ STC:
Prediction Is Everywhere ■■ DD: Infant Affordances ■■ ■■

Perceiving Motion (Chapter 8) Changes in motion perception over the
first year Motion and social perception ■■ STC: Motion, Motion, and More
Motion ■■ ■■

Perceiving Color (Chapter 9) Color and judging emotions of facial
expressions Reevaluation of the idea of "unique hues" ■■ Social
functions of color ■■ Color areas in cortex sandwiched between face and
place areas ■■ #TheDress and what it tells us about individual
differences and color constancy ■■ Novelty-preference procedure for
determining infant color categorization ■■ ■■

Preface

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Perceiving Depth and Size (Chapter 10) Praying mantis cinema used to
test binocular depth perception ■■ STC: The Changing Moon ■■

Hearing (Chapter 11) ■■

STC: Explaining Sound to an 11-Year-Old

Hearing in the Environment (Chapter 12) ■■ ■■

Human echolocation STC: Interactions Between Hearing and Vision

Perceiving Music (Chapter 13) New chapter, greatly expanding coverage of
music, which was part of Chapter 12 in the 10th edition ■■ Music and
social bonding ■■ Therapeutic effects of music ■■ Infant emotional
response to music ■■ Chemistry of musical emotions ■■ Effect of
syncopation on music-elicited movement ■■ Cross-cultural similarities ■■
Music and prediction ■■ Behavioral and physiological differences between
music and speech ■■ DD: How Infants Respond to the Beat ■■

Perceiving Speech (Chapter 14) Role of motor processes in speech
perception ■■ STC: Cochlear Implants ■■ DD: Infant-Directed Speech ■■

The Cutaneous Senses (Chapter 15) Social touch and CT afferents Cortical
responses to surface texture ■■ Top-down influences on social touch ■■
Pain reduction by social touching ■■ Pre- and post-partum touch
perception ■■ STC: Plasticity and the Brain ■■ DD: Social Touch in
Infants ■■ ■■

The Chemical Senses (Chapter 16) Comparing human and animal perception
of scent ■■ Music can influence flavor ■■ Color can influence flavor ■■
Odors can influence attention and performance ■■ Loss of smell in
COVID-19 and Alzheimer's ■■ STC: Community of the Senses ■■

Acknowledgments It is a pleasure to acknowledge the following people who
worked tirelessly to turn the manuscript into an actual book. Without
these people, this book would not exist, and both Laura and I are
grateful to all of them.

Cazzie Reyes, Associate Product Manager, for providing resources to
support the book. ■■ Jacqueline (Jackie) Czel, Content Manager, for
coordinating all of the components of the book as it was being produced.
■■ Lori Hazzard, Senior Project Manager of MPS Limited, for taking care
of the amazing number of details involved in turning my manuscript into
a book. Thank you, Lori, not only for taking care of details, but for
your flexibility and your willingness to take care of all of those
"special requests" that I made during the production process. ■■ Bethany
Bourgeois, for the striking cover. ■■ Heather Mann, for her expert and
creative copyediting. ■■

In addition to the help received from people on the editorial and
production side, Laura and I also received a great deal of help from
perception researchers. One of the things I have learned in my years of
writing is that other people's advice is crucial. The field of
perception is a broad one, and we have relied heavily on the advice of
experts in specific areas to alert us to emerging new research and to
check the content for accuracy. The following is a list of "expert
reviewers," who checked the relevant chapter from the 10th edition for
accuracy and completeness, and provided suggestions for updating.
Chapter 5 Joseph Brooks Keele University Chapter 6 Marisa Carrasco New
York University John McDonald Simon-Fraser University Chapter 7 Sarah
Creem-Reghr University of Utah Jonathan Marotta University of Manitoba
Chapter 8 Emily Grossman University of California, Irvine Duje Tadin
University of Rochester Chapter 9 David Brainard University of
Pennsylvania

Bevil Conway Wellesley College Chapter 10 Gregory DeAngeles University
of Rochester Jenny Read University of Newcastle Andrew Welchman
University of Cambridge Chapter 11 Daniel Bendor University College
London Nicholas Lesica University College London Chapter 12 Yale Cohen
University of Pennsylvania John Middlebrooks University of California,
Irvine William Yost Arizona State University

Preface

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

xix

Chapter 13 Bill Thompson Macquarie University

Chapter 15 Sliman Bensmaia University of Chicago

Chapter 14 Laura Dilley Michigan State University

Tor Wager Dartmouth College

Phil Monahan University of Toronto

Chapter 16 Donald Wilson New York University

Howard Nussbaum University of Chicago I also thank the following people
who donated photographs and research records for illustrations that are
new to this edition. Sliman Bensmaia University of Chicago

Jenny Reed University of Newcastle

Jack Gallant University of California, Berkeley

István Winkler University of Helsinki

Daniel Kish Visoneers.org

xx

A Note on the Writing of this Edition Taking the 10th edition as our
starting point, this edition was created by myself (B. G.) and Laura
Cacciamani. Laura revised Chapters 1--5, and is therefore responsible
for the greatly improved organization of Chapters 1--4, which introduce
the field of perception and which set the stage for the discussion of
the different aspects of perception in the chapters that follow. I
revised Chapters 6--16. We read and commented on each other's chapters
and made suggestions regarding both the writing and the content, so this
was, in a very real sense, a collaborative project.

Chen Yu Indiana University

Preface

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Eleventh Edition

Sensation and Perception

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Perception is a miracle. Somehow, the markings on this page become a
sidewalk, stone walls, and a quaint ivy-covered house. Even more
miraculous is that if you were standing in the real scene, the flat
image on the back of your eye is transformed into threedimensional space
that you can walk through. This book explains how this miracle happens.
Bruce Goldstein

Learning Objectives After studying this chapter, you will be able to ...
■■ Explain the seven steps of the perceptual process. ■■ Differentiate
between "top-down" and "bottom-up" processing. ■■ Describe how knowledge
can influence perception. ■■ Understand how perception can be studied by
determining

the relationships between stimulus and behavior, stimulus and
physiology, and physiology and behavior.

■■ Explain "absolute threshold" and "difference threshold" and the

various methods that can be used to measure them. ■■ Describe how
perception above threshold can be measured by

considering five questions about the perceptual world. ■■ Understand the
importance of the distinction between physical

stimuli and perceptual responses.

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

C ha p ter 1

Introduction to Perception Chapter Contents 1.1 Why Read This Book? 1.2
Why Is This Book Titled Sensation and Perception? 1.3 The Perceptual
Process Distal and Proximal Stimuli (Steps 1 and 2) Receptor Processes
(Step 3) Neural Processing (Step 4) Behavioral Responses (Steps 5--7)
Knowledge DEMONSTRATION: Perceiving a

METHOD: Magnitude Estimation

1.4 Studying the Perceptual Process The Stimulus--Behavior Relationship
(A) The Stimulus--Physiology Relationship (B) The Physiology--Behavior
Relationship (C) Test Yourself 1.1

SOMETHING TO CONSIDER: Why is the Difference Between Physical and
Perceptual Important? Test Yourself 1.2 THINK ABOUT IT

1.5 Measuring Perception Measuring Thresholds METHOD: Determining the
Threshold

Measuring Perception Above Threshold

Picture

Some Questions We Will Consider: ■■ Why should you read this book?
(p. 5) ■■ What is the sequence of steps from looking at a stimulus

like a tree to perceiving the tree? (p. 6) ■■ What is the difference
between perceiving something and

recognizing it? (p. 9) ■■ How do perceptual psychologists go about
measuring the

varied ways that we perceive the environment? (p. 11)

I

n July of 1958, the New York Times published an intriguing article
entitled, "Electronic 'Brain' Teaches Itself." The article described a
new, potentially revolutionary technological advancement: "... an
electronic computer named the Perceptron which, when completed in about
a year, is expected to be the first non-living mechanism able to
perceive, recognize, and identify its surroundings without human
training or control." The first Perceptron, created by psychologist
Frank Rosenblatt (1958), was a room-sized five-ton computer (Figure 1.1)
that could teach itself to distinguish between basic images, such as
cards with markings on the left versus on the right. Rosenblatt claimed
that this device could "... learn to recognize

similarities or identities between patterns of optical, electrical, or
tonal information, in a manner which may be closely analogous to the
perceptual processes of a biological brain" (Rosenblatt, 1957). A truly
astounding claim! And, in fact, Rosenblatt and other computer scientists
in the 1950s and 1960s proposed that it would take only about a decade
or so to create a "perceiving machine," like the Perceptron, that could
understand and navigate the environment with humanlike ease. So how did
Rosenblatt's Perceptron do in its attempt to duplicate human perception?
Not very well, since it took 50 trials to learn the simple task of
telling whether a card had a mark on the left or on the right, and it
was unable to carry out more complex tasks. It turns out that perception
is much more complex than Rosenblatt or his Perceptron could comprehend.
This invention therefore received mixed feedback from the field, and
ultimately this line of research was dropped for many years. However,
Rosenblatt's idea that a computer could be trained to learn perceptual
patterns laid the groundwork for a resurgence of interest in this area
in the 1980s, and many now consider Rosenblatt's work to be a key
precursor to modern artificial intelligence (Mitchell, 2019; Perez et
al., 2017).

3

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Now over 60 years later, although great strides have been made in
computer vision, computers still can't perceive as well as humans (Liu
et al., 2019). Consider Figure 1.2, which shows pictures similar to
those that were provided to a computer, which then created descriptions
for each image (Fei-Fei, 2015). For example, the computer identified a
scene similar to the one in Figure 1.2a as "a large plane sitting on a
runway." But mis-

(b) 
(c) 

© Cengage 2021

Division of Rare and Manuscript Collections, Cornell University Library

Figure 1.1 Frank Rosenblatt's original "Perceptron" machine.

takes occur, as when a picture similar to the one in Figure 1.2b was
identified as "a young boy holding a baseball bat." The computer's
problem is that it doesn't have the huge storehouse of information that
humans begin accumulating as soon as they are born. If a computer has
never seen a toothbrush, it identifies it as something with a similar
shape. And, although the computer's response to the airplane picture is
accurate, it is beyond the computer's capabilities to recognize that
this is a picture of airplanes on display, perhaps at an air show, and
that the people are not passengers but are visiting the air show. So on
one hand, we have come a very long way from the first attempts in the
1950s to design computervision systems, but to date, humans still
out-perceive computers. Why did early computer scientists think they
would be able to create a computer capable of human-like perception
within a decade or so, when it has actually taken over 60 years, and we
still aren't there yet? One answer to this question is that
perception---the experiences that result from stimulation of the
senses---is something we usually accomplish so easily that we often
don't even give it a second thought. Perception seems to "just happen."
We open our eyes and see a landscape, a campus building, or a group of
people. But the reality, as you will appreciate after reading this book,
is that the mechanisms responsible for perception are extremely complex.
Throughout this book, we'll see many more examples illustrating how
complex and amazing perception is. Our goal is to understand how humans
and animals perceive, starting with the detectors---located in the eyes,
ears, skin, tongue, nose, and mouth---and then moving on to the
"computer"---the brain. We want to understand how we sense things in the
environment and interact with them. In this chapter, we will consider
some practical reasons for studying perception, how perception occurs in
a sequence of steps, and how perception can be measured.

Figure 1.2 Pictures similar to one that a computer vision program
identified as (a) "a large plane sitting on a runway" and (b) "a young
boy holding a baseball bat." (Adapted from Fei-Fei, 2015) 4

Chapter 1  Introduction to Perception

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

1.1 Why Read This Book? The most obvious answer to the question "Why
read this book?" is that it is required reading for a course you are
taking. Thus, it is probably an important thing to do if you want to get
a good grade. But beyond that, there are a number of other reasons for
reading this book. For one thing, it will provide you with information
that may be helpful in other courses and perhaps even your future
career. If you plan to go to graduate school to become a researcher or
teacher in perception or a related area, this book will provide you with
a solid background to build on. In fact, many of the research studies
you will read about were carried out by researchers who read earlier
editions of this book when they were undergraduates. The material in
this book is also relevant to future studies in medicine or related
fields, because much of our discussion is about how the body operates.
Medical applications that depend on an understanding of perception
include devices to restore perception to people who have lost vision or
hearing and treatments for pain. Other applications include autonomous
vehicles that can find their way through unfamiliar environments, face
recognition systems that can identify people as they pass through
airport security, speech recognition systems that can understand what
someone is saying, and highway signs that are visible to drivers under a
variety of conditions. But reasons to study perception extend beyond the
possibility of creating or understanding useful applications. Studying
perception can help you become more aware of the nature of your own
perceptual experiences. Many of the everyday experiences that you take
for granted---such as tasting food, looking at a painting in a museum,
or listening to someone talking---can be appreciated at a deeper level
by considering questions such as "Why do I lose my sense of taste when I
have a cold?" "How do artists create an impression of depth in a
picture?" and "Why does an unfamiliar language sound as if it is one
continuous stream of sound, without breaks between words?" This book
will not only answer these questions but will answer other questions
that you may not have thought of, such as "Why don't I see colors at
dusk?" and "How come the scene around me doesn't appear to move as I
walk through it?" Thus, even if you aren't planning to become a
physician or an autonomous vehicle designer, you will come away from
reading this book with a heightened appreciation of both the complexity
and the beauty of the mechanisms responsible for your perceptual
experiences, and perhaps even with an enhanced awareness of the world
around you. Because perception is something you experience constantly,
knowing about how it works is interesting in its own right. To
appreciate why, consider what you are experiencing right now. If you
touch the page of this book, or look out at what's around you, you might
get the feeling that you are perceiving exactly what is "out there" in
the environment. After all, touching this page puts you in direct
contact with it, and it

seems likely that what you are seeing is what is actually there. But one
of the things you will learn as you study perception is that everything
you see, hear, taste, feel, or smell is the result of the activity in
your nervous system and your knowledge gained from past experience.
Think about what this means. There are things out there that you want to
see, hear, taste, smell, and feel. But the only way to achieve this is
by activating sensory receptors in your body designed to respond to
light energy, sound energy, chemical stimuli, and pressure on the skin.
When you run your fingers over the pages of this book, you feel the page
and its texture because the pressure and movement are activating small
receptors just below the skin. Thus, whatever you are feeling depends on
the activation of these receptors. If the receptors weren't there, you
would feel nothing, or if they had different properties, you might feel
something different from what you feel now. This idea that perception
depends on the properties of the sensory receptors is one of the themes
of this book. A few years ago, I received an email from a student (not
one of my own, but from another university) who was using an earlier
edition of this book.1 In her email, "Jenny" made a number of comments
about the book, but the one that struck me as being particularly
relevant to the question "Why read this book?" is the following: "By
reading your book, I got to know the fascinating processes that take
place every second in my brain, that are doing things I don't even think
about." Your reasons for reading this book may turn out to be totally
different from Jenny's, but hopefully you will find out some things that
will be useful, or fascinating, or both.

1.2 Why Is This Book Titled Sensation and Perception? You may have
noticed that so far in our discussion we've used the word perception
quite a lot, but haven't mentioned sensation, even though the title of
this book is Sensation and Perception. Why has sensation been ignored?
To answer this question, let's consider the terms sensation and
perception. When a distinction is made between sensation and perception,
sensation is often identified as involving simple "elementary" processes
that occur right at the beginning of a sensory system, such as when
light reaches the eye, sound waves enter the ear, or your food touches
your tongue. In contrast, perception is identified with complex
processes that involve higher-order mechanisms such as interpretation
and memory that involve activity in the brain---for instance,
identifying the food you're eating 1

Who is "I"? In various places in the book you will see first-person
references such as this one ("I received an email") or others, like "a
student in my class," or "I tell my students," or "I had an interesting
experience." Because this book has two authors, you may wonder who I or
my is. The answer is that, unless otherwise noted, it is author B. G.,
because most of the first-person references in this edition are carried
over from the 10th edition.

1.2 Why Is This Book Titled Sensation and Perception?

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

5

(a)

(b) 
(c) 

Figure 1.3 (a) One dot, (b) a triangle, (c) a house. What do these
stimuli tell us about sensations and perceptions? See text for
discussion.

and remembering the last time you had it. It is therefore often stated,
especially in introductory psychology textbooks, that sensation involves
detecting elementary properties of a stimulus (Carlson, 2010), and
perception involves the higher brain functions involved in interpreting
events and objects (Myers, 2004). Keeping this distinction in mind,
let's consider an example from the sense of vision in Figure 1.3. Figure
1.3a is extremely simple---a single dot. Let's for the moment assume
that this simplicity means that there is no interpretation or
higher-order processes, so sensation is involved. Looking at Figure
1.3b, with three dots, we might now think that we are dealing with
perception, because we interpret the three dots as creating a triangle.
Going even further, we can say that Figure 1.3c, which is made up of
many dots, is a "house." Surely this must be perception because it
involves many dots and our past experience with houses. But let's return
to Figure 1.3a, which we called a dot. As it turns out, even a stimulus
this simple can be seen in more than one way. Is this a black dot on a
white background or a hole in a piece of white paper? Now that
interpretation is involved, does our experience with Figure 1.3a become
perception? This example illustrates that deciding what is sensation and
what is perception is not always obvious, or even that useful. As we
will see in this book, there are experiences that depend heavily on
processes that occur right at the beginning of a sensory system, in the
sensory receptors or nearby, and there are other experiences that depend
on interpretation and past experiences, using information stored in the
brain. But this book takes the position that calling some processes
sensation and others perception doesn't add anything to our
understanding of how our sensory experiences are created, so the term
perception is used almost exclusively throughout this book. Perhaps the
main reason not to use the term sensation is that, with the exception of
papers on the history of perception research (Gilchrist, 2012), the term
sensation appears only rarely 6

in modern research papers (mainly in papers on the sense of taste, which
refer to taste sensations, and touch which refer to touch sensations),
whereas the term perception is extremely common. Despite the fact that
introductory psychology books may distinguish between sensation and
perception, most perception researchers don't make this distinction. So
why is this book called Sensation and Perception? Blame history.
Sensation was discussed in the early history of perceptual psychology,
and courses and textbooks followed suit by including sensation in their
titles. But while researchers eventually stopped using the term
sensation, the titles of the courses and books remained the same. So
sensations are historically important (we will discuss this briefly in
Chapter 5), but as far as we are concerned, everything that involves
understanding how we experience the world through our senses comes under
the heading of perception. With that bit of terminology out of the way,
we are now ready to describe perception as involving a number of steps,
which we will call the perceptual process. These steps begin with a
stimulus in the environment and end with perceiving the stimulus,
recognizing it, and taking action relative to it.

1.3 The Perceptual Process Perception happens at the end of what can be
described, with apologies to the Beatles, as a long and winding road
(McCartney, 1970). This road begins outside of you, with stimuli in the
environment---trees, buildings, birds chirping, smells in the air---and
ends with the behavioral responses of perceiving, recognizing, and
taking action. We picture this journey from stimuli to responses by the
seven steps in Figure 1.4, called the perceptual process. The process
begins with a stimulus in the environment (a tree in this example) and
ends with the conscious experiences of perceiving the tree, recognizing
the tree, and taking action with respect to the tree (like walking up to
take a closer look). Although this example of perceiving a tree is from
the sense of vision, keep in mind as we go through these steps that the
same general process applies to the other senses as well. Furthermore,
because this process is involved in everything we will be describing in
this book, it is important to note that Figure 1.4 is a simplified
version of what happens. First, many things happen within each "box."
For example, "neural processing" involves understanding not only how
cells called neurons work, but how they interact with each other and how
they operate within different areas of the brain. Another reason we say
that our process is simplified is that steps in the perceptual process
do not always unfold in a one-follows-the-other order. For example,
research has shown that perception ("I see something") and recognition
("That's a tree") may not always happen one after another, but could
happen at the same time, or even in reverse order (Gibson & Peterson,
1994; Peterson, 2019). And when perception or recognition leads to
action ("Let's have a closer look at the tree"), that action could
change perception and recognition ("Looking closer shows that what I
thought was an oak tree turns out to be a maple tree"). This is why
there are bidirectional arrows between perception, recognition, and
action. In addition, there is an arrow from "action"

Chapter 1  Introduction to Perception

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Perception 5

Recognition 6

Action 7

Distal stimulus Neural processing

Knowledge

4

Receptor processes 3

Stimulus hits the receptors 2

Stimulus in the environment 1

Figure 1.4 The perceptual process. These seven steps, plus "knowledge"
inside the person's brain, summarize the major events that occur between
the time a person looks at the stimulus in the environment (the tree in
this example) and perceives the tree, recognizes it, and takes action
toward it. Information about the stimulus in the environment (the distal
stimulus; Step 1) hits the receptors, resulting in the proximal stimulus
(Step 2), which is a representation of the stimulus on the retina.
Receptor processes (Step 3) include transduction and the shaping of
perception by the properties of the receptors. Neural processing (Step
4) involves interactions between the electrical signals traveling in
networks of neurons. Finally, the behavioral responses---perception,
recognition, and action---are generated (Steps 5--7).

back to the stimulus. This turns the perceptual process into a "cycle"
in which taking action---for example, walking toward the tree---changes
the observer's view of the tree. Even though the process is simplified
and only depicts the perceptual process in one sense, Figure 1.4
provides a good way to think about how perception occurs and introduces
some important principles that will guide our discussion of perception
throughout this book. In the first part of this chapter, we will briefly
describe each stage of the process; in the second part, we will consider
ways of measuring the relationship between stimuli and perception.

Distal and Proximal Stimuli (Steps 1 and 2) There are stimuli within the
body that produce internal pain and enable us to sense the positions of
our body and limbs. But for the purposes of this discussion, we will
focus on stimuli that exist "out there" in the environment, like a tree
in the woods that you can see, hear, smell, and feel (and taste, if you
wanted to be adventurous). Using this example, we will consider what
happens in the first two steps of the perceptual process in which
stimuli from the environment reach the sensory receptors.

We begin with the tree that the person is observing, which we call the
distal stimulus (Step 1). It is called distal because it is
"distant"---out there in the environment. The person's perception of the
tree is based not on the tree getting into his eye or ear (ouch!), but
on light reflected from the tree entering the eye and reaching the
visual receptors, and the pressure changes in the air caused by the
rustling leaves entering the ear and reaching the auditory receptors.
This representation of the tree on the receptors is the proximal
stimulus (Step 2), so called because it is "in proximity" to the
receptors. The light and pressure waves that stimulate the receptors
introduce one of the central principles of perception, the principle of
transformation, which states that stimuli and responses created by
stimuli are transformed, or changed, between the distal stimulus and
perception. For example, the first transformation occurs when light hits
the tree and is then reflected from the tree to the person's eyes. The
nature of the reflected light depends on properties of the light energy
hitting the tree (is it the midday sun, light on an overcast day, or a
spotlight illuminating the tree from below?), properties of the tree
(its textures, shape, the fraction of light hitting it that it
reflects), and properties of the atmosphere through which the light is
transmitted (is the air clear, dusty, or foggy?). As this reflected
light enters the eye, it is transformed again as it is focused by the
eye's optical system (discussed further in Chapter 3) onto the retina, a
0.4-mm-thick network of nerve cells which contains the receptors for
vision. The fact that an image of the tree is focused on the receptors
introduces another principle of perception, the principle of
representation, which states that everything a person perceives is based
not on direct contact with stimuli but on representations of stimuli
that are formed on the receptors and the resulting activity in the
person's nervous system. The distinction between the distal stimulus
(Step 1) and the proximal stimulus (Step 2) illustrates both
transformation and representation. The distal stimulus (the tree) is
transformed into the proximal stimulus, and this image represents the
tree in the person's eyes. But this transformation from "tree" to "image
of the tree on the receptors" is just the first in a series of
transformations. We're only on Step 2 of the perceptual process, and we
can already begin to understand the complexity of perception in these
transformations! The next transformation occurs within the receptors
themselves.

Receptor Processes (Step 3) Sensory receptors are cells specialized to
respond to environmental energy, with each sensory system's receptors
specialized to respond to a specific type of energy. Figure 1.5 shows
examples of receptors from each of the senses. Visual receptors respond
to light, auditory receptors to pressure changes in the air, touch
receptors to pressure transmitted through the skin, and smell and taste
receptors to chemicals entering the nose and mouth. When the sensory
receptors receive the information from the environment, such as light
reflected from the tree, they do two things: (1) They transform
environmental 1.3 The Perceptual Process

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

7

\*

- 
- 
- 
- 

(a) Vision

(b) Hearing

(c) Touch

(d) Smell

(e) Taste

Figure 1.5 Receptors for (a) vision, (b) hearing, (c) touch, (d) smell,
and (e) taste. Each of these receptors is specialized to transduce a
specific type of environmental energy into electricity. Stars indicate
the place on the receptor neuron where the stimulus acts to begin the
process of transduction.

energy into electrical energy; and (2) they shape perception by the way
they respond to different properties of the stimuli. The transformation
of environmental energy (such as light, sound, or thermal energy) to
electrical energy is called transduction. For example, if you were to
run your fingers over the bark of the tree, the stimulation of pressure
receptors in your fingers would cause these receptors to produce
electrical signals representing the texture of the bark. By transforming
environmental energy into electrical energy, your sensory receptors are
allowing the information that is "out there," like the texture of the
tree, to be transformed into a form that can be understood by your
brain. Transduction by the sensory receptors is, therefore, crucial for
perception. Another way to think about transduction is that your sensory
receptors are like a bridge between the external sensory world and your
internal (neural) representation of that world. In the next step of the
perceptual process, further processing of that neural representation
takes place.

The changes in these signals that occur as they are transmitted through
this maze of neurons is called neural processing. This processing will
be discussed in much more detail in later chapters as we describe each
sense individually. However, there are commonalities in neural
processing between the senses. For instance, the electrical signals
created through transduction are often sent to a sense's primary
receiving area in the cerebral cortex of the brain, as shown in Figure
1.6. The cerebral cortex is a 2-mm-thick layer that contains the
machinery for creating perceptions, as well as other functions, such as
language, memory, emotions, and thinking. The primary receiving area for
vision occupies most of the occipital lobe; the area for hearing is
located in part of the temporal lobe; and the area for the skin
senses---touch, temperature, and pain---is located in an area in the
parietal lobe. As we study each sense in detail, we will see that once
signals reach the primary receiving areas, they are then transmitted

Neural Processing (Step 4) Once transduction occurs, the tree becomes
represented by electrical signals in thousands of sensory receptors
(visual receptors if you're looking at the tree, auditory receptors if
you're hearing the leaves rustling, and so on). But what happens to
these signals? As we will see in Chapter 2, they travel through a vast
interconnected network of neurons that (1) transmit signals from the
receptors to the brain and then within the brain; and (2) change (or
process) these signals as they are transmitted. These changes occur
because of interactions between neurons as the signals travel from the
receptors to the brain. Because of this processing, some signals become
reduced or are prevented from getting through, and others are amplified
so they arrive at the brain with added strength. This processing then
continues as signals travel to various places in the brain. 8

Parietal lobe (skin senses)

Frontal lobe Occipital lobe (vision)

Temporal lobe (hearing)

Figure 1.6 The four lobes of the brain, with the primary receiving areas
for vision, hearing, and the skin senses (touch, temperature, and pain)
indicated.

Chapter 1  Introduction to Perception

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Perception

Recognition

Action

"I see something"

"It's an oak tree"

"Let's have a closer look"

Figure 1.7 The behavioral responses of the perceptual process:
perception, recognition, and action.

to many other structures in the brain. For example, the frontal lobe
receives signals from all of the senses, and it plays an important role
in perceptions that involve the coordination of information received
through two or more senses. The sequence of transformations that occurs
between the receptors and the brain, and then within the brain, means
that the pattern of electrical signals in the brain is changed compared
to the electrical signals that left the receptors. It is important to
note, however, that although these signals have changed, they still
represent the tree. In fact, the changes that occur as the signals are
transmitted and processed are crucial for achieving the next step in the
perceptual process, the behavioral responses.

Behavioral Responses (Steps 5--7) Finally, after all of that
transformation, transduction, transmission, and processing, we reach the
behavioral responses (Figure 1.7). This transformation is perhaps the
most miraculous of all, because electrical signals have been transformed
into the conscious experience of perception (Step 5), which then leads
to recognition (Step 6). We can distinguish between perception, which is
conscious awareness of the tree, and recognition, which is placing an
object in a category, such as "tree," that gives it meaning, by
considering the case of Dr. P., a patient described by neurologist
Oliver Sacks (1985) in the title story of his book The Man Who Mistook
His Wife for a Hat. Dr. P., a well-known musician and music teacher,
first noticed a problem when he began having trouble recognizing his
students visually, although he could immediately identify them by the
sound of their voices. But when Dr. P. began misperceiving common
objects, for example addressing a parking meter as if it were a person
or expecting a carved knob on a piece of furniture to engage him in
conversation, it became clear that his problem was more serious than
just a little forgetfulness. Was he blind, or perhaps crazy? It was
clear from an eye examination that he could see well, and by many other
criteria it was obvious that he was not crazy. Dr. P.'s problem was
eventually diagnosed as visual form agnosia---an inability to recognize
objects---that was caused by a brain tumor. He perceived the parts of
objects but couldn't identify the whole object, so when Sacks showed

him a glove, as in Figure 1.8, Dr. P. described it as "a continuous
surface unfolded on itself. It appears to have five outpouchings, if
this is the word." When Sacks asked him what it was, Dr. P. hypothesized
that it was "a container of some sort. It could be a change purse, for
example, for coins of five sizes." The normally easy process of object
recognition had, for Dr. P., been derailed by his brain tumor. He could
perceive the object and recognize parts of it, but he couldn't
perceptually assemble the parts in a way that would enable him to
recognize the object as a whole. Cases such as this show that it is
important to distinguish between perception and recognition. The final
behavioral response is action (Step 7), which involves motor activities
in response to the stimulus. For example, after having perceived and
recognized the tree, the person might decide to walk toward the tree,
touch the tree, have a picnic under it, or climb it. Even if he doesn't
decide to interact directly with the tree, he is taking action when he
moves his eyes and head to look at different parts of the tree, even if
he is standing in one place.

Figure 1.8 How Dr. P.---a patient with visual form agnosia--- responded
when his neurologist showed him a glove and asked him what it was. 1.3
The Perceptual Process

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

9

Some researchers see action as an important outcome of the perceptual
process because of its importance for survival. David Milner and Melvyn
Goodale (1995) propose that early in the evolution of animals, the major
goal of visual processing was not to create a conscious perception or
"picture" of the environment but to help the animal control navigation,
catch prey, avoid obstacles, and detect predators---all crucial
functions for the animal's survival. The fact that perception often
leads to action---whether it be an animal's increasing its vigilance
when it hears a twig snap in the forest or a person's deciding to
interact with an object or just look more closely at something that
looks interesting---means that perception is a continuously changing
process. For example, the visual and auditory representations of the
tree change every time the person moves his body relative to the tree,
as the tree might look and sound different from different angles, and
this change creates new representations and a new series of
transformations. Thus, although we can describe the perceptual process
as a series of steps that "begins" with the distal stimulus and "ends"
with perception, recognition, and action, the overall process is dynamic
and continually changing.

Knowledge Our diagram of the perceptual process includes one more
factor: knowledge. Knowledge is any information that the perceiver
brings to a situation, such as prior experience or expectations.
Knowledge is placed inside the person's brain in Figure 1.4 because it
can affect a number of the steps in the perceptual process. Knowledge
that a person brings to a situation can be information acquired years
ago or, as in the following demonstration, information just recently
acquired.

DEMONSTRATION

Perceiving a Picture

After looking at the drawing in Figure 1.9, close your eyes, turn to
page 12, and open and shut your eyes rapidly to briefly expose the
picture in Figure 1.13. Decide what the second picture is; then open
your eyes and read the explanation below it. Do this now, before reading
further.

Figure 1.9 See above. (Adapted from Bugelski & Alampay, 1961)

10

Did you identify Figure 1.13 as a rat (or a mouse)? If you did, you were
influenced by the clearly rat- or mouselike figure you observed
initially. But people who first observe Figure 1.16 instead of Figure
1.9 usually identify Figure 1.13 as a man. (Try this on someone else.)
This demonstration, which is called the rat--man demonstration, shows
how recently acquired knowledge ("that pattern is a rat") can influence
perception. An example of how knowledge acquired years ago can influence
the perceptual process is your ability to categorize---to place objects
into categories. This is something you do every time you name an object.
"Tree," "bird," "branch," "car," and everything else you can name are
examples of objects being placed into categories that you learned as a
young child and that have become part of your knowledge base. Another
way to describe the effect of information that the perceiver brings to
the situation is by distinguishing between bottom-up processing and
top-down processing. Bottom-up processing (also called data-based
processing) is processing that is based on the stimuli reaching the
receptors. These stimuli provide the starting point for perception
because, with the exception of unusual situations such as drug-induced
perceptions or "seeing stars" from a bump to the head, perception
involves activation of the receptors. The woman sees the moth on the
tree in Figure 1.10 because of processes triggered by the moth's image
on her visual receptors. The image is the "incoming data" that is the
basis of bottom-up processing. Top-down processing (also called
knowledge-based processing) refers to processing that is based on
knowledge. When the woman in Figure 1.10 labels what she is seeing as a
"moth" or perhaps a particular kind of moth, she is accessing what she
has learned about moths from prior experience. Knowledge such as this
isn't always involved in perception, but as we will see, it often
is---sometimes without our even being aware of it. To experience
top-down processing in action, try reading the following sentence: M*RY
H*D \* L*TTL* L\*MB If you were able to do this, even though all of the
vowels have been omitted, you probably used your knowledge of English
words, how words are strung together to form sentences, and your
familiarity with the nursery rhyme to create the sentence (Denes &
Pinson, 1993). Students often ask whether top-down processing is always
involved in perception. The answer to this question is that it is "very
often" involved. There are some situations, typically involving very
simple stimuli, in which top-down processing may not be involved. For
example, perceiving a single flash of easily visible light is probably
not affected by a person's prior experience. However, as stimuli become
more complex, the role of topdown processing increases. In fact, a
person's past experience is usually involved in perception of real-world
scenes, even though in most cases the person is unaware of this
influence. One of the themes of this book is that our knowledge of how
things usually appear in the environment, based on our past experiences,
can play an important role in determining what we perceive.

Chapter 1  Introduction to Perception

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Figure 1.10 Perception is determined by an interaction between bottom-up
processing, which starts with the image on the receptors, and top-down
processing, which brings the observer's knowledge into play. In this
example, (a) the image of the moth on the woman's visual receptors
initiates bottom-up processing; and (b) her prior knowledge of moths
contributes to top-down processing.

(b) Existing knowledge (top down)

"Moth"

(a) Incoming data (bottom up)

To better understand how the perceptual process is studied, we can
simplify it from seven steps (Figure 1.4) into three major components
(Figure 1.11): Stimulus (distal and proximal; Steps 1--2) Physiology
(receptors and neural processing; Steps 3--4) ■■ Behavior (perception,
recognition, action; Steps 5--7) ■■ ■■

IO

in g

LO

-R

G Y pt or s

S

ec

e

The stimulus--behavior relationship relates stimuli (Steps 1 and 2 in
Figure 1.4) to behavioral responses, such as perception, recognition,
and action (Steps 5--7). This was the main

A

C

P H YS

The Stimulus--Behavior Relationship (A)

Recognition ion -A ept ctio c r n Pe BEHAVIOR

Process

The goal of perceptual research is to understand the relationships
indicated by arrows A, B, and C between these three components. For
example, some research studies examine how we get from a stimulus to
behavior (arrow A), such as the pressure of someone touching your
shoulder (the stimulus) and feeling the touch and reacting to it (the
behavior). Other studies have investigated how a given stimulus affects
physiology (arrow B), such as how the pressure on your shoulder leads to
neural firing. And still other work addresses the relationship between
physiology and behavior (arrow C), such as how neural firing results in
the feeling on your shoulder. In the following sections, we use a visual
phenomenon called the oblique effect to show how each of these
relationships can be studied to understand the perceptual process. The
oblique effect is that people see vertical or horizontal lines better
than lines oriented obliquely (at any orientation other than vertical or
horizontal). We begin by considering how the oblique effect has been
studied in the context of the stimulus--behavior relationship.

relationship measured during the first 100 years of the scientific study
of perception, before physiological methods became widely available, and
it is still being studied today. One way to study the stimulus--behavior
relationship is using an approach called psychophysics, which measures
the relationships between the physical (the stimulus) and the
psychological (the behavioral response). We will discuss various
psychophysical methods in more detail later in this chapter. For now,
let's consider psychophysics using the example of the oblique effect.
The oblique effect has been demonstrated by presenting black and white
striped stimuli called gratings, and measuring grating acuity, the
smallest width of lines that participants can

TI M UL ro US xim al - Di s ta l

1.4 Studying the Perceptual Process

P

B Figure 1.11 Simplified perceptual process showing the three
relationships described in the text. The three boxes represent the three
major components of the seven-step perceptual process: Stimulus (Steps 1
and 2); Physiology (Steps 3 and 4); and the three Behavioral responses
(Steps 5--7). The three relationships that are usually measured to study
the perceptual process are (A) the stimulus--behavior relationship; (B)
the stimulus--physiology relationship; and (C) the physiology--behavior
relationship.

1.4 Studying the Perceptual Process

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

11

Figure 1.12 Measuring grating acuity. The finest line width at which a
participant can perceive the bars in a black-and-white grating stimulus
is that participant's grating acuity. Stimuli with different line widths
are presented one at a time, and the participant indicates the grating's
orientation until the lines are so close together that the participant
can no longer indicate the orientation.

detect. One way to measure grating acuity is to ask participants to
indicate the grating's orientation and testing with thinner and thinner
lines (Figure 1.12). Eventually, the lines are so thin that they can't
be seen, and the area inside the circle looks uniform, so participants
can no longer indicate the grating's orientation. The smallest
line-width at which the participant can still indicate the correct
orientation is the grating acuity. When grating acuity is assessed at
different orientations, the results show that acuity is best for
gratings oriented vertically or horizontally, rather than obliquely
(Appelle, 1972). This simple psychophysics experiment demonstrates a
relation between the stimulus and behavior; in this case, the stimulus
is oriented gratings, and the behavioral response is detecting the
grating's orientation.

by measuring brain activity (discussed further in Chapter 2). For
example, David Coppola and coworkers (1998) measured the oblique effect
physiologically by presenting lines with different orientations (Figure
1.14a) to ferrets. When they measured the ferret's brain activity using
a technique called optical brain imaging, they found that horizontal and
vertical orientations caused larger brain responses in visual brain
areas than oblique orientations (Figure 1.14b).2 This demonstrates how
the oblique effect has been studied in the context of the
stimulus--physiology relationship. Note that even though the
stimulus--behavior experiment was carried out on humans and the
stimulus--physiology experiment was carried out on ferrets, the results
are similar. Horizontal and vertical orientations result in better
acuity (behavioral response) and more brain activation (physiological
response) than oblique orientations. When behavioral and physiological
responses to stimuli are similar like this, researchers often infer the
relationship between physiology and behavior (Arrow C in Figure 1.11),
which in this case would be the association between greater
physiological responses to horizontals and verticals and better
perception of horizontals and verticals. But in some cases, instead of
just inferring this association, researchers have measured the
physiology--behavior relationship directly.

(a) Stimuli: vertical, horizontal, oblique

(b) Brain response: Bigger to vertical and horizontal orientations

The Stimulus--Physiology Relationship (B) Miroslav
Hlavko/Shutterstock.com

The second stimulus relationship (Arrow B in Figure 1.11) is the
stimulus--physiology relationship, the relationship between stimuli
(Steps 1--2) and physiological responses, like neurons firing (Steps
3--4). This relationship is often studied

Figure 1.14 Coppola and coworkers (1998) measured the relationship
between bar orientation (stimuli) and brain activity (physiology) in
ferrets. Verticals and horizontals generated the greatest brain
activity. 2

Figure 1.13 Did you see a "rat" or a "man"? Looking at the more ratlike
picture in Figure 1.9 increased the chances that you would see this as a
rat. But if you had first seen the man version (Figure 1.16), you would
have been more likely to perceive this figure as a man. (Adapted from
Bugelski & Alampay, 1961) 12

Because a great deal of physiological research has been done on animals,
students often express concerns about how these animals are treated. All
animal research in the United States follows strict guidelines for the
care of animals established by organizations such as the American
Psychological Association and the Society for Neuroscience. The central
tenet of these guidelines is that every effort should be made to ensure
that animals are not subjected to pain or distress. Research on animals
has provided essential information for developing aids for people with
sensory disabilities such as blindness and deafness and for helping
develop techniques to ease severe pain.

Chapter 1  Introduction to Perception

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

The Physiology--Behavior Relationship (C) The physiology--behavior
relationship relates physiological responses (Steps 3--4 in Figure 1.4)
and behavioral responses (Steps 5--7; Arrow C in Figure 1.11).
Christopher Furmanski and Stephen Engel (2000) determined the
physiology--behavior relationship for different grating orientations by
measuring both the brain response and behavioral sensitivity in the same
participants. The behavioral measurements were made by decreasing the
intensity difference between light and dark bars of a grating until the
participant could no longer detect the grating's orientation.
Participants were able to detect the horizontal and vertical
orientations at smaller light--dark differences than for the oblique
orientations. This means that participants were more sensitive to the
horizontal and vertical orientations (Figure 1.15a). The physiological
measurements were made using a technique called functional magnetic
resonance imaging (fMRI), which we will describe in Chapter 2 (see page
31). These measurements showed larger brain responses to vertical and
horizontal gratings than to oblique gratings (Figure 1.15b). The results
of this experiment, therefore, are consistent with the results of the
other two oblique effect experiments that we have discussed. The beauty
of this experiment is that the behavioral and physiological responses
were measured in the same participants, allowing for a more direct
assessment of the physiology--behavior relationship than in the
previously

Relative fMRI amplitude

45°

90°

.50

Relative detection sensitivity

1.0

0°

1.  What are some reasons for studying perception?

2.  Describe the process of perception as a series of seven steps,
    beginning with the distal stimulus and culminating in the behavioral
    responses of perceiving, recognizing, and acting.

3.  What does it mean to say that perception can be studied by measuring
    three relationships? Give an example of how the oblique effect was
    studied by measuring each relationship.

135°

(b) Physiological 45°

0

TEST YOuRSELF 1.1

3.  What is the role of higher-level or knowledge-based processes in
    perception? Be sure you understand the difference between bottom-up
    and top-down processing.

0°

0

described experiments. The reason for the visual system's preference for
horizontal and vertical orientations, which has to do with the
prevalence of verticals and horizontals in the environment, will be
discussed in Chapter 5. We have now seen how the perceptual process can
be studied by investigating the relationships between its three main
components: the stimulus, physiology, and behavior. One of the things
that becomes apparent when we step back and look at the three
relationships is that each one provides information about different
aspects of the perceptual process. An important message of this book is
that to truly understand perception, we have to study it by measuring
both behavioral and physiological relationships. Furthermore, as
discussed earlier in this chapter, it's important to consider how the
knowledge, memories, and expectations that people bring to a situation
can influence their perception (like in the rat--man demonstration in
Figure 1.13). Only by considering both behavior and physiology together,
along with potential influences of knowledge-based processing, can we
create a complete picture of the mechanisms responsible for perception.

90°

.50 135° 1.0 (a) Behavioral

Figure 1.15 Furmanski and Engel (2000) made both behavioral and
physiological measurements of participants' response to oriented
gratings. (a) Bars indicate sensitivity to gratings of different
orientations. Sensitivity is highest to the vertical (0 degree) and
horizontal (90 degree) orientations. (b) Bars indicate fMRI amplitude to
different orientations. Amplitudes were greater to the 0- and 90-degree
orientations.

1.5 Measuring Perception So far we've pictured the perceptual process as
having a number of steps (Figure 1.4), and we've demonstrated how we can
study the process by studying three different relationships (Figure
1.11). But what, exactly, do we measure to determine these
relationships? In this section we will describe a number of different
ways to measure behavioral responses. We will describe how to measure
physiological responses in Chapter 2. What is measured in an experiment
looking at the relationship between stimuli and behavior? The grating
acuity experiment described on page 12 (Figure 1.12) measured the
absolute threshold for seeing fine lines. The absolute threshold is the
smallest stimulus level that can just be detected. In the grating acuity
example, this threshold was the smallest line width that can be
detected. But we can also look to the other senses for more examples.
For instance, if you're making a stew and decide that you want to add
salt for flavor, the absolute threshold would be 1.5 Measuring
Perception

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

13

the smallest amount of salt that you would need to add in order to just
be able to taste it. For the sense of hearing, it might be the intensity
of a whisper that you can just barely hear. These examples show that
thresholds measure the limits of sensory systems; they are measures of
minimums---the smallest line-width that can be detected, the smallest
concentration of a chemical we can taste or smell, the smallest amount
of sound energy we can hear. Thresholds have an important place in the
history of perceptual psychology, and of psychology in general, so let's
consider them in more detail before describing other ways of measuring
perception. As we will now see, the importance of being able to
accurately measure thresholds was recognized very early in the history
of the scientific study of the senses.

Measuring Thresholds Gustav Fechner (1801--1887), professor of physics
at the University of Leipzig, introduced a number of ways of measuring
thresholds. Fechner had wide-ranging interests, having published papers
on electricity, mathematics, color perception, aesthetics (the judgment
of art and beauty), the mind, the soul, and the nature of consciousness.
But of all his accomplishments, the most significant one was providing a
new way to study the mind. Let's view Fechner's thinking about the mind
against the backdrop of how people thought about the mind in the
mid1800s. Prevailing thought at that time was that it was impossible to
study the mind. The mind and the body were thought to be totally
separate from one another. People saw the body as physical and therefore
something that could be seen, measured, and studied, whereas the mind
was considered not physical and was therefore invisible and something
that couldn't be measured and studied. Another reason proposed to
support the idea that the mind couldn't be studied was the assertion
that it is impossible for the mind to study itself. Against this
backdrop of skepticism regarding the possibility of studying the mind,
Fechner, who had been thinking about this problem for many years, had an
insight, the story goes, while lying in bed on the morning of October
22, 1850. His insight was that the mind and body should not be thought
of as totally separate from one another but as two sides of a single
reality (Wozniak, 1999). Most important, Fechner proposed that the mind
could be studied by measuring the relationship between changes in
physical stimulation (the body part of the relationship) and a person's
experience (the mind part). This proposal was based on the observation
that as physical stimulation is

increased---for example, by increasing the intensity of a light---the
person's perception of the brightness of the light also increases. Ten
years after having his insight about the mind, Fechner (1860/1966)
published his masterpiece, Elements of Psychophysics, in which he
proposed a number of methods for measuring stimulus--behavior
relationships using psychophysics. One of the major contributions of
Elements of Psychophysics was the proposal of three methods for
measuring the threshold: the method of limits, the method of constant
stimuli, and the method of adjustment. Taken together, these methods,
which are called the classical psychophysical methods, opened the way
for the founding of scientific psychology by providing methods to
measure thresholds for perceiving sensory stimuli. Every so often we
will introduce a new method by describing it in a "Method" section.
Students are sometimes tempted to skip these sections because they think
the content is unimportant. However, you should resist this temptation
because these methods are essential tools for the study of perception.
These "Method" sections are often related to experiments described
immediately afterward and also provide the background for understanding
experiments that are described later in the book. METHOD

Determining the Threshold

Fechner's classical psychophysical methods for determining the absolute
threshold of a stimulus are the method of limits, constant stimuli, and
adjustment. In the method of limits, the experimenter presents stimuli
in either ascending order (intensity is increased) or descending order
(intensity is decreased), as shown in Figure 1.17, which indicates the
results of an experiment that measures a person's threshold for hearing
a tone. 1

2

3

4

5

6

7

8

Intensity 103

Y

Y

Y

Y

102

Y

Y

Y

Y

101

Y

Y

Y

Y

Y

100

Y

Y

Y

Y

Y

Y

Y

99

Y

N

Y

N

Y

Y

Y

Y

98

N

N

Y

N

N

N

N

Y

97

N

N

N

N

N

96

N

N

N

N

95

N

N

N

N

Crossover values

98.5 99.5 97.5 99.5 98.5 98.5 98.5 97.5 Threshold = Mean of crossovers =
98.5

Figure 1.16 Man version of the rat--man stimulus. (Adapted from Bugelski
&

Figure 1.17 The results of an experiment to determine the threshold
using the method of limits. The dashed lines indicate the crossover
point for each sequence of stimuli. The threshold---the average of the
crossover values---is 98.5 in this experiment.

Alampay, 1961)

14

Chapter 1  Introduction to Perception

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

On the first series of trials, the experimenter begins by presenting a
tone with an intensity we will call 103, and the participant indicates
by a "yes" response that he or she hears the tone. This response is
indicated by a Y at an intensity of 103 in the far left column of the
table. The experimenter then presents another tone, at a lower
intensity, and the participant responds to this tone. This procedure
continues, with the participant making a judgment at each intensity
until the response is "no." This change from "yes" to "no," indicated by
the dashed line, is the crossover point, and the threshold for this
series is taken as the mean between 99 and 98, or 98.5. The next series
of trials begins below the participant's threshold, so that the response
is "no" on the first trial (intensity 95), and continues until "yes,"
when the intensity reaches 100. Notice that the crossover point when
starting below the threshold is slightly different. Because the
crossover points may vary slightly, this procedure is repeated a number
of times, starting above the threshold half the time and starting below
the threshold half the time. The threshold is then determined by
calculating the average of all of the crossover points. The method of
constant stimuli is similar to the method of limits in that different
stimulus intensities are presented one at a time, and the participant
must respond whether they perceive it ("yes" or "no") on each trial. The
difference is that in this method, the stimulus intensities are
presented in random order, rather than in descending or ascending order.
After presenting each intensity many times, the threshold is usually
defined as the intensity that results in detection on 50 percent of
trials. The method of adjustment is slightly different in that the
participant---rather than the experimenter---adjusts the stimulus
intensity continuously until he or she can just barely detect the
stimulus. For example, the participant might be asked to turn a knob to
decrease the intensity of a sound until it can no longer be heard, and
then to turn the knob back again so that the sound is just barely
audible. This just barely audible intensity is taken as the threshold.
The procedure is repeated numerous times, and the threshold is
determined by taking the average setting. The choice among these methods
is usually determined by the degree of accuracy needed and the amount of
time available. The method of constant stimuli is the most accurate
method because it involves many observations and stimuli are presented
in random order, which minimizes how presentation on one trial can
affect the participant's judgment of the stimuli presented on the next
trial. The disadvantage of this method is that it is time-consuming. The
method of adjustment is faster because participants can determine their
threshold in just a few trials by adjusting the stimulus intensity
themselves.

While the method of limits, constant stimuli, and adjustment have a key
role in history, it's important to note that they are still being used
in perceptual research today to measure the absolute threshold of
stimuli within the different senses under various conditions. Because of
the impact of Fechner's contributions to our understanding of measuring
thresholds,

October 22, the date Fechner awoke with his insight that led to the
founding of psychophysics, is known among psychophysical researchers as
"Fechner Day." Add that date to your calendar if you're looking for
another holiday to celebrate! (Another approach to measuring people's
sensitivity, called "The Signal Detection Approach," is described in
Appendix C, page 420.) So far in this section, we have discussed how one
might go about measuring the absolute threshold of a stimulus. But what
if instead of measuring the detection threshold of just one stimulus
("Can you taste any salt in this stew?"), the researcher wants to
measure the threshold between two stimuli? For instance, perhaps as you
hone your cooking skills, you make a second batch of stew in which you
start with the same amount of salt as the first batch. Now, you want to
know how much more salt you need to add to the second batch to detect a
difference in salt content between the two batches. In this case, you
would be interested in the difference threshold--- the smallest
difference between two stimuli that enables us to tell the difference
between them. In Elements of Psychophysics, Fechner not only proposed
his psychophysical methods but also described the work of Ernst Weber
(1795--1878), a physiologist who, a few years before the publication of
Fechner's book, measured the difference threshold for different senses.
See Appendix A (p. 417) for more details about difference thresholds.
Fechner's and Weber's methods not only made it possible to measure the
ability to detect stimuli but also made it possible to determine
mechanisms responsible for experiences. For example, consider what
happens when you enter a dark place and then stay there for a while. At
first you may not be able to see much (Figure 1.18a), but eventually
your vision gets better and you are able to see light and objects that
were invisible before (Figure 1.18b). This improved vision occurs
because your threshold for seeing light is becoming smaller and smaller
as you stay in the dark. By measuring how a person's threshold changes
moment by moment, we can go beyond simply saying that "we see better
when we spend time in the dark" to providing a quantitative description
of what is happening as a person's ability to see improves. We will
further discuss this particular aspect of vision (called the dark
adaptation curve) in Chapter 3. As significant as the methods for
measuring thresholds are, we know that perception includes far more than
just what happens at threshold. To understand the richness of
perception, we need to be able to measure other aspects of sensory
experience in addition to thresholds. In the next section, we describe
techniques of measuring perception when a stimulus is above threshold.

Measuring Perception Above Threshold In order to describe some of the
ways perceptual researchers measure sensory experience above threshold,
we will consider five questions about the perceptual world and the
techniques used to answer these questions. 1.5 Measuring Perception

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

15

Bruce Goldstein

(a) 
(b) 

Figure 1.18 (a) How a dark scene might be perceived when seen just after
being in the light. (b) How the scene would be perceived after spending
10 to 15 minutes adapting to the dark. The improvement in perception
after spending some time in the dark reflects a decrease in the
threshold for seeing light.

Question 1: What Is the Perceptual Magnitude of a Stimulus? Technique:
Magnitude Estimation Things are big and small (an elephant; a bug), loud
and soft (rock music; a whisper), intense and just perceptible
(sunlight; a dim star), overpowering and faint (heavy pollution; a faint
smell). Fechner was not only interested in measuring thresholds using
the classical psychophysical methods; he was also interested in
determining the relationship between physical stimuli (like rock music
and a whisper) and the perception of their magnitude (like perceiving
one to be loud and the other soft). Fechner created a mathematical
formula relating physical stimuli and perception; modern psychologists
have modified Fechner's equation based on a method not available in
Fechner's time called magnitude estimation (Stevens, 1957, 1961). METHOD

Magnitude Estimation

The procedure for a magnitude estimation experiment is relatively
simple: The experimenter first presents a "standard" stimulus to the
participant (let's say a sound of moderate intensity) and assigns it a
value of, say, 10. The participant then hears sounds of different
intensities, and is asked to assign a number to each of these sounds
that is proportional to the loudness of the original sound. If the sound
seems twice as loud as the standard, it gets a rating of 20; half as
loud, a 5; and so on. Thus, the participant assigns a loudness value to
each sound intensity. This number for "loudness" is the perceived
magnitude of the stimulus.

The example of magnitude estimation used here is from the sense of
hearing (judging the loudness of a sound), but as with other methods
introduced in this chapter, the same technique can be applied to the
other senses. As another example, the results of experiments using
magnitude estimation 16

to measure brightness (rather than loudness) are discussed in the
"Something to Consider" section at the end of this chapter, and the
mathematical formulas relating physical intensity and perceptual
magnitude for brightness are discussed in Appendix B (p. 418).

Question 2: What Is the Identity of the Stimulus? Technique: Recognition
Testing When you name things, you are categorizing them (see page 9).
The process of categorizing, which is called recognition, is measured in
many different types of perceptual experiments. One application is
testing the ability of people with brain damage. As we saw earlier in
this chapter, Dr. P.'s brain damage led him to have trouble recognizing
common objects, like a glove. The recognition ability of people with
brain damage is tested by asking them to name objects or pictures of
objects. Recognition is also used to assess the perceptual abilities of
people without brain damage. In Chapter 5 we will describe experiments
that show that people can identify rapidly flashed pictures ("It's a
docking area for boats lined with houses"), although seeing small
details ("The second house has five rows of windows") requires more time
(Figure 1.19). Recognition is not only visual; it can also include
hearing ("that's a car revving its engine"), touch ("that feels like an
apple"), taste ("mmm, chocolate"), and smell ("that's a rose"). Because
recognizing objects is so crucial for our survival, many perception
researchers have shifted their emphasis from asking "What do you see?"
(perception) to asking "What is that called?" (recognition). Question 3:
How Quickly Can I React to It? Technique: Reaction Time The speed with
which we react to something can be determined by measuring reaction
time---the time between presentation of a stimulus and the

Chapter 1  Introduction to Perception

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Bruce Goldstein

While directing attention to the top of the left rectangle, the
participant's task was to push a button as quickly as possible when a
dark target flashed anywhere on the display. The results, shown in
Figure 1.20b, indicate that the participant responded more quickly when
the target was flashed at A, where he or she was directing attention,
compared to B, off to the side (Egly et al., 1994). These findings are
relevant to a topic we will discuss in Chapter 7: How does talking on a
cellphone while driving affect the ability to drive?

Figure 1.19 A stimulus like those used in an experiment in which
participants are asked to recognize a rapidly flashed scene.
Participants can often recognize general properties of a rapidly flashed
scene, such as "houses near water and a boat," but need more time to
perceive the details.

person's reaction to it. An example of a reaction time experiment is to
ask participants to keep their eyes fixed on the + in the display in
Figure 1.20a and pay attention to location A on the left rectangle.
Because the participant is looking at the + but paying attention to the
top of the left rectangle, this task resembles what happens when you are
looking in one direction but are paying attention to something off to
the side. (a) A

B

Reaction time (ms)

- 

(b) 400

Question 5: How Can I Interact With It? Technique: Physical Tasks and
Judgments All of the other questions have focused on different ways of
measuring what we perceive. This last question is concerned not with
perception but with actions that follow perception (Step 7 of the
perceptual process). Many perceptual researchers believe that one of the
primary functions of perception is to enable us to take action within
our environment. Look at it this way: Morg the caveman sees a dangerous
tiger in the woods. He could stand there and marvel at the beauty of its
fur, or the power of its legs, but if he doesn't take action by either
hiding or getting away and the tiger sees him, his days of perceiving
will be over. On a less dramatic level, we need to be able to see a
saltshaker and then accurately reach across the table to pick it up, or
navigate from one place on campus to another to get to class. Research
on perception and action, which we will describe in Chapter 7, has
participants carry out tasks that involve both perception and action,
such as

350 300

A

B

Figure 1.20 (a) A reaction time experiment in which the participant is
told to look at the + sign, but pay attention to the location at A, and
to push a button as quickly as possible when a dark target flashes
anywhere on the display. (b) Reaction times in milliseconds, which
indicates that reaction time was faster when the target was flashed at
A, where the participant was attending, than when it was flashed at B,
where the participant was not attending. (Data from Egly et al., 1994)

Question 4: How Can I Describe What Is Out There? Technique:
Phenomenological Report Look around. Describe what you see. You could
name the objects you recognize, or you could describe the pattern of
lights and darks and colors, or how things are arranged in space, or
that two objects appear to be the same or different sizes or colors.
Describing what is out there is called phenomenological report. For
example, do you see a vase or two faces in Figure 1.21? We will see in
Chapter 5 that displays like this are used to study how people perceive
objects in front of backgrounds. Phenomenological reports are important
because they define the perceptual phenomena we want to explain, and
once a phenomenon is identified, we can then study it using other
methods.

Figure 1.21 Vase--face stimulus used to demonstrate how people perceive
objects in front of a background. 1.5 Measuring Perception

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

17

reaching for a target, navigating through a maze, or driving a car,
under different conditions. Physical tasks have also been studied by
having people make judgments about tasks before they actually carry them
out. For example, we will see in Chapter 7 that people with pain that
makes walking difficult will estimate an object as being farther away
than people who aren't in pain. The examples above provide a hint as to
the wide range of methods that are used in perception research. This
book discusses research using the methods described above, plus others
as well. Although we won't describe the details of the methods used in
every experiment we consider, we will highlight the most important
methods in "Method" sections like the ones on determining the threshold
and on magnitude estimation in this chapter. Additionally, many
physiological methods will be described in Methods sections in the
chapters that follow. What will emerge as you read this book is a story
in which important roles are played by both behavioral and physiological
methods, which combine to create a more complete understanding of
perception than is possible using either type of method alone.

(b) Two-bulbs; Intensity 5 20.

Figure 1.22 A participant (indicated by the eye) is viewing lights with
different physical intensities. The two lights at (b) have twice the
physical intensity as the single light at (a). However, when the
participant is asked to judge brightness, which is a perceptual
judgment, the light at (b) is judged to be only about 20 or 30 percent
brighter than the light at (a).

intensities of the lights with a light meter, we would find that the
person receives twice as much light in (b) as in (a). But what does the
person perceive? Perception of the light is measured not by determining
the intensity but by determining perceived brightness using a method
such as magnitude estimation (see page 16). What happens to brightness
when we double the intensity from (a) to (b)? The answer is that (b)
will appear brighter than (a), but not twice as bright. If the
brightness is judged to be 10 for light (a), the brightness of light (b)
will be judged to be about 12 or 13 (Stevens, 1962; also see Appendix B,
page 418). Thus, there is not a one-to-one relationship between the
physical intensity of the light and our perceptual response to the
light. As another example of the distinction between physical and
perceptual, consider the electromagnetic spectrum in Figure 1.23. The
electromagnetic spectrum is a band of energy ranging from gamma rays at
the short-wave end of the spectrum

SOMETHING TO CONSIDER:

Why Is the Difference Between Physical and Perceptual Important? One of
the most crucial distinctions in the study of perception is the
distinction between physical and perceptual. To illustrate the
difference, consider the two situations in Figure 1.22. In (a), the
light from one light bulb with a physical intensity of 10 is focused
into a person's eye. In (b), the light from two light bulbs, with a
total intensity of 20, is focused into the person's eye. All of this so
far has been physical. If we were to measure the Visible light

Ultraviolet

X-rays

Gamma rays 10--3

400

10--1

Ultraviolet rays 101

Infrared

Infrared rays 103

500

Observer

(a) One-bulb; Intensity 5 10.

105

Radar

FM

TV

AM

AC circuits

107

109

1011

1013

1015

600

700

Wavelength (nm)

Figure 1.23 The electromagnetic spectrum, shown on top, stretches from
gamma rays to AC circuits. The visible spectrum, shown exploded below,
accounts for only a small part of the electromagnetic spectrum. We are
blind to energy outside of the visible spectrum. 18

Chapter 1  Introduction to Perception

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

to AM radio and AC circuits at the long-wave end. But we see just the
small band of energy called visible light, sandwiched between the
ultraviolet and infrared energy bands. We are blind to ultraviolet and
shorter wavelengths (although hummingbirds can see ultraviolet
wavelengths that are invisible to us). We also can't see at the high end
of the spectrum, in the infrared and above, which is probably a good
thing---imagine the visual clutter we would experience if we could see
all of those cellphone conversations carrying their messages through the
air! What these examples illustrate is that what physical measuring
instruments record and what we perceive are two different things. Ludy
Benjamin, in his book A History of Psychology (1997), makes this point
when he observes that "If changes in physical stimuli always resulted in
similar changes in perception of those stimuli ... there would be no
need for psychology; human perception could be wholly explained by the
laws of the discipline of physics" (p. 120). But perception is
psychology, not physics, and

perceptual responses are not necessarily the same as the responses of
physical measuring devices. We will, therefore, be careful, throughout
this book, to distinguish between physical stimuli and the perceptual
responses to these stimuli. TEST YOuRSELF 1.2 1. What was Fechner's
contribution to psychology? 2. Describe the differences between the
method of limits, the method of constant stimuli, and the method of
adjustment. 3. Describe the five questions that can be asked about the
world out there and the measurement techniques that are used to answer
them. 4. Why is it important to distinguish between physical and
perceptual?

THINK ABOUT IT 1. This chapter argues that although perception seems
simple, it is actually extremely complex when we consider "behind the
scenes" activities that are not obvious as a person is experiencing
perception. Cite an example of a similar situation from your own
experience, in which an "outcome" that might seem as though it was
achieved easily actually involved a complicated process that most people
are unaware of.

2.  Describe a situation in which you initially thought you saw, heard,
    or felt something but then realized that your initial perception was
    in error. What was the role of bottom-up and top-down processing in
    this example of first having an incorrect perception and then
    realizing what was actually there?

KEY TERMS Absolute threshold (p. 14) Action (p. 9) Bottom-up processing
(data-based processing) (p. 10) Categorize (p. 10) Cerebral cortex
(p. 8) Classical psychophysical methods (p. 14) Difference threshold
(p. 15) Distal stimulus (p. 7) Electromagnetic spectrum (p. 18) Frontal
lobe (p. 9) Grating acuity (p. 11) Knowledge (p. 10) Magnitude
estimation (p. 16) Method of adjustment (p. 15)

Method of constant stimuli (p. 15) Method of limits (p. 14) Neural
processing (p. 8) Oblique effect (p. 11) Occipital lobe (p. 8) Parietal
lobe (p. 8) Perceived magnitude (p. 16) Perception (p. 4) Perceptual
process (p. 6) Phenomenological report (p. 17) Physiology--behavior
relationship (p. 13) Primary receiving area (p. 8) Principle of
representation (p. 7) Principle of transformation (p. 7) Proximal
stimulus (p. 7)

Psychophysics (p. 11) Rat--man demonstration (p. 10) Reaction time
(p. 16) Recognition (p. 9) Sensation (p. 5) Sensory receptors (p. 7)
Stimulus--behavior relationship (p. 11) Stimulus--physiology
relationship (p. 11) Temporal lobe (p. 8) Thresholds (p. 14) Top-down
processing (knowledgebased processing) (p. 10) Transduction (p. 8)
Visual form agnosia (p. 9)

Key Terms

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

19

Brain-imaging technology has made it possible to visualize the
structure, functioning, and activity of different areas of the brain.
Barry Blackman/Taxi/Getty Images

Learning Objectives After studying this chapter, you will be able to ...
■■ Identify the key components of neurons and their respective

functions. ■■ Explain how electrical signals are recorded from neurons
and

the basic properties of these signals. ■■ Describe the chemical basis of
electrical signals in neurons. ■■ Describe how electrical signals are
transmitted from one neuron

■■ Explain how brain imaging can be used to create pictures of the

locations of the brain's activity. ■■ Distinguish between structural and
functional connectivity

between brain areas and describe how functional connectivity is
determined. ■■ Discuss the mind--body problem.

to another. ■■ Understand the various ways that neurons can represent
our

sensory experiences.

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Ch HAPTER a p t e r 42

Basic Principles of Sensory Physiology Chapter Contents 2.1 Electrical
Signals in Neurons Recording Electrical Signals in Neurons METHOD: The
Setup for Recording

From a Single Neuron

Basic Properties of Action Potentials Chemical Basis of Action
Potentials Transmitting Information Across a Gap

2.2 Sensory Coding: How Neurons Represent Information Specificity Coding
Sparse Coding Population Coding

■■ How do neurons work, and how does neural firing under-

lie our perception? (p. 21) ■■ How do perceptual functions map onto the
structure of

the brain? (p. 30) ■■ How is brain activity measured both within a brain
area

and between different brain areas? (p. 31)

T

of Measuring Functional Connectivity

SOMETHING TO CONSIDER:

2.3 Zooming Out: Representation in the Brain

Some Questions We Will Consider:

wo cars start at the same place and drive to the same destination. Car A
takes an express highway, stopping only briefly for gas. Car B takes the
"scenic" route--- back roads that go through the countryside and small
towns, stopping a number of times along the way to see some sights and
meet some people. Each of Car B's stops can influence its route,
depending on the information its driver receives. Stopping at a
small-town general store, the driver of Car B hears about a detour up
the road, so changes the route accordingly. Meanwhile, Car A is speeding
directly to its destination. The way electrical signals travel through
the nervous system is more like Car B's journey. The pathway from
receptors to brain is not a nonstop expressway. Every signal leaving a
receptor travels through a complex network of interconnected signals,
often meeting, and being affected by, other signals along the way. What
is gained by taking a complex, indirect route? If the goal were just to
send a signal to the brain that a particular

Distributed Representation Connections Between Brain Areas METHOD: The
Resting State Method

TEST YOURSELF 2.1

Mapping Function to Structure

METHOD: Brain Imaging

The Mind--Body Problem TEST YOURSELF 2.2 THINK ABOUT IT

receptor had been stimulated, then the straight-through method would
work. But the purpose of electrical signals in the nervous system goes
beyond signaling that a receptor was stimulated. The information that
reaches the brain and then continues its journey within the brain is
much richer than this. As we will see in this and upcoming chapters,
there are neurons in the brain that respond to certain stimuli like
slanted lines, faces, movement across space in a specific direction,
movement across the skin in a specific direction, or salty tastes. These
neurons didn't achieve these properties by receiving signals through a
straight-line transmission system from receptors to brain. They achieve
these properties by neural processing---the interaction of the signals
of many neurons (see page 8). Because the activity of individual neurons
and neural processing carried out by large numbers of neurons create our
perceptual experiences, it is important to understand the basic
mechanisms behind neural responding and neural processing. We begin by
describing electrical signals in neurons.

2.1 Electrical Signals in Neurons Electrical signals occur in structures
called neurons, like the ones shown in Figure 2.1. The key components of
neurons, shown in the neuron on the right in Figure 2.1, are the cell
body, which

21

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Touch receptor Stimulus from environment

Dendrite Nerve fiber

Axon or nerve fiber Synapse Cell body

Electrical signal

Figure 2.1 The neuron on the right consists of a cell body, dendrites,
and an axon, or nerve fiber. The neuron on the left that receives
stimuli from the environment has a receptor in place of the cell body.

contains mechanisms to keep the cell alive; dendrites, which branch out
from the cell body to receive electrical signals from other neurons; and
the axon, or nerve fiber, which is filled with fluid that conducts
electrical signals. There are variations on this basic neuron structure:
Some neurons have long axons; others have short axons or none at all.
Especially important for perception are sensory receptors (see Figure
1.5), which are neurons specialized to respond to environmental stimuli.
The receptor on the left in Figure 2.1 responds to touch stimuli.
Individual neurons do not, of course, exist in isolation. There are
hundreds of millions of neurons in the nervous system, and each neuron
is connected to many other neurons. As we will discuss later in this
chapter, these connections are extremely important for perception. To
begin our discussion of how neurons and their connections give rise to
perception, we focus on individual neurons. One of the most important
ways of studying how electrical signals underlie perception is to record
signals from single neurons. We can appreciate the importance of being
able to record from single neurons by considering the following analogy:
You walk into a large room in which hundreds of people are talking about
a political speech they have just heard. There is a great deal of noise
and commotion in the room as people react to the speech. Based on
hearing this "crowd noise," all you can say about what is going on is
that the speech seems to have generated a great deal of excitement. To
get more specific information about the speech, you need to listen to
what individual people are saying. Just as listening to individual
people provides valuable information about what is happening in a large
crowd, recording from single neurons provides valuable information about
what is happening in the nervous system. Recording from single neurons
is like listening to individual voices. It is important to record from
as many neurons as possible, of course, because just as individual
people may have different opinions about the speech, different neurons
may respond differently to a particular stimulus or situation. The
ability to record electrical signals from individual neurons ushered in
the modern era of brain research, and in the 1950s and 1960s,
development of sophisticated electronics and the availability of
computers made possible more detailed analysis of how neurons function.
22

Recording Electrical Signals in Neurons Electrical signals are recorded
from the axons (or nerve fibers) of neurons using small electrodes to
pick up the signals. METHOD

Neuron

The Setup for Recording From a Single

Figure 2.2a shows a typical setup used for recording from a single
neuron. There are two electrodes: a recording electrode, shown with its
recording tip inside the neuron,1 and a reference electrode, located
some distance away so it is not affected by the electrical signals.
These two electrodes are connected to a meter that records the
difference in charge between the tips of the two electrodes. This
difference is displayed on a computer screen, like the one shown in
Figure 2.3, which shows electrical signals being recorded from a neuron.

When the axon, or nerve fiber, is at rest, the difference in the
electrical potential between the tips of the two electrodes is --70
millivolts (mV, where a millivolt is 1/1,000 of a volt), as shown on the
right in Figure 2.2a. This means that the inside of the axon is 70 mV
more negative than the outside. This value, which stays roughly the same
as long as there are no signals in the neuron, is called the resting
potential. Figure 2.2b shows what happens when the neuron's receptor is
stimulated so that a signal is transmitted down the axon. As the signal
passes the recording electrode, the charge inside the axon rises to +40
mV compared to the outside. As the signal continues past the electrode,
the charge inside the fiber reverses course and starts becoming negative
again (Figure 2.2c), until it returns to the resting level (Figure
2.2d). This signal, identified by the predictable rise and fall of the
charge inside the axon relative to the outside, is called the action
potential, and lasts about 1 millisecond (ms, 1/1,000 second). When we
refer to neurons as "firing," we are referring to the neuron having
action potentials. 1

In practice, most recordings are achieved with the tip of the electrode
positioned just outside the neuron because it is technically difficult
to insert electrodes into the neuron, especially if it is small.
However, if the electrode tip is close enough to the neuron, the
electrode can pick up the signals generated by the neuron.

Chapter 2  Basic Principles of Sensory Physiology

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Recording electrode (inside axon) Push

Nerve impulse

(b) 

Reference electrode (outside axon)

Resting potential --70 Time

Pressure-sensitive receptor Charge inside fiber relative to outside (mV)

(a) 

Meter

+40

--70

Figure 2.2 (a) When a nerve fiber is at rest, there is a difference in
charge of --70 mV between the inside and the outside of the fiber. This
difference, which is measured by the meter indicated by the blue circle,
is displayed on the right. (b) As the nerve impulse, indicated by the
red band, passes the electrode, the inside of the fiber near the
electrode becomes more positive. This positivity is the rising phase of
the action potential. (c) As the nerve impulse moves past the electrode,
the charge inside the fiber becomes more negative. This is the falling
phase of the action potential. (d) Eventually the neuron returns to its
resting state.

--70

(c) Back at resting level --70

(d) 

Your experience of seeing the words on this page, hearing the sounds
around you, and tasting your food all start with electrical signals in
neurons. In this chapter we will first describe how individual neurons
work, and will then consider how the activity of groups of neurons is
related to perception. We begin by describing basic properties of the
action potential and its chemical basis.

Bruce Goldstein

Basic Properties of Action Potentials

Figure 2.3 Electrical signals being displayed on a computer screen, in
an experiment in which responses are being recorded from a single
neuron. The signal on the screen shows the difference in voltage between
two electrodes as a function of time. In this example, many signals are
superimposed on one another, creating a thick white tracing.
(Photographed in Tai Sing Lee's laboratory at Carnegie Mellon
University)

An important property of the action potential is that it is a propagated
response---once the response is triggered, it travels all the way down
the axon without decreasing in size. This means that if we were to move
our recording electrode in Figure 2.2 to a position nearer the end of
the axon, the electrical response would take longer to reach the
electrode, but it would still be the same size (increasing from 270 to
140 mV) when it got there. This is an extremely important property of
the action potential because it enables neurons to transmit signals over
long distances. Another property is that the action potential remains
the same size no matter how intense the stimulus is. The three records
in Figure 2.4 represent the axon's response to three intensities of
pushing on the skin. Each action potential appears as a sharp spike in
these records because we have compressed the time scale 2.1 Electrical
Signals in Neurons

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

23

Na+

K+

Na+

(a) 

K+

Na+ (b) Na+

K+ K+

Na+ Na+

Na+

Figure 2.5 A nerve fiber showing the high concentration of sodium ions
(Na+) outside the fiber and potassium ions (K+) inside the fiber. Other
ions, such as negatively charged chlorine, are not shown.

(c) Time Pressure off

Figure 2.4 Response of a nerve fiber to (a) soft, (b) medium, and (c)
strong stimulation. Increasing the stimulus strength increases both the
rate and the regularity of nerve firing in this fiber, but has no effect
on the size of the action potentials.

to display a number of action potentials. Figure 2.4a shows how the axon
responds to gentle stimulation applied to the skin, and Figures 2.4b and
2.4c show how the response changes as the pressure is increased.
Comparing these three records leads to an important conclusion: Changing
the stimulus intensity does not affect the size of the action potentials
but does affect the rate of firing. Although increasing the stimulus
intensity can increase the rate of firing, there is an upper limit to
the number of nerve impulses per second that can be conducted down an
axon. This limit occurs because of another property of the axon called
the refractory period---the interval between the time one nerve impulse
occurs and the next one can be generated in the axon. Because the
refractory period for most neurons is about 1 millisecond, the upper
limit of a neuron's firing rate is about 500 to 800 impulses per second.
Another important property of action potentials is illustrated by the
beginning of each of the records in Figure 2.4. Notice that a few action
potentials are occurring even before the pressure stimulus is applied.
Action potentials that occur in the absence of stimuli from the
environment are called spontaneous activity. This spontaneous activity
establishes a baseline level of firing for the neuron. The presence of
stimulation usually causes an increase in activity above this
spontaneous level, but under some conditions, which we will describe
shortly, it can cause firing to decrease below the spontaneous level.

Chemical Basis of Action Potentials What causes these rapid changes in
charge that travel down the axon? Because this is a traveling electrical
charge, we might be tempted to equate it to the electrical signals that
are conducted along electrical power lines or the wires used for
household appliances. But action potentials create electricity not in
the dry environment of metal wires, but in the wet environment of the
body. 24

Na+

K+

K+

Pressure on

Na+

The key to understanding the "wet" electrical signals transmitted by
neurons is understanding the components of the neuron's liquid
environment. Neurons are bathed in a liquid solution rich in ions,
molecules that carry an electrical charge (Figure 2.5). Ions are created
when molecules gain or lose electrons, as happens when compounds are
dissolved in water. For example, adding table salt (sodium chloride,
NaCl) to water creates positively charged sodium ions (Na+) and
negatively charged chlorine ions (Cl--). The solution outside the axon
of a neuron is rich in positively charged sodium (Na+) ions, whereas the
solution inside the axon is rich in positively charged potassium (K+)
ions. This distribution of ions across the neuron's membrane at rest is
important to maintaining the --70 mV resting potential, as well as to
the initiation of the action potential itself. You can understand how
these ions result in the action potential by imagining yourself just
outside an axon next to a recording electrode (Figure 2.6a). (You will
have to shrink yourself down to a very small size to do this!)
Everything is quiet until incoming signals from other neurons trigger an
action potential to begin traveling down the axon. As it approaches, you
see positively charged sodium ions (Na+) rushing into the axon (Figure
2.6b). This occurs because channels in the membrane that are selective
to Na+ have opened, which allow Na+ to flow across the membrane and into
the neuron. This opening of sodium channels represents an increase in
the membrane's selective permeability to sodium, where permeability
refers to the ease with which a molecule can pass through the membrane
and selective means that the fiber is permeable to one specific type of
molecule, Na+ in this case, but not to others. The inflow of positively
charged sodium causes an increase in the positive charge inside the axon
from the resting potential of --70 mV until it reaches the peak of the
action potential of +40 mV. An increase in positive charge inside the
neuron is called depolarization. This quick and steep depolarization
from --70 mV to +40 mV during an action potential is referred to as the
rising phase of the action potential (Figure 2.6b). Continuing your
vigil, you notice that once the charge inside the neuron reaches +40 mV,
the sodium channels close (the membrane becomes impermeable to sodium)
and potassium channels open (the membrane becomes selectively permeable
to potassium). Because there were more potassium ions (K+) inside

Chapter 2  Basic Principles of Sensory Physiology

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Figure 2.6 How the flow of sodium and potassium creates the action
potential. (a) When the fiber is at rest, there is no flow of ions, and
the record indicates the --70 mV resting potential. (b) Ion flow occurs
when an action potential travels down the fiber. Initially, positively
charged sodium (Na+) flows into the axon, causing the inside of the
neuron to become more positive (rising phase of the action potential).
(c) Later, positively charged potassium (K+) flows out of the axon,
causing the inside of the axon to become more negative (falling phase of
the action potential). (d) When the action potential has passed the
electrode, the charge returns to the resting level.

Resting potential --70 Time (a) +40

K+

Charge inside fiber relative to outside (mV)

Na+

(b) K+ Na+

(c ) K+ Na+

--70

Sodium flows into axon

Potassium flows out of axon --70

Back at resting level

Receiving neuron Receptor

--70

(d) 

than outside the neuron while at rest, positively charged potassium
rushes out of the axon when the channels open, causing the charge inside
the axon to become more negative. An increase in negative charge inside
the neuron is called hyperpolarization. The hyperpolarization from +40
mV back to --70 mV is the falling phase of the action potential (Figure
2.6c). Once the potential has returned to the --70 mV resting level, the
K+ flow stops (Figure 2.6d), which means the action potential is over
and the neuron is again at rest. After reading this description of ion
flow, students often ask why the sodium-in, potassium-out flow that
occurs during the action potential doesn't cause sodium to build up
inside the axon and potassium to build up outside. The answer is that a
mechanism called the sodium-potassium pump keeps this buildup from
happening by continuously pumping sodium out and potassium into the
fiber.

Transmitting Information Across a Gap We have seen that action
potentials caused by sodium and potassium flow travel down the axon
without decreasing in size. But what happens when the action potential
reaches the end of the axon? How is the action potential's message
transmitted to other neurons? The problem is that there is a very small
space between neurons, known as a synapse (Figure 2.7). The discovery of
the synapse raised the question of how the electrical

Axon

Stimulus

Nerve impulse

(a) 

Neurotransmitter molecules Synaptic vesicle

Receiving neuron

Axon of sending neuron

(b) 

Neurotransmitter molecules

Receptor site

(c) 

Figure 2.7 Synaptic transmission from one neuron to another. (a) A
signal traveling down the axon of a neuron reaches the synapse at the
end of the axon. (b) The nerve impulse causes the release of
neurotransmitter molecules (red) from the synaptic vesicles of the
sending neuron. (c) The neurotransmitters fit into receptor sites that
are shaped like the transmitter and cause a voltage change in the
receiving neuron. 2.1 Electrical Signals in Neurons

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

25

Level of depolarization needed to trigger an action potential

--70 Depolarization (Excitatory)

(a) 

40

Charge inside fiber

Charge inside fiber

0

0

--70

Charge inside fiber

0

(c) 

Level to trigger an action potential

(b) 

Depolarization (Excitatory)

--70

Hyperpolarization (Inhibitory)

Figure 2.8 (a) Excitatory transmitters cause depolarization, an
increased positive charge inside the neuron. (b) When the level of
depolarization reaches threshold, indicated by the dashed line, an
action potential is triggered. (c) Inhibitory transmitters cause
hyperpolarization, an increased negative charge inside the axon.

signals generated by one neuron are transmitted across the space
separating the neurons. As we will see, the answer lies in a remarkable
chemical process that involves molecules called neurotransmitters. Early
in the 1900s, it was discovered that when action potentials reach the
end of a neuron, they trigger the release of chemicals called
neurotransmitters that are stored in structures called synaptic vesicles
at the end of the sending neuron (Figure 2.7b). The neurotransmitter
molecules flow into the synapse to small areas on the receiving neuron
called receptor sites that are sensitive to specific neurotransmitters
(Figure 2.7c). These receptor sites exist in a variety of shapes that
match the shapes of particular neurotransmitter molecules. When a
neurotransmitter makes contact with a receptor site matching its shape,
it activates the receptor site and triggers a voltage change in the
receiving neuron. A neurotransmitter is like a key that fits a specific
lock. It has an effect on the receiving neuron only when its shape
matches that of the receptor site. Thus, when an electrical signal
reaches the synapse, it triggers a chemical process that causes a new
electrical signal in the receiving neuron. The nature of this signal
depends on both the type of transmitter that is released and the nature
of the receptor sites in the receiving neuron. Two types of responses
can occur at these receptor sites, excitatory and inhibitory. An
excitatory response occurs when the neuron becomes depolarized, and thus
the inside of the neuron becomes more positive. 26

Figure 2.8a shows this effect. Notice, however, that this response is
much smaller than the depolarization that happens during an action
potential. To generate an action potential, enough excitation must occur
to increase depolarization to the level indicated by the dashed line.
How does the receiving neuron get enough excitation to reach this level?
The answer is that it might take more than one excitatory response, such
as what occurs when multiple neurotransmitters from a number of incoming
neurons all reach the receptor sites of the receiving neuron at once. If
the resulting depolarization is large enough, an action potential is
triggered (Figure 2.8b). Depolarization is an excitatory response
because it causes the charge to change in the direction that triggers an
action potential. An inhibitory response occurs when the inside of the
neuron becomes more negative, or hyperpolarized. Figure 2.8c shows this
effect. Hyperpolarization is an inhibitory response because it causes
the charge inside the axon to move away from the level of
depolarization, indicated by the dashed line, needed to generate an
action potential. We can summarize this description of the effects of
excitation and inhibition as follows: Excitation increases the chances
that a neuron will generate action potentials and is associated with
increasing rates of nerve firing. Inhibition decreases the chances that
a neuron will generate action potentials and is associated with lowering
rates of nerve firing. Since a typical neuron receives both excitation
and inhibition,

Chapter 2  Basic Principles of Sensory Physiology

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

the response of the neuron is determined by the interplay of excitation
and inhibition, as illustrated in Figure 2.9. In Figure 2.9a, excitation
(E) is much stronger than inhibition (I), so the neuron's firing rate is
high. However, as inhibition becomes stronger and excitation becomes
weaker, the neuron's firing decreases, until in Figure 2.9e, inhibition
has eliminated the neuron's spontaneous activity and has decreased
firing to zero. Why does inhibition exist? If one of the functions of a
neuron is to transmit its information to other neurons, what would be
the point of decreasing or eliminating firing in the next neuron? The
answer is that the function of neurons is not only to transmit
information but also to process it, and, as we will see in Chapter 3,
both excitation and inhibition are involved in this processing.

Excitation stronger E

Electrode (a)

Now that we have an understanding of the basics of neural functioning,
we can go one step further and think about how these neural processes
underlie perception. How is it that a neuron "represents" information,
like the taste of the salt in your stew? Is there a "salty" neuron in
your brain that only fires in response to salt, and so causes you to
perceive "saltiness"? Or does the pattern of firing of a group of
neurons in one brain area, or perhaps many brain areas, result in our
perception of saltiness? The problem of neural representation for the
senses has been called the problem of sensory coding, where the sensory
code refers to how neurons represent various characteristics of the
environment.

Specificity Coding

I

E (b) I

E (c) I

E (d) I

E (e) I Inhibition stronger

2.2 Sensory Coding: How Neurons Represent Information

Stimulus on

Stimulus off

Figure 2.9 Effect of excitatory (E) and inhibitory (I) input on the
firing rate of a neuron. The amount of excitatory and inhibitory input
to the neuron is indicated by the size of the arrows at the synapse. The
responses recorded by the electrode are indicated by the records on the
right. The firing that occurs before the stimulus is presented is
spontaneous activity. In (a), the neuron receives only excitatory
transmitter, which causes the neuron to fire. In (b) to (e), the amount
of excitatory transmitter decreases while the amount of inhibitory
transmitter increases. As inhibition becomes stronger relative to
excitation, firing rate decreases, until eventually the firing rate
becomes zero.

One way in which neurons can represent sensory information is
demonstrated in the "salty neuron" example above--- the idea that one
neuron can represent one perceptual experience, like the taste of salt.
This notion of a specialized neuron that responds only to one concept or
stimulus is called specificity coding. An example of specificity coding
from the sense of vision is illustrated in Figure 2.10, which shows how
a number of neurons respond to three different faces (the actual firing
rates don't matter; they're made up for the sake of this example). Only
neuron #4 responds to Bill's face, only #9 responds to Mary's face, and
only #6 responds to Raphael's face. Also note that the neuron
specialized to respond only to Bill, which we can call a "Bill neuron,"
does not respond to Mary or Raphael. In addition, other faces or types
of objects would not affect this neuron. It fires only in response to
Bill's face. This idea that one neuron can represent one stimulus or
concept, such as a face, dates back to the 1960s (Konorski, 1967; see
also Barlow, 1972; Gross, 2002). At that time, Jerome Lettvin proposed
the somewhat tongue-in-cheek idea that neurons could be so specific that
there could be one neuron in your brain that fires only in response to,
say, your grandmother. This highly specific type of neuron, dubbed by
Lettvin as a grandmother cell, would respond to your grandmother "...
whether animate or stuffed, seen from before or behind, upside down or
on a diagonal or offered by caricature, photograph or abstraction"
(Lettvin, as quoted in Gross, 2002). According to Lettvin, even just
thinking about the idea of your grandmother, not just the visual input,
could make your grandmother cell fire. Along this reasoning, you would
also have a "grandmother cell" for every face, stimulus, and concept
that you've ever encountered---a specific neuron to represent your
professor, one for your best friend, one for your dog, and so on.
Perhaps you even have grandmother cells that respond 2.2 Sensory Coding:
How Neurons Represent Information

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

27

Figure 2.10 Specificity coding, in which each face causes a different
neuron to fire. Firing of neuron 4 signals "Bill"; neuron 9 signals
"Mary"; neuron 6 signals "Raphael."

Firing rate

SPECIFICITY CODING

1

2

3

(a) Bill

4

5

6

7

8

9

10

8

9

10

8

9

10

Firing rate

Neuron number

1

(b) Mary

2

3

4

5

6

7

Firing rate

Neuron number

1 (c) Raphael

2

3

4

5

6

7

Neuron number

to specific information in other senses as well, like one neuron per
song that you know or food that you've eaten. Could it be that we have
such specific representations of stimuli and concepts that we've
encountered? Evidence that provided some insight into this question came
from R. Quian Quiroga and colleagues (2005; 2008) who recorded from the
temporal lobe of patients undergoing brain surgery for epilepsy.
(Stimulating and recording from neurons is a common procedure before and
during brain surgery, because it makes it possible to determine the
exact layout of a particular person's brain.) These patients were
presented with pictures of famous people from different viewpoints, as
well as other things such as other faces, buildings, and animals, in
order to see how the neurons responded. Not surprisingly, a number of
neurons responded to some of these stimuli. What was surprising,
however, was that some neurons responded to a number of different views
of just one person or building, or to a number of ways of representing
that person or building. For example, Figure 2.11 shows a particular
neuron that responded to pictures of the actor Steve Carell and not to
other people's faces (Quiroga et al., 2008). Neurons were also found
that responded to just the actress Halle Berry, including pictures of
her from different films, a sketch drawing of her, and even just the
words "Halle Berry" (Quiroga et al., 2005). Thus, these neurons were not
just responding to the visual input of the famous person's face, but
also to the concept of that particular person. Likewise, other neurons
were found that responded just to certain buildings, 28

like the Sydney Opera House, and not any other buildings or objects,
indicating that these specific cells can be found not just for people
but for other objects as well.

Figure 2.11 Records from a neuron in the temporal lobe that responded to
different pictures of Steve Carell similar to the ones shown here (top
records) but which did not respond to pictures of other well-known
people (bottom records). (From Quiroga et al., 2008) Photos: Frederick
M. Brown/Getty Images; AP Images/Todd Williamson; Photos 12/Alamy Stock
Photo; JStone/ Shutterstock.com; Young Nova/Shutterstock.com;
s_bukley/Shutterstock.com

Chapter 2  Basic Principles of Sensory Physiology

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Figure 2.12 Sparse coding, in which each face's identity is indicated by
the pattern of firing of a small number of neurons. Thus, the pattern
created by neurons 2, 3, 4, and 7 signals "Bill"; the pattern created by
4, 6, and 7 signals "Mary"; the pattern created by 1, 2, and 4 signals
"Raphael."

Firing rate

SPARSE CODING

1

2

3

(a) Bill

4

5

6

7

8

9

10

8

9

10

8

9

10

Firing rate

Neuron number

1

(b) Mary

2

3

4

5

6

7

Firing rate

Neuron number

1 (c) Raphael

2

3

4

5

6

7

Neuron number

At this point, you might be thinking that Quiroga and coworkers' study
provides evidence for grandmother cells. After all, these neurons were
responding to highly specific stimuli! While this finding does seem
consistent with the idea of grandmother cells, it does not prove that
they exist. The researchers themselves even say that their study doesn't
necessarily support the notion of grandmother cells. In fact, Quiroga
and colleagues point out that they had only 30 minutes to record from
these neurons, and that if more time were available, it is likely that
they would have found other faces, places, or objects that would have
caused these neurons to fire. In other words, the "Steve Carell neuron"
might actually have been responsive to other faces or objects as well,
had more options been tested. In fact, the idea of grandmother cells is
not typically accepted by neuroscientists today, given the lack of
confirmatory evidence and its biological implausibility. Do we really
have one neuron to represent every single concept we've encountered?
It's unlikely, given how many neurons would be required. An alternative
to the idea of specificity coding is that a number of neurons---rather
than just one---are involved in representing a perceptual experience.

Sparse Coding In their 2008 article, Quiroga and coworkers proposed that
sparse coding, rather than specificity coding, was more likely to
underlie their results. Sparse coding occurs when a particular stimulus
is represented by a pattern of firing of only a small group of neurons,
with the majority of neurons remaining

silent. Going back to our example of using faces as stimuli, as shown in
Figure 2.12a, sparse coding would represent Bill's face by the pattern
of firing of a few neurons (neurons 2, 3, 4, and 7). Mary's face would
be signaled by the pattern of firing of a few different neurons (neurons
4, 6, and 7; Figure 2.12b), but possibly with some overlap with the
neurons representing Bill, and Raphael's face would have yet another
pattern (neurons 1, 2, and 4; Figure 2.12c). Notice that a particular
neuron can respond to more than one stimulus. For example, neuron #4
responds to all three faces, although most strongly to Mary's. There is
evidence that the code for representing objects in the visual system,
tones in the auditory system, and odors in the olfactory system may
involve a pattern of activity across a relatively small number of
neurons, as sparse coding suggests (Olshausen & Field, 2004).

Population Coding While sparse coding proposes that the pattern of
firing across a small number of neurons underlies neural representation,
population coding proposes that our experiences are represented by the
pattern of firing across a large number of neurons. According to this
idea, Bill's face might be represented by the pattern of firing shown in
Figure 2.13a, Mary's face by a different pattern (Figure 2.13b), and
Raphael's face by another pattern (Figure 2.13c). An advantage of
population coding is that a large number of stimuli can be represented,
because large groups of neurons can create a huge number of different
patterns. As we will see in upcoming chapters, there 2.2 Sensory Coding:
How Neurons Represent Information

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

29

Figure 2.13 Population coding, in which the face's identity is indicated
by the pattern of firing of a large number of neurons.

Firing rate

POPULATION CODING

1

2

3

(a) Bill

4

5

6

7

8

9

10

8

9

10

8

9

10

Firing rate

Neuron number

1

(b) Mary

2

3

4

5

6

7

Firing rate

Neuron number

1 (c) Raphael

2

3

4

5

6

7

Neuron number

is good evidence for population coding in each of the senses, and for
other cognitive functions as well. Returning to the question about how
neural firing can represent perception, we can state that part of the
answer is that perceptual experiences---such as the experience of the
aroma of cooking or the appearance of the objects on the table in front
of you---are represented by the pattern of firing of groups of neurons.
Sometimes the groups are small (sparse coding), sometimes large
(population coding). TEST YOuRSELF 2.1 1. Describe the basic structure
of a neuron. 2. Describe how to record electrical signals from a neuron.
3. What are some of the basic properties of action potentials? 4.
Describe what happens when an action potential travels along an axon. In
your description, indicate how the charge inside the fiber changes, and
how that is related to the flow of chemicals across the cell membrane.
5. How are electrical signals transmitted from one neuron to another? Be
sure you understand the difference between excitatory and inhibitory
responses. 6. What is a grandmother cell? Describe Quiroga and
colleagues' experiments on recordings from neurons in patients
undergoing surgery for epilepsy. 7. What is the sensory code? Describe
specificity, sparse, and population coding. Which types of coding are
most likely to operate in sensory systems?

30

2.3 Zooming Out: Representation in the Brain So far in this chapter,
we've been focusing mainly on how neural firing "represents" information
that's out there in the world, such as a face. But as we'll see
throughout this book as we explore each of the senses, perception goes
beyond individual neurons or even groups of neurons. To get a more
complete picture of the physiology of perception, we must zoom out from
neurons and consider representation in the brain more broadly, including
different brain areas and the connections between those areas.

Mapping Function to Structure How do perceptual functions, like
perceiving faces, map onto the structure of the brain? The general
question of how different functions map onto different brain areas can
be dated all the way back to the 18th century with German physiologist
Franz Joseph Gall and his colleague Johann Spurzheim. Using prison
inmates and mental hospital patients as his participants, Gall claimed
to observe a correlation between the shape of a person's skull and their
abilities and traits, which he called "mental faculties." Based on his
observations, Gall concluded that there were about 35 different mental
faculties that could be mapped onto different brain areas based on the
bumps and contours on the person's skull, as in Figure 2.14--- an
approach that Spurzheim called phrenology. For instance,

Chapter 2  Basic Principles of Sensory Physiology

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Library of Congress, Prints & Photographs Division, Reproduction number
LC-DIG-pga-07838 (digital file from original item) LC-USZC4-4556 (color
film copy transparency) LC-USZCN4-195 (color film copy neg.)
LC-USZ62-2550 (b&w film copy neg.)

Wernicke's area

Figure 2.14 How different functions map onto the structure of the head,
according to phrenology.

a ridge on the back of your head might mean that you're a loving person,
while a bump on the side means that you're good at musical perception.
Although phrenology has now been debunked as a method, it was the first
proposal that different functions map onto different areas of the
brain---a concept that is still discussed today. The idea that specific
brain areas are specialized to respond to specific types of stimuli or
functions is called modularity, with each specific area called a module.
Early evidence supporting modularity of function came from case studies
of humans with brain damage. One such historical case study was
conducted by French physician Pierre Paul Broca (1824--1890), who saw a
patient with a very specific behavioral deficit: the patient could only
speak the word "tan," although his speech comprehension and other
cognitive abilities appeared to be intact. Examination of Tan's brain
after his death showed that he had a lesion to his left frontal lobe
(Figure 2.15). Soon thereafter, Broca saw other patients with similar
defects in speech production who had damage to that same area of the
brain. Broca therefore concluded that this area was the speech
production area, and it came to be known as Broca's area. Another early
researcher, Carl Wernicke (1848--1905) identified an area in the
temporal lobe which was involved in understanding speech, and which came
to be known as Wernicke's area (Figure 2.15).

Broca's area

Figure 2.15 Broca's and Wernicke's areas. Broca's area is in the frontal
lobe, and Wernicke's area is in the temporal lobe.

Broca's and Wernicke's areas provided early evidence for modularity, and
since Broca's and Wernicke's pioneering research, there have been many
other studies relating the location of brain damage to specific effects
on behavior---a field now known as neuropsychology. We will describe
more examples of neuropsychological case studies throughout this book,
including more on Broca and Wernicke in Chapter 14. While
neuropsychology has provided evidence for modularity, studying patients
with brain damage is difficult for numerous reasons, including the fact
that the extent of each patient's damage can differ greatly. A more
controlled way that modularity has been studied is by recording brain
responses in neurologically normal humans using brain imaging, which
makes it possible to create pictures of the location of the brain's
activity.

METHOD

Brain Imaging

In the 1980s, a technique called magnetic resonance imaging (MRI) made
it possible to create images of structures within the brain. Since then,
MRI has become a standard technique for detecting tumors and other brain
abnormalities. While this technique is excellent for revealing brain
structures, it doesn't indicate neural activity. Another technique,
functional magnetic resonance imaging (fMRI), has enabled researchers to
determine how various types of cognitions, or functions (hence the
"functional" part of functional MRI), activate different areas of the
brain. Functional magnetic resonance imaging takes advantage of the fact
that blood flow increases in areas of the brain that are activated. The
measurement of blood flow is based on the fact that hemoglobin, which
carries oxygen in the blood, contains a ferrous (iron) molecule and
therefore has magnetic properties. If a magnetic field is presented to
the brain, the hemoglobin molecules line up, like tiny magnets. Areas of
the brain that are more active consume more oxygen, so the hemoglobin
molecules lose some

2.3 Zooming Out: Representation in the Brain

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

31

of the oxygen they are transporting, which makes them more magnetic and
increases their response to the magnetic field. The fMRI apparatus, also
known as the "scanner," determines the relative activity of various
areas of the brain by detecting changes in the magnetic response of the
hemoglobin. The setup for an fMRI experiment is shown in Figure 2.16a,
with the person laying down such that their head is in the scanner. As a
person engages in a task, such as listening to certain sounds, the
activity of the brain is recorded. Importantly, fMRI is limited in that
it can't record activity from individual neurons. Instead, what's being
recorded is activity in subdivisions of the brain called voxels, which
are small cube-shaped areas of the brain about 2 or 3 mm on a side. Due
to their size, each voxel contains many neurons. Voxels are not brain
structures but are simply small units of analysis created by the fMRI
scanner. One way to think about voxels is that they are like the small
square pixels that make up the image on your computer screen, but
because the brain is threedimensional, voxels are small cubes rather
than small squares. Figure 2.16b shows the results of an fMRI scan.
Increases or decreases in brain activity associated with cognitive
activity are indicated by colors, with specific colors indicating the
amount of activation; usually "hotter" colors like red indicate higher
activation, while "cooler" colors like blue indicate lower activation.
Note how the brain appears pixelated in Figure 2.16b; each of those
small units is a voxel! It is important to realize that these colored
areas do not appear as the brain is being scanned. They are determined
by a calculation in which brain activity that occurred during the
cognitive task is compared to baseline activity (like while the
participant is at rest) or a different task. The results of this
calculation, which indicate increases or decreases in activity in
specific areas of the brain, are then converted into colored displays
like the one in Figure 2.16b.

Many researchers have used brain imaging techniques like fMRI in an
attempt to map a certain function onto a specific area of the brain. One
example of this from speech perception (continuing from our discussion
of Broca) is a study

Frontal cortex (FC)

Superior temporal sulcus (STS)

Occipital cortex (OC)

Temporal lobe

Fusiform gyrus (FG) (underside of the brain)

Figure 2.17 Location of the superior temporal sulcus (STS) in the
temporal lobe, where the "voice area" of the brain resides according to
a modular view of speech perception.

by Belin and coworkers (2000) which asked whether there is a brain area
that responds specifically when you hear a voice, compared to when you
hear other sounds. The participants reclined in the fMRI scanner and
passively listened to vocal sounds on some trials and non-vocal sounds,
like environmental sounds, on other trials. The results of the study
revealed an area in the temporal lobe---the superior temporal sulcus
(STS), (Figure   2.17)---that was activated significantly more in res­
ponse to vocal sounds than non-vocal sounds. This area was therefore
dubbed the "voice area" of the brain, given its highly specialized
response. The fact that a specific function---in this case, voice
perception---was able to be mapped onto a specific area of the brain in
this fMRI study supports a modular view of representation. Throughout
this book, we will see many more examples of modularity in the brain in
other senses. For instance, in Chapter 5, we'll discuss fMRI research
suggesting that there are specific brain areas for perceiving faces
versus other objects. But we'll also see how representation in the brain
often goes beyond individual modules. As we will see next, specific
perceptions are often associated with networks of brain areas that are
distributed across the cortex.

(a) 

32

Percent Activation --1

(b) 

0 +1 +2

Source: From Ishai et al., 2000

Photodisc/Jupiter Images

Figure 2.16 (a) A person in a brain scanner. (b) fMRI record. Each small
square represents a voxel, and the colors indicate whether brain
activity increased or decreased in each voxel. Red and yellow indicate
increases in brain activity; blue and green indicate decreases.

Chapter 2  Basic Principles of Sensory Physiology

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Distributed Representation

Sensory aspects, Location

Thinking about the brain in terms of modularity is still discussed today
(Kanwisher, 2010). But starting in the late 1900s, researchers also
began considering how multiple brain areas work together. One such
researcher is computer scientist Geoffrey Hinton, who, along with his
colleagues James McClelland and David Rumelhart, proposed that the brain
represents information in patterns distributed across the cortex, rather
than in one single brain area---a concept known as distributed
representation (Hinton et al., 1986). A distributed approach to
representation focuses on the activity in multiple brain areas and the
connections between those areas. Hinton's lifelong work on distributed
representation, as modeled by computer programs, won him a Turing Award
(also known as the "Nobel Prize of Computing") in 2018, which speaks to
the importance of this view of representation. One example of
distributed representation is how the brain responds to pain. When you
experience a painful stimulus, like accidentally touching a hot stove,
your perception involves multiple components. You might simultaneously
experience the sensory component ("it feels burning hot"), an emotional
component ("it's unpleasant"), and a reflexive motor component (pulling
your hand away). These different aspects of pain activate a number of
structures distributed across the brain (Figure 2.18). Thus, pain
presents a good example of how a single stimulus can cause widespread
activity. Another example of distributed representation is shown in
Figure 2.19. Figure 2.19a shows that the maximum activity for houses,
faces, and chairs occurs in separate areas in the cortex. This finding
is consistent with the idea that there are areas specialized for
specific stimuli. If, however, we look at Figure 2.19b, which shows all
of the activity for each type of stimulus, we see that houses, faces,
and chairs also cause activity over a

Unpleasantness Attention, Memory

Emotion, Avoidance

Figure 2.18 Areas that are involved in the perception of pain. Each area
serves a different aspect of pain perception.

wide area of the cortex (Ishai et al., 1999; 2000). We'll discuss
modular and distributed representation of objects further in Chapter 5.

Connections Between Brain Areas We've seen how a given perceptual
experience can involve multiple brain areas. But what about the
connections between those areas? Recent research has shown that
connections between brain areas may be just as important for perception
as the activity in each of those areas alone (Sporns, 2014). There are
two different approaches to exploring the connections between brain
areas. Structural connectivity is the "road map" of fibers connecting
different areas of the brain. Functional connectivity is the neural
activity associated

Houses (a) Segregation by category

Houses Chairs

Faces

Chairs

(b) Response magnitude Percent Activation

Faces No difference

--1

Bruce Goldstein

Maximal Response to:

Motivation, Temperature Relevance

0 +1 +2

Figure 2.19 fMRI responses of the human brain to houses, faces, and
chairs. (a) Areas activated most strongly by each type of stimulus; (b)
all areas activated by each type of stimulus, showing that each type of
stimulus activates multiple areas. (From Ishai et al., 2000) 2.3 Zooming
Out: Representation in the Brain

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

33

with a particular function that is flowing through this structural
network. The distinction between structural and functional connectivity
is similar to the one we described in "Method: Brain Imaging," where the
structure of the brain is measured using magnetic resonance imaging
(MRI) and the functioning of the brain is measured by functional
magnetic resonance imaging (fMRI). Here's another way to think about
structural and functional connectivity. Picture the road network of a
large city. On one set of roads, cars are streaming toward the shopping
area just outside the city, while on other roads cars are traveling
toward the city's business and financial district. One group of people
is using roads to reach places to shop; another group is using roads to
get to work or conduct business. Thus, the road map is analogous to the
brain's structural pathways and connections, and the different traffic
patterns are analogous to the brain's functional connectivity. Just as
different parts of the city's road network are involved in achieving
different goals, so different parts of the brain's neural network are
involved in carrying out different cognitive or motor goals. One way of
measuring functional connectivity involves using fMRI to measure resting
state activity of the brain. To understand what this means, let's return
to the "Brain Imaging" method on page 31. That method described the fMRI
measured as a person is engaged in a specific task, such as listening to
certain sounds. This type of fMRI is called task-related fMRI. It is
also possible to record fMRI when the brain is not involved in a
specific task. This fMRI is called the resting-state fMRI. Restingstate
fMRI is used to measure functional connectivity, as follows: Figure 2.20
How functional connectivity is determined by the resting-state fMRI
method. (a) Left hemisphere of the brain, showing the seed location
Motor (L) in the left motor cortex, and a number of test locations, each
indicated by a dot. Test location Motor (R) is in the right motor cortex
on the other side of the brain from Motor (L). Test location
Somatosensory is in the somatosensory cortex, which is involved in
perceiving touch. (b) Resting level fMRI response of the Motor (L) seed
location. (c) Resting level fMRI response of the Somatosensory test
location. The responses in (b) and (c) are 4 seconds long. (Responses
courtesy of Ying-Hui Chou)

Motor (L) Motor (R)

Resting-state functional connectivity is measured as follows: 1. Use
task-related fMRI to determine a brain location associated with carrying
out a specific task. For example, movement of the finger causes an fMRI
response at the location marked Motor (L) in Figure 2.20a. This location
is called the seed location. 2. Measure the resting-state fMRI at the
seed location. The resting-state fMRI of the seed location, shown in
Figure 2.20b, is called a time-series response because it indicates how
the response changes over time. 3. Measure the resting-state fMRI at
another location, which is called the test location. The response of the
test location Somatosensory, which is located in an area of the brain
responsible for sensing touch, is shown in Figure 2.20c. 4. Calculate
the correlation between the seed and test location responses. The
correlation is calculated using a complex mathematical procedure that
compares the seed and test responses at a large number of places along
the horizontal time axis. Figure 2.21a shows the response at the
Somatosensory test location superimposed on the seed response. The
correspondence between these responses results in a high correlation,
which indicates high functional connectivity. Figure 2.21b shows the
seed response and the response at another test location. The poor match
between these two responses results in a low correlation, which
indicates poor or no functional connectivity.

Somatosensory

(b) Response at Motor (L) seed location

(c) Response at Somatosensory test location

(d) 

Figure 2.21 Superimposed seed response (black) and test-location
response (red). (a) Response of the Somatosensory test location, which
is highly correlated with the seed response (correlation = 0.86). (b)
Response of another test location, which is poorly correlated with the
seed response (correlation = 0.04). (Responses courtesy of Yin-Hui Chou)

34

The Resting State Method of Measuring Functional Connectivity

METHOD

(a) Seed response (black) and test response at Somatosensory test
    location (red) Correlation = 0.86

(b) Seed response (black) and test response at another test location
    (red) Correlation = 0.04

Chapter 2  Basic Principles of Sensory Physiology

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Figure 2.22 shows the time-series for the seed location and a number of
test locations, and the correlations between the seed and test
locations. The test locations Somatosensory and Motor R are highly
correlated with the seed response and so have high functional
connectivity with the seed location. This is evidence that these
structures are part of a functional network. All of the other locations
have low correlations so are not part of the network. Resting-state
functional connectivity is one of the main methods for determining
functional connectivity, but there are also other methods. For example,
functional connectivity can be determined by measuring the task-related
fMRI at the seed and test locations and determining the correlations
between the two responses. It is important to note that saying two areas
are functionally connected does not necessarily mean that they directly
communicate by neural pathways. For example, the response from two areas
can be highly correlated because they are both receiving inputs from
another area. Functional connectivity and structural connectivity are
not, therefore, the same thing, but they are related, so regions with
high structural connectivity often show a high level of functional
connectivity (van den Heuvel & Pol, 2010). So why does it matter if
certain brain areas are functionally connected? What does this really
tell us about perception? One example of how functional connectivity can
help us understand perception is that it can be used to predict
behavior. A recent experiment by Sepideh Sadaghiani and coworkers (2015)
explored this by using fMRI to look at

Seed

0.74

Motor R

moment-to-moment functional connectivity across a network of brain
areas. The participants' task was to detect a very quiet sound that was
only perceptible 50 percent of the time---in other words, it was at the
participant's detection threshold (see Chapter 1, page 14). The
researchers found that the strength of functional connectivity
immediately before the detection task predicted how likely it was that
the person would hear the sound. So, the person was more likely to
report hearing the sound when their neural connections were stronger.
Other research has observed similar effects in other senses. For
example, a person's resting-state functional connectivity can predict
whether or not they will perceive a hot stimulus on their foot as
painful (Ploner et al., 2010). By examining the structural and
functional connectivity between brain areas in a network, in addition to
the activation in each brain area alone, researchers can get a more
comprehensive picture of how the brain represents our perceptual
experiences.

SOMETHING TO CONSIDER:

The Mind--Body Problem The main goal of our discussion so far has been
to explore the electrical signals that are the link between the
environment and our perception of the environment. The idea that nerve
impulses can represent things in the environment is what is

0.86

Somatosensory

0.09

--0.13

0.14

0.04

Figure 2.22 Resting-state fMRI responses for the Motor (L) seed, test
locations Motor (R), Somatosensory, and five test locations in other
parts of the brain. The numbers indicate correlations between the seed
response and each test-location response. Responses Motor (R) and
Somatosensory have been singled out because they have high correlations,
which indicates high functional connectivity with the seed. The other
locations have low correlations so are not functionally connected to the
seed location. (Responses courtesy of Yin-Hui Chou) Something to
Consider: The Mind--Body Problem

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

35

behind the following statement, written by Bernita Rabinovitz, a student
in my class. A human perceives a stimulus (a sound, a taste, etc.). This
is explained by the electrical impulses sent to the brain. This is so
incomprehensible, so amazing. How can one electrical impulse be
perceived as the taste of a sour lemon, another impulse as a jumble of
brilliant blues and greens and reds, and still another as bitter, cold
wind? Can our whole complex range of sensations be explained by just the
electrical impulses stimulating the brain? How can all of these varied
and very concrete sensations---the ranges of perceptions of heat and
cold, colors, sounds, fragrances and tastes---be merely and so
abstractly explained by differing electrical impulses? When Bernita asks
how hot and cold, colors, sounds, fragrances, and tastes can be
explained by electrical impulses, she is asking about the mind--body
problem: How do physical processes like nerve impulses (the body part of
the problem) become transformed into the richness of perceptual
experience (the mind part of the problem)? As we continue on to discuss
vision in the following chapters, and the other senses later in this
book, we'll see many examples of the connections between electrical
signals in the nervous system and what we perceive. We will see that
when we look out at a scene, countless neurons are firing---some to the
basic features of a stimulus (discussed in Chapter 4), and others to
entire objects, like faces or bodies (discussed in Chapter 5). You may
think that all of these connections between electrical signals and
perception provide a solution to the mind--body problem. This is not,
however, the case, because as impressive as these connections are, they
are all just correlations---demonstrations of relationships between
neural firing and perception (Figure 2.23a). But the mind--body problem
goes beyond asking how physiological responses correlate with
perception. It asks how physiological processes cause our experience.
Think about what this means. The mind--body problem is asking how the
flow of sodium and potassium ions across membranes that creates nerve
impulses becomes transformed into the experience we have when we see a
friend's face or when we experience the color of a red rose
(Figure   2.23b). Just showing that a neuron fires to a face or the
color red doesn't answer the question of how the firing creates the
experience of seeing a face or perceiving the color red. Thus, the
physiological research we describe in this book, although extremely
important for understanding the physiological mechanisms responsible for
perception, does not provide a solution to the mind--body problem.
Researchers (Baars, 2001; Crick & Koch, 2003) and philosophers (Block,
2009) may discuss the mind--body problem, but when researchers step into
the laboratory, their efforts are devoted to doing experiments like the
ones we have discussed so far, which search for correlations between
physiological responses and experience.

36

Experience

Correlation

"Susan's face"

"red"

(a) Typical physiological experiment

Experience Na+

Cause "Susan's face"

"red"

(b) Mind--body problem

Figure 2.23 (a) This illustrates the situation for most of the
physiological experiments we will be describing in this book, which
determine correlations between physiological responding such as nerve
firing and experiences such as perceiving "Susan's face" or "red." (b)
Solving the mind--body problem requires going beyond demonstrating
correlations to determine how ion flow or nerve firing causes the
experiences of "Susan's face" or the color "red."

TEST YOuRSELF 2.2 1. What is phrenology, and what insight did it provide
into neural representation? 2. Explain how neuropsychological case
studies can support a modular view of neural representation, using
Broca's research as an example. 3. Describe the technique of brain
imaging. How can fMRI be used to study modularity? 4. What is
distributed representation? Provide an example from one of the senses.
5. Discuss the difference between structural and functional
connectivity. Which technique might be used if one was interested in
studying the neural connections associated with a certain task, and why?
6. Describe how functional connectivity is determined. What is the
resting-state method? 7. How can functional connectivity provide insight
into perception? 8. What is the mind--body problem? Why do we say that
demonstrating connections between nerve firing and a particular stimulus
like a face or a color does not solve the mind--body problem?

Chapter 2  Basic Principles of Sensory Physiology

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

THINK ABOUT IT 1. Because the long axons of neurons look like electrical
wires, and both neurons and electrical wires conduct electricity, it is
tempting to equate the two. Compare the functioning of axons and
electrical wires in terms of their structure and the nature of the
electrical signal they conduct.

2.  We described pain as consisting of multiple components. Can you
    think of ways that other objects or experiences consist of multiple
    components? If you can, what does that say about the neural
    representation of these objects or experiences?

KEY TERMS Action potential (p. 22) Axon (p. 22) Brain imaging (p. 31)
Broca's area (p. 31) Cell body (p. 21) Dendrites (p. 22) Depolarization
(p. 24) Distributed representation (p. 33) Excitatory response (p. 26)
Falling phase of the action potential (p. 25) Functional connectivity
(p. 33) Functional magnetic resonance imaging (fMRI) (p. 31) Grandmother
cell (p. 27) Hyperpolarization (p. 25) Inhibitory response (p. 26)

Ions (p. 24) Magnetic resonance imaging (MRI) (p. 31) Mind--body problem
(p. 36) Modularity (p. 31) Module (p. 31) Nerve fiber (p. 22) Neurons
(p. 21) Neuropsychology (p. 31) Neurotransmitters (p. 26) Permeability
(p. 24) Phrenology (p. 30) Population coding (p. 29) Propagated response
(p. 23) Receptor sites (p. 26) Refractory period (p. 24)

Resting potential (p. 22) Resting-state fMRI (p. 34) Resting-state
functional connectivity (p. 34) Rising phase of the action potential
(p. 24) Seed location (p. 34) Sensory coding (p. 27) Sparse coding
(p. 29) Specificity coding (p. 27) Spontaneous activity (p. 24)
Structural connectivity (p. 33) Synapse (p. 25) Task-related fMRI
(p. 34) Test location (p. 34) Wernicke's area (p. 31)

Key Terms

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

37

One message of this book is that perceptual experience is shaped by
properties of the perceptual system. The sharp, colorful scene
represents perception created by activation of cone receptors in the
retina. The less focused, grey-scale scene represents perception created
by activation of rod receptors in the retina. Bruce Goldstein

Learning Objectives After studying this chapter, you will be able to ...
■■ Identify the key structures of the eye and describe how they

work together to focus light on the retina. ■■ Explain how light is
transduced into an electrical signal. ■■ Distinguish between the
influence of rods and cones on

■■ Describe how lateral inhibition and convergence underlie

center-surround antagonism in ganglion cell receptive fields. ■■
Understand the development of visual acuity over the first

year of life.

perception in both dark and light environments. ■■ Use your knowledge of
neural processing to explain how

signals travel through the retina.

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

C hapter 3

The Eye and Retina

Chapter Contents 3.3 Photoreceptor Processes

DEMONSTRATION: Foveal Versus

Transforming Light Energy Into Electrical Energy Adapting to the Dark

Ganglion Cell Receptive Fields

DEMONSTRATION: Becoming Aware

METHOD: Measuring the Dark

Events Are Powerful

DEMONSTRATION: Filling in the Blind

Spectral Sensitivity

Visual Acuity

METHOD: Measuring a Spectral

METHOD: Preferential Looking

3.1 Light, the Eye, and the Visual Receptors Light: The Stimulus for
Vision The Eye

of the Blind Spot Spot

Adaptation Curve

3.2 Focusing Light Onto the Retina

Sensitivity Curve

Accommodation

TEST YOURSELF 3.1

DEMONSTRATION: Becoming Aware

of What Is in Focus

3.4 What Happens as Signals Travel Through the Retina

Refractive Errors

Rod and Cone Convergence

Some Questions We Will Consider: ■■ How does the focusing system at the
front of our eye af-

fect our perception? (p. 43) ■■ How do chemicals in the eye called
visual pigments affect

our perception? (p. 45) ■■ How can the way neurons are "wired up" in the
retina

affect perception? (p. 51)

W

e begin with the story of Larry Hester---a retired tire salesman from
North Carolina. In his early 30s, Larry began noticing a rapid decline
in his vision. He had always had poor eyesight, but this was different;
it almost looked like the world was closing in on him. Upon seeing an
ophthalmologist, Larry was given the shocking news that he had a genetic
disorder of the eye called retinitis pigmentosa that would result in
total blindness, and that there was no stopping it (Graham, 2017). Larry
lost his vision and was left in complete darkness for the next 33 years.
But then something amazing happened: He had the opportunity to have some
of his vision restored. Larry was a candidate for a new technology
referred to as the bionic

Peripheral Acuity

SOMETHING TO CONSIDER: Early DEVELOPMENTAL DIMENSION: Infant

TEST YOURSELF 3.2 THINK ABOUT IT

eye---an array of electrodes implanted in the back of the eye that,
through a camera mounted on eyeglasses, sends signals to the visual
system about what is "out there" in the world (Da Cruz et al., 2013;
Humayun et al., 2016). While the bionic eye doesn't completely restore
vision, it allows the person to see contrasting lightness versus
darkness, such the edge between where one object ends and another
begins---a concept we'll return to later in this chapter. This might not
seem very impressive to someone with normal vision, but to Larry, who
couldn't see anything for half of his life, suddenly being able to see
the lines in the crosswalk or the edges of his wife's face was
monumental. It meant that he could use vision to interact with his world
again. As Larry once described in an interview, "Light is so basic and
probably wouldn't have significance to anybody else, but to me, it means
I can see." Larry's story demonstrates the importance of light, the
eyes, and the cells at the back of the eyes. A great deal of processing
takes place within the eyes. This chapter focuses on these processes and
marks the beginning of our journey into the sense of vision. After we
discuss the early stages of the visual perceptual process in this
chapter, Chapter 4 will go on to discuss the later stages of processing
that occur when the signals leave the eye and reach the brain. Then,
Chapters 5--10 will

39

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Cone

Seeing in focus STEP 1 Distal stimulus: The tree

STEP 2 Light is reflected and transformed to create an image of the tree
on the retina.

Rod

Seeing in dim light

Seeing fine details

STEP 3 Receptor processes: Receptors transform light into electricity.

STEP 4 Neural processing: Signals travel in a network of neurons.

Figure 3.1 Chapter preview. This chapter will describe the first three
steps of the perceptual process for vision and will introduce step 4.
Physical processes for each step are indicated along the bottom; the
perceptual outcomes of these processes are indicated in blue.

discuss more specific aspects of vision, such as how we perceive
objects, motion, and color. Figure 3.1 shows the first four steps of the
visual process. Following the sequence of the physical events in the
process, shown in black along the bottom of the figure, we begin with
Step 1, the distal stimulus (the tree); then move to Step 2, in which
light is reflected from the tree and enters the eye to create the
proximal stimulus on the visual receptors; then to Step 3, in which
receptors transform light into electrical signals; and finally to Step
4, in which electrical signals are "processed" as they travel through a
network of neurons. Our goal in this chapter is to show how these
physical events influence the following aspects of perception, shown in
blue in Figure 3.1: (1) seeing in focus, (2) seeing in dim light, and
(3) seeing fine details. We begin by describing light, the eye, and the
receptors in the retina that line the back of the eye.

wavelength---the distance between the peaks of the electromagnetic
waves. The wavelengths in the electromagnetic spectrum range from
extremely short-wavelength gamma rays (wavelength 5 about 10212 meters,
or one ten-billionth of a meter) to long-wavelength radio waves
(wavelength 5 about 104 meters, or 10,000 meters). Visible light, the
energy within the electromagnetic spectrum that humans can perceive, has
wavelengths ranging from about 400 to 700 nanometers (nm), where 1
nanometer 5 1029 meters, which means that the longest visible
wavelengths are slightly less than one-thousandth of a millimeter long.
For humans and some other animals, the wavelength of visible light is
associated with the different colors of the spectrum, with short
wavelengths appearing blue, middle wavelengths green, and long
wavelengths yellow, orange, and red.

3.1 Light, the Eye, and the Visual Receptors

The eyes contain the receptors for vision. The first eyes, which
appeared back in the Cambrian period (570--500 million years ago), were
eyespots on primitive animals such as flatworms that could distinguish
light from dark but couldn't detect features of the environment.
Detecting an object's details didn't become possible until more
sophisticated eyes evolved to include optical systems that could produce
images and therefore provide information about shapes and details of
objects and the arrangement of objects within scenes (Fernald, 2006).
Light reflected from objects in the environment enters the eye through
the pupil and is focused by the cornea and lens to form sharp images of
the objects on the retina, the network of neurons that covers the back
of the eye and that contains the receptors for vision, also known as
photoreceptors (Figure 3.2a). There are two types of photoreceptors,
rods and

The ability to see a tree, or any other object, depends on light being
reflected from that object into the eye.

Light: The Stimulus for Vision Vision is based on visible light, which
is a band of energy within the electromagnetic spectrum. The
electromagnetic spectrum is a continuum of electromagnetic energy that
is produced by electric charges and is radiated as waves (see Figure
1.23, page 18). The energy in this spectrum can be described by its 40

The Eye

Chapter 3  The Eye and Retina

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Optic nerve fibers

Photoreceptors (rods and cones)

Light

Back of eye

Rod

Pupil Cone

Fovea (point of central focus) Optic nerve

Cornea

Retina Pigment epithelium

Lens Retina (a)

(b) 

Figure 3.2 An image of the tree is focused on the retina, which lines
the back of the eye. The close-up of the retina on the right shows the
receptors and other neurons that make up the retina.

cones, so called because of the rod- and cone-shaped outer segments
(Figure 3.3). The outer segments are the part of the receptor that
contains light-sensitive chemicals called visual pigments that react to
light and trigger electrical signals. Signals from the receptors flow
through the network of neurons that make up the retina (Figure 3.2b) and
emerge from the back of the eye in the optic nerve, which contains a
million optic nerve fibers that conduct signals toward the brain. The
rod and cone receptors not only have different shapes, they are also
distributed differently across the retina. From Figure 3.4, which
indicates the rod and cone distributions, we can conclude the
following: 1. One small area, the fovea, contains only cones. When we
look directly at an object, the object's image falls on the fovea. 2.
The peripheral retina, which includes all of the retina outside of the
fovea, contains both rods and cones. It is important to note that
although the fovea has

One way to appreciate the fact that the rods and cones are distributed
differently in the retina is by considering what happens when
functioning receptors are missing from one area of the retina. A
condition called macular degeneration, which is most common in older
people, destroys the cone-rich fovea and a small area that surrounds it.
(Macula is a term usually associated with medical practice that includes
the fovea plus a small area surrounding the fovea.) This creates a blind
region in central vision, so when a person looks directly at something,
he or she loses sight of it (Figure 3.5a).

Figure 3.3 (a) Scanning electromicrograph of the rod and cone receptors
in the retina, showing the rod-shaped and cone-shaped receptor outer
segments. (b) Rod and cone receptors, showing the inner and outer
segments. The outer segments contain the light-sensitive visual pigment.
(From Lewis et al., 1969)

Rod Cone

only cones, there are also many cones in the peripheral retina. The
fovea is so small (about the size of this "o") that it contains only
about 1 percent, or 50,000, of the 6 million cones in the retina (Tyler,
1997a, 1997b). 3. The peripheral retina contains many more rods than
cones because there are about 120 million rods and only 6 million cones
in the retina.

Outer segment

Elsevier Science

Inner segment

Rod (a)

Cone

(b) 

3.1 Light, the Eye, and the Visual Receptors

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

41

180,000

Cones Rods

Blind spot (no receptors)

Fovea

808

808

608

608 Blind spot

408 208

08

Fovea

408 208

Number of receptors per square millimeter

160,000

Optic nerve

140,000 120,000 100,000 80,000 60,000 40,000 20,000 0 708 608 508 408
308 208 108

08

108 208 308 408 508 608 708 808

Angle (degree)

Figure 3.4 The distribution of rods and cones in the retina. The eye on
the left indicates locations in degrees relative to the fovea. These
locations are repeated along the bottom of the chart on the right. The
vertical brown bar near 20 degrees indicates the place on the retina
where there are no receptors because this is where the ganglion cells
leave the eye to form the optic nerve. (Adapted from Lindsay & Norman,
1977)

Bruce Goldstein

Another condition, retinitis pigmentosa, which led to Larry Hester's
blindness, is a degeneration of the retina that is passed from one
generation to the next (although not always affecting everyone in a
family). This condition first attacks the peripheral rod receptors and
results in poor vision in the peripheral visual field (Figure 3.5b).
Eventually, in severe cases, the foveal cone receptors are also
attacked, resulting in complete blindness. Before leaving the rod--cone
distribution shown in Figure 3.4, note that there is one area in the
retina, indicated by the vertical brown bar, where there are no
photoreceptors. Figure 3.6 shows a close-up of the place where this
occurs, which is where the nerve fibers that make up the optic nerve
leave the eye. Because of the absence of photoreceptors, this place is
called the blind spot. Although

(a) 
(b) Receptors

Bruce Goldstein

Blind spot

(b) 

Figure 3.5 (a) In a condition called macular degeneration, the fovea and
surrounding area degenerate, so the person cannot see whatever he or she
is looking at. (b) In retinitis pigmentosa, the peripheral retina
initially degenerates and causes loss of vision in the periphery. The
resulting condition is sometimes called "tunnel vision." 42

Optic nerve

Figure 3.6 There are no receptors at the place where the optic nerve
leaves the eye. This enables the receptors' ganglion cell fibers to flow
into the optic nerve. The absence of receptors in this area creates the
blind spot.

Chapter 3  The Eye and Retina

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

you are not normally aware of the blind spot, you can become aware of it
by doing the following demonstration. DEMONSTRATION

Becoming Aware of the Blind Spot

Place the book (or your electronic device if you are reading the ebook)
on your desk. Close your right eye, and position yourself above the
book/device so that the cross in Figure 3.7 is aligned with your left
eye. Be sure the book page is flat and, while looking at the cross,
slowly move closer. As you move closer, be sure not to move your eye
from the cross, but at the same time keep noticing the circle off to the
side. At some point, around 3 to 9 inches from the book/device, the
circle should disappear. When this happens, the image of the circle is
falling on your blind spot.

Figure 3.7 Blind spot demonstration.

Why aren't we usually aware of the blind spot? One reason is that the
blind spot is located off to the side of our visual field, where objects
are not in sharp focus. Because of this and because we don't know
exactly where to look for it (as opposed to the demonstration, in which
we are focusing our attention on the circle), the blind spot is hard to
detect. But the most important reason that we don't see the blind spot
is that a mechanism in the brain "fills in" the place where the image
disappears (Churchland & Ramachandran, 1996). The next demonstration
illustrates an important property of this filling-in process.
DEMONSTRATION

Filling in the Blind Spot

Close your right eye and, with the cross in Figure 3.8 lined up with
your left eye, move toward the wheel . When the center of the wheel
falls on your blind spot, notice how the spokes of the wheel fill in the
hole (Ramachandran, 1992).

These demonstrations show that the brain does not fill in the area
served by the blind spot with "nothing"; rather, it creates a perception
that matches the surrounding pattern--- the grey page in the first
demonstration, and the spokes of the wheel in the second one. This
"filling in" is a preview of one of the themes of the book: how the
brain creates a coherent perception of our world. For now, however, we
return to the visual process, as light reflected from objects in the
environment is focused onto the receptors (step 2 in Figure 3.1).

3.2 Focusing Light Onto the Retina Light reflected from an object into
the eye is focused onto the retina by a two-element optical system: the
cornea and the lens (Figure 3.9a). The cornea, the transparent covering
of the front of the eye, accounts for about 80 percent of the eye's
focusing power, but like the lenses in eyeglasses, it is fixed in place
so it can't adjust its focus. The lens, which supplies the remaining 20
percent of the eye's focusing power, can change its shape to adjust the
eye's focus for objects located at different distances. This change in
shape is achieved by the action of ciliary muscles, which increase the
focusing power of the lens (its ability to bend light) by increasing its
curvature (compare Figure 3.9b and Figure 3.9c). We can understand why
the eye needs to adjust its focus by first considering what happens when
the eye is relaxed and a person with normal (20/20) vision views a small
object that is far away. If the object is located more than about 20
feet away, the light rays that reach the eye are essentially parallel
(Figure 3.9a), and the cornea--lens combination brings these parallel
rays to a focus on the retina at point A. But if the object moves closer
to the eye, the light rays reflected from this object enter the eye at
more of an angle, and this pushes the focus point back so if the back of
the eye weren't there, light would be focused at point B (Figure 3.9b).
Because the light is stopped by the back of the eye before it reaches
point B, the image on the retina is out of focus, so if things remained
in this state, the person would see the object as blurred. The
adjustable lens, which controls a process called accommodation, comes to
the rescue to help prevent blurring.

Accommodation

Figure 3.8 View the pattern as described in the text, and observe what
happens when the center of the wheel falls on your blind spot. (Adapted
from Ramachandran, 1992)

Accommodation is the change in the lens's shape that occurs when the
ciliary muscles at the front of the eye tighten and increase the
curvature of the lens so that it gets thicker (Figure 3.9c). This
increased curvature increases the bending of the light rays passing
through the lens so the focus point is pulled from point B back to A to
create a sharp image on the retina. This means that as you look around
at different objects, your eye is constantly adjusting its focus by
accommodating, especially for nearby objects. The following
demonstration shows that this is necessary because everything is not in
focus at once. 3.2 Focusing Light Onto the Retina

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

43

Lens Retina

Cornea

A

(a) Object far--- eye relaxed

Focus on retina

(d) Myopia--- eye relaxed

Focus in front of retina

Moving object closer pushes focus point back B

(b) Object near--- eye relaxed

Focus behind retina Accommodation brings focus point forward

Corrective lens (e) Correction of myopia

A

(c) Object near--- accommodation

Focus on retina

Figure 3.9 Focusing of light rays by the eye. (a) Rays of light coming
from a small light source that is more than 20 feet away are
approximately parallel. The focus point for parallel light is at A on
the retina. (b) Moving an object closer to the relaxed eye pushes the
focus point back. Here the focus point is at B, but light is stopped by
the back of the eye, so the image on the retina is out of focus. (c)
Accommodation of the eye (indicated by the fatter lens) increases the
focusing power of the lens and brings the focus point for a near object
back to A on the retina, so it is in focus. This accommodation is caused
by the action of the ciliary muscles, which are not shown. (d) In the
myopic (nearsighted) eye, parallel rays from a distant spot of light are
brought to a focus in front of the retina, so distant objects appear
blurred. (e) A corrective lens bends light so it is focused on the
retina.

DEMONSTRATION

Becoming Aware of What Is in Focus

Accommodation occurs unconsciously, so you are usually unaware that the
lens is constantly changing its focusing power to let you see clearly at
different distances. This unconscious focusing process works so
efficiently that most people assume that everything, near and far, is
always in focus. You can demonstrate that this is not so by holding a
pen or a pencil, point up, at arm's length, closing one eye, and looking
past the pencil at an object that is at least 20 feet away. As you stay
focused on the faraway object, notice the pencil point without actually
looking at it (be sure to stay focused on the far object). The point
will probably appear slightly blurred. Then slowly move the pencil
toward you while still looking at the far object. Notice that as the
pencil moves closer, the point becomes more blurred. When the pencil is
about 12 inches away, shift your focus to the pencil point. This shift
in focus causes the pencil point to appear sharp, but the far object is
now out of focus. 44

When you changed focus from far away to the nearby pencil point during
this demonstration, you were changing your accommodation. Either near
objects or far objects can be in focus, but not both at the same time.
Accommodation, therefore, makes it possible to adjust vision for
different distances.

Refractive Errors While accommodation can help put things into focus,
it's not foolproof; sometimes focusing the image onto the retina fails,
even with accommodation. There are a number of errors that can affect
the ability of the cornea and/or lens to focus the visual input onto the
retina. Collectively, these are called refractive errors. The first
refractive error that we will discuss often occurs in normal aging. As
people get older, their ability to accommodate decreases due to
hardening of the lens and weakening of the ciliary muscles, and so they
become

Chapter 3  The Eye and Retina

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

unable to accommodate enough to see objects, or read, at close range.
This age-related loss of the ability to accommodate, called presbyopia
(for "old eye"), can be dealt with by wearing reading glasses, which
brings near objects into focus by replacing the focusing power that can
no longer be provided by the lens. Another refractive error that can be
solved by a corrective lens is myopia, or nearsightedness, an inability
to see distant objects clearly. The reason for this difficulty, which
affects more than 70 million Americans, is illustrated in Figure 3.9d.
Myopia occurs when the optical system brings parallel rays of light into
focus at a point in front of the retina, so the image that reaches the
retina is blurred. This problem can be caused by either of two factors:
(1) refractive myopia, in which the cornea and/or the lens bends the
light too much, or (2) axial myopia, in which the eyeball is too long.
Either way, images of faraway objects are not focused sharply.
Corrective lenses can solve this problem, as shown in Figure 3.9e.
Finally, people with hyperopia, or farsightedness, can see distant
objects clearly but have trouble seeing nearby objects because the focus
point for parallel rays of light is located behind the retina, usually
because the eyeball is too short. Young people can bring the image
forward onto the retina by accommodating. However, older people, who
have difficulty accommodating, often use corrective lenses that bring
the focus point forward onto the retina. Focusing an image clearly onto
the retina is the initial step in the process of vision, but although a
sharp image on the retina is essential for clear vision, we do not see
the image on the retina. Vision occurs not in the retina but in the
brain. Before the brain can create vision, the light on the retina must
activate the photoreceptors in the retina. This brings us to the next
step of the visual process (step 3 in Figure 3.1): processing by the
photoreceptors.

Molecule in dark

3.3 Photoreceptor Processes Now that we know how light is reflected and
focused onto the retina, we next need to understand how the
photoreceptors react to that incoming light. As we will see, the
light-sensitive visual pigments (see Figure 3.2) play a key role in
these photoreceptor processes. In this section, we first describe
transduction, and then how the photoreceptors shape perception.

Transforming Light Energy Into Electrical Energy Transduction is the
transformation of one form of energy into another form of energy (see
Chapter 1, page 8). Visual transduction occurs in photoreceptors (the
rods and cones) and transforms light into electricity. The starting
point for understanding how the rods and cones create electricity are
the millions of molecules of a light-sensitive visual pigment that are
contained in the outer segments of the photoreceptors (Figure 3.3).
Visual pigments have two parts: a long protein called opsin and a much
smaller lightsensitive component called retinal. Figure 3.10a shows a
model of a retinal molecule attached to opsin (Wald, 1968). Note that
only a small part of the opsin is shown here; it is actually hundreds of
times longer than the retinal. Despite its small size compared to the
opsin, retinal is the crucial part of the visual pigment molecule,
because when the retinal and opsin are combined, the resulting molecule
absorbs visible light. When incoming light hits the retina, the first
step of transduction is initiated: the visual pigment molecule absorbs
the light. This causes the retinal within that molecule to change its
shape, from being bent, as shown in Figure 3.10a, to straight, as shown
in Figure 3.10b. This change of shape, called

Retinal isomerized by light

Bruce Goldstein

Retinal

Opsin (a)

(b) 

Figure 3.10 Model of a visual pigment molecule. The horizontal part of
the model shows a tiny portion of the huge opsin molecule near where the
retinal is attached. The smaller molecule on top of the opsin is the
light-sensitive retinal. (a) The retinal molecule's shape before it
absorbs light. (b) The retinal molecule's shape after it absorbs light.
This change in shape, which is called isomerization, triggers a sequence
of reactions that culminates in generation of an electrical response in
the receptor. 3.3 Photoreceptor Processes

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

45

illumination. We will describe how the dark adaptation curve is
measured, and how the increase in sensitivity that occurs in the dark
has been linked to properties of the rod and cone visual pigments.

One visual pigment molecule

Measuring the Dark Adaptation Curve The study of dark adaptation begins
with measuring the dark adaptation curve, which is the function relating
sensitivity to light to time in the dark, beginning when the lights are
extinguished. METHOD

Figure 3.11 This sequence symbolizes the chain reaction that is
triggered when a single visual pigment molecule is isomerized by
absorption of light. In the actual sequence of events, each visual
pigment molecule activates hundreds more molecules, which, in turn, each
activate about a thousand molecules. Isomerization of just one visual
pigment molecule activates about a million other molecules, which
activates the receptor.

isomerization, creates a chemical chain reaction, illustrated in Figure
3.11, that activates thousands of charged molecules to create electrical
signals in receptors (Baylor, 1992; Hamer et al., 2005). Through this
amplification, the initial isomerization of just one visual pigment
molecule can ultimately lead to activation of the entire photoreceptor.
An electrical signal has now been created, which means that transduction
is complete. Next, we will demonstrate how properties of the visual
pigments influence perception. We do this by comparing the perceptions
caused by the rod and cone photoreceptors. As we will see, the visual
pigments in these two types of photoreceptors influence two aspects of
visual perception: (1) how we adjust to darkness, and (2) how well we
see light in different parts of the spectrum.

Measuring the Dark Adaptation Curve

The first step in measuring a dark adaption curve is to have the
participant look at a small fixation point while paying attention to a
flashing test light that is off to the side (Figure 3.12). Because the
participant is looking directly at the fixation point, its image falls
on the fovea, so the image of the test light, which is off to the side,
falls on the peripheral retina, which contains both rods and cones.
While still in the light, the participant turns a knob that adjusts the
intensity of the flashing light until it can just barely be seen (this
is the method of adjustment introduced in Chapter 1). This threshold for
seeing the light, the minimum amount of energy necessary to just barely
see the light, is then converted to sensitivity. Because sensitivity 5
1/threshold, this means that a high threshold corresponds to low
sensitivity. The sensitivity measured in the light is called the
light-adapted sensitivity, because it is measured while the eyes are
adapted to the light. Because the room (or adapting) lights are on, the
intensity of the flashing test light has to be high to be seen. At the
beginning of the experiment, then, the threshold is high and the
sensitivity is low. Once the light-adapted sensitivity to the flashing
test light is determined, the adapting light is extinguished so the
participant is in the dark. The participant continues adjusting the
intensity of the flashing light so he or she can just barely see it,
tracking the increase in sensitivity that occurs in the dark. As the
participant becomes more sensitive to the light, he or she must decrease
the light's intensity to keep it just barely visible. The result, shown
as the red curve in Figure 3.13, is a dark adaptation curve. Peripheral
retina

Fixation point

Adapting to the Dark When we discussed measuring perception in Chapter
1, we noted that when a person goes from a lighted environment to a dark
place, it may be difficult to see at first, but that after some time in
the dark, the person becomes able to make out lights and objects that
were invisible before (Figure 1.18, page 16). This process of increasing
sensitivity in the dark, called dark adaptation, is measured by
determining a dark adaptation curve. In this section we will show how
the rod and cone receptors control an important aspect of vision: the
ability of the visual system to adjust to dim levels of

46

Fovea Test light

Figure 3.12 Viewing conditions for a dark adaptation experiment. In this
example, the image of the fixation point falls on the fovea, and the
image of the test light falls on the peripheral retina.

Chapter 3  The Eye and Retina

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Pure cone curve Pure rod curve Both rods and cones Rod light-adapted
sensitivity

Logarithm of sensitivity

Low

Cone light-adapted sensitivity

Rod--cone break

Figure 3.13 Three dark adaptation curves. The red line is the two-stage
dark adaptation curve, with an initial cone branch and a later rod
branch, which occurs when the test light is in the peripheral retina, as
shown in Figure 3.12. The green line is the cone adaptation curve, which
occurs when the test light falls on the fovea. The purple curve is the
rod adaptation curve measured in a rod monochromat. Note that the
downward movement of these curves represents an increase in sensitivity.
The curves actually begin at the points indicating "lightadapted
sensitivity," but there is a slight delay between the time the lights
are turned off and when measurement of the curves begins.

C Maximum cone sensitivity

Dark-adapted sensitivity R Maximum rod sensitivity

High 10

20 Time in dark (min)

The dark adaptation curve shows that as adaptation proceeds, the
participant becomes more sensitive to the light. Note that higher
sensitivity is at the bottom of this graph, so movement of the dark
adaptation curve downward means that the participant's sensitivity is
increasing. The red dark adaptation curve indicates that the
participant's sensitivity increases in two phases. It increases rapidly
for the first 3 to 4 minutes after the light is extinguished and then
levels off. At about 7 to 10 minutes, it begins increasing again and
continues to do so until the participant has been in the dark for about
20 or 30 minutes (Figure 3.13). The sensitivity at the end of dark
adaptation, labeled dark-adapted sensitivity, is about 100,000 times
greater than the light-adapted sensitivity measured before dark
adaptation began. Dark adaptation was involved in a 2007 episode of the
Mythbusters program on the Discovery Channel, which was devoted to
investigating myths about pirates. One of the myths was that pirates
wore eye patches to preserve night vision in one eye so that when they
went from the bright light outside to the darkness below decks, moving
the patch to the light-adapted eye would enable them to see with the
dark-adapted eye. To determine whether this would work, the Mythbusters
carried out some tasks in a dark room just after both of their eyes had
been in the light and did some different tasks with an eye that had
previously been covered with a patch for 30 minutes. It isn't surprising
that they completed the tasks much more rapidly when using the eye that
had been patched. Anyone who has taken a course on sensation and
perception could have told the Mythbusters that the eye patch would work
because keeping an eye in

the dark triggers the process of dark adaptation, which causes the eye
to increase its sensitivity in the dark. Whether pirates actually used
patches to help them see below decks remains an unproven hypothesis. One
argument against the idea that pirates wore eye patches to keep their
sensitivity high is that patching one eye causes a decrease in depth
perception, which might be a serious disadvantage when the pirate is
working on deck. We will discuss why two eyes are important for depth
perception in Chapter 10. Although the Mythbusters showed that dark
adapting one eye made it easier to see with that eye in the dark, we
have a more specific goal. We are interested in showing that the first
part of the dark adaptation curve is caused by the cones and the second
part is caused by the rods. We will do this by running two additional
dark adaptation experiments, one measuring adaptation of the cones and
another measuring adaptation of the rods.

Measuring Cone Adaptation The reason the red curve in Figure 3.13 has
two phases is that the flashing test light fell on the peripheral
retina, which contains both rods and cones. To measure dark adaptation
of the cones alone, we have to ensure that the image of the test light
falls only on cones. We achieve this by having the participant look
directly at the test light so its image falls on the all-cone fovea, and
by making the test light small enough so that its entire image falls
within the fovea. The dark adaptation curve determined by this procedure
is indicated by the green line in Figure 3.13. This curve, 3.3
Photoreceptor Processes

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

47

which measures only the activity of the cones, matches the initial phase
of our original dark adaptation curve but does not include the second
phase. Does this mean that the second part of the curve is due to the
rods? We can show that the answer to this question is "yes" by doing
another experiment.

Measuring Rod Adaptation We know that the green curve in Figure 3.13 is
due only to cone adaptation because our test light was focused on the
all-cone fovea. Because the cones are more sensitive to light at the
beginning of dark adaptation, they control our vision during the early
stages of adaptation, so we can't determine what the rods are doing. In
order to reveal how the sensitivity of the rods is changing at the very
beginning of dark adaptation, we need to measure dark adaptation in a
person who has no cones. Such people, who have no cones because of a
rare genetic defect, are called rod monochromats. Their all-rod retinas
provide a way for us to study rod dark adaptation without interference
from the cones. (Students sometimes wonder why we can't simply present
the test flash to the peripheral retina, which contains mostly rods. The
answer is that there are enough cones in the periphery to influence the
beginning of the dark adaptation curve.) Because the rod monochromat has
no cones, the lightadapted sensitivity we measure just before we turn
off the lights is determined by the rods. The sensitivity we determine,
which is labeled "rod light-adapted sensitivity" in Figure 3.13,
indicates that the rods are much less sensitive than the cone
lightadapted sensitivity we measured in our original experiment. We can
also see that once dark adaptation begins, the rods increase their
sensitivity, as indicated by the purple curve, and reach their final
dark-adapted level in about 25 minutes (Rushton, 1961). The end of this
rod adaptation measured in our monochromat matches the second part of
the two-stage dark adaptation curve. Based on the results of our dark
adaptation experiments, we can summarize the process of dark adaptation.
As soon Figure 3.14 A frog retina was dissected from the eye in the dark
and then exposed to light. The top row shows how the relationship
between retinal and opsin changes after the retinal absorbs light. Only
a small part of the opsin molecule is shown. The photographs in the
bottom row show how the color of the retina changes after it is exposed
to light. (a) This picture of the retina was taken just after the light
was turned on. The dark red color is caused by the high concentration of
visual pigment in the receptors that are still in the unbleached state.
(b, c) After the retinal isomerizes, the retinal and opsin break apart,
and the retina becomes bleached, as indicated by the lighter color.

Visual Pigment Regeneration From our description of transduction earlier
in the chapter, we know that light causes the retinal part of the visual
pigment molecule, which is initially bent as shown in Figure 3.10a, to
change its shape as in Figure 3.10b. This change from bent to straight
is shown in the upper panels of Figure 3.14, which also shows how the
retinal eventually separates from the opsin part of the molecule. This
change in shape and separation from the opsin causes the molecule to
become lighter in color, a process called visual pigment bleaching. This
bleaching is shown in the lower panels of Figure 3.14. Figure 3.14a is a
picture of a frog retina that was taken moments after it was illuminated
with light.

Retinal

Opsin

Opsin

Bruce Goldstein

Opsin

(a) 

48

as the light is extinguished, the sensitivity of both the cones and the
rods begins increasing. However, because the cones are much more
sensitive than the rods at the beginning of dark adaptation, we see with
our cones right after the lights are turned out. One way to think about
this is that the cones have "center stage" at the beginning of dark
adaptation, while the rods are working "behind the scenes." However,
after about 3 to 5 minutes in the dark, the cones have reached their
maximum sensitivity, as indicated by the leveling off of the dark
adaptation curve. Meanwhile, the rods are still adapting behind the
scenes, and by about 7 minutes in the dark, the rods' sensitivity
finally catches up to the cones'. The rods then become more sensitive
than the cones, and rod adaptation, indicated by the second branch of
the dark adaptation curve, becomes visible. The place where the rods
begin to determine the dark adaptation curve instead of the cones is
called the rod--cone break. Why do the rods take about 20 to 30 minutes
to reach their maximum sensitivity (point R on the curve) compared to
only 3 to 4 minutes for the cones (point C)? The answer to this question
involves a process called visual pigment regeneration, which occurs more
rapidly in the cones than in the rods.

(b) 
(c) 

Chapter 3  The Eye and Retina

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

The red color is the visual pigment. As the light remains on, more and
more of the pigment's retinal is isomerized and breaks away from the
opsin, so the retina's color changes as shown in Figures 3.14b and
3.14c. When the pigments are in their lighter bleached state, they are
no longer useful for vision. In order to do their job of changing light
energy into electrical energy, the retinal needs to return to its bent
shape and become reattached to the opsin. This process of reforming the
visual pigment molecule is called visual pigment regeneration. Another
way to think about regeneration is to think about the visual pigment
molecule as a light switch. Once you flip a light switch on by changing
its position, you can't turn it on again until you turn it back off.
Likewise, once a visual pigment molecule has signaled the presence of
light by isomerizing (changing its position) and becoming bleached, as
in Figure 3.14c, it can't signal the presence of light again until
retinal reattaches to opsin, as in Figure 3.14a. Thus, the pigments
become unresponsive to light in their bleached state and need time to
regenerate before becoming responsive again. When you are in the light,
as you are now as you read this book, some of your visual pigment
molecules are isomerizing and bleaching, as shown in Figure 3.14, while
at the same time, others are regenerating. This means that in most
normal light levels, your eye always contains some bleached visual
pigment and some intact visual pigment. When you turn out the lights,
the bleached visual pigment continues to regenerate, but there is no
more isomerization, so eventually the concentration of regenerated
pigment builds up so your retina contains only intact visual pigment
molecules. This increase in visual pigment concentration that occurs as
the pigment regenerates in the dark is responsible for the increase in
sensitivity we measure during dark adaptation. This relationship between
pigment concentration and sensitivity was demonstrated by William
Rushton (1961), who devised a procedure to measure the regeneration of
visual pigment in humans by measuring the darkening of the retina that
occurs during dark adaptation. (Think of this as Figure 3.14 proceeding
from right to left.) Rushton's measurements showed that cone pigment
takes 6 minutes to regenerate completely, whereas rod pigment takes more
than 30 minutes. When he compared the course of pigment regeneration to
the dark adaptation curve, he found that the rate of cone dark
adaptation matched the rate of cone pigment regeneration and the rate of
rod dark adaptation matched the rate of rod pigment regeneration. These
results demonstrated two important connections between perception and
physiology: 1. Our sensitivity to light depends on the concentration of
a chemical---the visual pigment. 2. The speed at which our sensitivity
increases in the dark depends on a chemical reaction---the regeneration
of the visual pigment. What happens to vision if something prevents
visual pigments from regenerating? This is what occurs when a person's
retina becomes detached from the pigment epithelium (see Figure 3.2b), a
layer that contains enzymes necessary for pigment regeneration. This
condition, called detached retina, can occur as a result of traumatic
injuries of the eye or head,

as when a baseball player is hit in the eye by a line drive. When this
occurs, the bleached pigment's separated retinal and opsin can no longer
be recombined, and the person becomes blind in the area of the visual
field served by the separated area of the retina. This condition is
permanent unless the detached area of retina is reattached, which can be
accomplished by laser surgery.

Spectral Sensitivity Our discussion of rods and cones has emphasized how
they control our vision as we adapt to darkness. Rods and cones also
differ in the way they respond to light in different parts of the
visible spectrum (Figure 1.23, page 18). The differences in the rod and
cone responses to the spectrum have been studied by measuring the
spectral sensitivity of rod vision and cone vision, where spectral
sensitivity is the eye's sensitivity to light as a function of the
light's wavelength. Spectral sensitivity is measured by determining the
spectral sensitivity curve---the relationship between wavelength and
sensitivity.

Spectral Sensitivity Curves The following is the psychophysical method
used to measure a spectral sensitivity curve. METHOD

Measuring a Spectral Sensitivity Curve

To measure sensitivity to light at each wavelength across the spectrum,
we present one wavelength at a time and measure the participant's
sensitivity to each wavelength. Light of a single wavelength, called
monochromatic light, can be created by using special filters or a device
called a spectrometer. To determine a person's spectral sensitivity, we
determine the person's threshold for seeing monochromatic lights across
the spectrum using one of the psychophysical methods for measuring
threshold described in Chapter 1 (p. 14). The threshold is usually not
measured at every wavelength, but at regular intervals. Thus, we might
measure the threshold first at 400 nm, then at 410 nm, and so on. The
result is the curve in Figure 3.15a, which shows that the threshold is
higher at short and long wavelengths and lower in the middle of the
spectrum; that is, less light is needed to see wavelengths in the middle
of the spectrum than to see wavelengths at either the short- or
long-wavelength end of the spectrum. The ability to see wavelengths
across the spectrum is often plotted not in terms of threshold versus
wavelength, as in Figure 3.15a, but in terms of sensitivity versus
wavelength. Using the equation, sensitivity 5 1/threshold, we can
convert the threshold curve in Figure 3.15a into the curve in Figure
3.15b, which is called the spectral sensitivity curve. We measure the
cone spectral sensitivity curve by having a participant look directly at
a test light so that it stimulates only the cones in the fovea. We
measure the rod spectral sensitivity curve by measuring sensitivity
after the eye is dark adapted (so the rods control vision because they
are the most sensitive photoreceptors) and presenting test flashes in
the peripheral retina, off to the side of the fixation point.

3.3 Photoreceptor Processes

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

49

Threshold curve

Relative threshold

Relative sensitivity

High

0.8 0.6 0.4 0.2

400

400 (a)

500

600

700

Wavelength (nm) Spectral sensitivity curve

Relative sensitivity

High

Low 400

500

600

700

Wavelength (nm)

Figure 3.15 (a) The threshold for seeing a light as a function of
wavelength. (b) Relative sensitivity as a function of wavelength---the
spectral sensitivity curve. (Adapted from Wald, 1964)

The rod and cone spectral sensitivity curves in Figure 3.16 show that
the rods are more sensitive to short-wavelength light than are the
cones, with the rods being most sensitive to light of 500 nm and the
cones being most sensitive to light of 560 nm. This difference in the
sensitivity of cones and rods to different wavelengths means that as
vision shifts from the cones in the light-adapted eye to the rods after
the eye has become dark adapted, our vision shifts to become relatively
more sensitive to short-wavelength light---that is, light nearer the
blue and green end of the spectrum. You may have noticed an effect of
this shift to shortwavelength sensitivity if you have observed how green
foliage seems to stand out more near dusk. This enhanced perception of
short wavelengths during dark adaptation is called the Purkinje
(Pur-kin'-jee) shift after Johann Purkinje, who described this effect in
1825. You can experience this shift in color sensitivity during dark
adaptation by closing one eye for 5 to 10 minutes so it dark adapts,
then switching back and forth between your eyes and noticing how the
blue flower in Figure 3.17 is brighter compared to the red flower in
your dark-adapted eye. 50

Cone vision

0

Low

(b) 

Rod vision

1.0

500 600 Wavelength (nm)

700

Figure 3.16 Spectral sensitivity curves for rod vision (left) and cone
vision (right). The maximum sensitivities of these two curves have been
set equal to 1.0. However, the relative sensitivities of the rods and
the cones depend on the conditions of adaptation: The cones are more
sensitive in the light, and the rods are more sensitive in the dark. The
circles plotted on top of the rod curve are the absorption spectrum of
the rod visual pigment. (From Wald & Brown, 1958)

Rod- and Cone-Pigment Absorption Spectra Just as we can trace the
difference in the rate of rod and cone dark adaptation to a property of
the visual pigments (the cone pigment regenerates faster than the rod
pigment), we can trace the difference in the rod and cone spectral
sensitivity curves to the rod and cone pigment absorption spectra. A
pigment's absorption spectrum is a plot of the amount of light absorbed
versus the wavelength of the light. The absorption spectra of the rod
and cone pigments are shown in Figure 3.18. The rod pigment absorbs best
at 500 nm, the blue-green area of the spectrum. There are three
absorption spectra for the cones because there are three different cone
pigments, each contained in its own receptor. The short-wavelength
pigment (S) absorbs light best at about 419 nm; the medium-wavelength
pigment (M) absorbs light best at about 531 nm; and the long-wavelength
pigment (L) absorbs light best at about 558 nm. We will have more to say
about the three cone pigments in Chapter 9, because they are the basis
of our ability to see colors. The absorption of the rod visual pigment
closely matches the rod spectral sensitivity curve (Figure 3.18), and
the short-, medium-, and long-wavelength cone pigments add together to
result in a psychophysical spectral sensitivity curve that

Figure 3.17 Flowers for demonstrating the Purkinje shift. See text for
explanation.

Chapter 3  The Eye and Retina

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

S

Relative proportion of light absorbed

1.0

R

M

Figure 3.18 Absorption spectra of the rod pigment (R), and the short-
(S), medium- (M), and long-wavelength (L) cone pigments. (Based on
Dartnall, Bowmaker,

L

& Mollon, 1983)

.75 .50 .25 0

400

450

500

550

600

650

Wavelength (nm)

peaks at 560 nm. Because there are fewer short-wavelength receptors and
therefore much less of the short-wavelength pigment, the cone spectral
sensitivity curve is determined mainly by the medium- and
long-wavelength pigments (Bowmaker & Dartnall, 1980; Stiles, 1953). It
is clear from the evidence we have presented that the increase in
sensitivity that occurs in the dark (dark adaptation) and the
sensitivity to different wavelengths across the spectrum (spectral
sensitivity) are determined by the properties of the rod and cone visual
pigments. Thus, even though perception---the experience that results
from stimulation of the senses---does not occur in the eye, our
experience is definitely affected by what happens there. We have now
traveled through the first three steps in the perceptual process for the
sense of vision. The tree (Step 1) reflects light, which is focused onto
the retina by the eye's optical system (Step 2). The photoreceptors
shape perception as they transform light energy into electrical energy
(Step 3). We are now ready to discuss the processing that occurs after
the photoreceptors as the signal moves through the other cells in the
retina (Step 4). TEST YOURSELF 3.1 1. Describe light, the structure of
the eye, and the rod and cone receptors. How are the rods and cones
distributed across the retina? 2. How does moving an object closer to
the eye affect how light reflected from the object is focused on the
retina? 3. How does the eye adjust the focusing of light by
accommodation? Describe the following refractive errors that can cause
problems in focusing: presbyopia, myopia, hyperopia. How are these
problems solved through either accommodation or corrective lenses? 4.
Where on the retina does a researcher need to present a stimulus to test
dark adaptation of the cones? How is this related to the distribution of
the rods and cones on the retina? How can the adaptation of cones be
measured without any interference from the rods? How can adaptation of
the rods be measured without any interference from the cones?

5.  Describe how rod and cone sensitivity changes starting when the
    lights are turned off and how this change in sensitivity continues
    for 20 to 30 minutes in the dark. When do the rods begin adapting?
    When do the rods become more sensitive than the cones?
6.  What happens to visual pigment molecules when they

<!-- -->

(a) absorb light and (b) regenerate? What is the connection between
    visual pigment regeneration and dark adaptation?

<!-- -->

7.  What is spectral sensitivity? How is a cone spectral sensitivity
    curve determined? A rod spectral sensitivity curve?
8.  What is a pigment absorption spectrum? How do rod and cone pigment
    absorption spectra compare, and what is their relationship to rod
    and cone spectral sensitivity?

3.4 What Happens as Signals Travel Through the Retina We have now seen
how the photoreceptors are critical to perception since they transduce
incoming light into an electrical signal. They also influence perception
in the different ways that rods versus cones adapt to the dark and
respond to different wavelengths of light. As we will discuss in this
section, the way in which the photoreceptors and the other cells in the
retina are "wired up" also has a substantial effect on our perception.

Rod and Cone Convergence Figure 3.19a is a cross section of a monkey
retina that has been stained to reveal the retina's layered structure.
Figure 3.19b shows the five types of neurons that make up these layers
and that create neural circuits---interconnected groups of
neurons---within the retina. Signals generated in the receptors (R)
travel to the bipolar cells (B) and then to the ganglion cells (G). The
receptors and bipolar cells do not have long axons, but the ganglion
cells do. These long axons transmit signals out of the retina in the
optic nerve (see Figure 3.6). 3.4 What Happens as Signals Travel Through
the Retina

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

51

Figure 3.19 (a) Cross section of a monkey retina, which has been stained
to show the various layers. Light is coming from the bottom. The purple
circles are cell bodies of the receptors, bipolar cells, and ganglion
cells. (b) Cross section of the primate retina showing the five major
cell types and their interconnections: receptors (R), bipolar cells (B),
ganglion cells (G), horizontal cells (H), and amacrine cells (A).
Signals from the three highlighted rods on the right reach the
highlighted ganglion cell. This is an example of convergence. (Based on
Dowling &

Receptors

Boycott, 1966)

Bipolar cells Ganglion cells (a)

Outer segment Rod and cone receptors (R)

Inner segment R R

R

R

R

R

R

R

R

Horizontal cell (H) Bipolar cells (B)

H B

B

B A

B

A

Amacrine cells (A)

Ganglion cells (G)

B

B

G

G

G

G

Optic nerve fibers Light rays

(b) 

In addition to the photoreceptors, bipolar cells, and ganglion cells,
there are two other types of neurons that connect neurons across the
retina: horizontal cells and amacrine cells. Signals can travel between
receptors through the horizontal cells, and between bipolar cells and
between ganglion cells through the amacrine cells. We will return to the
horizontal and amacrine cells later in this chapter. For now we will
focus on the direct pathway from the photoreceptors to the ganglion
cells. We focus specifically on the property of neural convergence. 52

Perception Is Shaped by Neural Convergence Neural convergence (or just
convergence for short) occurs when a number of neurons synapse onto a
single neuron. A great deal of convergence occurs in the retina because
each eye has 126 million photoreceptors but only 1 million ganglion
cells. Thus, on the average, each ganglion cell receives signals from
126 photoreceptors. We can show how convergence can affect perception by
returning to the rods and cones. An important difference between rods
and cones is that the signals from

Chapter 3  The Eye and Retina

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

the rods converge more than do the signals from the cones. We can
appreciate this difference by noting that there are 120 million rods in
the retina, but only 6 million cones. Thus, on the average, about 120
rods send their signals to one ganglion cell, but only about 6 cones
send signals to a single ganglion cell. This difference between rod and
cone convergence becomes even greater when we consider the cones in the
fovea. (Remember that the fovea is the small area that contains only
cones.) Many of these foveal cones have "private lines" to ganglion
cells, so that each ganglion cell receives signals from only one cone,
with no convergence. The greater convergence of the rods compared to the
cones translates into two differences in perception: (1) the rods result
in better sensitivity than the cones, and (2) the cones result in better
detail vision than the rods.

Convergence Causes the Rods to Be More Sensitive Than the Cones In the
dark-adapted eye, rod vision is more sensitive than cone vision (see
"dark-adapted sensitivity" in the dark adaptation curve of Figure 3.13).
This is why in dim light we use our rods to detect faint stimuli. A
demonstration of this effect, which has long been known to astronomers
and amateur stargazers, is that some very dim stars are difficult to
detect when looked at directly (because the star's image falls on the
cones in the fovea), but these same stars can often be seen when they
are located off to the side of where the person is looking (because then
the star's image falls on the rod-rich peripheral retina). One reason
for this greater sensitivity of rods, compared to cones, is that it
takes less light to generate a response from an individual rod receptor
than from an individual cone receptor (Barlow & Mollon, 1982; Baylor,
1992). But there is another reason as well: The rods have greater
convergence than the cones. Keeping this basic principle in mind, we can
see how the difference in rod and cone convergence translates into
differences in the maximum sensitivities of the rods and the cones by
considering the two circuits in Figure 3.20. The left panel shows five
rod receptors converging onto one ganglion cell, and the

2

2

2

2

2

2

2

2

2

2

+10 Response

No response

Figure 3.20 The wiring of the rods (left) and the cones (right). The
yellow dot and arrow above each receptor represents a "spot'' of light
that stimulates the receptor. The numbers represent the number of
response units generated by the rods and the cones in response to a spot
intensity of 2.

right panel shows five cone receptors each sending signals onto their
own ganglion cells. This represents the greater convergence of the rods
than the cones. Note that we have left out the bipolar, horizontal, and
amacrine cells in these circuits for simplicity, but our conclusions are
not affected by these omissions. For the purposes of our discussion, we
will assume that we can present small spots of light to individual rods
and cones. Let's also say that when a photoreceptor is stimulated by one
spot of light, it causes one unit of excitation in the ganglion cell,
and it takes 10 units of excitation for the ganglion cell to fire. Each
ganglion cell can sum all of the inputs from the photoreceptors to try
to reach this 10-unit threshold. If we present spots of light with an
intensity of 1 to each photoreceptor, the rod ganglion cell receives 5
units of excitation, 1 from each of the 5 rod receptors. In contrast,
each cone ganglion cell receives just 1 unit of excitation, 1 from each
cone receptor. Thus, when intensity 5 1, the rod ganglion cell receives
more excitation than the cone ganglion cells because of convergence, but
not enough to cause it to fire. If, however, we increase the intensity
to 2, as shown in Figure 3.20, the rod ganglion cell receives 2 units of
excitation from each of its 5 receptors, for a total of 10 units of
excitation. This causes the ganglion cell to fire, and the light is
perceived. Meanwhile, at the same intensity, the cones' ganglion cells
are each receiving only 2 units of excitation, so those ganglion cells
have no response, since the 10-unit threshold has not been reached. For
the cones' ganglion cells to fire, we would have to increase the
intensity of the light. This example shows that it takes less incoming
light to stimulate a ganglion cell that is receiving input from rods,
since many rods converge onto that one ganglion cell. On the contrary,
it takes much more light to stimulate a ganglion cell that receives
input from cones, since fewer cones converge onto each ganglion cell.
This demonstrates how the rods' high sensitivity compared to the cones'
is caused by the rods' greater convergence. The fact that rod and cone
sensitivity is determined not by individual receptors but by groups of
receptors converging onto other neurons means that when we describe "rod
vision" and "cone vision" we are actually referring to the way groups of
rods and cones participate in determining our perceptions.

Less Convergence Causes the Cones to Have Better Acuity Than the Rods
While rod vision is more sensitive than cone vision because the rods
have more convergence, the cones have better visual acuity because they
have less convergence. Acuity refers to the ability to see details;
thus, being able to see very small letters on an eye chart in the
optometrist's or ophthalmologist's office translates into high acuity.
(Also, remember grating acuity from Chapter 1, page 12.) One way to
appreciate the high acuity of the cones is to think about the last time
you were looking for one thing that was hidden among many other things.
This could be searching for your cellphone on the clutter of your desk
or locating a friend's face in a crowd. To find what you are looking
for, you usually need to move your eyes from one place to another. When
you move your eyes to look at different things in this way, what you are
doing is scanning with your cone-rich fovea (remember that when you 3.4
What Happens as Signals Travel Through the Retina

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

53

look directly at something, its image falls on the fovea). This is
necessary because your visual acuity is highest in the fovea; objects
that are imaged on the peripheral retina are not seen as clearly.
DEMONSTRATION

Foveal Versus Peripheral Acuity

D I H C N R L A Z I F W N S M Q Z K D X You can demonstrate that foveal
vision is superior to peripheral vision for seeing details by looking at
the X on the right and, without moving your eyes, seeing how many
letters you can identify to the left. If you do this without cheating
(resist the urge to look to the left!), you will find that although you
can read the letters right next to the X, which are imaged on or near
the fovea, it is difficult to read letters that are further off to the
side, which are imaged on the peripheral retina.

This demonstration shows that acuity is better in the fovea than in the
periphery. Because you were light adapted, the comparison in this
demonstration was between the foveal cones, which are tightly packed,
and the peripheral cones, which are more widely spaced. Comparing the
foveal cones to the rods results in even greater differences in acuity.
We can make this comparison by noting how acuity changes during dark
adaptation. The picture of the bookcase in Figure 3.21 simulates the
change in acuity that occurs during dark adaptation. The

books on the top shelf represent the details seen when viewing the books
in the light, when cones are controlling vision. The middle shelf
represents detail perception midway through dark adaptation, when the
rods are beginning to determine vision, and the books on the bottom
shelf represent the poor detail vision of the rods. Also note that color
has disappeared, because color vision depends on the cones, as we will
see in Chapter 9. The chapter-opening picture on page 38 also
illustrates what happens when vision shifts from cones to rods. We can
understand how differences in rod and cone wiring explain the cones'
greater acuity by returning to our rod and cone neural circuits. First
consider the rod circuit in Figure 3.22a. When we present two spots of
light next to each other, as on the left, the rods' signals cause the
ganglion cell to fire. When we separate the two spots, as on the right,
the two separated rods still feed into the same ganglion cell and cause
it to fire. In both cases, the ganglion cell fires. Thus, firing of the
ganglion cell provides no information about whether there are two spots
close together or two separated spots. We now consider the cones in
Figure 3.22b, each of which synapses on its own ganglion cell. When we
present a light that

(a) 

Bruce Goldstein

(b) 

Figure 3.21 Simulation of the change from colorful sharp perception to
colorless fuzzy perception that occurs during the shift from cone vision
to rod vision during dark adaptation. The top shelf simulates cone
vision; the bottom shelf, rod vision. 54

Figure 3.22 How the wiring of the rods and cones determines detail
vision. (a) Rod neural circuits. On the left, stimulating two
neighboring rods causes the ganglion cell to fire. On the right,
stimulating two separated rods causes the same effect. (b) Cone neural
circuits. On the left, stimulating two neighboring cones causes two
neighboring ganglion cells to fire. On the right, stimulating two
separated cones causes two separated ganglion cells to fire. This firing
of two neurons, with a space between them, indicates that two spots of
light have been presented to the cones.

Chapter 3  The Eye and Retina

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

stimulates two neighboring cones, as on the left, two adjacent ganglion
cells fire. But when we separate the spots, as on the right, two
separate ganglion cells fire. This separation between two firing cells
provides information that there are two separate spots of light. Thus,
the cones' lack of convergence causes cone vision to have higher acuity
than rod vision. Convergence is therefore a double-edged sword. High
convergence results in high sensitivity but poor acuity (the rods). Low
convergence results in low sensitivity but high acuity (cones). The way
the rods and cones are wired up in the retina, therefore, influences
what we perceive. We now continue our description of processing in the
retina, by looking at a property discovered in retinal ganglion cells
called the receptive field.

Ganglion Cell Receptive Fields Signals from the photoreceptors travel
through the retina and eventually reach the retinal ganglion cells
(Figure 3.19). The axons of the ganglion cells leave the retina as
fibers of the optic nerve (Figure 3.23). Pioneering research by H.
Keffer Hartline (1938, 1940), which won him the Nobel Prize in
Physiology and Medicine in 1967, led to the discovery of a property of
neurons called the neuron's receptive field.

Hartline's Discovery of Receptive Fields In his seminal research,
Hartline isolated a single ganglion cell axon in the opened eyecup of a
frog (Figure 3.24) by teasing apart the optic nerve near where it leaves
the eye. While recording from this axon, Hartline illuminated different
areas of the retina and found that the cell he was recording from
responded only when a small area of the retina was illuminated. He
called the area that caused the neuron to fire the ganglion cell's
receptive field (Figure 3.24a), which he defined as "the region of the
retina that must receive illumination in order to obtain a response in
any given fiber" (Hartline, 1938, p. 410). Hartline went on to emphasize
that a ganglion cell's receptive field covers a much greater area than a
single photoreceptor.

Nerve

Nerve fiber

Figure 3.23 The optic nerve, which leaves the back of the eye, contains
about one million ganglion cell axons, or nerve fibers.

Receptive fields of three fibers

Receptive field

Recording from single fiber

Retina (flattened)

Eyecup of frog (top dissected off)

(a) 

Optic nerve

(b) 

Figure 3.24 (a) Hartline's experiment in which he presented stimuli to
the retina by dissecting a frog's eye and removing the top to create an
"eyecup." He then presented light to the retina to determine which area
of a frog's retina caused firing in one of the ganglion cell fibers in
the optic nerve. This area is called the receptive field of that
ganglion cell. (b) Receptive fields of three ganglion cells. These
receptive fields overlap, so stimulating at a particular point on the
retina will generally activate a number of fibers in the optic nerve.

The fact that a ganglion cell's receptive field covers hundreds or even
thousands of receptors means that the cell is receiving converging
signals from all of these photoreceptors, as we saw in the previous
section. Finally, Hartline noted that the receptive fields of many
different ganglion cells overlap (Figure 3.24b). This means that shining
light on a particular point on the retina activates many ganglion cells.
One way to think about receptive fields is to imagine a football field
and a grandstand full of spectators, each with a pair of binoculars
trained on one small area of the field. Each spectator is monitoring
what is happening in his or her own small area, and all of the
spectators together are monitoring the entire field. Since there are so
many spectators, some of the areas they are observing will wholly or
partially overlap. To relate this football field analogy to Hartline's
receptive fields, we can equate each spectator to a ganglion cell, the
football field to the retina, and the small areas viewed by each
spectator to receptive fields. Just as each spectator monitors a small
area of the football field, but collectively all spectators take in
information about what is happening on the entire football field, each
ganglion cell monitors a small area of retina. However, because there
are many ganglion cells, just as there are many spectators, all of them
together take in information about what is happening over the entire
retina.

Kuffler's Discovery of Center-Surround Receptive Fields Following
Hartline's research on the receptive fields of ganglion cells in the
frog's retina, Stephen Kuffler (1953) measured ganglion cell receptive
fields in the cat and reported a property of these receptive fields that
Hartline had not observed in the frog. In the cat (and, as it turns out,
in other mammals such as monkeys and humans), the ganglion cells have
center-surround receptive fields that are arranged like concentric
circles in a center-surround organization, as shown in Figure 3.25. In
these receptive fields, the area in the 3.4 What Happens as Signals
Travel Through the Retina

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

55

Center

--

- 
- 
- 
- 

-- -- Surround

- 

--

- --

- 

--

(a) 

- 

-- -- -- --

- 
- 
- 

(b) 

Figure 3.25 Center-surround receptive fields: (a) excitatory center,
inhibitory surround; (b) inhibitory center, excitatory surround.

"center" of the receptive field responds differently to light than the
area in the "surround" of the receptive field (Barlow et al., 1957;
Hubel & Wiesel, 1965; Kuffler, 1953). For example, for the receptive
field in Figure 3.25a, presenting a spot of light to the center
increases firing, so it is called the excitatory area of the receptive
field. In contrast, stimulation of the surround causes a decrease in
firing, so it is called the inhibitory area of the receptive field. This
receptive field is called an excitatory-center, inhibitory-surround
receptive field. The receptive field in Figure 3.25b, which responds
with inhibition when the center is stimulated and excitation when the
surround is stimulated, is an inhibitory-center, excitatory-surround
receptive field. Both of these types of center-surround ganglion cell
receptive fields are present in the mammalian retina. The discovery that
receptive fields can have oppositely responding areas made it necessary
to modify Hartline's definition of receptive field to "the retinal
region over which a cell in the visual system can be influenced (excited
or inhibited) by light" (Hubel & Wiesel, 1961). The word influenced and
reference to excitation and inhibition make it clear that any change in
firing---either an increase or a decrease---needs to be taken into
account in determining a neuron's receptive field. The discovery of
center-surround receptive fields was also important because it showed
that ganglion cells respond best to specific patterns of illumination.
This is illustrated by an effect called center-surround antagonism,
illustrated in Figure 3.26. A small spot of light presented to the
excitatory

Figure 3.26 Response of an excitatorycenter, inhibitory-surround
ganglion cell receptive field as stimulus size is increased. Color
indicates the area stimulated with light. The response to the stimulus
is indicated below each receptive field. (a) Small response to a small
dot in the excitatory center. (b) Increased response when the whole
excitatory area is stimulated. (c) Response begins to decrease when the
size of the spot is increased so that it stimulates part of the
inhibitory surround; this illustrates centersurround antagonism. (d)
Covering all of the inhibitory surround decreases the response further.
56

-- --

center of the receptive field causes a small increase in the rate of
nerve firing (a); increasing the light's size so that it covers the
entire center of the receptive field increases the cell's response, as
shown in (b). Center-surround antagonism comes into play when the spot
of light becomes large enough that it begins to cover the inhibitory
area, as in (c) and (d). Stimulation of the inhibitory surround
counteracts the center's excitatory response, causing a decrease in the
neuron's firing rate. Thus, because of centersurround antagonism, this
neuron responds best to a spot of light that is the size of the
excitatory center of the receptive field. How does center-surround
antagonism work? To answer this question, we need to return to our
discussion of neural convergence and consider how inhibition and
convergence work together. Specifically, the inhibition involved in
center-surround ganglion cell receptive fields is known as lateral
inhibition--- inhibition that is transmitted across the retina
(laterally).

Lateral Inhibition Underlies Center-Surround Antagonism The pioneering
work on lateral inhibition was carried out by Keffer Hartline, Henry
Wagner, and Floyd Ratliff (1956) on a primitive animal called the
Limulus, more familiarly known as the horseshoe crab (Figure 3.27). They
chose the Limulus because the structure of its eye makes it possible to
stimulate individual receptors. The Limulus eye is made up of hundreds
of tiny structures called ommatidia, and each ommatidium has a small
lens on the eye's surface that is located directly over a single
receptor. Each lens and receptor is roughly the diameter of a pencil
point (very large compared to human receptors), so it is possible to
illuminate and record from a single receptor without illuminating its
neighboring receptors. When Hartline and coworkers recorded from the
nerve fiber of receptor A, as shown in Figure 3.28, they found that
illumination of that receptor caused a large response (Figure 3.28a).
But when they added illumination to the three nearby receptors at B, the
response of receptor A decreased (Figure 3.28b). They also found that
further increasing the illumination of B decreased A's response even
more (Figure 3.28c). Thus, illumination of the neighboring

--

- 

- --

- 

- 

-- --

On

On (a)

On (b)

On (c)

(d) 

Chapter 3  The Eye and Retina

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Bruce Goldstein

Lateral eye

Figure 3.27 A Limulus, or horseshoe crab. Its large eyes are made up of
hundreds of ommatidia, each containing a single receptor.

Light B

Light A

Lateral plexus

Electrode recording from A

A only (a)

A+B

receptors at B inhibited the firing caused by stimulation of receptor A.
This decrease in the firing of receptor A is caused by lateral
inhibition that is transmitted from B to A across the Limulus's eye by
the fibers of the lateral plexus, shown in Figure 3.28. Just as the
lateral plexus transmits signals laterally in the Limulus, the
horizontal and amacrine cells (see Figure 3.19, page 52) transmit
inhibitory signals laterally across the monkey and human retina. This
lateral inhibition by the horizontal and amacrine cells is what
underlies center-surround antagonism in center-surround ganglion cell
receptive fields. To help us understand the relationship between
centersurround receptive fields, convergence, and lateral inhibition,
let's consider an example of a neural circuit in the retina that
demonstrates all of these principles working together. Figure 3.29 shows
a neural circuit consisting of seven photoreceptors. These neurons, with
the aid of lateral inhibition, help create the excitatory-center,
inhibitory-surround receptive field of neuron B. Receptors 1 and 2
converge on neuron A; receptors 3, 4, and 5 converge on neuron B; and
receptors 6 and 7 converge on neuron C. All of these synapses are
excitatory, as indicated by the blue Y-shaped connections and + signs.
Additionally, neurons A and C (representing horizontal/amacrine cells)
are laterally connected to neuron B, with both of these synapses being
inhibitory, as indicated by the red T-shaped connections and -- signs.
Let's now consider how stimulating the photoreceptors will affect the
firing of B. Stimulating receptors 3, 4, and 5 causes B's firing to
increase because their synapses with B are excitatory. This is what we
would expect, because receptors 3, 4, and 5 are located in the
excitatory center of the receptive field. Now consider what happens when
we stimulate receptors 1 and 2, located in the surround of the receptive
field. These receptors connect to neuron A with excitatory synapses, so
illuminating these receptors causes A's firing to increase. A's signal
then travels to neuron B, but because its synapse onto B is inhibitory,
this signal causes B's firing to decrease. This is what we would expect,
because receptors 1 and 2 are located in the inhibitory surround of the
receptive field. The same thing happens when we illuminate receptors 6
and 7, which are also located in the inhibitory surround. Thus,
stimulating anywhere in the center (green area) causes B's firing to
increase. Stimulating anywhere in the surround (red area) causes B's
firing to decrease due to lateral inhibition.

(b) 
(c) 

A+B (increased intensity at B)

Figure 3.28 A demonstration of lateral inhibition in the Limulus. The
records show the response recorded by the electrode in the nerve fiber
of receptor A: (a) when only receptor A is stimulated; (b) when receptor
A and the receptors at B are stimulated together; (c) when A and B are
stimulated, with B stimulated at an increased intensity. (Adapted from
Ratliff, 1965)

1

2

3

(+)

4

5

(--)

B

7 (+)

(+)

A

6

C (--)

Figure 3.29 A seven-receptor neural circuit underlying a centersurround
receptive field. Receptors 3, 4, and 5 are in the excitatory center, and
receptors 1, 2, 6, and 7 are in the inhibitory surround. 3.4 What
Happens as Signals Travel Through the Retina

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

57

58

High Light intensity

Center-Surround Receptive Fields and Edge Enhancement Center-surround
receptive fields illustrate how the interaction between excitatory and
inhibitory connections can shape the responding of individual neurons,
as when ganglion cells respond best to small spots of light (Figure
3.26). But in addition to determining optimal stimuli for ganglion
cells, center-surround receptive fields contribute to edge
enhancement---an increase in perceived contrast at borders between
regions of the visual field. In other words, they help to make edges
look more distinct so that we can see them more easily. To illustrate
edge enhancement, let's look at Figure 3.30a, which shows two
side-by-side rectangles. An important feature of these rectangles is
revealed by plotting the light intensity that would be measured by a
light meter scanning the rectangles along the line from A to D (Figure
3.30b). Notice that the light intensity remains the same across the
entire distance between A and B, then at the border the intensity drops
to a lower level and remains the same between C and D. However, you may
notice that although the intensity is the same from A to B, and then
from C to D, the perception of lightness is not. At the border between B
and C there is a lightening at B to the left of the border and a
darkening at C to the right of the border. The perceived light and dark
bands at the borders, which are not present in the actual physical
stimuli, is called the Chevreul illusion named after the French chemist
MichelEugene Chevreul (1789--1889) who, as the director of dyes at the
Gobelin tapestry works, became interested in how placing colors side by
side could alter their appearance. The perceived light and dark bands
are represented in Figure 3.30c, which shows the light band at B as an
upward bump in lightness, and the dark band at C as a downward bump in
lightness. By appearing lighter on one side of the border and darker on
the other, the edge itself looks sharper and more distinct, which
demonstrates edge enhancement. Illusory light and dark bars at borders
also occur in the environment, especially in shadows. You might notice
this if there are shadows nearby, or see if you can find light and dark
bars in Figure 3.31. This figure shows a fuzzy shadow border between
light and dark, rather than the sharp border in the Chevreul display.
Light and dark bands created at fuzzy borders are called Mach bands,
after German physicist Ernst Mach (1836--1916). The same mechanism is
thought to be responsible for the Mach and Chevreul effects.

A

B C

A

B

D

(a) 

C

Low

D

Distance (b) B

High A

Perception of lightness

Neuron B sums all of its incoming signals to produce one response---a
general principle of neurons introduced in Chapter 2. So, when the
entire receptive field is illuminated, neuron B receives both an
excitatory signal from the center and inhibitory signals from the
surround through lateral inhibition. Those inputs would counteract each
other, causing center-surround antagonism. Although an actual ganglion
cell neuron receives signals from many more than seven photoreceptors,
and the wiring diagram is much more complex than shown in our example,
the basic principle described here operates: Center-surround receptive
fields are created by the interplay of excitation and lateral
inhibition.

D Low

C Distance

(c) 

Figure 3.30 The Chevreul illusion. Look at the borders between light and
dark. (a) Just to the left of the border, near B, a faint light band can
be perceived, and just to the right at C, a faint dark band can be
perceived. (b) The physical intensity distribution of the light, as
measured with a light meter. Because the intensity plot looks like a
step in a staircase, this illusion is also called the staircase
illusion. (c) A plot showing the perceptual effect described in (a). The
bump in the curve at B indicates the light band, and the dip in the
curve at C indicates the dark band. The bumps that represent our
perception of the bands are not present in the physical intensity
distribution.

We can understand how center-surround receptive fields can explain the
edge enhancement in the Chevreul and Mach illusions by looking at Figure
3.32, which shows the locations of the excitatory-center
inhibitory-surround receptive fields (RFs) of four ganglion cells. The
key to understanding how these neurons could cause edge enhancement is
to compare the amount of inhibition for the different cells. Let's
consider A and B first. The inhibitory area of A's receptive field is
all in the lighter region (indicated by the dots), so generates a lot of
inhibition, which decreases the cell's firing rate. But only part of the
inhibitory area of cell B's receptive field is in the lighter region, so
the cell B's response is greater than cell A's response, thereby
creating the light band at B. Now let's consider C and D. Cell C
generates more inhibition, because part of its inhibitory surround is in
the lighter

Chapter 3  The Eye and Retina

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Of course, the actual situation is more complicated than this because
hundreds or thousands of ganglion cells could be firing to the two
rectangles. But our example illustrates how neural processing involving
excitation and inhibition can create perceptual effects---in this case,
enhanced borders. The important message for our purposes is that
perception is the outcome of neural processing---in this example, the
neural processing that occurs in center-surround receptive fields in the
retina. As we will see in the next chapter when we consider the brain,
receptive fields change as we move to higher levels of the visual system
so that the higher-level neurons respond to more complex stimuli (like
objects instead of just edges), which makes it easier to draw
relationships between neural processing and perception.

Light band

Bruce Goldstein

Dark band

Figure 3.31 Shadow-casting technique for observing illusory bands in
shadows. Illuminate a light-colored surface with a lamp and cast a
shadow with a piece of paper. When the transition from light to dark is
gradual, rather than a step as in the Chevreul illusion, the bands are
called Mach bands.

region, whereas none of D's inhibitory surround is in the lighter
region. Thus, cell C's response is less than cell D's response, which
creates the dark band at C. Less inhibition than A, so appears lighter
(light bar)

- 
- 

A

B

- 
- 

C

D

More inhibition than D, so appears darker (dark bar)

Figure 3.32 How the Chevreul illusions can be explained by
centersurround receptive fields and lateral inhibition. Areas of the
inhibitory surround of the receptive field that are illuminated by the
more intense light on the light side of the border are indicated by
dots. Greater areas of high illumination result in more inhibition,
which decreases neural responding.

SOMETHING TO CONSIDER:

Early Events Are Powerful In 1990, a rocket blasted off from Cape
Canaveral to place the Hubble space telescope into earth orbit. The
telescope's mission was to provide high-resolution images from its
vantage point above the interference of the earth's atmosphere. But it
took only a few days of data collection to realize that something was
wrong. Images of stars and galaxies that should have been extremely
sharp were blurred (Figure 3.33a). The cause of the problem, it turned
out, was that the telescope's lens was ground to the wrong curvature.
Although a few of the planned observations were possible, the
telescope's mission was severely compromised. Three years later, the
problem was solved when a corrective lens was fitted over the original
one. The new Hubble, with its "eyeglasses," could now see stars as sharp
points (Figure 3.33b). This diversion to outer space emphasizes that
what happens early in a system can have a large, often crucial, effect
on the outcome. No matter how sophisticated Hubble's electronic computer
and processing programs were, the distorted image caused by the faulty
lens had fatal effects on the quality of the telescope's image.
Similarly, if problems in the eye's focusing system deliver degraded
images to the retina, no amount of processing by the brain can create
sharp perception. What we see is also determined by the energy that can
enter the eye and can activate the photoreceptors. Although there is a
huge range of electromagnetic energy in the environment, the visual
pigments in the receptors limit our sensitivity by absorbing only a
narrow range of wavelengths, as we introduced in this chapter. One way
to think about the effect of pigments is that they act like filters,
only making available for vision the wavelengths they absorb. Thus, at
night, when we are perceiving with our rods, we see only wavelengths
between about 420 and 580 nm, with the best sensitivity at 500 nm.
However, in daylight, when we are perceiving with our cones, we become
more sensitive to longer wavelengths, as the best sensitivity shifts to
560 nm. This idea of visual pigments as limiting our range of seeing is
dramatically illustrated by the honeybee, which, as we will see
Something to Consider: Early Events Are Powerful

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

59

perceive ultraviolet wavelengths that are invisible to us, so the
honeybee can see markings on flowers that reflect ultraviolet light
(Figure 3.34). Thus, although perception does not occur in the eye, what
we see is affected by what happens there. Similar effects occur in the
other senses as well. Damage to the receptors in the ear is the main
cause of hearing loss (Chapter 11, page 284); differences in the number
of "bitter" receptors on people's tongues can cause two people to have
different taste experiences to the same substance (Chapter 15, page
396).

Wide field planetary camera 1 (a)

Before

NASA Images

(a) 

Wide field planetary camera 2 After correction

Bjorn Rorslett

(b) 

Figure 3.33 (a) Image of a galaxy taken by the Hubble telescope before
the lens was corrected. (b) The same galaxy after the lens was
corrected.

in the chapter on color vision, has a visual pigment that absorbs light
all the way down to 300 nm (see Figure 9.44, page 224). This
very-short-wavelength pigment enables the honeybee to

(b) 

Figure 3.34 (a) A black-and-white photograph of a flower as seen by a
human. (b) The same flower, showing markings that become visible to
sensors that can detect ultraviolet light. Although we don't know
exactly what honeybees see, their short-wavelength cone pigment makes it
possible for them to sense these markings.

developmental dimension Infant Visual Acuity Most chapters in this book
include "Developmental Dimensions," such as this one, which describe
perceptual capacities of infants and young children that are related to
material in the chapter.

clever ways to determine what infants or young children are perceiving.
One method that has been used to measure infant visual acuity is the
preferential looking (PL) technique.

One of the challenges of determining infant capacities is that infants
can't respond by saying "yes, I perceive it" or "no, I don't perceive
it" in reaction to a stimulus. But this difficulty has not stopped
developmental psychologists from devising

METHOD

60

Preferential Looking

The key to measuring infant perception is to pose the correct question.
To understand what we mean by this, let's consider how we might
determine infants' visual acuity, their ability to

Chapter 3  The Eye and Retina

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

How well can infants see details? The red curve in Figure 3.36 shows
acuity over the first year of life measured with

the preferential looking technique, in which infants are tested with
gratings, as in Figure 3.35. The blue curve indicates acuity determined
by measuring an electrical signal called the visual evoked potential
(VEP), which is recorded by disc electrodes placed on the infant's head
over the visual part of the brain. For this technique, researchers
alternate a gray field with a grating or checkerboard pattern. If the
stripes or checks are large enough to be detected by the visual system,
the visual system generates an electrical response called the visual
evoked potential. If, however, the stripes are too fine to be detected
by the visual system, no response is generated. Thus, the VEP provides
an objective measure of the visual system's ability to detect details.
The VEP usually indicates better acuity than does preferential looking,
but both techniques indicate that visual acuity is poorly developed at
birth (about 20/400 to 20/600 at 1 month). (The expression 20/400 means
that the infant must view a stimulus from 20 feet to see the same thing
that an adult with normal 50 40 30 20 Visual acuity (cycles/degree)

see details. To test adults, we can ask them to read the letters or
symbols on an eye chart. But to test infant acuity, we have to ask
another question and use another procedure. A question that works for
infants is "Can you tell the difference between the stimulus on the left
and the one on the right?" The way infants answer this question is by
looking more at one of the stimuli. In the preferential looking (PL)
technique, two stimuli like the ones the infant is observing in Figure
3.35 are presented, and the experimenter watches the infant's eyes to
determine where the infant is looking. In order to guard against bias,
the experimenter does not know which stimulus is being presented on the
left or right. If the infant looks at one stimulus more than the other,
the experimenter concludes that he or she can tell the difference
between them. The reason preferential looking works is that infants have
spontaneous looking preferences; that is, they prefer to look at certain
types of stimuli. For example, infants choose to look at objects with
contours over ones that are homogeneous (Fantz et al., 1962). Thus, when
we present a grating stimulus (alternating white and black bars like the
one shown in Figure 3.35) with large bars on one side, and a gray ﬁeld
that reﬂects the same total amount of light that the grating would
reﬂect on the other side (again, like the one shown in Figure 3.35), the
infant can easily see the bars and therefore looks at the side with the
bars more than the side with the gray ﬁeld. If the infant looks
preferentially at the side with the bars when the bars are switched
randomly from side to side on different trials, he or she is telling the
experimenter "I see the grating." But decreasing the size of the bars
makes it more difficult for the infant to tell the difference between
the grating and gray stimulus. Eventually, the infant begins to look
equally at each display, which tells the experimenter that very fine
lines and the gray field are indiscriminable. Therefore, we can measure
the infant's acuity by determining the narrowest stripe width that
results in looking more at the grating stimulus.

10

5

2

1

2

3

4

5

6

7

8

9

10 11 12 13

Age (months)

Figure 3.35 An infant being tested using the preferential looking
technique. The parent holds the infant in front of the display, which
consists of a grating on the right and a homogeneous gray ﬁeld on the
left. The grating and the gray ﬁeld have the same average light
intensity. An experimenter, who does not know which side the grating is
on in any given trial, looks through the peephole between the grating
and the gray ﬁeld and judges whether the infant is looking to the left
or to the right.

Figure 3.36 Acuity over the ﬁrst year of life, measured by the visual
evoked potential technique (top curve) and the preferential looking
technique (bottom curve). The vertical axis indicates the ﬁneness, in
cycles per degree, of a grating stimulus that the infant can detect. One
cycle per degree corresponds to one pair of black and white lines on a
circle the size of a penny viewed from a distance of about a meter.
Higher numbers indicate the ability to detect finer lines on the
penny-sized circle. The dashed line is adult acuity (20/20 vision). (VEP
curve adapted from Norcia & Tyler, 1985; PL curve adapted from Gwiazda
et al., 1980, and Mayer et al., 1995.)

Continued Something to Consider: Early Events Are Powerful

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

61

vision can see from 400 feet.) Acuity increases rapidly over the ﬁrst 6
to 9 months (Banks & Salapatek, 1978; Dobson & Teller, 1978; Harris et
al., 1976; Salapatek et al., 1976). This rapid improvement of acuity is
followed by a leveling-off period, and full adult acuity is not reached
until sometime after 1 year of age. From our discussion of how adult rod
and cone visual acuity depends on the wiring of the rods and cones, it
would make sense to consider the possibility that infants' low acuity
might be traced to the development of their photoreceptors. If we look
at the newborn's retina, we ﬁnd that this is the case. Although the
rod-dominated peripheral retina appears adultlike in the newborn, the
all-cone fovea contains widely spaced and very poorly developed cone
receptors (Abramov et al., 1982). Figure 3.37a compares the shapes of
newborn and adult foveal cones. Remember from our discussion of
transduction that the visual pigments are contained in the receptor's
outer segments. These outer segments sit on top of the other part of the
receptor, the inner segment. The newborn's cones have fat inner segments
and very small outer segments, whereas the adult's inner and outer
segments are larger and are about the same diameter (Banks & Bennett,
1988; Yuodelis & Hendrickson, 1986). These differences in shape and size
have a number of consequences. The small size of the outer segment

means that the newborn's cones contain less visual pigment and therefore
do not absorb light as effectively as adult cones. In addition, the fat
inner segment creates the coarse receptor lattice shown in Figure 3.37b,
with large spaces between the outer segments. In contrast, when the
adult cones have become thin, they can become packed closely together to
create a ﬁne lattice that is well suited to detecting small details.
Martin Banks and Patrick Bennett (1988) calculated that the cone
receptors' outer segments effectively cover 68 percent of the adult
fovea but only 2 percent of the newborn fovea. This means that most of
the light entering the newborn's fovea is lost in the spaces between the
cones and is therefore not useful for vision. Thus, adults have good
acuity because the cones have low convergence compared to the rods and
the receptors in the fovea are packed closely together. In contrast, the
infant's poor acuity can be traced to the fact that the infant's cones
are spaced far apart. Another reason for the infant's poor acuity is
that the visual area of the brain is poorly developed at birth, with
fewer neurons and synapses than in the adult cortex. The rapid increase
in acuity that occurs over the first 6 to 9 months of life can thus be
traced to the fact that during that time, more neurons and synapses are
being added to the cortex, and the infant's cones are becoming more
densely packed.

Adult cone (Actual length relative to newborn cone is 2x greater than
shown)

Newborn cone Inner segment Outer segment

Newborn cone lattice

(a) 

Adult cone lattice

(b) 

Figure 3.37 (a) Idealized shapes of newborn and adult foveal cones.
(Real cones are not so perfectly straight and cylindrical.) Foveal cones
are much narrower and longer than the cones elsewhere in the retina, so
these look different from the one shown in Figure 3.3. (b) Receptor
lattices for newborn and adult foveal cones. The newborn cone outer
segments, indicated by the red circles, are widely spaced because of the
fat inner segments. In contrast, the adult cones, with their slender
inner segments, are packed closely together. (Adapted from Banks &
Bennett, 1988)

TEST YOuRSELF 3.2 1. What is convergence, and how can the differences in
the convergence of rods and cones explain (a) the rods' greater
sensitivity and (b) the cones' better detail vision? 2. What is a
receptive field? What did Hartline's research indicate about receptive
fields?

62

3.  Describe the experiment that demonstrated the effect of lateral
    inhibition in the Limulus.
4.  What is center-surround antagonism? Describe how lateral inhibition
    and convergence underlie center-surround antagonism.

Chapter 3  The Eye and Retina

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

5. Discuss how lateral inhibition and center-surround receptive fields
can lead to edge enhancement. 6. What is the Chevreul illusion? What
does it illustrate about the difference between physical and perceptual?

8.  What is the young infant's visual acuity, and how does it change
    over the first year of life? What is the reason for

<!-- -->

(a) low acuity at birth and (b) the increase in acuity over the first 6
    to 9 months?

<!-- -->

7.  What does it mean to say that early events are powerful shapers of
    perception? Give examples.

THINK ABOUT IT 1. Ellen is looking at a tree. She sees the tree because
light is reflected from the tree into her eyes, as shown in Figure 3.38.
One way to describe this is to say that information about the tree is
contained in the light. Meanwhile, Roger is off to the side, looking
straight ahead. He doesn't see the tree because he is looking away from
it. He is, however, looking right at the space through which the light
that is carrying information from the tree to Ellen is passing. But
Roger doesn't see any of this information. Why does this occur? (Hint
#1: Consider the idea that "objects make light visible." Hint #2: Outer
space contains a great deal of light, but it looks dark, except where
there are objects.) 2. In the demonstration "Becoming Aware of What Is
in Focus" on page 44, you saw that we see things clearly only when we
are looking directly at them so that their image falls on the cone-rich
fovea. But consider the common observation that the things we aren't
looking at do not appear fuzzy, that the entire scene appears sharp or
in focus. How can this be, in light of the results of the demonstration?
3. Here's an exercise you can do to get more in touch with the process
of dark adaptation: Find a dark place where you can make some
observations as you adapt to the dark. A closet is Light

a good place to do this because you can regulate the intensity of light
inside the closet by opening or closing the door. The idea is to create
an environment in which there is dim light (no light at all is too
dark). Take this book into the closet, opened to this page. (If you are
reading an ebook on your device, make a paper copy of Figure 3.39 to
take into the closet.) Close the closet door all the way, and then open
the door slowly until you can just barely make out the white circle on
the far left in Figure 3.39 but can't see the others or see them as
being very dim. As you sit in the dark, become aware that your
sensitivity is increasing by noting how the circles to the right in the
figure slowly become visible over a period of about 20 minutes. Also
note that once a circle becomes visible, it gets easier to see as time
passes. If you stare directly at the circles, they may fade, so move
your eyes around every so often. Also, the circles will be easier to see
if you look slightly above them. 4. Look for shadows, both inside and
outside, and see if you can see Mach bands at the borders of the
shadows. Remember that Mach bands are easier to see when the border of a
shadow is slightly fuzzy. Mach bands are not actually present in the
pattern of light and dark, so it is important to be sure that the bands
are not really in the light but are created by the nervous system.
Figure 3.38 Ellen sees the tree because light is reflected from the tree
into her eyes. Roger doesn't see the tree because he is not looking at
it, but he is looking directly across the space where light from the
tree is reflected into Ellen's eyes. Why isn't he aware of the
information contained in this light?

Ellen

Roger

Figure 3.39 Dark adaptation test circles. Think About It

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

63

KEY TERMS Absorption spectrum (p. 50) Accommodation (p. 43) Amacrine
cells (p. 52) Axial myopia (p. 45) Bipolar cells (p. 51) Blind spot
(p. 42) Center-surround antagonism (p. 55) Center-surround receptive
field (p. 55) Chevreul illusion (p. 58) Cone spectral sensitivity
(p. 49) Cones (p. 41) Convergence (p. 52) Cornea (p. 40) Dark adaptation
(p. 46) Dark adaptation curve (p. 46) Dark-adapted sensitivity (p. 47)
Detached retina (p. 49) Edge enhancement (p. 58) Excitatory area (p. 56)
Excitatory-center, inhibitory-surround receptive field (p. 56) Eyes
(p. 40)

64

Farsightedness (p. 45) Fovea (p. 41) Ganglion cells (p. 51) Horizontal
cells (p. 52) Hyperopia (p. 45) Inhibitory area (p. 56)
Inhibitory-center, excitatory-surround receptive field (p. 56)
Isomerization (p. 46) Lateral inhibition (p. 56) Lens (p. 40)
Light-adapted sensitivity (p. 46) Mach bands (p. 58) Macular
degeneration (p. 41) Monochromatic light (p. 49) Myopia (p. 45)
Nearsightedness (p. 45) Neural circuits (p. 51) Neural convergence
(p. 52) Ommatidia (p. 56) Optic nerve (p. 41) Outer segments (p. 41)
Peripheral retina (p. 41) Photoreceptors (p. 40)

Preferential looking technique (p. 60) Presbyopia (p. 45) Pupil (p. 40)
Purkinje shift (p. 50) Receptive field (p. 55) Refractive errors (p. 44)
Refractive myopia (p. 45) Retina (p. 40) Retinitis pigmentosa (p. 42)
Rod monochromats (p. 48) Rod--cone break (p. 48) Rod spectral
sensitivity curve (p. 49) Rods (p. 40) Spectral sensitivity (p. 49)
Spectral sensitivity curve (p. 49) Transduction (p. 45) Visible light
(p. 40) Visual acuity (p. 53) Visual evoked potential (p. 61) Visual
pigment bleaching (p. 48) Visual pigment regeneration (p. 49) Visual
pigments (p. 41) Wavelength (p. 40)

Chapter 3  The Eye and Retina

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

The brain is a complex structure that creates our perceptions when
electrical signals occur in specific areas, and then flow along pathways
leading from one area to another. This artistically embellished image of
the brain symbolizes the mysteries of the brain's operation. E. M.
Pasieka/Science Photo Library/Corbis

Learning Objectives After studying this chapter, you will be able to ...
■■ Explain how visual signals travel from the eye to the lateral ge-

niculate nucleus, and then to the visual cortex. ■■ Distinguish between
the different types of cells in the visual cor-

tex and their role in perception. ■■ Describe experiments that
illustrate the connection between

neurons called "feature detectors" and perception. ■■ Discuss how
perception of visual objects and scenes depends

■■ Describe visual pathways beyond the visual cortex, including

the what and where streams and how the functions of these streams have
been studied. ■■ Describe higher-level neurons, how they are involved in

perceiving objects, and the connection between higher-level neurons and
visual memories. ■■ Explain what is meant by "flexible" receptive
fields.

on neural "maps" and "columns" in the cortex.

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

C ha p ter 4

The Visual Cortex and Beyond Chapter Contents 4.1 From Retina to Visual
Cortex Pathway to the Brain Receptive Fields of Neurons in the Visual
Cortex

4.3 Spatial Organization in the Visual Cortex The Neural Map in the
Striate Cortex (V1)

METHOD: Brain Ablation

Streams for Information About What and How METHOD: Double Dissociations

METHOD: Presenting Stimuli to

DEMONSTRATION: Cortical

in Neuropsychology

Determine Receptive Fields

Magnification of Your Finger

4.2 The Role of Feature Detectors in Perception Selective Adaptation

The Cortex Is Organized in Columns How V1 Neurons and Columns Underlie
Perception of a Scene

Responses of Neurons in Inferotemporal Cortex Where Perception Meets
Memory

METHOD: Psychophysical

TEST YOURSELF 4.1

Measurement of the Effect of Selective Adaptation to Orientation

Selective Rearing

SOMETHING TO CONSIDER: "Flexible"

4.4 Beyond the Visual Cortex

Receptive Fields

Streams for Information About What and Where

TEST YOURSELF 4.2

Some Questions We Will Consider: ■■ Where does the transduced visual
signal go once it leaves

the retina? (p. 68) ■■ How is visual information organized in the
cortex? (p. 75) ■■ How do the responses of neurons change as we move

higher in the visual system? (p. 79)

I

4.5 Higher-Level Neurons

n Chapter 3, as we began our exploration of the perceptual process for
vision, we saw that a number of transformations take place in the
retina, before we get to the brain. Now we will focus our attention on
the later stages of the visual process by looking at how electrical
signals are sent from the eye to the visual cortex, what happens once
they get there, and where they go next. Historically, understanding
functions of different parts of the brain often began with case studies
of people with brain damage. Our knowledge of how the brain responds to
visual input can be traced back to the Russo-Japanese War of 1904--
1905. During this war, Japanese physician Tatsuji Inouye was treating
soldiers who survived gunshot wounds to the head,

THINK ABOUT IT

and in doing so, he made an interesting observation. He noticed that if
a soldier had a wound to the back of the head, his vision was impaired.
And not only that, but the area of the head that was injured was
correlated with the area of vision that was lost. For example, if the
bullet wound was to the right side of the brain, then visual impairments
were noticed on the left side of the soldier's visual field, and vice
versa (Glickstein & Whitteridge, 1987). While there was other early
research on the brain's role in vision prior to Inouye's observations in
humans (Colombo et al., 2002), his contributions spoke not only to
function (that the back of the brain is involved in vision), but also to
organization (that the location in the brain maps onto the location in
the visual field).

4.1 From Retina to Visual Cortex How does the visual signal get from the
retina to the visual area of the cortex? And once it has reached the
cortex, how is it processed?

67

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Pathway to the Brain The pathway from the retina to the brain is
illustrated in Figure 4.1. The first thing that happens on this journey
is that the visual signals from both eyes leave the back of the eye in
the optic nerve and meet at a location called the optic chiasm. The
optic chiasm is an x-shaped bundle of fibers on the underside of the
brain. Interestingly, if you were to hold a human brain and flip it
over, you could actually see the optic chiasm. At the optic chiasm, some
of the fibers cross to the opposite side of the brain from the eye they
came from. The result of this crossing is that all fibers corresponding
to the right visual field, regardless of eye, end up on the left
side---or hemisphere---of the brain, and vice versa. In this way, each
hemisphere of the brain responds to the opposite, or contralateral, side
of the visual field. This can be seen in the color coding in Figure
4.1b. The visual field is determined based on where the person is
fixating; anything to right of the point of central focus is the right
visual field (processed by the left hemisphere), and anything to the
left is the left visual field (processed by the right hemisphere).
Importantly, both eyes can see both visual fields. You can determine
this for yourself by holding up your finger and looking directly at it,
and noticing that you can still see to the left and right of your finger
even if you close your left or right eye. After meeting at the optic
chiasm and crossing to the contralateral hemisphere, the visual signal's
journey to the cortex Figure 4.1 (a) Side view of the visual system,
showing the major sites along the primary visual pathway where
processing takes place: the eye, the optic nerve, the lateral geniculate
nucleus, and the visual receiving area of the cortex. (b) Visual system
seen on the underside of the brain, showing the superior colliculus,
which receives some of the signals from the eye. The optic chiasm is the
place where some of the fibers from each eye cross over to the other
side of the brain, so they reach the contralateral (opposite) hemisphere
of the visual cortex. This is illustrated by the colors, with red
indicating fibers transmitting information about the right visual field
and blue indicating fibers transmitting information about the left
visual field.

continues. Approximately 90 percent of the signals from the retina
proceed to the lateral geniculate nucleus (LGN), located in the thalamus
of each hemisphere, while the other 10 percent of fibers travel to the
superior colliculus (Figure 4.1b), a structure involved in controlling
eye movements. In vision and in other senses as well, the thalamus
serves as a relay station where incoming sensory information often makes
a stop before reaching the cerebral cortex. In Chapter 3, we introduced
receptive fields and showed that ganglion cells receptive fields have a
center-surround organization (Kuffler, 1953). As it turns out, neurons
in the LGN also have center-surround receptive fields (Hubel & Wiesel,
1961). The fact that little change occurred in receptive fields when
moving from the retina to the LGN made researchers wonder about the
function of the LGN. One proposal of LGN function is based on the
observation that the signal sent from the LGN to the cortex is smaller
than the input the LGN receives from the retina (Figure 4.2). This
decrease in the signal leaving the LGN has led to the suggestion that
one of the purposes of the LGN is to regulate neural information as it
flows from the retina to the cortex (Casagrande & Norton, 1991; Humphrey
& Saul, 1994). Another important characteristic of the LGN is that it
receives more signals from the cortex than from the retina (Sherman &
Koch, 1986; Wilson et al., 1984). This "backward" flow of information,
called feedback, could also be involved in Lateral geniculate nucleus in
thalamus

Visual receiving area (striate cortex)

Eye Light energy

Optic nerve (a)

Left visual field

- 

Right visual field

Optic nerve Optic chiasm Lateral geniculate nucleus Superior colliculus

(b) 

68

Visual cortex

Chapter 4  The Visual Cortex and Beyond

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

From cortex To cortex

LGN

From retina

Figure 4.2 Information flow into and out of the LGN. The sizes of the
arrows indicate the sizes of the signals.

regulation of information flow, the idea being that the information the
LGN receives back from the brain may play a role in determining which
information is sent up to the brain. As we will see later in the book,
there is good evidence for the role of feedback in perception (Gilbert &
Li, 2013). From the LGN, the visual signal then travels to the occipital
lobe, which is the visual receiving area---the place where signals from
the retina and LGN first reach the cortex. The visual receiving area is
also called the striate cortex, because it has a striped appearance when
viewed in cross section, or area V1 to indicate that it is the first
visual area in the cortex. As indicated by the blue arrows in Figure
4.1a, signals also travel to other places in the cortex---a fact that we
will return to later in this chapter. We have seen how the signals
leaving the eye cross at the optic chiasm, make a stop in the LGN, and
then proceed to the visual cortex. Next, we'll see how neurons in the
visual cortex respond to those incoming signals.

Receptive Fields of Neurons in the Visual Cortex Our discussion of
receptive fields in Chapter 3 focused on the center-surround receptive
fields of ganglion cells. Once the concept of receptive fields was
introduced, researchers realized that they could follow the effects of
processing through different levels of the visual system by determining
which patterns of light are most effective in generating a response in
neurons at each level. This was the strategy adopted by David Hubel and
Thorsten Wiesel, who made substantial contributions to the study of
receptive fields. In fact, their work was so important to the field that
it earned them the Nobel Prize in Physiology and Medicine in 1981. Hubel
and Wiesel (1965) state their tactic for understanding receptive fields
as follows: One approach ... is to stimulate the retina with patterns of
light while recording from single cells or fibers at various points
along the visual pathway. For each cell, the optimum stimulus can be
determined, and one can note the characteristics common to cells at each
level in the visual pathway, and compare a given level with the next.
(Hubel & Wiesel, 1965, p. 229)

In their study of receptive fields, Hubel and Wiesel (1965) modified
earlier procedures that were used to present light to the retina.
Instead of shining light directly into the animal's eye, Hubel and
Wiesel had animals look at a screen on which they projected stimuli.
METHOD

Presenting Stimuli to Determine Receptive Fields

A neuron's receptive field is determined by presenting a stimulus, such
as a spot of light, to different places on the retina to determine which
areas result in no response, an excitatory response, or an inhibitory
response. Hubel and Wiesel projected stimuli onto a screen (Figure 4.3).
The animal, usually a cat or monkey, was anesthetized and looked at the
screen, its eyes focused with glasses so that whatever was presented on
the screen would be in focus on the back of the eye. Because the cat's
eye remains stationary, each point on the screen corresponds to a point
on the cat's retina. Thus, a stimulus at point A on the screen creates
an image on point A on the retina, B creates an image on B, and C on C.
There are many advantages to projecting an image on a screen. Stimuli
are easier to control compared to projecting light directly into the eye
(especially for moving stimuli); they are sharper; and it is easier to
present complex stimuli such as faces or scenes. An important thing to
remember about receptive fields, which is always true no matter what
method is used, is that the receptive field is always on the receptor
surface. The receptor surface is the retina in our examples, but as we
will see later, there are also receptive fields in the touch system on
the surface of the skin. It is also important to note that it doesn't
matter where the neuron is--- the neuron can be in the retina, the
cortex serving vision or touch, or elsewhere in the brain, but the
receptive field is always on the receptor surface, because that is where
the stimuli are received. A

C

B

B

C

A

A

B

C

Figure 4.3 Method of determining receptive fields using a projection
screen to present the stimuli. Each location on the projection screen
corresponds to a location on the retina. Receptive fields can be
recorded from neurons anywhere in the visual system, but the receptive
field is always located on the retina. 4.1 From Retina to Visual Cortex

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

69

-- -- + + ---- -- -- + -- ---- ---- -- ++ -- -- -- ++ -- -- + ---- -- --
---- + + ------ +

on

Receptive field--- simple cortical cell (a)

off

on

off

(c) 
(d) 

Impulses per second

Orientation tuning curve 25

10

40

20

0

20

40

Vertical (d)

Orientation (degrees)

Figure 4.4 (a) An example of a receptive field of a simple cortical
cell. (b) This cell responds best to a vertical bar of light that covers
the excitatory area of the receptive field. (c) The response decreases
as the bar is tilted so that it also covers the inhibitory area. (d)
Orientation tuning curve of a simple cortical cell for a neuron that
responds best to a vertical bar (orientation 5 0).

By flashing spots of light on different places in the retina, Hubel and
Wiesel found cells in the striate cortex with receptive fields that,
like center-surround receptive fields of neurons in the retina and LGN,
have excitatory and inhibitory areas. However, these areas are arranged
side by side rather than in the center-surround configuration (Figure
4.4a). Cells with these side-by-side receptive fields are called simple
cortical cells. We can tell from the layout of the excitatory and
inhibitory areas of the simple cell shown in Figure 4.4a that a cell
with this receptive field would respond best to a vertical bar of light,
line, or edge. Hubel and Wiesel found that not only do the simple cells
respond to bars, but to bars of particular orientations. As shown in
Figure 4.4b, a vertical bar that illuminates only the excitatory area
causes high firing, but as the bar is tilted so the inhibitory area is
illuminated, firing decreases (Figure 4.4c). The relationship between
orientation and firing is indicated by a neuron's orientation tuning
curve, which is determined by measuring the responses of a simple
cortical cell to bars with different orientations. The tuning curve in
Figure 4.4d shows that the cell responds with 25 nerve impulses per
second to a vertically oriented bar and that the cell's response
decreases as the bar is tilted away from the vertical and begins
stimulating inhibitory areas of the neuron's receptive field. Notice
that a bar tilted 20 degrees from the vertical elicits only a small
response. This particular simple cell responds best to a bar with a
vertical orientation (in other words, it "prefers" vertically oriented
bars), but there are other simple cells in the visual cortex that
respond to other orientations, so 70

there are neurons that respond to all of the orientations that exist in
the environment. Although Hubel and Wiesel were able to use small spots
of light to map the receptive fields of simple cortical cells like the
one in Figure 4.4, they found that many of the cells they encountered in
the striate cortex and nearby visual areas did not respond to small
spots of light. In his Nobel lecture, Hubel described how he and Wiesel
were becoming increasingly frustrated in their attempts to get these
neurons to fire, when something startling happened: As they inserted a
glass slide containing a spot stimulus into their slide projector,1 a
visual cortex neuron "went off like a machine gun" (Hubel, 1982). The
neuron, as it turned out, was responding not to the spot at the center
of the slide that Hubel and Wiesel had planned to use as a stimulus, but
to the image of the slide's edge moving downward on the screen as the
slide dropped into the projector (Figure 4.5). Upon realizing this,
Hubel and Wiesel changed their stimuli from small spots to moving lines
and were then able to find cells that responded to oriented moving bars.
As with simple cells, a particular neuron had a preferred orientation.
Hubel and Wiesel (1965) discovered that many cortical neurons respond
best to moving barlike stimuli with specific orientations. Complex
cells, like simple cells, respond best to 1 A slide projector is a
device that, until the advent of digital technology, was the method of
choice for projecting images onto a screen. Slides were inserted into
the projector and the images on the slides were projected onto the
screen. Although slides and slide projectors have been replaced by
digital imaging devices, it is still possible to purchase slide
projectors on the Internet; however, the popular Kodachrome slide film
used to shoot family vacation pictures was discontinued in 2009.

Chapter 4  The Visual Cortex and Beyond

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Edge of slide

Figure 4.5 When Hubel and Wiesel dropped a slide into their slide
projector, the image of the edge of the slide moving down unexpectedly
triggered activity in a visual cortex neuron.

bars of a particular orientation. However, unlike simple cells, which
respond to small spots of light or to stationary stimuli, most complex
cells respond only when a correctly oriented bar of light moves across
the entire receptive field. Further, many complex cells respond best to
a particular direction of movement (Figure 4.6a). Because these neurons
don't respond to stationary flashes of light, their receptive fields are
indicated not by pluses and minuses but by outlining the area that, when
stimulated, elicits a response in the neuron. Another type of cell in
the visual cortex, called endstopped cells, fires to moving lines of a
specific length or to moving corners or angles. Figure 4.6b shows a
light corner

(No response to downward movement)

(No response to movement)

stimulus that is being moved up and down across the retina. The records
to the right indicate that the neuron responds best to a medium-sized
corner that is moving upward. Hubel and Wiesel's finding that some
neurons in the visual cortex respond only to oriented lines and others
respond best to corners was an extremely important discovery because it
extended the idea first proposed in connection with centersurround
receptive fields that neurons respond to some patterns of light and not
to others. This makes sense because the purpose of the visual system is
to enable us to perceive objects in the environment, and many objects
can be at least crudely represented by simple shapes and lines of
various orientations. Thus, Hubel and Wiesel's discovery that neurons
respond selectively to oriented lines and stimuli with specific lengths
was an important step toward determining how neurons respond to more
complex objects. Table 4.1, which summarizes the properties of the
neurons we have described so far, illustrates an important fact about
neurons in the visual system: As we travel farther from the retina,
neurons fire to more complex stimuli. Retinal ganglion cells respond
best to spots of light, whereas cortical endstopped cells respond best
to bars of a certain length that are moving in a particular direction.
Because simple, complex, and end-stopped cells fire in response to
specific features of the stimulus, such as orientation or direction of
movement, they have also been called feature detectors. Next, we will
discuss how these feature detectors in the visual cortex are important
to perception.

Figure 4.6 (a) Response of a complex cell recorded from the visual
cortex of the cat. The stimulus bar is moved back and forth across the
receptive field. This cell fires best when the bar is positioned with a
specific orientation and is moved from left to right. (b) Response of an
end-stopped cell recorded from the visual cortex of the cat. The
stimulus is indicated by the light area on the left. This cell responds
best to a medium-sized corner that is moving up.

(b) 
(c) 4.1 From Retina to Visual Cortex

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

71

Table 4.1 Properties of Neurons in the Retina, LGN,

and Visual Cortex

Type of Cell

Characteristics of Receptive Field

Ganglion cell

Center-surround receptive field. Responds best to small spots, but will
also respond to other stimuli.

Lateral geniculate

Center-surround receptive fields very similar to the receptive field of
a ganglion cell.

Simple cortical

Excitatory and inhibitory areas arranged side by side. Responds best to
bars of a particular orientation.

Complex cortical

Responds best to movement of a correctly oriented bar across the
receptive field. Many cells respond best to a particular direction of
movement.

End-stopped cortical

Responds to corners, angles, or bars of a particular length moving in a
particular direction.

4.2 The Role of Feature Detectors in Perception Neural processing endows
neurons in the visual cortex with properties that make them feature
detectors that respond best to a specific type of stimulus. When
researchers show that neurons respond to oriented lines, they are
measuring the stimulus--physiology relationship (arrow B in Figure 4.7),
n - Recognition ptio Ac tio rce e n P BEHAVIOR

Selective Rearing

A

C

IO

in g

P H YS

Process

TI M UL ro US xim al - Di s ta l

Selective Adaptation

LO

-R

G Y pt or s

ec

S

e

P

B Figure 4.7 Three-part version of the perceptual process, repeated from
Figure 1.13, showing the three basic relationships: (A)
stimulus--behavior, (B) stimulus--physiology, and (C) physiology--
behavior. "Selective Adaptation" and "Selective Rearing" refer to
experiments described in the text that were designed to measure
relationship C.

72

introduced in Chapter 1. But just measuring this relationship does not
prove that these neurons have anything to do with the perception of
oriented lines. To demonstrate a link between physiology and perception,
it is necessary to measure the physiology--behavior relationship (arrow
C). One way this has been accomplished is by using a psychophysical
procedure called selective adaptation.

Selective Adaptation When we view a stimulus with a specific property,
neurons tuned to that property fire. The idea behind selective
adaptation is that this firing causes neurons to eventually become
fatigued, or adapt. This adaptation causes two physiological effects:
(1) the neuron's firing rate decreases, and (2) the neuron fires less
when that stimulus is immediately presented again. According to this
idea, presenting a vertical line causes neurons that respond to vertical
lines to respond, but as these presentations continue, these neurons
eventually begin to fire less to vertical lines. Adaptation is selective
because only the neurons that were responding to verticals or
near-verticals adapt, and neurons that were not firing do not adapt.

Psychophysical Measurement of the Effect of Selective Adaptation to
Orientation

METHOD

Measuring the effect of selective adaptation to orientation involves the
following three steps: 1. Measure a person's contrast threshold to
gratings with a number of different orientations (Figure 4.8a). A
grating's contrast threshold is the minimum intensity difference between
two adjacent bars that can just be detected. The contrast threshold for
seeing a grating is measured by changing the intensity difference
between the light and dark bars until the bars can just barely be seen.
For example, it is easy to see the four gratings on the left of Figure
4.9, because the difference in intensity between the bars is above
threshold. However, there is only a small intensity difference between
the bars of the grating on the far right, so it is close to the contrast
threshold. The intensity difference at which the bars can just barely be
seen is the contrast threshold. 2. Adapt the person to one orientation
by having the person view a high-contrast adapting stimulus for a minute
or two. In this example, the adapting stimulus is a vertical grating
(Figure 4.8b). 3. Remeasure the contrast threshold of all the test
stimuli presented in step 1 (Figure 4.8c).

Chapter 4  The Visual Cortex and Beyond

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

(a) Measure contrast threshold at a number of orientations.

(b) Adapt to a high-contrast grating.

(c) Remeasure contrast thresholds for same orientations as above.

Figure 4.8 Procedure for carrying out a selective adaptation experiment.
See text for details.

Figure 4.9 The contrast threshold for a grating is the minimum
difference in intensity at which the observer can just make out the
bars. The grating on the left is far above the contrast threshold. The
ones in the middle have less contrast but are still above threshold. The
grating on the far right is near the contrast threshold. (From
Womelsdorf

(a) 

Large

30

Effect of selective adaptation Impulses/sec

Increase in contrast threshold

et al., 2006)

The rationale behind this procedure is that if the adaptation to the
high-contrast grating in step 2 decreases the functioning of neurons
that determine the perception of verticals, this should cause an
increase in contrast threshold so it is more difficult to see
low-contrast vertical gratings. In other words, adapting vertical
feature detectors should make it necessary to increase the difference
between the black and white vertical bars in order to see them. Figure
4.10a shows that this is exactly what happens. The peak of the contrast
threshold curve, which indicates that a large increase in the difference
between the bars was needed to see the bars, occurs at the vertical
adapting orientation. The important result of this experiment is that
our psychophysical curve shows that adaptation selectively affects only
some orientations, just as neurons selectively respond to only some
orientations. In fact, comparing the psychophysically determined
selective adaptation curve (Figure 4.10a) to the orientation tuning
curve for a simple cortical neuron (Figure 4.10b) reveals that they are
very similar. (The psychophysical curve is slightly wider because the
adapting stimulus affects some neurons that respond to orientations near
the adapting orientation.) The near match between the orientation
selectivity of neurons and the perceptual effect of selective adaptation
supports the idea that feature detectors---in this case, simple cells in
the visual cortex---play a role in perception. The selective adaptation
experiment is measuring how a physiological effect (adapting the feature
detectors that respond to a specific orientation) causes a perceptual
result (decrease in sensitivity to that orientation). This evidence that
feature detectors have something to do with perception means that when
you look at a complex scene, such as a city street or a crowded shopping
mall, feature detectors that are firing to the orientations in the scene
are helping to construct your perception of the scene.

Orientation tuning

20

10

Adapting orientation

Small 408

208

0

208

408

408

208

0

208

(Vertical)

(Vertical)

Orientation of grating

Orientation of grating

408

(b) 

Figure 4.10 (a) Results of a psychophysical selective adaptation
experiment. This graph shows that the person's adaptation to the
vertical grating causes a large decrease in the ability to detect the
vertical grating when it is presented again but has less effect on
gratings that are tilted to either side of the vertical. (b) Orientation
tuning curve of the simple cortical neuron from Figure 4.4.

4.2 The Role of Feature Detectors in Perception

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

73

Selective Rearing Further evidence that feature detectors in the visual
cortex are involved in perception is provided by selective rearing
experiments. The idea behind selective rearing is that if an animal is
reared in an environment that contains only certain types of stimuli,
then neurons that respond to these stimuli will become more prevalent.
This follows from a phenomenon called neural plasticity or
experience-dependent plasticity---the idea that the response properties
of neurons can be shaped by perceptual experience. According to this
idea, rearing an animal in an environment that contains only vertical
lines should result in the animal's visual cortex having simple cells
that respond predominantly to verticals. This result may seem to
contradict the results of the selective adaptation experiment just
described, in which exposure to verticals decreases the response to
verticals. However, adaptation is a short-term effect. Presenting the
adapting orientation for a few minutes decreases responding to that
orientation. In contrast, selective rearing is a longer-term effect.
Presenting the rearing orientation over a period of days or even weeks
keeps the neurons that respond to that orientation active. Meanwhile,
neurons that respond to orientations that aren't present are not active,
so they lose their ability to respond to those orientations. One way to
describe the results of selective rearing experiments is "Use it or lose
it." This effect was demonstrated in a classic experiment by Colin
Blakemore and Grahame Cooper (1970) in which they placed kittens in
striped tubes like the one in Figure 4.11a, so that each kitten was
exposed to only

one orientation, either vertical or horizontal. The kittens were kept in
the dark from birth to 2 weeks of age, at which time they were placed in
the tube for 5 hours a day; the rest of the time they remained in the
dark. Because the kittens sat on a Plexiglas platform, and the tube
extended both above and below them, there were no visible corners or
edges in their environment other than the stripes on the sides of the
tube. The kittens wore cones around their head to prevent them from
seeing vertical stripes as oblique or horizontal stripes by tilting
their heads; however, according to Blakemore and Cooper, "The kittens
did not seem upset by the monotony of their surroundings and they sat
for long periods inspecting the walls of the tube" (p. 477). When the
kittens' behavior was tested after 5 months of selective rearing, they
seemed blind to the orientations that they hadn't seen in the tube. For
example, a kitten that was reared in an environment of vertical stripes
would pay attention to a vertical rod but ignore a horizontal rod.
Following behavioral testing, Blakemore and Cooper recorded from simple
cells in the visual cortex and determined the stimulus orientation that
caused the largest response from each cell. Figure 4.11b shows the
results of this experiment. Each line indicates the orientation
preferred by a single neuron in the cat's cortex. This cat, which was
reared in a vertical environment, has many neurons that respond best to
vertical or nearvertical stimuli, but none that respond to horizontal
stimuli. The horizontally responding neurons were apparently lost
because they hadn't been used. The opposite result occurred for the
horizontally reared cats. The parallel between the orientation
selectivity of neurons in the cat's cortex and the cat's

Vertically reared cat

Horizontally reared cat

Vertical

Vertical

Horizontal

Vertical (a)

Horizontal

Vertical

(b) 

Figure 4.11 (a) Striped tube used in Blakemore and Cooper's (1970)
selective rearing experiments. (b) Distribution of optimal orientations
for 72 simple cells from a cat reared in an environment of vertical
stripes, on the left, and for 52 simple cells from a cat reared in an
environment of horizontal stripes, on the right. (Blakemore & Cooper,
1970)

74

Chapter 4  The Visual Cortex and Beyond

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

behavioral response to the same orientation provides more evidence that
feature detectors are involved in the perception of orientation. This
connection between feature detectors and perception was one of the major
discoveries of vision research in the 1960s and 1970s. Related to this
result is the oblique effect discussed in Chapter 1 (p. 12)---the fact
that people perceive vertical and horizontal lines better than slanted
lines. What is important about the oblique effect is not only that
people see horizontals and verticals better, but that the brain's
response to detecting horizontals and verticals is larger than when
detecting slanted lines (Figure 1.16, page 14). Possibly, just as the
orientation selectivity of the kitten's neurons matched its horizontal
or vertical environment, the response of human neurons reflects the fact
that horizontals and verticals are more common than slanted lines in our
environment (Coppola et al., 1998). So far, we have seen how visual
cortex neurons fire in response to certain features, like specific
orientations of lines that comprise edges of objects in our visual
world. We've also seen how these "feature detectors" are related to
perception, as demonstrated in the selective adaptation and selective
rearing experiments. We now consider how these neurons are organized in
the visual cortex.

4.3 Spatial Organization in the Visual Cortex When we look out at a
scene, things are organized across our visual field. There's a house on
the left, a tree next to the house, and a car parked in the driveway on
the other side of the house. This organization of objects in visual
space becomes transformed into organization in the eye, when an image of
the scene is created on the retina. It is easy to appreciate spatial
organization at the level of the retinal image because this image is
essentially a picture of the scene. But once the house, the tree, and
the car have been transformed into electrical signals, the signals
created by each object then become organized in the form of "neural
maps," so that objects that create images near each other on the retina
are represented by neural signals that are near each other in the
cortex.

The Neural Map in the Striate Cortex (V1) To begin describing neural
maps, let's describe how points in the retinal image are represented
spatially in the striate cortex (area V1). We determine this by
stimulating various places on the retina and noting where neurons fire
in the cortex. Figure 4.12 shows a man looking at a tree so that points
A, B, C, and D on the tree stimulate points A, B, C, and D on his
retina. Moving to the cortex, the image at point A on the retina causes
neurons at point A to fire in the cortex. The image at point B causes
neurons at point B to fire, and so on. This example shows how points on
the retinal image cause activity in the cortex.

A B

D C

D C

B

B A

A

C D

Figure 4.12 A person looking at a tree, showing how points A, B, C, and
D are imaged on the retina and where these retinal activations cause
activity in the brain. Although the distances between A and B and
between C and D are about the same on the retina, the distance between A
and B is much greater on the cortex. This is an example of cortical
magnification, in which more space is devoted to areas of the retina
near the fovea.

This example also shows that locations on the cortex correspond to
locations on the retina. This electronic map of the retina on the cortex
is called a retinotopic map. This organized spatial map means that two
points that are close together on an object and on the retina will
activate neurons that are close together in the brain (Silver & Kastner,
2009). But let's look at this retinotopic map a little more closely,
because it has a very interesting property that is relevant to
perception. Although points A, B, C, and D in the cortex correspond to
points A, B, C, and D on the retina, you might notice something about
the spacing of these locations. Considering the retina, we note that the
man is looking at the leaves at the top of the tree, so points A and B
are both near the fovea and the images of points C and D at the bottom
of the trunk are in the peripheral retina. But although A and B and C
and D are the same distance apart on the retina, the spacing is not the
same on the cortex. A and B are farther apart on the cortex than C and
D. What this means is that electrical signals associated with the part
of the tree near where the person is looking are allotted more space on
the cortex than signals associated with parts of the tree that are
located off to the side---in the periphery. In other words, the spatial
representation of the visual scene on the cortex is distorted, with more
space being allotted to locations near the fovea than to locations in
the peripheral retina. Even though the fovea accounts for only 0.01
percent of the retina's area, signals from the fovea account for 8 to 10
percent of the retinotopic map on the cortex (Van Essen & Anderson,
1995). This apportioning of a large area on the cortex to the small
fovea is called cortical magnification. The size of this magnification,
which is called the cortical magnification factor, is depicted in Figure
4.13. 4.3 Spatial Organization in the Visual Cortex

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

75

Visual field Visual cortex

Retina

8--10% of cortical map's area

Fovea: 0.01% of retinal area

Figure 4.13 The magnification factor in the visual system. The small
area of the fovea is represented by a large area on the visual cortex.

Cortical magnification has been determined in the human cortex using
brain imaging (see Chapter 2, page 31). Robert Dougherty and coworkers
(2003) had participants in the fMRI scanner look at stimuli like the one
shown in Figure 4.14a. The participant looked directly at the center of

Visual field

158 108 58

(a) 

Cortex

Figure 4.15 Demonstration of the magnification factor. A person looks at
the red spot on the text on the left. The area of brain activated by
each letter of the text is shown on the right. The arrows point to the
letter a in the text on the left, and the area in the brain activated by
the a on the right. (Based on Wandell et al., 2009)

the screen, so the dot at the center fell on the fovea. During the
experiment, stimulus light was presented in two places: (1) near the
center (red area), which illuminated a small area near the fovea; and
(2) farther from the center (blue area), which illuminated an area in
the peripheral retina. The areas of the visual cortex activated by these
two stimuli are indicated in Figure 4.14b. This activation illustrates
cortical magnification because stimulation of the small area near the
fovea activated a greater area on the cortex (red) than stimulation of
the larger area in the periphery (blue). (Also see Wandell, 2011.) The
large representation of the fovea in the cortex is also illustrated in
Figure 4.15, which shows the space that would be allotted to words on a
page (Wandell et al., 2009). Notice that the letter "a," which is near
where the person is looking (red arrow), is represented by a much larger
area in the cortex than letters that are far from where the person is
looking. The extra cortical space allotted to letters and words at which
the person is looking provides the extra neural processing needed to
accomplish tasks such as reading that require high visual acuity
(Azzopardi & Cowey, 1993). What cortical magnification means when you
look at a scene is that information about the part of the scene you are
looking at takes up a larger space on your visual cortex than an area of
equal size that is off to the side. Another way to appreciate the
magnification factor is to do the following demonstration. DEMONSTRATION

Finger

(b) 

Figure 4.14 (a) Red and blue areas show the extent of stimuli that were
presented while a person was in an fMRI scanner. (b) Red and blue
indicate areas of the brain activated by the stimulation in (a).

Visual field representation in the brain (V1)

Cortical Magnification of Your

Hold your left hand at arm's length, holding your index finger up. As
you look at your finger, hold your right hand at arm's length, about a
foot to the right of your finger and positioned so the back of your hand
is facing you. When you have done this, your left index finger (which
you are still looking at) activates an area of cortex as large as the
area activated by your whole right hand.

(From Dougherty et al., 2003)

76

Chapter 4  The Visual Cortex and Beyond

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

An important thing to note about this demonstration is that even though
the image of your finger on the fovea takes up about the same space on
the cortex as the image of your hand on the peripheral retina, you do
not perceive your finger as being as large as your hand. Instead, you
see the details of your finger far better than you can see details on
your hand. That more space on the cortex translates into better detailed
vision rather than larger size is an example of the fact that what we
perceive doesn't exactly match the "picture" in the brain. We will
return to this idea shortly.

A

B

White matter

The Cortex Is Organized in Columns We determined the retinotopic map on
the brain by measuring activity near the surface of the cortex. We are
now going to consider what is happening below the surface by looking at
the results of experiments in which a recording electrode was inserted
into the visual cortex.

Location and Orientation Columns Hubel and Wiesel (1965) carried out a
series of experiments in which they recorded from neurons they
encountered as they lowered electrodes into the visual cortex. When they
inserted an electrode perpendicular to the surface of a cat's cortex,
they found that every neuron they encountered had its receptive field at
about the same location on the retina. Their results are shown in Figure
4.16a, which shows four neurons along the electrode track, and Figure
4.16b, which shows that these neurons' receptive fields are all located
at about the same place on the retina. From this result, Hubel and
Wiesel concluded that the striate cortex is organized into location
columns that are perpendicular to the surface of the cortex, so that all
of the neurons within a location column have their receptive fields at
the same location on the retina. As Hubel and Wiesel lowered their
electrodes perpendicular to the surface of the cortex, they noted not
only that the Surface of cortex 1 2 3 4

Surface of cortex

Cortex

Figure 4.17 Orientation columns in the visual cortex. All of the
cortical neurons encountered along track A respond best to horizontal
bars (indicated by the red lines cutting across the electrode track).
All of the neurons along track B respond best to bars oriented at 45
degrees.

neurons along this track had receptive fields with the same location on
the retina, but that these neurons all preferred stimuli with the same
orientation. Thus, all cells encountered along the electrode track at A
in Figure 4.17 fired the most to horizontal lines, whereas all those
along electrode track B fired the most to lines oriented at about 45
degrees. Based on this result, Hubel and Wiesel concluded that the
cortex is also organized into orientation columns, with each column
containing cells that respond best to a particular orientation. Hubel
and Wiesel also showed that adjacent orientation columns have cells with
slightly different preferred orientations. When they moved an electrode
through the cortex obliquely (not perpendicular to the surface), so that
the electrode cut across orientation columns, they found that the
neurons' preferred orientations changed in an orderly fashion, so a
column of cells that respond best to 90 degrees is right next to the
column of cells that respond best to 85 degrees (Figure 4.18). Hubel and
Wiesel also found that as they moved Oblique electrode

(a) Side view of cortex 1 3 2 4
(b) Receptive field locations on retina

Figure 4.16 Location column in the visual cortex. When an electrode
penetrates the cortex perpendicularly, the receptive fields of the
neurons encountered along this track overlap. The receptive field
recorded at each numbered position along the electrode track (a) is
indicated by a correspondingly numbered square (b).

Preferred orientations of neurons in each column

Figure 4.18 If an electrode is inserted obliquely into the visual
cortex, it crosses a sequence of orientation columns. The preferred
orientation of neurons in each column, indicated by the bars, changes in
an orderly way as the electrode crosses the columns. The distance the
electrode is advanced is exaggerated in this illustration. 4.3 Spatial
Organization in the Visual Cortex

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

77

their electrode 1 millimeter across the cortex, their electrode passed
through orientation columns that represented the entire range of
orientations. Interestingly enough, this 1-mm dimension is the size of
one location column.

One Location Column: Many Orientation Columns This 1-mm dimension for
location columns means that one location column is large enough to
contain orientation columns that cover all possible orientations. Thus,
the location column shown in Figure 4.19 serves one location on the
retina (all the neurons in the column have their receptive fields at
about the same place on the retina) and contains neurons that respond to
all possible orientations. Think about what this means. Neurons in that
location column receive signals from a particular location on the
retina, which corresponds to a small area in the visual field. Because
this location column contains some neurons that respond to every
possible orientation, any oriented edge or line that falls within the
location column's area on the retina will be able to be represented by
some of the neurons in this location column. A location column with all
of its orientation columns was called a hypercolumn by Hubel and Wiesel.
A hypercolumn receives information about all possible orientations that
fall within a small area of the retina; it is therefore well suited for
processing information from a small area in the visual field.2 1 mm

Location column Orientation columns within the location column

How V1 Neurons and Columns Underlie Perception of a Scene Now that we've
discussed visual cortex neurons, what they respond to, and how they are
organized into columns, let's put it all together to understand how
these processes are involved in our perception of a visual scene.
Determining how the millions of neurons in the cortex respond when we
look at a scene such as the one in Figure 4.20a is an ambitious
undertaking. We will simplify the task by focusing on one small part of
the scene---the tree trunk in Figure 4.20b. We focus specifically on the
part of the trunk shown passing through the three circles, A, B, and C.
Figure 4.21a shows how the image of this part of the tree trunk is
projected onto the retina. Each circle represents the area served by a
location column. Figure 4.21b shows the location columns in the cortex.
Remember that each of these location columns contains a complete set of
orientation columns (Figure 4.19). This means that the vertical tree
trunk will activate neurons in the 90-degree orientation columns in each
location column, as indicated by the orange areas in each column. Thus,
the continuous tree trunk is represented by the firing of neurons
sensitive to a specific orientation in a number of separate columns in
the cortex. Although it may be a bit surprising that the tree is
represented by separate columns in the cortex, it simply confirms a
property of our perceptual system that we mentioned earlier: The
cortical representation of a stimulus does not have to resemble the
stimulus; it just has to contain information that represents the
stimulus. The representation of the tree in the visual cortex is
contained in the firings of neurons in separate cortical columns. As
we'll soon discuss, at some point in the cortex, the information in
these separated columns must be combined to create our perception of the
tree. Before leaving our description of how objects are represented by
neural activity in the visual cortex, let's return to our scene (Figure
4.22). Each circle or ellipse in the scene

A

Figure 4.19 A location column that contains the full range of
orientation columns. A column such as this, which Hubel and Wiesel
called a hypercolumn, receives information about all possible
orientations that fall within a small area of the retina.

In addition to location and orientation columns, Hubel and Wiesel also
described ocular dominance columns. Most neurons respond better to one
eye than to the other. This preferential response to one eye is called
ocular dominance, and neurons with the same ocular dominance are
organized into ocular dominance columns in the cortex. This means that
each neuron encountered along a perpendicular electrode track responds
best to either the left eye or the right eye. There are two ocular
dominance columns within each hypercolumn, one for the left eye and one
for the right.

78

C

(a) 

© Bruce Goldstein

2

B

(b) 

Figure 4.20 (a) A scene from the Pennsylvania woods. (b) Focusing in on
part of a tree trunk. A, B, and C represent the parts of the tree trunk
that fall on receptive fields in three areas of the retina.

Chapter 4  The Visual Cortex and Beyond

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

the tree. As we will now see, signals from the striate cortex travel to
a number of other places in the cortex for further processing.

A A

B C

B

TEST YOuRSELF 4.1

C 90° orientation columns (a) Retina

(b) Cortex

Figure 4.21 (a) Receptive fields A, B, and C, located on the retina, for
the three sections of the tree trunk from Figure 4.20b. The neurons
associated with each of these receptive fields are in different location
columns. (b) Three location columns in the cortex. Neurons that fire to
the tree trunk's orientation are within the orange areas of the location
column.

represents an area that sends information to one location column.
Working together, these columns cover the entire visual field, an effect
called tiling. Just as a wall can be covered by adjacent tiles, the
visual field is served by adjacent (and often overlapping) location
columns (Nassi & Callaway, 2009). (Does this sound familiar? Remember
the football field analogy for ganglion cell receptive fields on page 55
of Chapter 3, in which each spectator was observing a small area of the
field. In that example, the spectators were tiling the football field.)
The idea that each part of a scene is represented by activity in many
location columns means that a scene containing many objects is
represented in the striate cortex by an amazingly complex pattern of
firing. Just imagine the process we described for the three small areas
on the tree trunk multiplied by hundreds or thousands. Of course, this
representation in the striate cortex is only the first step in
representing

1.  Describe the pathway from the retina to the brain. What does it mean
    when we say that the visual system has a contralateral organization?
2.  What function has been suggested for the LGN? How are LGN receptive
    fields similar to ganglion cell receptive fields?
3.  Describe the characteristics of simple, complex, and endstopped
    cells in the visual cortex. Why have these cells been called feature
    detectors?
4.  How has the psychophysical procedure of selective adaptation been
    used to demonstrate a link between feature detectors and the
    perception of orientation? Be sure you understand the rationale
    behind a selective adaptation experiment and also how we can draw
    conclusions about physiology from the results of this psychophysical
    procedure.
5.  How has the procedure of selective rearing been used to demonstrate
    a link between feature detectors and perception? Be sure you
    understand the concept of neural plasticity.
6.  How is the retina mapped onto the striate cortex? What is cortical
    magnification, and what function does it serve?
7.  Describe location columns and orientation columns. What do we mean
    when we say that location columns and orientation columns are
    "combined"? What is a hypercolumn?
8.  How do V1 neurons and columns underlie perception of a scene? Start
    by describing how a tree trunk is represented in the cortex and then
    expand your view to the whole forest scene.

Bruce Goldstein

4.4 Beyond the Visual Cortex

Figure 4.22 The yellow circles and ellipses superimposed on the forest
scene each represent an area that sends information to one location
column in the visual cortex. There are actually many more columns than
shown here, and they overlap, so that they cover the entire scene. The
way these location columns cover the entire scene is called tiling.

At this point, we have discussed how the visual signal travels from the
retina to the visual cortex, and how those V1 neurons represent the
basic elements or features of the visual scene (edges and lines). Now,
we will discuss what happens to these signals as they progress through
the visual system. After being processed in the striate cortex (V1), the
visual signal proceeds to other visual areas in the occipital lobe and
beyond--- areas conveniently known as V2, V3, V4, and V5 (Figure 4.23).
These areas collectively are often referred to as the extrastriate
cortex, since they are outside of the striate cortex. As we move from V1
to higher-level extrastriate areas, the receptive field sizes gradually
increase. V1 neurons respond to a very small area of the retina (which
corresponds to a very small area of the visual field); their receptive
fields, as we have seen, 4.4 Beyond the Visual Cortex

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

79

W he /h re ow

Inferotemporal cortex (IT)

V5 V4 V3 V2 What

V1

Figure 4.23 The hierarchy of cortical areas and pathways in the visual
system. The visual signal flows from striate cortex (area V1) at the
back of the brain to extrastriate cortex (areas V2--V5) through the what
(ventral) and where/how (dorsal) pathways.

are just large enough to encompass a line or an edge. V2 neuron
receptive fields are slightly larger, V3 even larger, and so on (Smith
et al., 2001). In this way, the representation of the visual scene
builds as we move up this hierarchy of extrastriate cortex areas, adding
more and more aspects of the visual scene such as corners, colors,
motion, and even entire shapes and objects. When the visual signal
leaves the occipital lobe, it continues through different "streams" or
pathways that serve different functions. Some of the first research on
these pathways was carried out by Leslie Ungerleider and Mortimer
Mishkin, who presented evidence for two streams serving different
functions that transmit information from the striate and extrastriate
cortex to other areas of the brain.

Ungerleider and Mishkin presented monkeys with two tasks: (1) an object
discrimination problem and (2) a landmark discrimination problem. In the
object discrimination problem, a monkey was shown one object, such as a
rectangular solid, and was then presented with a two-choice task like
the one shown in Figure 4.24a, which included the "target" object (the
rectangular solid) and another stimulus, such as the triangular solid.
If the monkey was able to discriminate between the two objects and thus
push aside the target object, it received the food reward that was
hidden in a well under the object. The landmark discrimination problem
is shown in Figure 4.24b. Here, the monkey's task was to remove the
cover of the food well that was closest to the "landmark"---in this
case, a tall cylinder. In the ablation part of the experiment, part of
the temporal lobe was removed in some monkeys. After ablation,
behavioral testing showed that the object discrimination problem was
very difficult for these monkeys. This result indicates that the pathway
that reaches the temporal lobes is responsible for determining an
object's identity. Ungerleider and Mishkin therefore called the pathway
leading from the striate cortex to the temporal lobe the what pathway
(Figure 4.23). Other monkeys had their parietal lobes removed, as in
Figure 4.24b, and they had difficulty solving the landmark
discrimination problem. This result indicates that the pathway that
leads to the parietal lobe is responsible for determining an object's
location. Ungerleider and Mishkin therefore called the

Streams for Information About What and Where Ungerleider and Mishkin
(1982) used a technique called ablation (also called lesioning) to
better understand the functional organization of the visual system.
Ablation refers to the destruction or removal of tissue in the nervous
system. METHOD

Area removed (temporal lobe)

(a) Object discrimination

Area removed (parietal lobe)

Brain Ablation

The goal of a brain ablation experiment is to determine the function of
a particular area of the brain. First, an animal's ability to carry out
a specific task is determined by behavioral testing. Most ablation
experiments have used monkeys because of the similarity of their visual
system to that of humans and because monkeys can be trained in ways that
enable researchers to determine perceptual capacities such as acuity,
color vision, depth perception, and object perception (Mishkin et al.,
1983). Once the animal's performance on a task has been measured, a
particular area of the brain is ablated (removed or destroyed), either
by surgery or by injecting a chemical that destroys tissue near the
place where it is injected. Ideally, one particular area is removed and
the rest of the brain remains intact. After ablation, the monkey is
retested to determine how performance has been affected by the ablation.

(b) Landmark discrimination

Figure 4.24 The two types of discrimination tasks used by Ungerleider
and Mishkin. (a) Object discrimination: Pick the correct shape.
Lesioning the temporal lobe (shaded area) makes this task difficult. (b)
Landmark discrimination: Pick the food well closer to the cylinder.
Lesioning the parietal lobe makes this task difficult. (From Mishkin et
al., 1983)

80

Chapter 4  The Visual Cortex and Beyond

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

pathway leading from the striate cortex to the parietal lobe the where
pathway (Figure 4.23). The what and where pathways are also called the
ventral pathway (what) and the dorsal pathway (where), because the lower
part of the brain, where the temporal lobe is located, is the ventral
part of the brain, and the upper part of the brain, where the parietal
lobe is located, is the dorsal part of the brain. The term dorsal refers
to the back or the upper surface of an organism; thus, the dorsal fin of
a shark or dolphin is the fin on the back that sticks out of the water.
Figure 4.25 shows that for upright, walking animals such as humans, the
dorsal part of the brain is the top of the brain. (Picture a person with
a dorsal fin sticking out of the top of his or her head!) Ventral is the
opposite of dorsal; hence it refers to the lower part of the brain. The
discovery of two pathways in the cortex---one for identifying objects
(what) and one for locating objects (where)---led some researchers to
look back at the retina and the lateral geniculate nucleus (LGN). Using
the techniques of both recording from neurons and ablation, they found
that properties of the ventral and dorsal streams are established by two
different types of ganglion cells in the retina, which transmit signals
to different layers of the LGN (Schiller et al., 1990). Thus, the
cortical ventral and dorsal streams can actually be traced back to the
retina and LGN. Although there is good evidence that the ventral and
dorsal pathways serve different functions, it is important to note that
(1) the pathways are not totally separated but have connections between
them and (2) signals flow not only "up" the pathway from the occipital
lobe toward the parietal and temporal lobes but "back" as well (Gilbert
& Li, 2013; Merigan & Maunsell, 1993; Ungerleider & Haxby, 1994). It
makes sense that there would be communication between the pathways
because in our everyday behavior we need to both identify and locate
objects, and we routinely coordinate these two activities every time we
identify something ("there's a pen") and notice where it is ("it's over
there, next to the computer"). Thus, there are two distinct pathways,
but some information is shared between them. The "backward" flow of
information, called feedback, provides information from higher centers
that can influence the signals flowing into the system (Gilbert & Li,
2013). This feedback is one of the mechanisms behind topdown processing,
introduced in Chapter 1 (p. 10). Dorsal for brain Ventral for brain

Dorsal for back

Figure 4.25 Dorsal refers to the back surface of an organism. In upright
standing animals such as humans, dorsal refers to the back of the body
and to the top of the head, as indicated by the arrows and the curved
dashed line. Ventral is the opposite of dorsal.

Streams for Information About What and How Although the idea of ventral
and dorsal streams has been generally accepted, David Milner and Melvyn
Goodale (1995; see also Goodale & Humphrey, 1998, 2001) have suggested
that the dorsal stream does more than just indicate where an object is.
Milner and Goodale propose that the dorsal stream is for taking action,
such as picking up an object. Taking this action would involve knowing
the location of the object, consistent with the idea of where, but it
goes beyond where to involve a physical interaction with the object.
Thus, reaching to pick up a pen involves information about the pen's
location plus information about how a person should move his or her hand
toward the pen. According to this idea, the dorsal stream provides
information about how to direct action with regard to a stimulus.
Evidence supporting the idea that the dorsal stream is involved in how
to direct action is provided by the discovery of neurons in the parietal
cortex that respond (1) when a monkey looks at an object and (2) when it
reaches toward the object (Sakata et al., 1992; also see Taira et al.,
1990). But the most dramatic evidence supporting the idea of a dorsal
how or action stream comes from neuropsychology---the study of the
behavioral effects of brain damage in humans (see Chapter 2, page 31).

METHOD

Double Dissociations in Neuropsychology

One of the basic principles of neuropsychology is that we can understand
the effects of brain damage by determining double dissociations, which
involve two people: In one person, damage to one area of the brain
causes function A to be absent while function B is present; in the other
person, damage to another area of the brain causes function B to be
absent while function A is present. Ungerleider and Mishkin's monkeys
provide an example of a double dissociation. The monkey with damage to
the temporal lobe was unable to discriminate objects (function A) but
had the ability to solve the landmark problem (function B). The monkey
with damage to the parietal lobe was unable to solve the landmark
problem (function B) but was able to discriminate objects (function A).
These two findings, taken together, are an example of a double
dissociation. The fact that object discrimination and the landmark task
can be disrupted separately and in opposite ways means that these two
functions operate independently of one another. An example of a double
dissociation in humans is provided by two hypothetical patients. Alice,
who has suffered damage to her temporal lobe, has difficulty naming
objects but has no trouble indicating where they are located (Table
4.2a). Bert, who has parietal lobe damage, has the opposite problem---he
can identify objects but can't tell exactly where they are located
(Table 4.2b). The cases of Alice and Bert, taken together, rep­ resent a
double dissociation and enable us to conclude that recognizing objects
and locating objects operate independently of each other.

4.4 Beyond the Visual Cortex

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

81

Table 4.2 A Double Dissociation Able to Name Objects?

Able to Determine Object's Location?

(a) ALICE: Temporal lobe damage (ventral stream)

NO

YES

(b) BERT: Parietal lobe damage (dorsal stream)

YES

NO

© Cengage Learning 2014

The Behavior of Patient D.F. Milner and Goodale (1995) used the method
of determining double dissociations to study D.F., a 34-year-old woman
who suffered damage to her ventral pathway from carbon monoxide
poisoning caused by a gas leak in her home. One result of her brain
damage was that D.F. was not able to match the orientation of a card
held in her hand to different orientations of a slot. This is shown in
the left circle in Figure 4.26a, which indicates D.F.'s attempts to
match the orientation of a vertical slot. Perfect matching performance
would be indicated by a vertical line for each trial, but D.F.'s
responses are widely scattered across many different orientations. The
right circle shows the accurate performance of the normal controls.
Because D.F. had trouble orienting a card to match the orientation of
the slot, it would seem reasonable that she would also have trouble
placing the card through the slot, because to do this she would have to
turn the card so that it was lined up with the slot. But when D.F. was
asked to "mail" the card through the slot, she could do it! Even though
D.F. could not turn the card to visually match the slot's orientation,
once she started moving the card toward the slot, she was able to rotate

D.F. (a) Static orientation matching

Control

D.F. (b) Active "posting"

Control

Figure 4.26 Performance of patient D.F. and a person without brain
damage on two tasks: (a) judging the orientation of a slot and (b)
placing a card through the slot. Vertical lines indicate perfect
matching performance. (Milner & Goodale, 1995) 82

it to match the orientation of the slot (Figure 4.26b). Thus, D.F.
performed poorly in the static orientation-matching task but did well as
soon as action was involved (Murphy et al., 1996). Milner and Goodale
interpreted D.F.'s behavior as showing that there is one mechanism for
judging orientation and another for coordinating vision and action
(Goodale, 2014). These results for D.F. are part of a double
dissociation because there are other patients whose symptoms are the
opposite of D.F.'s. These people can judge visual orientation, but they
can't accomplish the task that combines vision and action. As we would
expect, whereas D.F.'s ventral stream is damaged, these other people
have damage to their dorsal streams. Based on these results, Milner and
Goodale suggested that the ventral pathway should still be called the
what pathway, as Ungerleider and Mishkin suggested, but that a better
description of the dorsal pathway would be the how pathway, or the
action pathway, because it determines how a person carries out an
action. As sometimes occurs in science, not everyone uses the same
terms. Thus, some researchers call the dorsal stream the where pathway,
and some call it the how or action pathway.

The Behavior of People Without Brain Damage In our normal daily
behavior, we aren't aware of two visual processing streams, one for what
and the other for how, because they work together seamlessly as we
perceive objects and take actions toward them. Cases like that of D.F.,
in which one stream is damaged, reveal the existence of these two
streams. But what about people without damaged brains? Psychophysical
experiments that measure how people perceive and react to visual
illusions have demonstrated the dissociation between perception and
action that was evident for D.F. Figure 4.27a shows the stimulus used by
Tzvi Ganel and coworkers (2008) in an experiment designed to demonstrate
a separation of perception and action in non-brain-damaged participants.
This stimulus creates a visual illusion: Line 1 is actually longer than
line 2 (see Figure 4.27b), but line 2 appears longer. Ganel and
coworkers presented participants with two tasks: (1) a length estimation
task in which they were asked to indicate how they perceived the lines'
length by spreading their thumb and index finger, as shown in Figure
4.27c; and (2) a grasping task in which they were asked to reach toward
the lines and grasp each line by its ends. Sensors on the participants'
fingers measured the separation between the fingers as the participants
grasped the lines. These two tasks were chosen because they depend on
different processing streams. The length estimation task involves the
ventral or what stream. The grasping task involves the dorsal or
where/how stream. The results of this experiment, shown in Figure 4.27d,
indicate that in the length estimation task, participants judged line 1
(the longer line) as looking shorter than line 2, but in the grasping
task, they separated their fingers farther apart for line 1 to match its
longer length. Thus, the illusion works for perception (the length
estimation task), but not for action (the grasping task). These results
support the idea that perception and action are served by different
mechanisms. An idea about functional organization that originated with
observations of

Chapter 4  The Visual Cortex and Beyond

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

1

Distance between fingers (mm)

(a) 

2

12

(b) 
(c) 

68 66 64 62 60 58 Long line (1)

56

Short line (2)

54 Length Estimations

Grasping

(d) 

Figure 4.27 (a) The size illusion used by Ganel and coworkers (2008) in
which line 2 looks longer than line 1. The numbers were not present in
the display seen by the participants. (b) The two vertical lines from
(a), showing that line 2 is actually shorter than line 1. (c)
Participants in the experiment adjusted the space between their fingers
either to estimate the length of the lines (length estimation task) or
to reach toward the lines to grasp them (grasping task). The distance
between the fingers is measured by sensors on the fingers. (d) Results
of the length estimation and grasping tasks in the Ganel et
al. experiment. The length estimation task indicates the illusion,
because the shorter line (line 2) was judged to be longer. In the
grasping task, participants separated their fingers more for the longer
line (line 1), which was consistent with the physical lengths of the
lines. (From Ganel et al., 2008)

patients with brain damage is therefore supported by the performance of
participants without brain damage.

4.5 Higher-Level Neurons At this point in the chapter, we have seen how
the visual signal travels from the retina to the visual cortex, then on
to extrastriate areas, and the what and where/how processing streams
(keeping in mind that signals flow both "up" and "down" these pathways).
Now we'll discuss how information is represented at higher levels of the
visual system by considering the responses of individual neurons within
these areas.

Responses of Neurons in Inferotemporal Cortex We focused on the firing
of individual neurons in this chapter when we discussed the feature
detectors in the visual cortex and how they respond to basic elements of
a visual scene. Now,

let's again consider neural responses, but at a higher level---in the
temporal lobe. An area in the temporal lobe that has been the focus of
much research is the inferotemporal (IT) cortex (Figure 4.23). Recall
that earlier in this chapter, we mentioned how receptive fields of
neurons in the visual system become larger as we move to higher levels,
like from striate to extrastriate cortex. As it turns out, this increase
in receptive field size continues through the what stream so that
neurons at the apex of this stream in IT cortex have the largest
receptive fields---large enough to encompass whole objects in one's
visual field. So, it would make sense that instead of responding to
simple features like lines or edges, like V1 neurons, IT neurons would
respond to more complex objects that occupy a larger portion of the
visual field. This knowledge comes from early experiments conducted by
Charles Gross and coworkers (1972), who recorded from single neurons in
the monkey's IT cortex. In these experiments, Gross's research team
presented a variety of stimuli to anesthetized monkeys. Using the
projection screen procedure, they presented lines, squares, and circles.
Some stimuli were light, 4.5 Higher-Level Neurons

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

83

Bruce Goldstein

Firing rate

20

10

0 Faces

(Based on data from Rolls & Tovee, 1995)

Clearly, Charles Gross was onto something with his discovery of IT
neuron specificity---an idea which was later supported by these studies
finding face-selective neurons grouped together in IT cortex. As it
turns out, evidence for face selectivity has been observed in the human
brain as well (Kanwisher et al., 1997; McCarthy et al., 1997). We'll
discuss how the human brain responds to faces and other complex objects
in the next chapter. We can take this idea of neural activity for
complex objects a step further by considering that the processes we have
been describing not only create perceptions, but they also provide
information that is stored in our memory so we can remember perceptual
0.6 0.4 0.2 0 --0.2

1

1

1

2

3

3

4

4

5

84

16

Fa c

es

6

Figure 4.28 Some of the shapes used by Gross and coworkers (1972) to
study the responses of neurons in the monkey's inferotemporal cortex.
The shapes are arranged in order of their ability to cause the neuron to
fire, from none (1) to little (2 and 3) to maximum (6). (From Gross et
al., 1972)

Nonfaces

Figure 4.29 Size of response of a neuron in the monkey's IT cortex that
responds to face stimuli but not to nonface stimuli.

Mean response

and some dark. The dark stimuli were created by placing cardboard
cutouts against the transparent projection screen. The discovery that
neurons in the IT cortex respond to complex stimuli came a few days into
one of their experiments, when they found a neuron that refused to
respond to any of the standard stimuli like oriented lines or circles or
squares. Nothing worked, until one of the experimenters pointed at
something in the room, casting a shadow of his hand on the screen. When
this hand shadow caused a burst of firing, the experimenters knew they
were on to something and began testing the neuron to see what kinds of
stimuli caused it to respond. They used a variety of stimuli, including
cutouts of a monkey's hand. After a great deal of testing, they
determined that this neuron responded to a handlike shape with fingers
pointing up (Figure 4.28) (Rocha-Miranda, 2011; also see Gross, 2002,
2008). After expanding the types of stimuli presented, they also found
some neurons that responded best to faces. Finding neurons that
responded to real-life, complex objects like hands and faces was a
revolutionary result. Apparently, neural processing that occurred beyond
the initial receiving areas studied by Hubel and Wiesel had created
neurons that responded best to very specific types of stimuli. But
sometimes revolutionary results aren't accepted immediately, and Gross's
results were largely ignored when they were published in 1969 and 1972
(Gross et al., 1969, 1972). Finally, in the 1980s, other experimenters
began recording from neurons in the IT cortex of the monkey that
responded to faces and other complex objects (Perrett et al., 1982;
Rolls, 1981). In 1995, Edmund Rolls and Martin Tovee found many neurons
in the IT cortex in monkeys that responded best to faces, further
confirming Gross's initial findings that IT neurons respond to specific
types of complex stimuli. Figure 4.29 shows the results for a neuron
that responded to faces but hardly at all to other types of stimuli.
What is particularly significant about such "face neurons" is that, as
it turns out, there are areas in the monkey temporal lobe that are
particularly rich in these neurons. Doris Tsao and coworkers (2006)
presented 96 images of faces, bodies, fruits, gadgets, hands, and
scrambled patterns to two monkeys while recording from cortical neurons
inside this "face area." They classified neurons as "face selective" if
they responded at least twice as strongly to faces as to nonfaces. Using
this criterion, they found that 97 percent of the cells were face
selective. The high level of face selectivity within this area is
illustrated in Figure 4.30, which shows the average response for both
monkeys to each of the 96 objects. The response to the 16 faces, on the
left, is far greater than the response to any of the other objects.

32

Bo

di

es

48

Fr ui

ts

64

G

ad

ge

ts

80 Ha nd s

96

Sc

ra m

bl

ed

Figure 4.30 Results of the Tsao et al. (2006) experiment in which
activity of neurons in the monkey's temporal lobe was recorded in
response to faces, other objects, and a scrambled stimulus. (From Tsao
et al., 2006)

Chapter 4  The Visual Cortex and Beyond

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

experiences later. This link between perception and memory has been
studied in a number of experiments that have measured responding in
single neurons in the human hippocampus, an area associated with forming
and storing memories.

Where Perception Meets Memory Some of the signals leaving the IT cortex
reach structures in the medial temporal lobe (MTL), such as the
parahippocampal cortex, the entorhinal cortex, and the hippocampus
(Figure 4.31). These MTL structures are extremely important for memory.
The classic demonstration of the importance of one of the structures in
the MTL, the hippocampus, is the case of H.M., who had his hippocampus
on both sides of his brain removed in an attempt to eliminate epileptic
seizures that had not responded to other treatments (Scoville & Milner,
1957). The operation eliminated H.M.'s seizures, but it also eliminated
his ability to store experiences in his memory. Thus, when H.M.
experienced something, such as a visit from his doctor, he was unable to
remember the experience, so the next time the doctor appeared, H.M. had
no memory of having seen him. H.M.'s unfortunate situation occurred
because in 1953, the surgeons did not realize that the hippocampus is
crucial for the formation of long-term memories. Once they realized the
devastating effects of removing the hippocampus on both sides of the
brain, H.M.'s operation was never repeated. Parahippocampal cortex

Amygdala

Entorhinal cortex

Hippocampus

Figure 4.31 Location of the hippocampus, entorhinal cortex,
parahippocampal cortex, and amygdala on the underside of the brain.

We saw evidence of the link between the hippocampus and vision in
Chapter 2 when we discussed specificity coding and the research by
Quiroga and coworkers (2005, 2008). Recall that these studies showed
that there are neurons in the hippocampus that respond to specific
stimuli, like the Sydney Opera House or Steve Carell (see Figure 2.11,
page 28). As it turns out, these hippocampal and MTL neurons respond not
only to the visual perception of specific objects or concepts, but also
the memories of those concepts. Evidence of this link between MTL
neurons that respond to visual stimuli and memories comes from an
experiment by Hagan Gelbard-Sagiv and coworkers (2008). These
researchers had epilepsy patients view a series of 5- to 10-second video
clips a number of times while recording from neurons in the MTL. The
clips showed famous people, landmarks, and nonfamous people and animals
engaged in various actions. As the person was viewing the clips, some
neurons responded better to certain clips. For example, a neuron in one
of the patients responded best to a clip from The Simpsons TV program.
The firing to specific video clips is similar to what Quiroga found for
viewing still pictures. However, this experiment went a step further by
asking the patients to think back to any of the film clips they had seen
while the experimenter continued to record from the MTL neurons. One
result is shown in Figure 4.32, which indicates the response of the
neuron that fired to The Simpsons. The patient's description of what he
was remembering is shown at the bottom of the figure. First the patient
remembered "something about New York," then "the Hollywood sign." The
neuron responds weakly or not at all to those two memories. However,
remembering The Simpsons causes a large response, which continues as the
person continues remembering the episode (indicated by the laughter).
Results such as this support the idea that the neurons in the MTL that
respond to perceiving specific objects or events may also be involved in
remembering these objects and events. (Also see Cerf and coworkers
\[2010\] for more on how thoughts can influence the firing of
higher-level neurons.) Throughout this chapter, we have seen how
individual neurons at various stages respond to visual input, and even
memories of that visual input. We will continue to discuss more specific
aspects of vision in upcoming chapters, including the functions of other
brain areas within the processing streams we've introduced here.

Sound Amp.

Firing rate

15 10 5 0

Something about New York

The Hollywood sign

ahhmmm

laughing

ahhmmm

The Simpsons

Figure 4.32 Activity of a neuron in the MTL of an epilepsy patient
remembering the things indicated below the record. A response occurs
when the person remembered The Simpsons TV program. Earlier, this neuron
had been shown to respond to viewing a video clip of The Simpsons. (From
Gelbard-Sagiv et al., 2008) 4.5 Higher-Level Neurons

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

85

45

"Flexible" Receptive Fields 0

45

0 (b) 45

0

spikes/sec

86

(a) 

spikes/sec

In Chapter 3, we introduced the idea of a neuron's receptive field by
defining the receptive field as the area of the retina that, when
stimulated, influences the firing of the neuron. Later, as we worked our
way to higher levels of the visual system, the receptive field was still
the area that affected firing, but the stimulus required became more
specific---oriented lines, geometrical shapes, and faces. Nowhere in our
discussion of receptive fields did we say that the area defining the
receptive field can change. Receptive fields are, according to what we
have described so far, static, wired-in properties of neurons. However,
one of the themes of this book---and of a great deal of research in
perception---is that because we exist in an ever-changing environment,
because we are often moving, experiencing new situations, and creating
our own goals and expectations, we need a perceptual system that is
flexible and adapts to our needs and to the current situation. This
section---in which we introduce the idea that the visual system is
flexible and that neurons can change depending on changing
conditions---is a "preview of coming events," because the idea that
sensory systems are flexible will recur throughout the book. An example
of how a neuron's response can be affected by what is happening outside
the neuron's receptive field is illustrated by the results of an
experiment by Mitesh Kapadia and coworkers (2000), in which they
recorded from neurons in a monkey's visual cortex. Figure 4.33a
indicates the response of a neuron to a vertical bar located inside the
neuron's receptive field, which is indicated by the bar inside the
square. Figure 4.33b shows that two vertical bars located outside the
neuron's receptive field cause little change in the neuron's response.
But Figure 4.33c shows what happens when the "outside the receptive
field" bars are presented along with the "inside the field" bar. There
is a large increase in firing! Thus, although our definition of a
neuron's receptive field as the area of retina which, when stimulated,
influences the neuron's firing, is still correct, we can now see that
the response to stimulation within the receptive field can be affected
by what's happening outside the receptive field. The effect of
stimulating outside the receptive field is called contextual modulation.
The large response that occurs when the three lines are presented
together may be related to an example of a perceptual phenomenon called
perceptual organization, illustrated in Figure 4.33d, which shows how
lines of the same orientation are perceived as a group that stands out
from the surrounding clutter. We will consider perceptual organization
further in Chapter 5, "Perceiving Objects and Scenes." In Chapter 6,
"Visual Attention," we will consider the many effects of paying
attention. When we pay attention to something, we become more aware of
it, we can respond more

spikes/sec

SOMETHING TO CONSIDER:

(c) 
(d) 

Figure 4.33 A neuron in the monkey's temporal lobe responds (a) with a
small response to a vertical bar flashed inside the receptive field
(indicated by the square); (b) with little or no response to two
vertical bars presented outside the receptive field; (c) with a large
response when the three bars are presented together. This enhanced
response caused by stimuli presented outside the receptive field is
called contextual modulation. (d) A pattern in which the three aligned
lines stand out. (a--c from Kapadia et al., 2000)

rapidly to it, and we may even perceive it differently. And as we will
see, attention can even shift the location of a neuron's receptive field
(Figure 6.23). This shifting of the receptive field is an amazing result
because it means that attention is changing the organization of part of
the visual system. Receptive fields, it turns out, aren't fixed in place
but can change in response to where someone is paying attention. This
concentrates neural processing power at the place that is important to
the person at that moment. As we continue exploring how the nervous
system creates our perceptions, we will encounter other examples of how
the flexibility of our nervous system helps us function within our
ever-changing environment.

Chapter 4  The Visual Cortex and Beyond

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

TEST YOuRSELF 4.2 1. What is the extrastriate cortex? How do receptive
fields of extrastriate neurons differ from receptive fields of striate
neurons? 2. How has ablation been used to demonstrate the existence of
the ventral and dorsal processing streams? What is the function of these
streams? 3. How has double dissociation been used to show that one of
the functions of the dorsal stream is to process information about
coordinating vision and action? How do the results of a behavioral
experiment support the idea of two primary streams in people without
brain damage?

4.  Describe Gross's experiments on neurons in the inferotemporal cortex
    of the monkey. Why do you think his results were initially ignored?
5.  Describe the connection between vision and memory, as illustrated by
    experiments that recorded from neurons in the MTL and hippocampus.
6.  Describe the two experiments that demonstrated the "flexibility" of
    receptive fields.

THINK ABOUT IT 1. Ralph is hiking along a trail in the woods. The trail
is bumpy in places, and Ralph has to avoid tripping on occasional rocks,
tree roots, or ruts in the trail. Nonetheless, he is able to walk along
the trail without constantly looking down to see exactly where he is
placing his feet. That's a good thing because Ralph enjoys looking out
at the woods to see whether he can spot interesting birds or animals.
How can you relate this description of Ralph's behavior to the operation
of the dorsal and ventral streams in the visual system? (p. 80)

2.  Cell A responds best to vertical lines moving to the right. Cell B
    responds best to 45-degree lines moving to the right. Both of these
    cells have an excitatory synapse with cell C. How will cell C fire
    to vertical lines? To 45-degree lines? What if the synapse between B
    and C is inhibitory?
3.  We have seen that the neural firing associated with an object in the
    environment does not necessarily look like, or resemble, the object.
    Can you think of situations that you encounter in everyday life in
    which objects or ideas are represented by things that do not exactly
    resemble those objects or ideas?

KEY TERMS Ablation (p. 80) Action pathway (p. 82) Area V1 (p. 69)
Complex cells (p. 70) Contextual modulation (p. 86) Contralateral
(p. 68) Contrast threshold (p. 72) Cortical magnification (p. 75)
Cortical magnification factor (p. 75) Dorsal pathway (p. 81) Double
dissociations (p. 81) End-stopped cell (p. 71) Experience-dependent
plasticity (p. 74)

Extrastriate cortex (p. 79) Feature detectors (p. 71) Hippocampus
(p. 85) How pathway (p. 82) Hypercolumn (p. 78) Inferotemporal (IT)
cortex (p. 83) Landmark discrimination problem (p. 80) Lateral
geniculate nucleus (LGN) (p. 68) Location columns (p. 77) Neural
plasticity (p. 74) Object discrimination problem (p. 80)

Optic chiasm (p. 68) Orientation columns (p. 77) Orientation tuning
curve (p. 70) Retinotopic map (p. 75) Selective adaptation (p. 72)
Selective rearing (p. 74) Simple cortical cell (p. 70) Striate cortex
(p. 69) Superior colliculus (p. 68) Tiling (p. 79) Ventral pathway
(p. 81) Visual receiving area (p. 69) What pathway (p. 80) Where pathway
(p. 81)

Key Terms

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

87

Wherever you look, whether it's walking across campus, sitting in your
room, or looking at this hillside town of Manarola, Italy, you perceive
small details, individual objects, and larger scenes created from these
objects. Although you usually achieve these perceptions with ease, they
are created by extremely complex, hidden, processes. Bruce Goldstein

Learning Objectives After studying this chapter, you will be able to ...
■■ Discuss why object perception is challenging for both humans

and computers. ■■ Explain Gestalt psychology and the laws of perceptual
organization. ■■ Define figure--ground segregation and identify the
properties

that determine which area is perceived as figure. ■■ Describe the
recognition by components theory and how it

­accounts for our ability to recognize objects from different ­viewpoints.

■■ Explain the role of past experience, inference, and prediction

in perception. ■■ Describe experiments that show how the brain responds
to

faces, bodies, and scenes, and what is meant by "neural mind reading."
■■ Analyze the evidence for and against the idea that faces are

"special." ■■ Discuss the development of face recognition in infants.

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

C hapter 5

Perceiving Objects and Scenes Chapter Contents DEMONSTRATION: Perceptual
Puzzles

TEST YOURSELF 5.1

in a Scene

5.3 Recognition by Components

5.1 Why Is It So Difficult to Design a Perceiving Machine?

5.4 Perceiving Scenes and Objects in Scenes

The Stimulus on the Receptors Is Ambiguous Objects Can Be Hidden or
Blurred Objects Look Different From Different Viewpoints

5.2 Perceptual Organization The Gestalt Approach to Perceptual Grouping
Gestalt Principles of Perceptual Organization Perceptual Segregation

Perceiving the Gist of a Scene METHOD: Using a Mask to Achieve

Brief Stimulus Presentations

5.5 Connecting Neural Activity and Object/Scene Perception Brain
Responses to Objects and Faces Brain Responses to Scenes The
Relationship Between Perception and Brain Activity Neural Mind Reading
METHOD: Neural Mind Reading

Regularities in the Environment: Information for Perceiving

SOMETHING TO CONSIDER: The Puzzle

DEMONSTRATION: Visualizing Scenes

and Objects

of Faces

DEVELOPMENTAL DIMENSION: Infant

The Role of Inference in Perception

Face Perception

TEST YOURSELF 5.2

TEST YOURSELF 5.3

Some Questions We Will Consider: ■■ Why are even the most sophisticated
computers unable to

match a person's ability to perceive objects? (p. 91) ■■ Why do some
perceptual psychologists say "The whole

THINK ABOUT IT

All of Roger's perceptions come naturally to him and require little
effort. But when we look closely at the scene, it ­becomes apparent that
the scene poses many "puzzles." The following demonstration points out a
few of them.

differs from the sum of its parts"? (p. 96) ■■ Can we tell what people
are perceiving by monitoring

their brain activity? (p. 110) ■■ Are faces special compared to other
objects like cars or

houses? (p. 116) ■■ How do infants perceive faces? (p. 118)

S

itting in the upper deck in PNC Park, home of the Pittsburgh Pirates,
Roger looks out over the city (Figure 5.1). He sees a group of about 10
buildings on the left and can easily tell one building from another.
Looking straight ahead, he sees a small building in front of a larger
one, and has no trouble telling that they are two separate buildings.
Looking down toward the river, he notices a horizontal yellow band above
the right field bleachers. It is obvious to him that this is not part of
the ballpark but is located across the river.

DEMONSTRATION

Perceptual Puzzles in a Scene

The questions below refer to the areas labeled in Figure 5.1. Your task
is to answer each question and indicate the reasoning behind each
answer:    What is the dark area at A?    Are the surfaces at B and C
facing in the same or different directions?    Are areas B and C on the
same or on different buildings?    Does the building at D extend behind
the one at A?

Although it may have been easy to answer the questions, it was probably
somewhat more challenging to indicate what

89

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

B

C

D

Bruce Goldstein

A

Figure 5.1 It is easy to tell that there are a number of different
buildings on the left and that straight ahead there is a low rectangular
building in front of a taller building. It is also possible to tell that
the horizontal yellow band above the bleachers is across the river.
These perceptions are easy for humans but would be quite difficult for a
computer vision system. The letters on the left indicate areas referred
to in the Demonstration on page 89.

your "reasoning" was. For example, how did you know the dark area at A
is a shadow? It could be a dark-colored building that is in front of a
light-colored building. Or on what basis might you have decided that
building D extends behind building A? It could, after all, simply end
right where A begins. We could ask similar questions about everything in
this scene because, as we will see, a particular pattern of shapes can
be created by a large number of objects. One of the messages of this
chapter is that we need to go beyond the pattern of illumination that a
scene creates on the retina to determine what is "out there." One way to
appreciate the importance of this "going beyond" process is to consider
how difficult it has been to program even the most powerful computers to
accomplish perceptual tasks that humans achieve with ease. We saw an
example of computer errors in Chapter 1 (p. 4) when we discussed a
recent study showing how computers, when learning how to identify
objects in a scene, sometimes

90

make errors that humans wouldn't make, like mistaking a toothbrush for a
baseball bat (see Figure 1.2). In that study, the computer was
programmed to generate descriptions of a scene based on the objects that
it detected in the image (Karpathy & Fei-Fei, 2015). To create the
description, "a young boy is holding a baseball bat" the computer first
had to detect the objects in the image and then match those objects to
existing, stored representations of what those objects are---a process
known as object recognition. In this case, the computer recognized the
objects as (1) a boy and (2) a baseball bat, and then created a
description of the scene. Other computer vision systems have been
designed to learn how to recognize objects and determine not a
description of a scene, but rather, the precise locations of objects in
that scene. Figure 5.2 shows how computers can do this by placing boxes
around the recognized objects (Redmon et al., 2016, 2018). This object
recognition and localization can occur, with little delay, in real-time
(Boyla et al., 2019).

Chapter 5  Perceiving Objects and Scenes

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

(a)

(b) 

Figure 5.2 Examples of output from a computer system designed to detect
and recognize objects in images. The labels and boxes in these images
were created by the computer program. (a) The computer correctly
identified the boat and the person. (b) The computer correctly
identified the cars but calls the person an "aeroplane." (From Redmon et
al., 2016)

This type of technology is what underlies developments like autonomous
vehicles, which require fast and precise identification of objects in
order to smoothly navigate the environment. Think of all the objects
that such a vehicle must quickly and accurately detect and recognize
while driving down the road---pedestrians, cyclists, potholes, and
curbs, as well as road markings like lane dividers. This technology is
extremely impressive, and is probably in your pocket, as cellphones rely
on object recognition all the time, whether it be learning to recognize
your face across different angles and lighting conditions in order to
unlock your device, or an app that identifies a specific type of plant
in a picture that you took. The field of computer vision has clearly
come a long way, especially in the past 10 or so years. Just from 2012
to 2017, average error rates of these computer vision systems dropped
from 16 percent to a mere 2 percent (Liu et al., 2019), meaning that
they were wrong only 2 percent of the time---a number that is likely to
have dropped even lower from the time this book was written to when you
are reading it. Computers are becoming so accurate, in fact, that in
some situations, their object detection performance sometimes matches or
even exceeds that of humans (Geirhos et al., 2018). An exciting (and
perhaps scary) prospect! And yet, even with all of these advancements,
these computer systems still fail to fully replicate the intricacies of
human vision. Where they often fall short is in identifying objects
under degraded conditions---like when an image is blurry---or in
uncommon or unexpected situations. An example of one such error arising
from an uncommon situation is shown in the image of the car chase in the
bottom row of Figure 5.2b (Redmon et al., 2016). We as humans can
clearly identify the object in the air as a person leaping from car to
car, but since it's not common for a person to be flying through the
air, and also due to the shape of the image, the computer program
misidentified that object as an airplane.

What's interesting is that while object and scene perception is
difficult for computers, it is easy for humans---so easy, in fact, that
we often don't even have to think about it ("Of course that's a person
leaping through the air and not an airplane!"). This is yet another
example of how, although we're getting closer, we haven't yet created a
"perceiving machine" that accounts for all of the complexities of human
perception.

5.1 Why Is It So Difficult to Design a Perceiving Machine? We will now
describe a few of the problems involved in designing a "perceiving
machine." Remember that the point of these problems is that although
they pose difficulties for computers, humans solve them easily.

The Stimulus on the Receptors Is Ambiguous When you look at a page of a
book, the image cast by the borders of the page on your retina is
ambiguous. It may seem strange to say that, because (1) the rectangular
shape of the page is obvious, and (2) once we know the page's shape and
its distance from the eye, determining its image on the retina is a
simple geometry problem, which, as shown in Figure 5.3, can be solved by
extending "rays" from the red corners of the page into the eye. But the
perceptual system is not concerned with determining an object's image on
the retina. It starts with the image on the retina, and its job is to
determine the object "out there" that created the image. The task of
determining the object

5.1 Why Is It So Difficult to Design a Perceiving Machine?

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

91

Figure 5.3 The projection of the book (red object) onto the retina can
be determined by extending rays (solid lines) from the corners of the
book into the eye. The principle behind the inverse projection problem
is illustrated by extending rays out from the eye past the book (dashed
lines). When we do this, we can see that the image created by the book
can be created by an infinite number of objects, among them the tilted
trapezoid and the large rectangle shown here. This is why we say that
the image on the retina is ambiguous.

Image on retina

responsible for a particular image on the retina is called the inverse
projection problem, because it involves starting with the retinal image
and extending rays out from the eye. When we do this, as shown by
extending the lines in Figure 5.3, we see that the retinal image created
by the rectangular page could have been created by a number of other
objects, including a tilted trapezoid, a much larger rectangle, and an
infinite number of other objects located at different distances. When we
consider that the flat two-dimensional image on the retina can be
created by many different objects in the environment, it is easy to see
why we say that the image on the retina is ambiguous. Artists have taken
advantage of the fact that two-­ dimensional projections, like the image
on the retina, can be created by many different objects, to create
interesting

Objects that create the same image on the retina

"art constructions." Consider, for example, Figure 5.4, which shows an
art installation by Shigeo Fukuda, in which a spotlight shining on a
stack of bottles and glassware casts a two-dimensional shadow on the
wall that looks like a silhouette of a woman with an umbrella. This
shadow is a twodimensional projection of the stack of bottles that
occurs when the light casting the shadow is placed in just the right
location. Similarly, it is possible that the two-dimensional image on
the retina may not accurately reflect what is "out there" in the
environment. The ambiguity of the image on the retina is also
illustrated by Figure 5.5a, which, when viewed from one specific
location, creates a circular image on the retina and appears to be a
circle of rocks. However, moving to another viewpoint

Estate of Shigeo Fukuda

Figure 5.4 "Bonjour Madamoiselle" art piece by Shigeo Fukuda, showing
how the image projected onto a surface (the shadow on the wall that
looks like a woman) doesn't always accurately depict what is out there
in the environment (a precarious stack of bottles and glassware).

92

Chapter 5  Perceiving Objects and Scenes

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Thomas Macaulay

(a) 
(b) 

Figure 5.5 An environmental sculpture by Thomas Macaulay. (a) When
viewed from the exact right vantage point (the second-floor balcony of
the Blackhawk Mountain School of Art, Black Hawk, Colorado), the stones
appear to be arranged in a circle. (b) Viewing the stones from the
ground floor reveals a truer indication of their configuration.
(Courtesy of Thomas Macaulay, Blackhawk Mountain School of Art,
Blackhawk, CO)

reveals that the rocks aren't arranged in a circle after all (­Figure
5.5b). Thus, just as a rectangular image on the retina can be created by
trapezoids and other nonrectangular objects, a circular image on the
retina can be created by objects that aren't circular. The art pieces
depicted in Figures 5.4 and 5.5 are designed to fool us by relying on
viewing conditions that create images that don't correspond to the
actual object. Most of the time, erroneous perceptions like this don't
occur; the visual system solves the inverse projection problem and
determines which object out of all the possible objects is responsible
for a particular image on the retina. However, as easy as this is for
the human perceptual system, solving the inverse projection problem
poses serious challenges to computer vision systems.

Objects Can Be Hidden or Blurred Sometimes objects are hidden or
blurred. For example, look for the pencil and eyeglasses in Figure 5.6
before reading further. Although it might take a little searching,
people can find the pencil in the foreground and the glasses frame
sticking out from behind the computer, next to the picture, even though
only a small portion of these objects is visible. People can also easily
identify the book, scissors, and paper, even though they are partially
hidden by other objects. This problem of hidden objects occurs anytime
one object obscures---or "occludes"---part of another object. This
occurs frequently in the environment, but people easily understand that
the part of an object that is covered continues to exist,

Bruce Goldstein

Figure 5.6 A portion of the mess on the author's desk. Can you locate
the hidden pencil (easy) and the author's glasses (hard)?

5.1 Why Is It So Difficult to Design a Perceiving Machine?

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

93

Figure 5.7 Who are these people? See page 120 for the answers. (bukley/
Shutterstock.com; Featureflash/Shutterstock.com; dpa picture alliance
archive/Alamy Stock Photo; Peter Muhly/Alamy Stock Photo; s_bukley/
Shutterstock.com; Joe Seer/Shutterstock.com; DFree/Shutterstock.com)

and they are able to use their knowledge of the environment to determine
what is likely to be present. People are also able to recognize objects
that are not in sharp focus, such as the faces in Figure 5.7. See how
many of these people you can identify, and then consult the answers on
page 120. Despite the degraded nature of these images, people can often
identify most of them, whereas computers still perform poorly on this
type of task (Li et al., 2018).

Objects Look Different From Different Viewpoints Another problem facing
any perceiving machine is that objects are often viewed from different
angles. This means that the images of objects are continually changing,
depending on the angle from which they are viewed. Thus, although humans
continue to perceive the object in Figure 5.8 as the same chair viewed
from different angles, this isn't so obvious to a computer. The ability
to recognize an object seen from different viewpoints is called
viewpoint invariance---a task that is difficult for computers. The
difficulties facing any perceiving machine illustrate that the process
of perception is more complex than it seems (something you already knew
from the perceptual process in Figure 1.1, page 4). But how do humans
overcome these complexities? We begin answering this question by
considering perceptual organization.

5.2 Perceptual Organization Perceptual organization is the process by
which elements in a person's visual field become perceptually grouped
and segregated to create a perception. During this process, incoming

stimulation is organized into coherent units such as objects. The
process of perceptual organization involves two components: grouping and
segregation (Figure 5.9; Peterson & Kimchi, 2013). Grouping is the
process by which elements in a visual scene are "put together" into
coherent units or objects. Thus, when Roger sees each building in
Pittsburgh as an individual unit, he has grouped the visual elements in
the scene to create each building. If you can perceive the Dalmatian dog
in Figure 5.10, you have perceptually grouped some of the dark areas to
form a Dalmatian, with the other dark areas being seen as shadows on the
ground. The process of grouping works in conjunction with segregation,
which is the process of separating one area or object from another.
Thus, seeing two buildings in Figure 5.9 as separate from one another,
with borders indicating where one building ends and the other begins,
involves segregation.

The Gestalt Approach to Perceptual Grouping What causes some elements to
become grouped so they are part of one object? Answers to this question
were provided in the early 1900s by the Gestalt psychologists---where
Gestalt, roughly translated, means configuration. "How," asked the
Gestalt psychologists, "are configurations formed from smaller
elements?" We can understand the Gestalt approach by first considering
an approach that came before Gestalt psychology, called structuralism,
which was proposed by Wilhelm Wundt, who established the first
laboratory of scientific psychology at the University of Leipzig in
1879. Structuralism distinguished between sensations---elementary
processes that occur in response to stimulation of the senses---and
perceptions, more complex conscious experiences such as our awareness of
objects. The structuralists saw sensations as analogous to the atoms of

Bruce Goldstein

Figure 5.8 Your ability to recognize each of these views as being of the
same chair is an example of viewpoint invariance.

(a) 

94

(b) 
(c) 

Chapter 5  Perceiving Objects and Scenes

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Segregation The building on the right is in front of the one on the
left.

Segregation The two buildings are separated from one another, with a
border between them.

Figure 5.9 Examples of grouping and segregation in a city scene.

chemistry. Just as atoms combine to create complex molecular structures,
sensations combine to create complex perceptions. Sensations might be
linked to very simple experiences, such as seeing a single flash of
light, but perception accounts for the vast majority of our sensory
experiences. For example, when you look at Figure 5.11, you perceive a
face, but according to structuralists, the starting point would be many
sensations, which are indicated by the small dots. The Gestalt
psychologists rejected the idea that perceptions were formed only by
"adding up" sensations. An early observation that led to rejecting the
idea of adding up sensations occurred in 1911, as psychologist Max
Wertheimer was on vacation taking a train ride through Germany (Boring,
1942). When Wertheimer got off the train to stretch his legs in
Frankfort, he made an observation that involved a phenomenon called
apparent movement.

© Cengage 2021

Bruce Goldstein

Grouping Everything in the white areas belongs to one object (the
building).

Figure 5.10 Some black and white shapes that become perceptually
organized into a Dalmatian. See page 120 for an outline of the
Dalmatian.

illusion of movement by rapidly alternating two slightly different
pictures, caused Wertheimer to wonder how the structuralist's idea that
experience is created by sensations could explain the illusion of
movement he observed. Figure 5.12 diagrams the principle behind the
illusion of movement created by the stroboscope, which is called
apparent movement because although movement is perceived, nothing is
actually moving. The three components that create apparent movement (in
this case, using flashing lights) are shown in Figure 5.12: (a) One
light flashes (Figure 5.12a); (b) there is a

(a) One light flashes

Apparent Movement On the train platform in Frankfort, Wertheimer bought
a toy stroboscope from a vendor. The stroboscope, which is a mechanical
device that creates an (b) Darkness

(c) The second light flashes

(d) Flash---dark---flash

Figure 5.11 According to structuralism, a number of sensations
(represented by the dots) add up to create our perception of the face.

Figure 5.12 The conditions for creating apparent movement. (a) One light
flashes, followed by (b) a short period of darkness, followed by (c)
another light flashing in a different position. The resulting
perception, symbolized in (d), is a light moving from left to right.
Movement is seen between the two lights even though there is only
darkness in the space between them.

5.2 Perceptual Organization

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

95

period of darkness, lasting a fraction of a second (­Figure 5.12b); and
(c) the second image flashes (Figure 5.12c). Physically, then, there are
two images flashing separated by a period of darkness. But we don't see
the darkness because our perceptual system adds something during the
period of darkness--- the perception of an image moving through the
space between the flashing lights (Figure 5.12d). Modern examples of
apparent movement are electronic signs like the one in ­Figure 5.13,
which display moving advertisements or news headlines and movies. The
perception of movement in these displays is so compelling that it is
difficult to imagine that they are made up of stationary lights flashing
on and off (for the news headlines) or still images flashed one after
another (for the movies). Wertheimer drew two conclusions from the
phenomenon of apparent movement. His first conclusion was that apparent
movement can't be explained by sensations alone, because there is
nothing in the dark space between the flashing lights. His second
conclusion, which became one of the basic principles of Gestalt
psychology, is the whole is different than the sum of its parts, because
the perceptual system creates the perception of movement where there
actually is none. The idea that the whole is different than the sum of
its parts became the battle cry of the Gestalt psychologists. "Wholes"
were in; "sensations" were out (see page 5 for more on sensations).

Corbis Bridge/Alamy Stock Photo

Illusory Contours Another demonstration that argues against sensations
and for the idea that the whole is different than the sum of its parts
is shown in Figure 5.14. This demonstration involves circles with a
"mouth" cut out, which resemble "Pac Man" figures from the classic video
game introduced in the 1980s. We begin with the Pac Men in Figure 5.14a.
You may see an edge running between the "mouths" of the Pac Men, but if
you cover up one of them, the edge vanishes. This single edge becomes
part of a triangle when we add the third Pac Man, in Figure 5.14b. The
three Pac Men have created the perception of a triangle, which becomes
more obvious by adding lines, as shown in Figure 5.14c. The edges that
create

Figure 5.13 The stock ticker in Times Square, New York. The letters and
numbers that appear to be moving smoothly across the screen are created
by hundreds of small lights that are flashing on and off. 96

(a) 
(b) 
(c) 

Figure 5.14 The illusory contours clearly visible in (b) and (c) cannot
be caused by sensations, because there is only white there.

the triangle are called illusory contours because there are actually no
physical edges present. Sensations can't explain illusory contours,
because there aren't any sensations along the contours. The idea that
the whole is different than the sum of its parts led the Gestalt
psychologists to propose a number of principles of perceptual
organization to explain the way elements are grouped together to create
larger objects.

Gestalt Principles of Perceptual Organization Having questioned the idea
that perceptions are created by adding up sensations, the Gestalt
psychologists proposed that perception depends on a number of principles
of perceptual organization, which determine how elements in a scene
become grouped together. The starting points for the principles of
organization are things that usually occur in the environment. Consider,
for example, how you perceive the rope in ­ Figure 5.15a. Although there
are many places where one strand is overlapped by another strand, you
probably perceive the rope not as a number of separate pieces but as a
continuous strand, as illustrated by the highlighted segment of rope in
Figure 5.15b. The Gestalt psychologists, being keen observers of
perception, used this kind of observation to formulate the principle of
good continuation.

Good Continuation The principle of good continuation states the
following: Points that, when connected, result in straight or smoothly
curving lines are seen as belonging together, and the lines tend

Chapter 5  Perceiving Objects and Scenes

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Bruce Goldstein

(a) 
(b) 

Figure 5.15 (a) Rope on the beach. (b) Good continuation helps us
perceive the rope as a single strand.

to be seen in such a way as to follow the smoothest path. The wire
starting at A in Figure 5.16 flowing smoothly to B is an example of
lines following the smoothest path. The path from A does not go to C or
D because those paths would violate good continuation by making sharp
turns. The principle of good continuation also states that objects that
are partially ­covered by other objects are seen as continuing behind the
covering ­object. The rope in Figure 5.15 illustrates how covered objects
are seen as continuing behind the object that covers them.

Pragnanz Pragnanz, roughly translated from the German, means "good
figure." The principle of pragnanz, also called the principle of good
figure or the principle of simplicity, states: Every stimulus pattern is
seen in such a way that the resulting structure is as simple as
possible. The familiar Olympic symbol in Figure 5.17a is an example of
the principle of simplicity at work. We see this display as five circles
and not as a larger number of more complicated shapes such as the ones
in the "exploded view" of the Olympic symbol in Figure 5.17b. The

C

D

(a) 
(b) 

Figure 5.17 (a) The Olympic symbol is perceived as five circles, not as
the nine shapes in (b).

principle of good continuation also contributes to perceiving the five
circles. Can you see why this is so?

Similarity Most people perceive Figure 5.18a as either horizontal rows
of circles, vertical columns of circles, or a square

B

Figure 5.16 Good continuation helps us perceive two separate wires, even
though they overlap.

Bruce Goldstein

A

(a) 
(b) 

Figure 5.18 (a) These dots are perceived as horizontal rows or vertical
columns or both. (b) These dots are perceived as vertical columns. 5.2
Perceptual Organization

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

97

Proximity (Nearness) Our perception of Figure 5.20 as three groups of
candles illustrates the principle of proximity, or nearness: Things that
are near each other appear to be grouped together.

Wilma Hurskainen

Common Fate According to the principle of common fate, things that are
moving in the same direction appear to be grouped together. Thus, when
you see a flock of hundreds of birds all flying together, you tend to
see the flock as a unit; if some of the birds start flying in another
direction, this creates a new

Figure 5.19 This photograph, Waves, by Wilma Hurskainen, was taken at
the exact moment that the front of the white water aligned with the
white area on the woman's clothing. Similarity of color causes grouping;
differently colored areas of the dress are perceptually grouped with the
same colors in the scene. Also notice how the front edge of the water
creates grouping by good continuation across the woman's dress. 98

Bruce Goldstein

filled with evenly spaced dots. But when we change the color of some of
the columns, as in Figure 5.18b, most people perceive vertical columns
of circles. This perception illustrates the principle of similarity:
Similar things appear to be grouped together. This law causes circles of
the same color to be grouped together. A striking example of grouping by
similarity of color is shown in Figure 5.19. Grouping can also occur
because of similarity of shape, size, or orientation. Grouping also
occurs for auditory stimuli. For example, notes that have similar
pitches and that follow each other closely in time can become
perceptually grouped to form a melody. We will consider this and other
auditory grouping effects when we describe organizational processes in
hearing in Chapter 12 and perceiving music in Chapter 13.

Figure 5.20 The candles are grouped by proximity to create three
separate groups. Can you identify additional Gestalt principles in the
patterns on the menorah?

unit. Note that common fate can work even if the objects in a group are
dissimilar. The key to common fate is that a group of objects is moving
in the same direction. Common fate can apply not only to changes in
spatial position, as the original Gestalt psychologists proposed, but
also to changes in illumination when elements of our visual field that
become lighter or darker simultaneously are perceived as being grouped
into a unit (Sekuler & Bennett, 2001). For example, if you're at a rock
concert and some of the stage lights are flickering on and off at the
same time, you might perceive them as one group. The principles we have
just described were proposed by the Gestalt psychologists in the early
1900s. The following additional principles have been proposed by modern
perceptual psychologists.

Common Region Figure 5.21a illustrates the principle of common region:
Elements that are within the same region of space appear to be grouped
together. Even though the circles inside the ovals are farther apart
than the circles that are next to each other in neighboring ovals, we
see the circles inside the ovals as belonging together. This occurs
because each oval is seen as a separate region of space (Palmer, 1992;
Palmer & Rock, 1994). Notice that in this example, common region
overpowers proximity, because proximity would predict that the nearby
circles would be perceived together. But even though the circles that
are in different regions are close to each other in space, they do not
group with each other, as they did in Figure 5.20. Uniform Connectedness
According to the principle of uniform connectedness, a connected region
of the same visual properties, such as lightness, color, texture, or
motion, is perceived as a single unit (Palmer & Rock, 1994). For
example, in Figure 5.21b, the connected circles are perceived as grouped
together, just as they were when they were in the same region in Figure
5.21a. Again, connectedness overpowers proximity.

Chapter 5  Perceiving Objects and Scenes

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

for granted and label them as "obvious." But the reality is that the
Gestalt principles are nothing less than the basic operating
characteristics of our visual system that determine how our perceptual
system organizes elements of the environment into larger units.

(a) 
(b) 

Perceptual Segregation

Figure 5.21 Grouping by (a) common region and (b) uniform connectedness.

The Gestalt principles we have described predict what we will perceive,
based on what usually happens in the environment. Many of my students
react to this idea by saying that the Gestalt principles therefore
aren't anything special, because all they are doing is describing the
obvious things we see every day. When they say this, I remind them that
the reason we perceive scenes like the city buildings in Figure 5.1 or
the scene in Figure 5.22 so easily is that we use observations about
commonly occurring properties of the environment to organize the scene.
Thus, we assume, without even thinking about it, that the men's legs in
Figure 5.22 extend behind the gray boards, because generally in the
environment when two visible parts of an object (like the men's legs)
have the same color and are "lined up," they belong to the same object
and extend behind whatever is blocking it. People don't usually think
about how we perceive situations like this as being based on assumptions
or predictions, but that is, in fact, what is happening---an idea that
we'll return to later in the chapter. The reason the "assumption" seems
so obvious is that we have had so much experience with things like this
in the environment. That the "assumption" is actually almost a "sure
thing" may cause us to take the Gestalt principles

The Gestalt psychologists were also interested in determining
characteristics of the environment responsible for perceptual
segregation---the perceptual separation of one object from another, as
occurred when you saw the buildings in Figure 5.1 as separate from one
another. One approach to studying perceptual segregation is to consider
the problem of figure--ground segregation. When we see a separate
object, it is usually seen as a figure that stands out from its
background, which is called the ground. For example, sitting at your
desk, you would probably see a book or papers on your desk as figure and
the surface of your desk as ground, or stepping back from the desk, you
might see the desk as figure and the wall behind it as ground. The
Gestalt psychologists were interested in determining the properties of
the figure and the ground and what causes us to perceive one area as
figure and the other as ground.

Properties of Figure and Ground One way the ­Gestalt psychologists
studied the properties of figure and ground was by considering patterns
like the one in Figure 5.23, which was introduced by Danish psychologist
Edgar Rubin in 1915. This pattern is an example of reversible
figure--ground because it can be perceived alternately either as two
dark blue faces looking at each other, in front of a gray background, or
as a gray vase on a dark blue background. Some of the properties of the
figure and ground are: The figure is more "thinglike" and more memorable
than the ground. Thus, when you see the vase as figure, it appears as an
object that can be remembered later. However, when you see the same
light area as ground, it does not appear to be an object but is just
"background" and is therefore not particularly memorable.

Bruce Goldstein

■■

Figure 5.22 A usual occurrence in the environment: Objects (the men's
legs) are partially hidden by another object (the gray boards). In this
example, the men's legs continue in a straight line and are the same
color above and below the boards, so it is highly likely that they
continue behind the boards.

Figure 5.23 A version of Rubin's reversible face--vase figure. 5.2
Perceptual Organization

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

99

(a) (b)

The figure is seen as being in front of the ground. Thus, when the vase
is seen as figure, it appears to be in front of the dark background
(Figure 5.24a), and when the faces are seen as figure, they are on top
of the light background (Figure 5.24b). ■■ Near the borders it shares
with the figure, the ground is seen as unformed material, without a
specific shape, and seems to extend behind the figure. This is not to
say that grounds lack shape entirely. Grounds are often shaped by
borders distant from those they share with the figure; for instance, the
backgrounds in Figure 5.24 are rectangles. ■■ The border separating the
figure from the ground appears to belong to the figure. Consider, for
example, the Rubin face--vase in Figure 5.23. When the two faces are
seen as figure, the border separating the blue faces from the grey
background belongs to the faces. This property of the border belonging
to one area is called border ownership. When perception shifts so the
vase is perceived as figure, border ownership shifts as well, so now the
border belongs to the vase. ■■

Properties of the Image That Determine Which Area Is Figure In an image
like the Rubin face--vase in Figure 5.23, how does your visual system
decide which region "owns" the border and is therefore perceived as the
figure? To answer this, we return to the Gestalt psychologists, who
specified a number of figural cues within the image that determine which
areas are perceived as figure. These Gestalt figural cues are not to be
confused with the Gestalt principles of perceptual organization; while
the principles of organization determine how elements of an image are
grouped together, figural cues determine how an image is segregated into
figure and ground. One figural cue proposed by the Gestalt psychologists
was that areas lower in the field of view are more likely to be
perceived as figure (Ehrenstein, 1930; Koffka, 1935). This idea was
confirmed experimentally years later by Shaun Vecera and coworkers
(2002), who briefly flashed stimuli like the ones in F ­ igure 5.25a and
determined which area was seen as figure, the red area or the green
area. The results, shown in ­Figure 5.25b, indicate that for the
upper--lower displays, observers were more likely to perceive the lower
area as figure, but for the left--right displays, they showed only a
small preference for the left region. From this result, Vecera concluded
that 100

100 75 50 25

Lower seen as figure

Left seen as figure

(b) 

Figure 5.25 (a) Stimuli from the Vecera et al. (2002) experiment. (b)
Percentage of trials on which lower or left areas were seen as figure.

there is a preference for seeing objects lower in the display as figure.
This conclusion makes sense when we consider a scene like the one in
Figure 5.26, in which the lower part of the scene is figure and the sky
is ground. What is significant about this scene is that it is typical of
scenes we perceive every day. In our normal experience, the "figure" is
much more likely to be below the horizon. Another Gestalt proposal was
that figures are more likely to be perceived on the convex side of
borders (borders that bulge outward) (Kanizsa & Gerbino, 1976). Mary
Peterson and Elizabeth Salvagio (2008) demonstrated this by presenting

Bruce Goldstein

Figure 5.24 (a) When the vase is perceived as figure, it is seen in
front of a homogeneous dark background. (b) When the faces are seen as
figure, they are seen in front of a homogeneous light background.

Percent of trials

(a) 

Figure 5.26 The field, in the bottom half of the visual field, is seen
as figure. The sky, in the upper half of the visual field, is seen as
ground.

Chapter 5  Perceiving Objects and Scenes

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

(a)

(b) 
(c) 

Figure 5.27 Stimuli from Peterson and Salvagio's (2008) experiment: (a)
8-component display; (b) 2-component display; (c) 4-component display.
The red squares appeared on different areas on different trials. The
participant's task was to judge whether the red square was "on" or "off"
the area perceived as figure.

displays like the one in Figure 5.27a and asking observers to indicate
whether the red square was "on" or "off" the area perceived as figure.
Thus, if they perceived the dark area in this example as being a figure,
they would say "on." If they perceived the dark area as ground, they
would say "off." The result, in agreement with the Gestalt proposal, was
that convex regions, like the dark regions in Figure 5.27a, were
perceived as figure 89 percent of the time. But Peterson and Salvagio
went beyond simply confirming the Gestalt proposals by also presenting
displays like the ones in Figures 5.27b and 5.27c, which had fewer
components. Doing this greatly decreased the likelihood that convex
displays would be seen as figure, with the black convex region in the
two-component display (Figure 5.27b) being seen as figure only 58
percent of the time. What this result means, according to Peterson and
Salvagio, is that to understand how segregation occurs we need to go
beyond simply identifying factors like convexity. Apparently,
segregation is determined not by just what is happening at a single
border but by what is happening in the wider scene. This makes sense
when we consider that perception generally occurs in scenes that extend
over a wide area. We will return to this idea later in the chapter when
we consider how we perceive scenes.

The Role of Perceptual Principles and Experience in Determining Which
Area Is Figure The Gestalt psychologists' emphasis on perceptual
principles led them to minimize the role of a person's past experiences
in determining perception. They believed that although perception can be
affected by experience, built-in principles can override experience. The
Gestalt psychologist Max Wertheimer (1912) provided the following
example to illustrate how built-in principles could override experience:
Most people recognize the display in Figure 5.28a as a "W" sitting on
top of an "M," largely because of our past experiences with those two
letters. However, when the letters are arranged as in Figure 5.28b, most
people see two uprights plus a pattern in between them. The uprights,
which are created by the principle of good continuation, are the
dominant perception and override the effects of past experience with Ws
or Ms. The Gestalt idea that past experience and the meanings of stimuli
(like the W and M) play a minor role in perceptual

(a) 
(b) 

Figure 5.28 (a) W on top of M. (b) When combined, a new pattern emerges,
overriding the meaningful letters. (From Wertheimer, 1912)

organization is also illustrated by the Gestalt proposal that one of the
first things that occurs in the perceptual process is the segregation of
figure from ground. They contended that the figure must stand out from
the ground before it can be recognized. In other words, the figure has
to be separated from the ground before we can assign a meaning to the
figure. But Bradley Gibson and Mary Peterson (1994) did an experiment
that argued against this idea by showing that figure-- ground formation
can be affected by the meaningfulness of a stimulus. They demonstrated
this by presenting a display like the one in Figure 5.29a, which can be
perceived in two ways: (1) a standing woman (the black part of the
display) or (2) a less meaningful shape (the white part of the display).
When they presented stimuli such as this for a fraction of a second and
asked observers which region seemed to be the figure, they found that
observers were more likely to say that the meaningful part of the
display (the woman, in this example) was the figure. Why were the
observers more likely to perceive the woman? One possibility is that
they recognized that the black area was a familiar object. In fact, when
Gibson and Peterson turned the display upside down, as in Figure 5.29b,
so that it was more difficult to recognize the black area as a woman,
participants were less likely to see that area as being the figure. The
fact that meaningfulness can influence the assignment of an area as
figure means that the process of recognition must be occurring 5.2
Perceptual Organization

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

101

5. What properties of a stimulus tend to favor perceiving an area as
"figure"? Be sure you understand Vecera's experiment that showed that
the lower region of a display tends to be perceived as figure, and why
Peterson and Salvagio stated that to understand how segregation occurs
we have to consider what is happening in the wider scene. 6. Describe
the Gestalt ideas about the role of meaning and past experience in
determining figure--ground segregation. 7. Describe Gibson and
Peterson's experiment that showed that meaning can play a role in
figure--ground segregation.

5.3 Recognition by Components (a)

(b) 

Figure 5.29 Gibson and Peterson's (1994) stimulus. (a) The black area is
more likely to be seen as figure because it is meaningful. (b) This
effect does not occur when meaningfulness is decreased by turning the
picture upside down.

either before or at the same time as the figure is being separated from
the ground (Peterson, 1994, 2001, 2019). So far, the principles and
research we have been describing have focused largely on how our
perception of individual objects depends on organizing principles and
principles that determine which parts of a display will be seen as
figure and which will be seen as ground. If you look back at the
illustrations in this section, you will notice that most of them are
simple displays designed to illustrate a specific principle of
perceptual organization. In the next section, we'll consider a more
modern approach to object perception called recognitionby-components
theory.

TEST YOuRSELF 5.1 1. What are some of the problems that make object
perception difficult for computers but not for humans? 2. What is
structuralism, and why did the Gestalt psychologists propose an
alternative to this way of explaining perception? 3. How did the Gestalt
psychologists explain perceptual organization? 4. How did the Gestalt
psychologists describe figure-- ground segregation? What are some basic
properties of figure and ground?

102

In the previous section, we discussed how we organize a visual image by
grouping elements of that image into coherent units and by separating
objects from their backgrounds. Now, we'll move from organization to
recognition and discuss how we recognize those individual objects. For
instance, how do you recognize the black region in Figure 5.29a as a
silhouette of a woman? A theory of object recognition called recognition
by components (RBC) theory was proposed by Irving Biederman in the 1980s
(Biederman, 1987). RBC theory states that objects are comprised of
individual geometric components called geons, and we recognize objects
based on the arrangement of those geons. Geons are three-dimensional
shapes, like pyramids, cubes, and cylinders. Figure 5.30a shows some
examples of geons, but this is just a small sample; Biederman proposed
that there are 36 different geons from which most objects we encounter
can be assembled and recognized. Geons are the building blocks of
objects and the same geons can be arranged in different ways to form
different objects, as illustrated in Figure 5.30b. For instance, the
cylinder could be part of the mug or the base of the lamp. An important
aspect of the RBC theory is that it accounts for viewpoint invariance,
the fact that a given object can be recognized from different
viewpoints. RBC theory accounts for this because even if the mug in
Figure 5.30b was viewed from the side rather than from the front, it is
still comprised of the same geons, so it is still recognized as a mug.
RBC theory provided a simple and elegant way of approaching object
perception that included viewpoint invariance. However, there are many
aspects of object perception that the RBC theory could not explain. For
instance, it doesn't account for grouping or organization like the
Gestalt principles do, and some objects simply can't be represented by
assemblies of geons (like clouds in the sky that typically don't have
geometric components). The RBC theory also doesn't allow for
distinguishing between objects within a given category, such as two
different types of coffee mugs or species of birds that might be
composed of the same basic shapes. Thus, while

Chapter 5  Perceiving Objects and Scenes

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

5 1

2

5 3

2

2

5 3

4

4

5 5 3

3 (a) Geons

3

Figure 5.30 (a) Some geons. (b) Some objects created from these geons.
The numbers on the objects indicate which geons are present. Note that
recognizable objects can be formed by combining just two or three geons.
Also note that the relations between the geons matter, as illustrated by
the cup and the pail. (Biederman, 1987)

(b) Objects

the RBC theory played its role in history in that it got people thinking
about how the visual system represents objects, research in the field
has moved on to consider objects not just as a collection of geometric
components, but also as part of more meaningful, real-world scenes.

5.4 Perceiving Scenes and Objects in Scenes A scene is a view of a
real-world environment that contains (1) background elements and (2)
multiple objects that are organized in a meaningful way relative to each
other and the background (Epstein, 2005; Henderson & Hollingworth,
1999). One way of distinguishing between objects and scenes is that
objects are compact and are acted upon, whereas scenes are extended in
space and are acted within. For example, if we are walking down the
street and mail a letter, we would be acting upon the mailbox (an
object) and acting within the street (the scene).

Perceiving the Gist of a Scene Perceiving scenes presents a paradox. On
one hand, scenes are often large and complex. However, despite this size
and

complexity, you can identify important properties of most scenes after
viewing them for only a fraction of a second. This general description
of the type of scene is called the gist of a scene. An example of your
ability to rapidly perceive the gist of a scene is the way you can
rapidly flip from one TV channel to another, yet still grasp the meaning
of each picture as it flashes by---a car chase, quiz contestants, or an
outdoor scene with mountains---even though you may be seeing each
picture for a second or less and so may not be able to identify specific
objects. When you do this, you are perceiving the gist of each scene
(Oliva & Torralba, 2006). Exactly how long does it take to perceive the
gist of a scene? Mary Potter (1976) showed observers a target picture
and then asked them to indicate whether they saw that picture as they
viewed a sequence of 16 rapidly presented pictures. Her observers could
do this with almost 100 percent accuracy even when the pictures were
flashed for only 250 ms (ms 5 milliseconds; 250 ms 5 1/4 second). Even
when the target picture was only specified by a written description,
such as "girl clapping," observers achieved an accuracy of almost 90
percent (Figure 5.31). Another approach to determining how rapidly
people can perceive scenes was used by Li Fei-Fei and coworkers (2007),
who presented pictures of scenes for exposures ranging from 27 ms to 500
ms and asked observers to write a description

Description

250 ms

250 ms

250 ms

Bruce Goldstein

Girl clapping

Figure 5.31 Procedure for Potter's (1976) experiment. She first
presented either a target photograph or, as shown here, a description,
and then rapidly presented 16 pictures for 250 ms each. The observer's
task was to indicate whether the target picture had been presented. In
this example, only 3 of the 16 pictures are shown, with the target
picture being the second one presented. On some trials, the target
picture was not included in the series of 16 pictures. 5.4 Perceiving
Scenes and Objects in Scenes

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

103

of what they saw. This method of determining the observer's response is
a nice example of the phenomenological report, described in Chapter 1
(p. 17). Fei-Fei used a procedure called masking to be sure the
observers saw the pictures for exactly the desired duration.

Using a Mask to Achieve Brief Stimulus Presentations

METHOD

What if we want to present a stimulus that is visible for only 100 ms?
Although you might think that the way to do this would be to flash a
stimulus for 100 ms, this won't work because of a phenomenon called
persistence of vision---the perception of a visual stimulus continues
for about 250 ms (1/4 second) after the stimulus is extinguished. Thus,
a picture that is presented for 100 ms will be perceived as lasting
about 350 ms. But the persistence of vision can be eliminated by
presenting a visual masking stimulus, usually a random pattern that
covers the original stimulus, so if a picture is flashed for 100 ms
followed immediately by a masking stimulus, the picture is visible for
just 100 ms. A masking stimulus is therefore often presented immediately
after a test stimulus to stop the persistence of vision from increasing
the duration of the test stimulus.

Alice O'Donnell

Typical results of Fei-Fei's experiment are shown in ­Figure 5.32. At
brief durations, observers saw only light and dark areas of the
pictures. By 67 ms they could identify some large objects (a person, a
table), and when the duration was

27 ms

Looked like something black in the center with four straight lines
coming out of it against a white background. (Subject: AM)

40 ms

The first thing I could recognize was a dark splotch in the middle. It
may have been rectangular-shaped, with a curved top... but that's just a
guess. (Subject: KM)

67 ms

A person, I think, sitting down or crouching. Facing the left side of
the picture. We see their profile mostly. They were at a table or where
some object was in front of them (to their left side in the picture).
(Subject: EC)

500 ms

This looks like a father or somebody helping a little boy. The man had
something in his hands, like an LCD screen or a laptop. They looked like
they were standing in a cubicle. (Subject: WC)

Figure 5.32 Observer's description of a photograph presented in
Fei-Fei's (2007) experiment. Viewing durations are indicated on the
left. (From Fei-Fei et al., 2007) 104

increased to 500 ms (half a second) they were able to identify smaller
objects and details (the boy, the laptop). For a picture of an ornate
1800s living room, observers were able to identify the picture as a room
in a house at 67 ms and to identify details, such as chairs and
portraits, at 500 ms. Thus, the overall gist of the scene is perceived
first, followed by perception of details and smaller objects within the
scene. What enables observers to perceive the gist of a scene so
rapidly? Aude Oliva and Antonio Torralba (2001, 2006) propose that
observers use information called global image features, which can be
perceived rapidly and are associated with specific types of scenes. Some
of the global image features proposed by Oliva and Torralba are: Degree
of naturalness. Natural scenes, such as the ocean and forest in Figure
5.33, have textured zones and undulating contours. Man-made scenes, such
as the street, are dominated by straight lines and horizontals and
verticals. ■■ Degree of openness. Open scenes, such as the ocean, often
have a visible horizon line and contain few objects. The street scene is
also open, although not as much as the ocean scene. The forest is an
example of a scene with a low degree of openness. ■■ Degree of
roughness. Smooth scenes (low roughness) like the ocean contain fewer
small elements. Scenes with high roughness like the forest contain many
small elements and are more complex. ■■ Degree of expansion. The
convergence of parallel lines, like what you see when you look down
railroad tracks that appear to vanish in the distance, or in the street
scene in Figure 5.33, indicates a high degree of expansion. This feature
is especially dependent on the observer's viewpoint. For example, in the
street scene, looking directly at the side of a building would result in
low expansion. ■■ Color. Some scenes have characteristic colors, like
the ocean scene (blue) and the forest (green and brown). (Castelhano &
Henderson, 2008; Goffaux et al., 2005) ■■

Global image features are holistic and rapidly perceived. They are
properties of the scene as a whole and do not depend on time-consuming
processes such as perceiving small details, recognizing individual
objects, or separating one object from another. Another property of
global image features is that they contain information about a scene's
structure and spatial layout. For example, the degree of openness and
the degree of expansion refer directly to characteristics of a scene's
layout; naturalness also provides layout information that comes from
knowing whether a scene is from nature or contains humanmade structures.
Global image properties not only help explain how we can perceive the
gist of scenes based on features that can be seen in brief exposures,
they also illustrate the following general property of perception: Our
past experiences in perceiving properties of the environment play a role
in determining our perceptions. We learn, for example, that blue is
associated with open sky, that landscapes are often green and smooth,
and that verticals and horizontals are associated with buildings.
Characteristics of the environment such as this, which occur

Chapter 5  Perceiving Objects and Scenes

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Aude Oliva

Figure 5.33 Three scenes that have different global image properties.
See text for description.

frequently, are called regularities in the environment. We will now
describe these regularities in more detail.

Regularities in the Environment: Information for Perceiving

Physical Regularities Physical regularities are regularly occurring
physical properties of the environment. For example, there are more
vertical and horizontal orientations in the environment than oblique
(angled) orientations. This occurs in human-made environments (for
example, buildings contain many horizontals and verticals) and also in
natural environments (trees and plants are more likely to be vertical or
horizontal than slanted) (Coppola et al., 1998) (Figure 5.34). It is,
therefore, no coincidence that people can perceive horizontals and
verticals more easily than other orientations---the oblique effect we
introduced in Chapter 1 (see page 12) (Appelle, 1972; Campbell et al.,
1966; Orban et al., 1984). Another example of a physical regularity is
that when one object partially covers another one, the contour of the
partially covered object "comes out the other side," as occurs for the
rope in Figure 5.15. Yet another example is provided by the pictures in
Figure 5.35. Figure 5.35a shows indentations created by people walking
in the sand. But when we turn this picture upside down, as in Figure
5.35b, the indentations in the sand become rounded mounds. Our
perception in these two situations has been explained by the
light-from-above assumption: we usually assume that light is coming from
above, because light in the environment, including the sun and most
artificial light, usually comes from above (Kleffner & Ramachandran,
1992). Figure 5.35c shows how light coming from above and to the left
illuminates an indentation, leaving a shadow on the left. Figure 5.35d
shows how the same light illuminates a bump, leaving a shadow on the
right. Our perception of illuminated shapes is influenced by how they
are shaded, combined with the brain's assumption that light is coming
from above. One of the reasons humans are able to perceive and recognize
objects and scenes so much better than computers is that our perceptual
system is adapted to respond to physical characteristics of our
environment, such as the orientation of objects and the direction of
light. But this adaptation goes

Bruce Goldstein

Modern perceptual psychologists have introduced the idea that perception
is influenced by two types of regularities: physical regularities and
semantic regularities.

Figure 5.34 In these two scenes from nature, horizontal and vertical
orientations are more common than oblique orientations. These scenes are
special examples, picked because the large proportion of verticals.
However, randomly selected photos of natural scenes also contain more
horizontal and vertical orientations than oblique orientations. This
also occurs for human-made buildings and objects.

beyond physical characteristics. It also occurs because we have learned
about what types of objects typically occur in specific types of scenes.

Semantic Regularities In language, semantics refers to the meanings of
words or sentences. Applied to perceiving scenes, semantics refers to
the meaning of a scene. This meaning is often related to what happens
within a scene. For example, food preparation, cooking, and perhaps
eating occur 5.4 Perceiving Scenes and Objects in Scenes

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

105

Bruce Goldstein

Figure 5.35 (a) Indentations made by people walking in the sand. (b)
Turning the picture upside down turns indentations into rounded mounds.
(c) How light from above and to the left illuminates an indentation,
causing a shadow on the left. (d) The same light illuminating a bump
causes a shadow on the right.

(a) 
(b) 

Shadow

(c) 

Shadow

INDENTATION

in a kitchen; waiting around, buying tickets, checking luggage, and
going through security checkpoints happen in airports. Semantic
regularities are the characteristics associated with activities that are
common in different types of scenes. One way to demonstrate that people
are aware of semantic regularities is simply to ask them to imagine a
particular type of scene or object, as in the following demonstration.
DEMONSTRATION

Visualizing Scenes and Objects

Your task in this demonstration is simple. Close your eyes and then
visualize or simply think about the following scenes and objects: 1. An
office 2. The clothing section of a department store 3. A microscope 4.
A lion

Most people who have grown up in modern society have little trouble
visualizing an office or the clothing section of a department store.
What is important about this ability, for our purposes, is that part of
this visualization involves details within these scenes. Most people see
an office as having a desk with a computer on it, bookshelves, and a
chair. The 106

(d) 

BUMP

department store scene may contain racks of clothes, a changing room,
and perhaps a cash register. What did you see when you visualized the
microscope or the lion? Many people report seeing not just a single
object, but an object within a setting. Perhaps you perceived the
microscope sitting on a lab bench or in a laboratory, and the lion in a
forest or on a savannah or in a zoo. The point of this demonstration is
that our visualizations contain information based on our knowledge of
different kinds of scenes. This knowledge of what a given scene
typically contains is called a scene schema. An example of how a scene
schema can influence perception is an experiment by Stephen Palmer
(1975), which used stimuli like the picture in Figure 5.36. Palmer first
presented a context scene such as the one on the left and then briefly
flashed one of the target pictures on the right. When Palmer asked
observers to identify the object in the target picture, they correctly
identified an object like the loaf of bread (which is appropriate to the
kitchen scene) 80 percent of the time, but correctly identified the
mailbox or the drum (two objects that don't fit into the scene) only 40
percent of the time. Apparently, Palmer's observers were using their
knowledge about kitchens to help them perceive the briefly flashed loaf
of bread. The effect of semantic regularities is also illustrated in
Figure 5.37, which is called "the multiple personalities of a blob"
(Oliva & Torralba, 2007). The blob (a) is perceived as different objects
depending on its orientation and the context within which it is seen. It
appears to be an object on a table

Chapter 5  Perceiving Objects and Scenes

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

A

B

C

Context scene

Target object

Figure 5.36 Stimuli used in Palmer's (1975) experiment. The scene at the
left is presented first, and the observer is then asked to identify one
of the rapidly flashed objects on the right.

in (b), a shoe on a person bending down in (c), and a car and a person
crossing the street in (d), even though it is the same shape in all of
the pictures. Although people make use of regularities in the
environment to help them perceive, they are often unaware of the
specific information they are using. This aspect of perception is
similar to what occurs when we use language. Even though people easily
string words together to create sentences in conversations, they may not
know the rules of grammar that specify how these words are being
combined. Similarly, we easily use our knowledge of regularities in the
environment to help us perceive, even though we may not be able to
identify the specific information we are using.

blob (b)

(c) 
(d) 

People use their knowledge of physical and semantic regularities such as
the ones we have been describing to infer what is present in a scene.
The idea that perception involves inference is nothing new; it was
introduced in the 18th century by Hermann von Helmholtz (1866/1911), who
proposed the theory of unconscious inference.

Helmholtz's Theory of Unconscious Inference Helmholtz made many
discoveries in physiology and physics, developed the ophthalmoscope (the
device that an optometrist or ophthalmologist uses to look into your
eyes), and proposed theories of object perception, color vision, and
hearing. One of Helmholtz's contributions to perception was based on his
realization that the image on the retina is ambiguous. We have seen that
retinal ambiguity means that a particular pattern of stimulation on the
retina can be caused by many different possible objects in the
environment (see Figure 5.3). For example, what does the pattern of
stimulation in Figure 5.38a represent? For most people, this pattern
results in perception of a blue rectangle in front of a red rectangle,
as shown in Figure 5.38b. But as Figure 5.38c indicates, this display
could

Antonio Torralba

(a) 

The Role of Inference in Perception

Figure 5.37 "Multiple personalities of a blob." What we expect to see in
different contexts influences our interpretation of the identity of the
"blob" inside the circles. (From Oliva & Torralba, 2007)

(a) 
(b) 
(c) 

Figure 5.38 The display in (a) is usually interpreted as being (b) a
blue rectangle in front of a red rectangle. It could, however, be (c) a
blue rectangle and an appropriately positioned six-sided red figure. 5.4
Perceiving Scenes and Objects in Scenes

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

107

have been caused by a six-sided red shape positioned in front of,
behind, or right next to the blue rectangle. Helmholtz's question was,
"How does the perceptual system 'decide' that this pattern on the retina
was created by overlapping rectangles?" His answer was the likelihood
principle, which states that we perceive the object that is most likely
to have caused the pattern of stimuli we have received. This judgment of
what is most likely occurs, according to Helmholtz, by a process called
unconscious inference, in which our perceptions are the result of
unconscious assumptions, or inferences, that we make about the
environment. Thus, we infer that it is likely that Figure 5.38a is a
rectangle covering another rectangle because of experiences we have had
with similar situations in the past. Helmholtz's description of the
process of perception resembles the process involved in solving a
problem. For perception, the problem is to determine which object caused
a particular pattern of stimulation, and this problem is solved by a
process in which the perceptual system uses the observer's knowledge of
the environment to infer what the object might be. The idea that
inference is important for perception has recurred throughout the
history of perception research in various forms. In modern research,
Helmholtz's theory of unconscious inference has been reconceptualized as
prediction---the idea that our past experiences help us make informed
guesses about what we will perceive. But what, exactly, is the process
by which we make predictions? One approach that provides insight into
how predictions can be used in object perception is called Bayesian
inference.

Bayesian Inference In 1763, Thomas Bayes proposed what is known as
Bayesian inference (Geisler, 2008, 2011; ­Kersten et al., 2004; Yuille &
Kersten, 2006). According to Bayes, our estimate of the probability of
an outcome is determined by two factors: (1) the prior probability, or
simply the prior, which is our initial estimate of the probability of an
outcome, and (2) the extent to which the available evidence is
consistent with the outcome. This second factor is called the likelihood
of the outcome. To illustrate Bayesian inference, let's first consider
Figure 5.39a, which shows Maria's priors for three types of health
problems. Maria believes that having a cold or heartburn

108

"Prior": Mary's belief about frequency

"Likelihood": Chances of causing coughing

Conclusion: Cough is most likely due to a cold

High

Probability

Figure 5.39 These graphs present hypothetical probabilities, to
illustrate the principle behind Bayesian inference. (a) Maria's beliefs
about the relative frequency of having a cold, lung disease, and
heartburn. These beliefs are her priors. (b) Further data indicate that
colds and lung disease are associated with coughing, but heartburn is
not. These data contribute to the likelihood. (c) Taking the priors and
likelihood together results in the conclusion that Charles's cough is
probably due to a cold.

is likely to occur, but having lung disease is less likely. With these
priors in her head (along with lots of other beliefs about
health-related matters), Maria notices that her friend Charles has a bad
cough. She guesses that three possible causes could be a cold,
heartburn, or lung disease. Looking further into possible causes, she
does some research and finds that coughing is often associated with
having either a cold or lung disease, but isn't associated with
heartburn (Figure 5.39b). This additional information, which is the
likelihood, is combined with Maria's priors to produce the conclusion
that Charles probably has a cold (Figure 5.39c) (Tenenbaum et al.,
2011). In practice, Bayesian inference involves a mathematical procedure
in which the prior is multiplied by the likelihood to determine the
probability of the outcome. Thus, people start with a prior, then use
additional evidence to update the prior and reach a conclusion (Wolpert
& Ghahramani, 2005). Applying this idea to object perception, let's
return to the inverse projection problem from Figure 5.3. Remember that
the inverse projection problem occurs because a huge number of possible
objects could be associated with a particular image on the retina. So
the problem is how to determine what is "out there" that is causing a
particular retinal image. Luckily, we don't have to rely only on the
retinal image, because we come to most perceptual situations with prior
probabilities based on our past experiences. One of the priors you have
in your head is that books are rectangular. Thus, when you look at a
book on your desk, your initial belief is that it is likely that the
book is rectangular. The likelihood that the book is rectangular is
provided by additional evidence such as the book's retinal image,
combined with your perception of the book's distance and the angle at
which you are viewing the book. If this additional evidence is
consistent with your prior that the book is rectangular, the likelihood
is high and the perception "rectangular" is strengthened. Further
testing by changing your viewing angle and distance can further
strengthen the conclusion that the shape is a rectangle. Note that you
aren't necessarily conscious of this testing process---it occurs
automatically and rapidly. The important point about this process is
that while the retinal image is still the starting point for perceiving
the shape of the book, adding the person's prior beliefs reduces the
possible shapes that could be causing that image.

=

Low

Cold

Lung Heartdisease burn

Cold

Lung Heartdisease burn

Cold

Lung Heartdisease burn

Chapter 5  Perceiving Objects and Scenes

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

What Bayesian inference does is to restate Helmholtz's idea that we
perceive what is most likely to have created the stimulation we have
received in terms of probabilities. It isn't always easy to specify
these probabilities, particularly when considering complex perceptions.
However, because Bayesian inference provides a specific procedure for
determining what might be out there, researchers have used it to develop
computer vision systems that can apply knowledge about the environment
to more accurately translate the pattern of stimulation on their sensors
into conclusions about the environment. We've now discussed how our past
experiences can be used to predict what is most likely to be "out there"
in the world. But how does the brain actually implement these
predictions? One recent theory---called predictive coding---provides an
explanation.

How the Brain Implements Prediction Predictive coding is a theory that
describes how the brain uses our past experiences---or our "priors," as
Bayes put it---to predict what we will perceive (Panichello et al.,
2013; Rao & Ballard, 1999). This explanation begins by stating that our
brain's predictions about the world are represented at higher levels of
the visual system---for instance, toward the top of the "what" and
"where/ how" pathways introduced in Chapter 4, where the neurons respond
to more complex information, like entire objects and scenes. According
to predictive coding, when new incoming visual input reaches the
receptors and is sent upward in the visual system, that signal is
compared to the predictions flowing downward from higher levels (Figure
5.40a). In other words, the brain determines whether what we're seeing
matches with what we expect to be seeing. If the incoming signal matches
the higher-level prediction, nothing happens, as in (a). However, if the
incoming signal doesn't match the prediction, then a prediction error
signal is generated, which is sent back up to higher levels so that the
existing prediction can be modified High-level predictions

High-level predictions PE

No difference = no PE

Difference

Info from receptors

Unexpected info from receptors

(a) 
(b) 

Figure 5.40 The general idea behind predictive coding. (a) Information
from the receptors flows upward and is compared to the brain's
predictions about what we expect to perceive, flowing downward from
higher levels. If there is no difference between the signal from the
receptors and the high-level predictions, then there is no prediction
error (PE). (b) When something unexpected happens, the receptor
information does not match the brain's predictions, and a prediction
error signal is sent upward to make corrections to the predictions.

(Figure 5.40b). In this way, our current experiences can change existing
representations in the brain in order to make better predictions and
"learn" what to expect. As an example of how predictive coding works in
the real world, let's say you're walking across campus to get to class,
and everything is as expected---there's nothing odd or out of place in
your visual scene. In this case, your incoming visual input closely
matches your brain's predictions of what you should be seeing, based on
the many times you've walked that same path before, so there is no
prediction error signal (­Figure 5.40a). But now, let's say a chicken
jumps out of the bushes and runs across your path. You've never seen a
chicken on campus before, so that visual information does not match your
brain's expectations, and an error signal is generated and sent to
higher levels of the visual system (Figure 5.40b). Now, your brain's
representation of campus has been updated to reflect the possibility of
a chicken passing through. This is a good thing, because now you're more
likely to be on the lookout for a rogue chicken the next time you walk
that path, which ultimately may make you better able to respond to that
situation, should it arise again. Predictive coding is similar to
Helmholtz's idea that we use our past experiences to make inferences
about what we will perceive. But predictive coding takes unconscious
inference a step further by linking prediction to what is happening in
the brain. The idea that the brain makes predictions is an important
concept that has become prominent in recent perception research---not
only for object perception, but for other types of perception as well.
One example, which we will consider later, is taste perception. If you
expect a certain flavor---like chamomile tea, because that's what you
ordered at the coffee shop---but the taste of coffee hits your tongue
instead, you'd probably be surprised, which for your brain, would create
an error signal, since the prediction did not match the experience.
We'll continue to see other examples illustrating the importance of
prediction in later chapters. As we've discussed it here, though,
predictive coding doesn't provide the details about what is happening in
the brain. In the next section we will look at what we know about neural
responding to objects and scenes. TEST YOuRSELF 5.2 1. What is the
recognition by components theory? How does it account for viewpoint
invariance? 2. What is the evidence that we can perceive the gist of a
scene very rapidly? What information helps us identify the gist? 3. What
are regularities in the environment? Give examples of physical
regularities, and discuss how these regularities are related to the
Gestalt laws of organization. 4. What are semantic regularities? How do
semantic regularities affect our perception of objects within scenes?
What is the relation between semantic regularities and the scene schema?

5.4 Perceiving Scenes and Objects in Scenes

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

109

Front of the brain

5.  Describe Helmholtz's theory of unconscious inference. What does this
    have to say about inference and perception?
6.  Describe Bayesian inference. Be sure you understand the "sickness"
    example in Figure 5.39 and how Bayesian inference can be applied to
    object perception.
7.  What is predictive coding? Describe an example of how the brain
    might use prediction to perceive a real-world situation.

5.5 Connecting Neural Activity and Object/Scene Perception

LOC

In the last chapter, we discussed how the ventral ("what") pathway of
the brain, extending from the occipital lobe into the temporal lobe, is
involved in recognizing objects. One area that has been isolated within
this pathway in humans is called the lateral occipital complex (LOC).
Figure 5.41 shows the location of the LOC as well as a few other brain
areas that we'll be discussing in this section. Research studies using
brain imag­ ing (see Method, page 31) have found that the LOC is active
when the person views any kind of object---such as an animal, face,
house, or tool---but not when they view a texture, or an object with the
parts scrambled (Malach et al., 1995; Grill-Spector, 2003). Furthermore,
the LOC is activated by objects regardless of their size, orientation,
position, or other basic features. The LOC builds upon the processing
that took place in lower-level visual regions, like V1 where the neurons
responded to simple lines and edges (see Chapter 4, page 69). By the
time the signal proceeds up the ventral pathway and reaches the LOC,
those lines have been put together into a whole object. Importantly,
though, while the LOC appears to play a role in

110

PPA

Objects Faces

EBA

We look around. We see objects arranged in space, which creates a scene.
So far in our discussion of objects and scenes, we have focused on how
perception is determined by aspects of stimuli. We now consider the
neural side of object and scene perception. Recall that in Chapter 4, we
discussed how singlecell recording studies in monkeys have shown that
certain neurons, grouped together into a certain area within the
temporal lobe, can respond to specific complex stimuli, like faces. But
what about in humans? How do our brains respond when we perceive faces,
objects, and scenes?

Brain Responses to Objects and Faces

FFA

Body parts Houses v4

v3 v2 v1

Back of the brain Figure 5.41 A few brain areas involved in different
aspects of object and scene perception, shown here on the underside of
one hemisphere of the brain. These areas are shown relative to visual
areas V1--V4, which we introduced in Chapter 4 (Figure 4.23). LOC =
lateral occipital complex; FFA = fusiform face area; PPA =
parahippocampal place area; EBA = extrastriate body area. (From
Grill-Spector, 2009)

object perception, it does not differentiate between different types of
objects, like faces versus other objects. Next, we'll discuss how more
specific object categories are represented.

The Neural Correlates of Face Perception In a seminal 1997 study, Nancy
Kanwisher and coworkers used fMRI to determine brain activity in
response to pictures of faces and other objects such as household
objects, houses, and hands. When they subtracted the response to the
other objects from the response to the faces, Kanwisher and coworkers
found that activity remained in an area they called the fusiform face
area (FFA), which is located in the fusiform gyrus on the underside of
the brain directly below the inferotemporal (IT) cortex introduced in
Chapter 4 (Figure 5.41). This area is roughly equivalent to the face
areas in the temporal cortex of the monkey. Kanwisher's results, plus
the results of many

Chapter 5  Perceiving Objects and Scenes

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

other experiments, have suggested that the FFA is specialized to respond
to faces (Kanwisher, 2010). Additional evidence of the role of the FFA
in face perception is that damage to this area can cause
prosopagnosia--- difficulty recognizing the faces of familiar people.
Even very familiar faces are affected, so people with prosopagnosia may
not be able to recognize close friends or family members---or even their
own reflection in the mirror---­ although they can easily identify such
people as soon as they hear them speak (Burton et al., 1991; Hecaen &
Angelerques, 1962; Parkin, 1996). So, the FFA seems to play a key role
in face perception. This finding supports a modular view of neural
representation, which, if you recall from Chapter 2 (p. 31), is the idea
that activity in a certain brain area (or module) represents a certain
function. But other research suggests that the FFA is not the only area
involved in face perception. Consider, for example, that when we view a
face, our experience extends beyond simply identifying it ("that's a
face"). We can also respond to the following additional aspects of
faces: (1) emotional aspects ("she is smiling, so she is probably
happy," "looking at his face makes me happy"); (2) where someone is
looking ("she's looking at me"); (3) how parts of the face move ("I can
understand him better by watching his lips move"); (4) how attractive a
face is ("he has a handsome face"); and (5) whether the face is familiar
("I remember her from somewhere"). Faces are complex; they cause many
different reactions, and, as shown in Figure 5.42 and Table 5.1, these
different reactions are associated with activity in many different
places in the brain---a concept consistent with a distributed view of
neural representation.

Evaluation of attractiveness (FL)

Awareness of gaze direction, mouth and face movements (STS)

Initial processing (OC) Familiarity (A, plus other areas)

Emotional reactions (A)

Basic face processing (FFA)

Figure 5.42 Areas of the brain that are activated by different aspects
of faces. OC 5 occipital cortex; FFA 5 fusiform face area; A 5 amygdala;
FL 5 frontal lobe; STS 5 superior temporal sulcus. The dashed line for
the amygdala indicates that it is located inside the brain, below the
cortex.

of bodies, but not by faces or other objects, as shown in ­ igure 5.43
(Downing et al., 2001; Grill-Spector & Weiner, F

2014). Still other research suggests that categories such as animate
versus inanimate objects might activate specific areas as well (Konkle &
Caramazza, 2013; Martin, 2007; Martin et al., 1996). Even though some
specialized brain areas have been identified, it is unrealistic to think
that we would have a distinct brain area to represent every category of
object that we encounter. What's more likely is that neural
representation of objects is distributed across brain areas, as we saw
for faces (Figure 5.42). Evidence supporting this idea is provided by an
fMRI experiment by Alex Huth and coworkers (2012) in which participants
viewed 2 hours of film clips while in a brain scanNeural Representation
of Other Categories of ner. To analyze how individual brain areas were
activated by Objects How are other (non-face) categories of objects
repdifferent objects and actions in the films, Huth created a list of
resented in the brain? In addition to the FFA, which contains 1,705
different objects and action categories and determined neurons that are
activated by faces, another specialized area which categories were
present in each film scene. in the temporal cortex has been identified.
The extrastriate Figure 5.44 shows four movie clips and the categories
body area (EBA) is activated by pictures of bodies and parts (labels)
associated with them. By determining how different brain areas were
activated by each movie clip and then analyzing his reTable 5.1 Brain
Areas Activated by Different Aspects of Faces sults using a complex
statistical procedure, Huth was able to determine AREA OF BRAIN FUNCTION
what kinds of stimuli each brain Occipital cortex (OC) Initial
processing area responded to. For example, one Fusiform face area (FFA)
Basic face processing area responded well when streets, Amygdala (A)
Emotional reactions (face expressions and observer's buildings, roads,
interiors, and veemotional reactions) hicles were present. Familiarity
(familiar faces cause more activation in Figure 5.45 shows some of the
amygdala and other areas associated with emotions) categories that cause
different areas Frontal lobe (FL) Evaluation of attractiveness across
the surface of the brain to reSuperior temporal sulcus (STS) Gaze
direction spond. Objects and actions similar Mouth movements to each
other are located near each General face movements other in the brain.
The reason there Based on Calder et al., 2007; Gobbini & Haxby, 2007;
Grill-Spector et al., 2004; Ishai et al., 2004; Natu & O'Toole, 2011;
Pitcher et al., 2011; Puce et al., 1998; are two areas for humans and
two for Winston et al., 2007.

5.5 Connecting Neural Activity and Object/Scene Perception

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

111

Figure 5.44 Four frames from the movies viewed by participants in Huth
et al.'s (2012) experiment. The words on the right indicate categories
that appear in the frames (n 5 noun; v 5 verb).

Movie Clip

(Huth et al., 2012)

Figure 5.45 The results of Huth et al.'s (2012) experiment, showing
locations on the brain where the indicated categories are most likely to
activate the brain. Colors indicate areas that respond similarly. For
example, both areas marked "Animals" are yellow. (Courtesy of Alex Huth)

Labels

© Cengage 2021

nonpreferred

EBA

preferred

Figure 5.43 The extrastriate body area (EBA) is activated by bodies
(top), but not by other stimuli (bottom). (Kanwisher, 2003)

Movie Clip

Labels

butte.n desert.n sky.n cloud.n brush.n

city.n expressway.n skyscraper.n traffic.n sky.n

woman.n talk.v gesticulate.v book.n

bison.n walk.v grass.n stream.n

Athletes Talking

Animals

Landscape

Buildings Indoor scenes

Talking

Humans Animals

Humans

112

Chapter 5  Perceiving Objects and Scenes

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

animals is that each area represents different features related to
humans or animals. For example, the area labeled "human" at the bottom
of the brain (which is actually on the underside of the brain)
corresponds to the FFA, which responds to all aspects of faces. The
human area higher on the brain responds specifically to facial
expressions. The conclusion from all this neuroimaging research is while
some research suggests that there may be different modules for different
functions, representation often goes beyond those modules, so that a
combination of modular and distributed representation appears to
underlie our perception of objects and faces.

Brain Responses to Scenes

nonpreferred

PPA

preferred

Not long after the discovery of the role of the FFA in face perception,
Russell Epstein and Nancy Kanwisher identified another specialized area
in the temporal lobe---one that responds to places, but not objects or
faces. They called this region the parahippocampal place area (PPA),
which can be seen in ­Figure 5.46. Using fMRI, Epstein and Kanwisher
showed that the PPA was activated by pictures depicting indoor and
outdoor scenes (Aguirre et al., 1998; Epstein et al., 1999; Epstein &
Kanwisher, 1998). Apparently what is important for this area is
information about spatial layout, because increased activation occurs
both to empty rooms and to rooms that are completely furnished
(Kanwisher, 2003). But what function does the PPA actually serve? Some
researchers have asked whether the PPA really is a "place" area, as the
name implies. Some of these researchers prefer the term parahippocampal
cortex (PHC), which identifies the location of the area in the brain
without making a statement about its function. One hypothesis for what
the PPA/PHC does is Russell Epstein's (2008) spatial layout hypothesis,
which proposes that the PPA/PHC responds to the surface geometry or
geometric layout of a scene. This proposal is based partially on the
fact that scenes cause larger responses than buildings. But Epstein
doesn't think buildings are totally irrelevant, because the ­response to
buildings is larger than for objects in general. Epstein explains this
by stating that buildings are "partial scenes" that are associated with
space, and concludes that the function of the PPA/PHC is to respond to
qualities of objects that are relevant to navigation through a scene or
locating a place

Figure 5.46 The parahippocampal place area (PPA) is activated by places
(top) but not by other stimuli (bottom). (Kanwisher, 2003)

(also see Troiani et al., 2014). When we discuss navigation through
scenes in more detail in Chapter 7, we will consider more evidence
linking the parahippocampal cortex to navigation and will see that other
nearby brain areas are also involved in ­navigation. The spatial layout
hypothesis is just one proposed function of the PPA/PHC. Others have
suggested that the role of the PPA/PHC is to represent three-dimensional
space more generally, even if there is no scene (Mullally & Maguire,
2011). This hypothesis is based on fMRI studies showing that the PPA/
PHC is activated not only by full scenes, but also by objects that
create a sense of surroundings and by images that create an impression
of three-dimensional space---for instance, like the foreground of the
scene in Figure 5.26 (Zeidman et al., 2012). Still other researchers
have proposed that the function of the PPA/PHC is to represent
contextual relations---how related objects are organized in space, such
as items that belong in a kitchen (Aminoff et al., 2013). Others have
presented evidence that the PPA/PHC is subdivided into different areas
that may have different functions, such as one subregion for the visual
analysis of a scene and another subregion for connecting that visual
information to a memory of the scene (Baldassano et al., 2016; Rémy et
al., 2014). Although discussion of the function of the PPA/PHC is
continuing among researchers, it is generally agreed that it is
important for perceiving space, whether the space is defined by single
objects or the more extensive areas associated with scenes. Importantly,
like we saw with the FFA and face perception, the PPA is not the only
area involved in scene perception. In fact, there are at least two other
areas in the occipital and temporal lobes that appear to respond
selectively to scenes (Epstein & Baker, 2019), and studies assessing
functional connectivity (see Methods box, page 33) have even shown that
these areas are activated along with the PPA (Baldassano et al., 2016).
This provides further evidence for distributed representation---that
when we view a scene, multiple interconnected brain areas are involved.

The Relationship Between Perception and Brain Activity As you walk
around in your daily life, such as on campus to get to class, you likely
come across many faces and buildings. However, you might not be aware
that you encountered all of those faces and buildings. For instance, if
you're focused on navigating to the building ahead, you might miss your
friend waving at you right along the way. In this situation, is your FFA
still activated in response to your friend's face? Or must you switch
your perception from the building to your friend's face in order for
your FFA to respond? This relationship between perception (like
perceiving the face or the building) and brain activity has been
explored using a technique in which different images are presented to
the left and right eyes. In normal everyday perception, our two eyes
receive slightly different images because the eyes are in two slightly
5.5 Connecting Neural Activity and Object/Scene Perception

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

113

while Tong used fMRI to measure activity in the participant's PPA and
FFA. When participants were perceiving the house, activity increased in
the PPA (and decreased in the FFA); when they were perceiving the face,
activity increased in the FFA (and decreased in the PPA). Even though
the images on the retina remained the same throughout the experiment,
activity in the brain changed depending on what the person was
experiencing. This experiment and others like it generated a great deal
of excitement among brain researchers because they measured brain
activation and perception simultaneously and demonstrated a dynamic
relationship between perception and brain activity in which changes in
perception and changes in brain activity mirrored each other.

Frank Tong

Neural Mind Reading

Figure 5.47 Observers in Tong et al.'s (1998) experiment viewed the
overlapping red house and green face through red--green glasses, so the
house image was presented to the right eye and the face image to the
left eye. Because of binocular rivalry, the observers' perception
alternated back and forth between the face and the house. When the
observers perceived the house, activity occurred in the parahippocampal
place area (PPA) in the left and right hemispheres (red ellipses). When
observers perceived the face, activity occurred in the fusiform face
area (FFA) in the left hemisphere (green ellipse). (From Tong et al.,
1998)

different locations. These two images, however, are similar enough that
the brain can combine them into a single perception. But if the two eyes
receive totally different images, the brain can't combine the two images
and a condition called binocular rivalry occurs, in which the observer
perceives either the left-eye image or the right-eye image, but not both
at the same time.1 Frank Tong and coworkers (1998) used binocular
rivalry to connect perception and neural responding by presenting a
picture of a person's face to one eye and a picture of a house to the
other eye and having observers view the pictures through colored
glasses, as shown in Figure 5.47. The colored glasses caused the face to
be presented to the left eye and the house to the right eye. Because
each eye received a different image, binocular rivalry occurred, so
while the images remained the same on the retina, observers perceived
just the face or just the house, and these perceptions alternated back
and forth every few seconds. The participants pushed one button when
they perceived the house and another button when they perceived the
face, 1

This all-or-none effect of rivalry, in which one image is seen at a time
(the house or the face), occurs most reliably when the image presented
to each eye covers a small area of the visual field. When larger images
are presented, observers sometimes see parts of the two images at the
same time. In the experiment described here, observers generally saw
either the house or the face, alternating back and forth.

114

We've presented numerous examples of experiments in which stimuli---like
objects, faces, and scenes---are presented, and the brain response is
measured. Some researchers have reversed the process, measuring the
person's brain response and determining the stimuli that generated that
response. They achieve this using a procedure we will call neural mind
reading.

METHOD

Neural Mind Reading

Neural mind reading refers to using a neural response, usually brain
activation measured by fMRI, to determine what a person is perceiving or
thinking. As we saw in Chapter 2 (page 31), fMRI measures the activity
in voxels, which are small cubeshaped volumes in the brain about 2 or 3
mm on a side. In neural mind reading, what's important is the pattern of
activation across multiple voxels, which is often measured using a
technique called multivoxel pattern analysis (MVPA). The pattern of
voxels activated depends on the task and the nature of the stimulus
being perceived. For example, Figure 5.48a shows eight voxels that are
activated by a black-and-white grating stimulus slanted to the right.
Viewing a different orientation (for instance, slanted to the left)
activates a different pattern of voxels. Figure 5.49 illustrates the
basic procedure for neural mind reading using these oriented gratings as
an example (Kamitani & Tong, 2005). First, the relationship between the
stimulus and the voxel pattern is determined by measuring the brain's
response to a number of different stimuli (in this case, different
orientations) using fMRI (Figure 5.49a). We'll call this the calibration
phase. Then, these data are used to create a decoder, which is a
computer program that can predict the most likely stimulus based on the
voxel activation patterns observed in the calibration phase (Figure
5.49b). Finally, in the testing phase, the performance of the decoder is
tested by measuring brain activation as a person is looking at different
stimuli, as before, but this time using the decoder to predict the
stimulus the person is perceiving (Figure 5.49c). If this works, it
should be possible to predict what stimulus a person is looking at based
on his or her brain activation alone.

Chapter 5  Perceiving Objects and Scenes

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

fMRI voxels

(a) Stimulus

Prediction

(b) 

Figure 5.48 (a) Viewing an oriented grating like the one on the left
causes a pattern of activation of voxels. The cubes in the brain
represent the response of eight voxels. The differences in shading
represent the pattern of activation of the orientation being viewed. (b)
Results of Kamitani and Tong's (2005) experiment for two orientations.
The gratings are the stimuli presented to the observer. The line is the
orientation predicted by the decoder. The decoder was able to accurately
predict the orientations of all eight of the gratings tested. (From
Kamitani & Tong, 2005) Measure voxel activation patterns to different
orientations.

(a) 

Create decoder based on measured data. Decoder

(b) 

Decoder

Activation pattern

Use decoder to predict which orientation the person is looking at.

(c) 

Figure 5.49 Principle behind neural mind reading. (a) In the calibration
phase, the participant looks at different orientations, and fMRI is used
to determine voxel activation patterns for each orientation. (b) A
decoder is created based on the voxel patterns collected in (a). (c) In
the testing phase, a participant looks at an orientation, and the
decoder analyzes the voxel pattern recorded from the participant's
visual cortex. Based on this voxel pattern, the decoder predicts the
orientation that the participant is observing.

When Yukiyasu Kamitani and Frank Tong (2005) used the procedure above,
they were able to predict, based on the pattern of activity of 400
voxels in the visual cortex, the orientations of eight different
gratings that a person was observing (Figure 5.48b). Creating a decoder
that can determine from brain activity what orientation a person is
perceiving was an impressive achievement. But what about complex stimuli
like objects and scenes? Expanding our stimulus set from eight grating
orientations to every possible object and scene in the environment is
quite a jump! But amazingly, recent work toward creating such complex
decoders has had some success. One such study was conducted by Shinji
Nishimoto and coworkers (2011), whose goal was to see if they could make
a decoder that was able to reconstruct, or recreate, what the
participant was seeing in a movie using just their brain activation.
First, in the calibration phase, they showed participants over 7,000
seconds of movie clips in the fMRI scanner while recording the patterns
of voxel activation in the visual cortex. The decoder was given these
voxel activation patterns and trained to learn how the participant's
brain typically responds to different visual stimuli. As one of the
authors on this study, Jack Gallant, once described, it's almost like
the computer is building a "dictionary" that translates between the
stimuli in the movie clips and the participant's brain responses to
those stimuli (Ross, 2011). Then in the testing phase, the participants
saw new movie clips that they had not seen during the calibration phase.
The goal was to determine whether the decoder could use
"dictionary"---the participant's activation patterns acquired during the
calibration phase---to predict what they were seeing on a
second-by-second basis in these new movie clips. In other words, the
decoder would read the brain activation pattern acquired during the
testing phase, and then "look up" that pattern in the dictionary that it
created during the calibration phase in order to predict what stimulus
the person was seeing. The interesting part about this study was that
not only could the decoder look up the stimulus that most closely
matched the brain activation (say, that the participant was viewing a
"person"), it could also create a reconstruction of what that stimulus
might have looked like (the "person" was on the left side of the screen
and had dark hair). To make this reconstruction, the decoder consulted a
database of 18 million seconds of movie clips collected from the
Internet (clips not used during calibration or testing) and was
programmed to pick the clips that most closely matched the stimulus that
it had "looked up" in the brain activation pattern dictionary (i.e.,
clips showing a person on the left side of the screen with dark hair).
The decoder then took an average of all of those matching clips to
produce a visual reconstruction, or a "best guess," of what the
participant was most likely seeing during each second of the movie.
Altogether, an astounding computational feat! So, could the decoder
actually read participants' minds and guess what they were seeing in the
movie clips during the testing phase? The left column in Figure 5.50
shows images from film clips that the participant observed. The right
column 5.5 Connecting Neural Activity and Object/Scene Perception

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

115

(a) Presented Clip

larger image databases will result in matches that are much closer to
the target. Accuracy will also increase as we learn more about how the
neural activity of various areas of the brain represents the
characteristics of objects and scenes. Of course, the ultimate decoder
won't need a calibration phase or to compare its output to huge image
databases. It will just analyze the voxel activation patterns and
recreate the image of the scene. Presently, there is only one "decoder"
that has achieved this, and that is your own brain! Although it is worth
noting that your brain does make use of a "database" of information
about the environment, as we know from the role of regularities of the
environment in perceiving scenes. This ultimate decoder is still far
from being achieved in the laboratory. However, the decoders that
presently exist are amazing achievements, which only recently might have
been classified as "science fiction."

(b) Clip reconstructed from brain activity

SOMETHING TO CONSIDER:

Jack Gallant

The Puzzle of Faces

Figure 5.50 Results of a neural mind reading experiment. Left column:
Images from film clips. Right column: Image created by the computer.
(Nishimoto et al., 2011)

shows the computer's best guess of what the participant was seeing,
based on their brain activation. Although some fine detail is missing,
you can see that overall, the decoder did a pretty good job! It could
determine that the participant was looking at a face, for instance,
versus some abstract shape. Nishimoto and coworkers' study provides some
strong evidence that neural mind reading is becoming possible, and other
recent studies continue to confirm this in vision and even in other
senses, like hearing (Formisano et al., 2008; Huth et al., 2016). What's
still limiting about the current mind reading methods is that there
needs to be a "calibration" phase; in other words, researchers first
need to present the stimulus to determine the brain activation pattern
in order to use the brain activation pattern to determine the stimulus.
So, it's not that decoders can just read anyone's mind; they need to
first see how someone's brain responds to certain inputs. This means
that the stimuli that the researchers choose as the inputs (and
potential outputs) is also limiting. Eventually, though, much

116

Having described perceptual organization and how we perceive objects and
scenes, we now focus again on one specific type of object: faces. We
focus on faces because faces are pervasive in the environment, and they
are important sources of information. Faces establish a person's
identity, which is important for social interactions (who is the person
who just said hello to me?) and for security surveillance (checking
people as they pass through airport security). Faces also provide
information about a person's mood and where the person is looking, and
can elicit evaluative judgments (the person seems unfriendly, the person
is attractive, and so on). Faces have also been the topic of a great
deal of research. Some of this research argues that there is something
special about faces. For example, when people are asked to look as
rapidly as possible at a picture of either a face, an animal, or a
vehicle, faces elicit the fastest eye movements, occurring within 138
ms, compared to 170 ms for animals and 188 ms for vehicles (Crouzet et
al., 2010). Results such as these have led to the suggestion that faces
have special status that allows them to be processed more efficiently
and faster than other classes of objects (Crouzet et al., 2010; Farah et
al., 1998). One research finding that had been repeated many times is
that inverting a picture of a face (turning it upside down) makes it
more difficult to identify the face or to tell if two inverted faces are
the same or different (Busigny & Rossion, 2010). Similar effects occur
for other objects, such as cars, but the effect is much smaller (Figure
5.51). Because inverting a face makes it more difficult to process
configurational information---the relationship between features such as
the eyes, nose, and mouth---the inversion effect has been interpreted as
providing evidence that faces are processed

Chapter 5  Perceiving Objects and Scenes

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Car

Face Upright Inverted % of correct responses

Upright

Inverted

100 90 80 70 60 50

Car

Face Objects

(a) 
(b) 

Figure 5.51 (a) Stimuli from Busigny and Rossion's (2010) experiment in
which participants were presented with a front view of a car or a face
and were asked to pick the three-quarter view that was the same car or
face. For example, the car on the right in the upper panel is the same
car as the one shown in front-view above. (b) Performance for upright
cars and faces (blue bars) and inverted cars and faces (orange bars).
Notice that inverting the cars has little or no effect on performance
but that inverting faces causes performance to decrease from 89 percent
to 73 percent. (From Busigny & Rossion, 2010)

response in the FFA, can be explained by the fact that we have become
"experts" in perceiving faces because we've been exposed to them for our
entire lives. In support of the expertise hypothesis, Isabel Gauthier
and coworkers (1999) used fMRI to determine the FFA response to faces
and to objects called "Greebles"---families of computergenerated
"beings" that all have the same basic configuration but differ in the
shapes of their parts (Figure 5.52a). Initially, the participants were
shown both human faces and Greebles. The results for this part of the
experiment, shown by the left pair of bars in Figure 5.52b, indicate
that the FFA neurons responded poorly to the Greebles but well to the
faces. Greebles Faces

FFA response

holistically (Freire et al., 2000). Thus, while all faces contain the
same basic features---two eyes, a nose, and a mouth---our ability to
distinguish thousands of different faces seems to be based on our
ability to detect the configuration of these features--- how they are
arranged relative to each other on the face. This research suggests that
faces are special because they are processed differently (more
holistically) than other objects. Faces may also be special because, as
we've discussed in this and previous chapters, there are neurons that
respond selectively to faces, and there are specialized places in the
brain that are rich in these neurons. One such area, which we described
earlier in this chapter, is the area in the fusiform gyrus that Nancy
Kanwisher dubbed the fusiform face area (FFA), because it seemed to
respond selectively to faces (Kanwisher et al., 1997). As it turns out,
however, later research has shown that there are neurons in the FFA that
respond to objects other than faces (Haxby et al., 2001), but the name
fusiform face area has stuck, and it is likely that even if the FFA may
not respond exclusively to faces, it plays an important role in face
perception. Of course, there's more to the physiology of face perception
than the FFA; as we saw when we described the neural correlates of face
perception on page 110, numerous areas in addition to the FFA are
involved in face perception (see Figure 5.42). Faces are therefore
special both because of the role they play in our environment and
because of the widespread activity they trigger in the brain. But we're
not finished with faces yet, because faces have been at the center of
one of the more interesting controversies in perception, which involves
the expertise hypothesis, the idea that our proficiency in perceiving
faces, and the large face

(a) 
(b) 

Before training

After training

Figure 5.52 (a) Greeble stimuli used by Gauthier. Participants were
trained to name each different Greeble. (b) Brain responses to Greebles
and faces before and after Greeble training. (From Gauthier et al.,
1999)

Something to Consider: The Puzzle of Faces

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

117

The participants were then trained in "Greeble recognition" for 7 hours
over a 4-day period. After the training sessions, participants had
become "Greeble experts," as indicated by their ability to rapidly
identify many different Greebles by the names they had learned during
the training. The right pair of bars in Figure 5.52b shows that after
the training, the FFA responded about as well to Greebles as to faces.
Based on this result, Gauthier suggested that the FFA might not be a
"face area" after all, but may instead represent any object with which
the person is an expert (which happens to include faces). In fact,
Gauthier has also shown that the FFA of people who are experts in
recognizing cars or birds responds well not only to human faces but to
cars (for the car experts) and to birds (for the bird experts; Gauthier
et al., 2000). Similarly, another study showed that viewing

the positions of chess pieces on a chess board causes a larger
activation of the FFA in chess experts than in nonexperts (Bilalić et
al., 2011). What does all this mean? All we can say for sure is that the
question of whether faces are inherently special, or whether their
"specialness" is simply due to our extensive experience with them, is
still controversial. Some researchers agree that experience is important
for establishing the FFA as a module for faces (Bukach et al., 2006;
Tanaka & Curran, 2001); others argue that the FFA's role as a face area
is based largely on built-in wiring that doesn't depend on experience
(Kanwisher, 2010). This debate about face specificity and the FFA
demonstrates that even with all of the research on face and object
perception that has occurred over the past 30 years, controversies and
uncertainties still exist.

DEVELOPMENTAL DIMENSION Infant Face Perception What do newborns and
young infants see? In the Developmental Dimension in Chapter 3 (p. 60),
we saw that infants have poor detail vision compared to adults but that
the ability to see details increases rapidly over the first year of
life. We should not conclude from young infants' poor detail vision,
however, that they can see nothing at all. At very close distances, a
young infant can detect some gross features, as indicated in Figure
5.53, which simulates how infants perceive a face from a distance of
about 2 feet. At birth, the contrast perceived between light and dark
areas is so low that it is difﬁcult to

(b) 4-week-old

(c) 8-week-old

(d) 3-month-old

(e) 6-month-old

(f) Adult

Bruce Goldstein

(a) Newborn

Figure 5.53 Simulations of perceptions of a mother located 124 inches
from an observer, as seen by newborns and various ages. (Simulations
courtesy of Alex Wade)

118

determine it is a face, but it is possible to see very high contrast
areas. By 8 weeks, however, the infant's ability to perceive the
contrast between light and dark perception has improved so that the
image looks clearly facelike. By 3 to 4 months, infants can tell the
difference between faces that look happy and those that show surprise,
anger, or are neutral (LaBarbera et al., 1976; Young-Browne et al.,
1977) and can also tell the difference between a cat and a dog (Eimas &
Quinn, 1994). Human faces are among the most important stimuli in an
infant's environment. As a newborn or young infant stares up from the
crib, numerous faces of interested adults appear in the infant's ﬁeld of
view. The face that the infant sees most frequently is usually the
mother's, and there is evidence that young infants can recognize their
mother's face shortly after they are born. Using preferential looking in
which 2-day-old infants were given a choice between their mother's face
and a stranger's, Ian Bushnell and coworkers (1989) found that newborns
looked at the mother's face about 63 percent of the time. This result is
above the 50 percent chance level, so Bushnell concluded that the
2-day-olds could recognize their mother's face. To determine what
information the infants might be using to recognize the mother's face,
Olivier Pascalis and coworkers (1995) showed that when the mother and
the stranger wore pink scarves that covered their hairline, the
preference for the mother disappeared. The high-contrast border between
the mother's dark hairline and light forehead apparently provides
important information about the mother's physical characteristics that
infants use to recognize the mother (see Bartrip et al., 2001, for
another experiment that shows this). In an experiment that tested
newborns within an hour after they were born, John Morton and Mark
Johnson (1991) presented stimuli (see bottom of Figure 5.54) to the
newborns and then moved the stimuli to the left and right. As they did
this, they videotaped the infant's face. Later, scorers who were

Chapter 5  Perceiving Objects and Scenes

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Average rotation of eyes

50 40 30 20 10 0

Figure 5.54 The magnitude of infants' eye movements in response to
movement of each stimulus. The average rotation of the infants' eyes was
greater for the facelike stimulus than for the scrambled-face stimulus
or the blank stimulus. (Adapted from Morton & Johnson, 1991)

unaware of which stimulus had been presented viewed the tapes and noted
whether the infant turned its head or eyes to follow the moving
stimulus. The results in the graph in Figure 5.54 show that the newborns
looked at the moving face more than at the other moving stimuli, which
led Morton and Johnson to propose that infants are born with some
information about the structure of faces. In support of this proposal, a
neuroimaging study by Teresa Farroni and coworkers (2013) found that in
1to 5-day old newborns, moving face stimuli---such as a video of an
adult playing the game "peek-a-boo"---elicited more activity in visual
brain areas than moving non-face stimuli, such as a video of cogs and
pistons. This neuroimaging study adds to the behavioral evidence
suggesting an innate predisposition for perceiving faces. But there is
also evidence for a role of experience in infant face perception. Ian
Bushnell (2001) observed newborns over the ﬁrst 3 days of life to
determine whether there was a relationship between their looking
behavior and the amount of time they were with their mother. He found
that at 3 days of age, when the infants were given a choice between
looking at a stranger's face or their mother's face, the infants who had
been exposed to their mother longer were more likely to prefer her over
the stranger. The two infants with the lowest exposure to the mother (an
average of 1.5 hours) divided their looking evenly between the mother
and stranger, but the two infants with the longest exposure (an average
of 7.5 hours) looked at the mother 68 percent of the time. Analyzing the
results from all of the infants led Bushnell to conclude that face
perception emerges very rapidly after birth, but that experience in
looking at faces does have an effect.

Although the infant's ability to recognize faces develops rapidly over
the first few months, these impressive gains are only a starting point,
because even though 3- to 4-month-old infants can recognize some facial
expressions, their ability to identify faces doesn't reach adult levels
until adolescence or early adulthood (Grill-Spector et al., 2008;
Mondloch et al., 2003, 2004). What about physiology? A recent study
measured functional connectivity in the brains of infants who were 27
days old. Functional connectivity occurs when the neural activity in two
different areas of the brain is correlated (see page 34). Frederik Kamps
and coworkers (2020) measured functional connectivity in sleeping
infants using the resting state fMRI method (see Method, Chapter 2, page
34) and identified a functional connection between the visual cortex,
where information about faces first reaches the cortex, and the fusiform
face area---two areas that are not yet well developed in infants, but
that are associated with face perception in adults. Kamps concludes from
this result that "connectivity precedes function" in the developing
cortex. According to this idea, the connection between the visual cortex
and what will become the FFA is prewired, setting the stage for further
development of the infant's face perception abilities. But the
development of the physiology of face perception stretches out over many
years. Figure 5.55 shows that the FFA, indicated by red, is small in an
8-year-old child compared to the FFA in an adult (Golarai et al., 2007;
Grill-Spector et al., 2008). In contrast, the PPA, indicated by green,
is similar in the 8-year-old child and the adult. It has been suggested
that this slow development of the specialized face area may be related
to the maturation of the ability to recognize faces and their emotions,
and especially the ability to perceive the overall configuration of
facial ­features (Scherf et al., 2007). Thus, the specialness of faces
extends from birth, when newborns can react to some aspects of faces, to
late adolescence, when the true complexity of our responses to faces
finally emerges.

Figure 5.55 Face (red), place (green), and object (blue) selective
activations for one representative 8-year-old and one representative
adult. The place and object areas are well developed in the child, but
the face area is small compared to the adult. (From Grill-Spector et
al., 2008)

Something to Consider: The Puzzle of Faces

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

119

TEST YOuRSELF 5.3 1. What is the role of the lateral occipital complex
in perception?

response, measured using fMRI, to predict what a person is seeing.

2.  Describe the evidence suggesting that the FFA is involved in
    perceiving faces. Be sure your answer includes a description of
    prosopagnosia.

3.  Describe two experiments showing that neural mind reading is
    possible. What are some limitations of these experiments?

4.  Discuss how other (non-face) categories of objects are represented
    in the brain, including the fMRI study by Huth and coworkers. What
    does this say about modular versus distributed representation?

5.  Why do some researchers believe that faces are "special"? What do
    the eye movement experiments and the face inversion experiments
    show?

6.  What is the role of the PPA/PHC in scene perception? Describe the
    function of the PPA/PHC according to the spatial layout hypothesis.
    What are some other proposed functions of this area?

7.  Describe Tong's experiment in which he presented a picture of a
    house to one eye and a picture of a face to the other eye. What did
    the results indicate?

8.  What is multivoxel pattern analysis? Describe how "decoders" have
    enabled researchers to use the brain's

THINK ABOUT IT 1. Reacting to the announcement of the Google driverless
car, Harry says, "Well, we've finally shown that computers can perceive
as well as people." How would you respond to this statement? 2. Vecera
showed that regions in the lower part of a stimulus are more likely to
be perceived as figure (p. 120). How does this result relate to the idea
that our visual system is tuned to regularities in the environment? 3.
When you first look at Figure 5.56, do you notice anything funny about
the walkers' legs? Do they initially ap-

9.  What are some areas in addition to the fusiform face area that are
    involved in perceiving faces?
10. What is the expertise hypothesis? Describe the fMRI evidence
    supporting this idea. Describe the experiment which studied faces by
    measuring functional connectivity.
11. What is the evidence that newborns and young infants can perceive
    faces? What is the evidence that perceiving the full complexity of
    faces does not occur until late adolescence or adulthood?

pear tangled? What is it about this picture that makes the legs appear
to be perceptually organized in that way? Can you relate your perception
to any of the laws of perceptual organization? To cognitive processes
based on assumptions or past experience? (pp. 96, 105) 4. Continued
research on neural mind reading has explored potential applications of
decoding one's neural activity. For instance, MVPA has been shown to be
able to determine whether someone is telling the truth or lying, just
based on their typical pattern of brain activation associated with each
(Davatzikos et al., 2005; Jiang et al., 2015). Can you think of any
other real-world applications of neural mind reading? What (if any) are
the ethical implications of this intriguing technique? Answers for
Figure 5.7

Figure 5.56 Is there something wrong with these people's legs? (Or is it
just a problem in perception?) 120

© Cengage 2021

Charles Feil

Will Smith, Taylor Swift, Barak Obama, Hillary Clinton, Jackie Chan, Ben
Affleck, Oprah Winfrey

Figure 5.57 The Dalmatian in Figure 5.10.

Chapter 5  Perceiving Objects and Scenes

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

KEY TERMS Apparent movement (p. 95) Bayesian inference (p. 108)
Binocular rivalry (p. 114) Border ownership (p. 100) Decoder (p. 114)
Expertise hypothesis (p. 117) Extrastriate body area (EBA) (p. 111)
Figural cues (p. 100) Figure (p. 99) Figure--ground segregation (p. 99)
Fusiform face area (FFA) (p. 110) Geons (p. 102) Gestalt psychologist
(p. 94) Gist of a scene (p. 103) Global image features (p. 104) Ground
(p. 99) Grouping (p. 94) Illusory contour (p. 96) Inverse projection
problem (p. 92) Lateral occipital complex (LOC) (p. 110)
Light-from-above assumption (p. 105)

Likelihood (p. 108) Likelihood principle (Helmholtz) (p. 108) Multivoxel
pattern analysis (MVPA) (p. 114) Neural mind reading (p. 114) Object
recognition (p. 90) Parahippocampal place area (PPA) (p. 113) Perceptual
organization (p. 94) Persistence of vision (p. 104) Physical
regularities (p. 105) Pragnanz (p. 97) Prediction (p. 108) Predictive
coding (p. 109) Principle of common fate (p. 98) Principle of common
region (p. 98) Principle of good continuation (p. 96) Principle of good
figure (p. 97) Principle of pragnanz (p. 97) Principle of proximity
(nearness) (p. 98) Principle of similarity (p. 98)

Principle of simplicity (p. 97) Principle of uniform connectedness
(p. 98) Principles of perceptual organization (p. 96) Prior (p. 108)
Prior probability (p. 108) Prosopagnosia (p. 111) Recognition by
components (RBC) theory (p. 102) Regularities in the environment
(p. 105) Reversible figure--ground (p. 99) Scene (p. 103) Scene schema
(p. 106) Segregation (p. 94) Semantic regularities (p. 106) Spatial
layout hypothesis (p. 113) Structuralism (p. 94) Unconscious inference
(p. 108) Viewpoint invariance (p. 94) Visual masking stimulus (p. 104)

Key Terms

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

121

PNC Park, with the city of Pittsburgh in the background. The red lines
and yellow fixation dots show where one person looked when viewing this
picture for three seconds. This person first looked just above the right
field bleachers and then scanned the ball game. Eye movement records
such as these indicate that people pay attention to some things and
ignore others. Photo by Bruce Goldstein; Eye movement record courtesy of
John Henderson

Learning Objectives After studying this chapter, you will be able to ...
■■ Describe early attention experiments using the techniques of

dichotic listening, precueing, and visual search. ■■ Describe how we
scan a scene by moving our eyes, and why

these eye movements don't cause us to perceive the scene as smeared. ■■
Describe four different factors that determine where we look

and the experiments that support each factor. ■■ Describe how attention
affects physiological responding.

■■ Understand what happens when we don't attend and when

distraction disrupts attention. ■■ Describe how disorders of attention
teach us about basic

mechanisms of attention. ■■ Understand the connection between
meditation, attention,

and mind-wandering. ■■ Describe how head-mounted eye tracking has been
used to

study how infants learn the names of objects.

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Chapter 6

Visual Attention

Chapter Contents 6.1 What Is Attention? 6.2 The Diversity of Attention
Research Attention to an Auditory Message: Cherry and Broadbent's
Selective Listening Experiments Attention to a Location in Space:
Michael Posner's Precueing Experiment Method: Precueing

Attention as a Mechanism for Binding Together an Object's Features: Anne
Treisman's Feature Integration Theory Demonstration: Visual Search

6.3 What Happens When We Scan a Scene by Moving Our Eyes? Scanning a
Scene With Eye Movements How Does the Brain Deal With What Happens When
the Eyes Move?

6.4 Things That Influence Visual Scanning

6.7 What Happens When We Don't Attend?

Visual Salience

Demonstration: Change Detection

Demonstration: Attentional

The Observer's Interests and Goals Scene Schemas Task Demands TEST
YOURSELF 6.1

6.5 The Benefits of Attention Attention Speeds Responding Attention
Influences Appearance

6.6 The Physiology of Attention Attention to Objects Increases Activity
in Specific Areas of the Brain Attention to Locations Increases Activity
in Specific Areas of the Brain Attention Shifts Receptive Fields

Some Questions We Will Consider: ■■ Why do we pay attention to some
parts of a scene but not

to others? (p. 130) ■■ Does paying attention to an object make the
object

"stand out"? (p. 134) ■■ How does distraction affect driving? (p. 138)
■■ How does damage to the brain affect where a person at-

tends to in space? (p. 141)

I

6.8 Distraction by Smartphones

Capture

n Chapter 5 we saw that our perception of objects and scenes can't be
explained by considering only the image on the retina. Although the
image on the retina is important, we also need to consider explanations,
such as Helmholtz's

Smartphone Distractions While Driving Distractions Beyond Driving

6.9 Disorders of Attention: Spatial Neglect and Extinction SOMETHING TO
CONSIDER: Focusing

Attention by Meditating

DEVELOPMENTAL DIMENSION: Infant Attention and Learning Object Names
Method: Head-Mounted Eye

Tracking

TEST YOURSELF 6.2 THINK ABOUT IT

unconscious inference and predictive coding, that involve mental
processing to understand how the information ­provided by the image on
the retina becomes transformed into perception. The idea that mental
processing plays an important role in determining our perceptions is a
story that continues throughout this book. This chapter continues this
story by describing how we pay attention to certain things and ignore
others, and what that means for visual processing. The idea of paying
attention to some things while ignoring others was described in the 19th
century by William James (1842--1910), the first professor of psychology
at Harvard. James relied not on the results of experiments but rather on
his own personal observations when making statements such

123

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

as the following description of attention, from his 1890 textbook
Principles of Psychology: Millions of items ... are present to my senses
which never properly enter my experience. Why? Because they have no
interest for me. My experience is what I agree to attend to ... Everyone
knows what attention is. It is the taking possession by the mind, in
clear and vivid form, of one out of what seem several simultaneously
possible objects or trains of thought ... It implies withdrawal from
some things in order to deal effectively with others. Thus, according to
James, we focus on some things to the exclusion of others. As you walk
down the street, the things you pay attention to---a classmate you
recognize, the "Don't Walk" sign at a busy intersection, the fact that
just about everyone except you seems to be carrying an umbrella---stand
out more than many other things in the environment. The reason you are
paying attention to those things is that saying hello to your friend,
not crossing the street against the light, and your concern that it
might rain later in the day are all important to you. But there is also
another reason for paying attention to some things and ignoring others.
Your perceptual system has a limited capacity for processing information
(Carrasco, 2011; Chun et al., 2011). Thus, to prevent overloading the
system and therefore not processing anything well, the visual system, in
James's words, "withdraws from some things in order to deal more
effectively with others."

6.1 What Is Attention? Attention is the process which results in certain
sensory information being selectively processed over other information.
The key words in the definition above are selectively processed, because
they mean that something special is happening to whatever is being
attended. This definition, while correct, does not capture the wide
range of things we attend to, such as a specific object (a football
player running down the field; a building on campus), a particular place
(a location where someone is supposed to meet you; the projection screen
at the front of a classroom), a specific sound (a conversation at a
party; a siren in the street), or a particular stream of thought ("what
am I going to do tonight?" "What is the solution to this math
problem?"). In addition to attending to different types of things, we
can attend in different ways. One way of paying attention, called overt
attention, occurs when you move your eyes from one place to another, to
focus on a particular object or location. Another way of paying
attention, called covert attention, occurs when you shift attention
without moving your eyes, as might occur when you are looking at the
person you are talking to but are keeping track of another person who is
off to the side. 124

6.2 The Diversity of Attention Research Continuing with the idea that we
direct our attention to different things and in different ways, we will
now describe three different approaches to attention. These approaches
both illustrate the diversity of attention research, and provide
examples of classic experiments from the early history of modern
attention research, which began in the 1950s.

Attention to an Auditory Message: Cherry and Broadbent's Selective
Listening Experiments One of the first modern attention experiments
involved hearing. Colin Cherry (1953) used a technique called dichotic
listening, where dichotic refers to presenting different stimuli to the
left and right ears. Cherry's experiment involved selective attention,
because the participant's task was to selectively focus on the message
in one ear, called the attended ear, and to repeat what he or she is
hearing out loud. This procedure of repeating the words as they are
heard is called shadowing (Figure 6.1). Cherry found that although his
participants could easily shadow a spoken message presented to the
attended ear, and they could report whether the unattended message was
male or female, they couldn't report what was being said in the
unattended ear. Other dichotic listening experiments confirmed that
people are not aware of the information being presented to the
unattended ear. For example, Neville Moray (1959) showed that
participants were unaware of a word that had been repeated 35 times in
the unattended ear. The ability to focus on one stimulus while filtering
out other stimuli is called the cocktail party effect, because at noisy
parties people are able to focus on what one person is saying even
though there are many conversations happening at the same time.

The meaning of life is...

The yellow dog chased...

The yellow dog chased...

Figure 6.1 In the shadowing procedure, which involves dichotic
listening, a person repeats out loud the words in the attended message
as they hear them. This ensures that participants are focusing their
attention on the attended message.

Chapter 6  Visual Attention

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Messages

Sensory memory

Filter

Detector

Figure 6.2 Flow diagram of Broadbent's filter model of attention.

To memory

Attended message

Based on results such as these, Donald Broadbent (1958) created a model
of attention designed to explain how it is possible to focus on one
message and why information isn't taken in from other messages. The
model proposed something that was revolutionary in the psychology of the
1950s---a flow diagram that pictured attention as a sequence of steps.
This flow diagram provided researchers with a way of thinking about
attention in terms of processing information, much like computer flow
diagrams that were being introduced at that time. Broadbent's flow
diagram, shown in Figure 6.2, shows a number of messages entering a
filter unit, which lets through the attended message, and filters out
all of the other messages. The attended message is then detected by the
detector unit and is perceived. We won't go into the details of
Broadbent's model here. Its main importance is that it provides a
mechanism by which attention makes the attended stimulus available for
more processing. As we will see next, Michael Posner took another
approach, which highlighted the effect of attention on processing.

presented off to the side (as shown in the right panel). The trial shown
in Figure 6.3a is a valid trial because the square appears on the side
indicated by the cue arrow. The location indicated by the cue arrow was
valid 80 percent of the time, but 20 percent of the trials were invalid;
that is, the arrow cue indicated that the target was going to be
presented on one side but it actually appeared on the other side, as
shown in Figure 6.3b. For this invalid trial, the arrow cue indicates
that the participant should attend to the left, but the target is
presented on the right.

The results of this experiment, shown in Figure 6.3c, indicate that
participants reacted more rapidly in a detection task on valid trials
than on invalid trials. Posner interpreted this result as showing that
information processing is more effective

Attention to a Location in Space: Michael Posner's Precueing Experiment

METHOD

- 
- 
- 
- 

(b) Invalid trial 325

Precueing

The general principle behind a precueing experiment is to determine
whether presenting a cue indicating where a test stimulus will appear
enhances the processing of the test stimulus. The participants in Posner
and coworkers' (1978) experiment kept their eyes stationary throughout
the experiment, always looking at the 1 in the display in Figure 6.3.
They first saw an arrow cue (as shown in the left panel) indicating on
which side of the target a stimulus was likely to appear. In Figure
6.3a, the arrow cue indicates that participants should focus their
attention to the right. (Remember, they do this without moving their
eyes, so this is an example of covert attention.) The participants' task
was to press a key as rapidly as possible when a target square was

Respond to target

(a) Valid trial

Reaction time (ms)

We often pay attention to specific locations, as when paying attention
to what is happening in the road directly in front of our car when
driving. Paying attention informs us about what is happening at a
location, and also enables us to respond more rapidly to anything that
happens in that location. Attention to a specific location is called
spatial attention. In a classic series of studies on spatial attention,
Michael Posner and coworkers (1978) asked whether paying covert
attention to a location improves a person's ability to respond to
stimuli presented there. To answer this question, Posner used the
precueing procedure.

See cue

300 275 250 225 200 0

(c) Results

Valid

Invalid

Figure 6.3 Procedure for the (a) valid task and (b) invalid task in
Posner and coworkers' (1978) precueing experiment. See text for details.
(c) The results of the experiment. The average reaction time was 245 ms
for valid trials but 305 ms for invalid trials. (Figures a--b from
Posner et al., 1978)

6.2 The Diversity of Attention Research

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

125

Object

Preattentive stage

Focused attention stage

Features separated

Features combined

Perception

Figure 6.6 Flow diagram of Treisman's (1988) feature integration theory
in which features like color and shape exist independently in the
preattentive stage and then are combined to create objects in the
focused attention stage. Figure 6.4 Spatial attention can be compared to
a spotlight that scans a scene.

at the place where attention is directed. This result and others like it
gave rise to the idea that attention operates like a spotlight or zoom
lens that improves processing when directed toward a particular location
(Figure 6.4; Marino & Scholl, 2005).

Attention as a Mechanism for Binding Together an Object's Features: Anne
Treisman's Feature Integration Theory Consider the following stimulus
used by Anne Treisman and Hilary Schmidt (1982). A participant briefly
saw a display like the one in Figure 6.5 and was told to report the
identity of the black numbers first and then to report what they saw at
each of the four locations where the shapes had been. Participants
reported the numbers correctly, but on about one-fifth of the trials
they reported seeing objects at the other locations that were made up of
a combination of features from two different stimuli. For example, after
being presented with the display in Figure 6.5, in which the small
triangle is red and the small circle is green, they might report seeing
a small green triangle. This combination of features is called an
illusory conjunction.

1

8

Figure 6.5 Stimuli for Treisman and Schemidt's (1982) experiment. When
participants first attended to the black numbers and then to the other
objects, some illusory conjunctions, such as "green triangle" occurred.
126

Based on results such as these plus the results of other experiments,
Treisman proposed feature integration theory (FIT) (Treisman & Gelade,
1980; Treisman, 1985). An early version of the theory, shown in Figure
6.6, proposes that the first step in processing an object is the
preattentive stage. In this stage, features of the object are analyzed
rapidly and unconsciously, and at this stage the features exist
independently of one another. So the red triangle would be analyzed into
independent features "red" and "triangle" and the green circle into
"green" and "circular." In the second stage, the focused attention
stage, attention becomes involved, and conscious perception occurs.
Conscious perception involves a process called binding, in which
individual features are combined, so the viewer sees "red triangle" or
"green circle." In other words, during normal viewing, attention
combines an object's features so we perceive the object correctly.
Illusory conjunctions can occur when attention to the object is
distracted, as in the experiment. Treisman's theory resulted in many
experiments supporting the idea of two stages of processing, one
unconscious and not involving attention (the preattentive stage) and the
other conscious and involving attention (the focused attention stage).
One type of experiment used to make this distinction involved a
procedure called visual search, which is something we do anytime we look
for an object among a number of other objects, like trying to find Waldo
in a Where's Waldo? picture (Handford, 1997). DEMONSTRATION

Visual Search

In this demonstration, your task is to find a target item that is
surrounded by other, distractor, items. First, try finding the
horizontal line in Figure 6.7a. Then find the horizontal green line in
Figure 6.7b. The first task is called a feature search, because you
could find the target by looking for a single feature, "horizontal." The
second task is a conjunction search, because you had to search for a
combination (or conjunction) of two or more features, "horizontal" and
"green." In this task you couldn't just focus on horizontal, because
there were horizontal red bars, and you couldn't just focus on green,
because there were vertical green bars. You had to look for the
conjunction between green and horizontal.

Chapter 6  Visual Attention

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

(a)

(b) 

Mean reaction time (ms)

900 800 700 600 500 4

8 Display size

16

Figure 6.7 Find the horizontal line in (a) and then the green horizontal
line in (b). Which task took longer? (c) Typical result of visual search
experiments in which the number of distractors is varied for different
search tasks. The reaction time for the feature search (green line) is
not affected by the number of distractors, but adding distractors
increases reaction time for the conjunction search (red line).

This demonstration illustrates the difference between the two stages in
Treisman's theory. The feature search was accomplished rapidly and
easily because you didn't have to search for the horizontal bar. It just
"popped out" of the display. This is an example of automatic processing,
which does not require a conscious search. In contrast, the conjunction
search did require a conscious search, corresponding to the focused
attention stage of FIT, because you had to find the conjunction between
two properties. Treisman demonstrated a difference between the two types
of search by having participants find targets, as in the demonstration,
and varying the number of distractors. Figure 6.7c, which shows the
results of a typical experiment of this kind, indicates that the speed
of feature search is unaffected

by the number of distractors (green line), but the speed of the
conjunction search (red line) becomes slower as distractors are added.
This difference corresponds to the difference between the rapid
preattentive and the slower focused attention stages of FIT. In a paper
commemorating the 40th anniversary of FIT, Arri Kristjansson and Howard
Egeth (2019) point out that research during the 40 years since the
theory was proposed has not always supported the theory, so that parts
of the theory have had to be modified or abandoned. But despite the fact
that FIT is now mainly of historical importance, Kristjansson and Egeth
point out that FIT was extremely important in early research on
attention and that "because of FIT, attention now plays a major role in
any account of visual perception." The work of Cherry, Broadbent,
Posner, and Treisman provided evidence that attention is a central
process in perception. Broadbent's work was important because it
proposed the first flow diagram to explain how attention separates
stimuli being attended from stimuli not being attended. Posner's
research showed how covert attention can enhance processing of a
stimulus. Treisman's research emphasized the role of attention in
perceiving coherent objects and also led many other researchers to carry
out visual search experiments as a way to study attention. What all of
these early experiments have in common is that they showed, in different
ways, how attention can influence perception, and they set the stage for
much of the research on attention we will describe in the remainder of
this chapter, which focuses on studying (1) the mechanisms that create
attention and (2) different ways in which attention influences how we
experience our environment. In the next section we will consider an
important mechanism that creates attention---the shifting of attention
from one place to another by eye movements.

6.3 What Happens When We Scan a Scene by Moving Our Eyes? Overt
attention is attention that occurs when you move your eyes from one
place to another. One way to get it touch with the nature of eye
movements is to look for things in a scene. Try this by seeing how many
birds with white heads you can find in Figure 6.8.

Scanning a Scene With Eye Movements As you looked for the white-headed
birds you probably noticed that you had to scan the scene, looking from
one place to another. Scanning is necessary because good detail vision
occurs only for things which you are looking at directly, as illustrated
by the task on page 54, which showed that while looking at the letter on
the right, it was difficult to identify the letters to the left. 6.3
What Happens When We Scan a Scene by Moving Our Eyes?

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

127

Bruce Goldstein

Figure 6.8 Count the number of birds that have white heads (no yellow)
in this scene.

scanning this scene is an example of overt attention, but that covert
attention---attending off to the side---is also involved because it
helps determine where we are going to look next. Thus, covert attention
and overt attention often work together.

This difference occurs because of the way the retina is constructed.
Objects you are looking at (central vision) fall on the fovea (see page
41), which has much better detail vision than objects off to the side
(peripheral vision), which fall on the peripheral retina. Thus, as you
scanned the scene in Figure 6.8, you were aiming your fovea at one
object after another. Each time you paused briefly, you were making a
fixation. When you moved your eye to the next object, you were making a
saccadic eye movement---a rapid, jerky movement from one fixation to the
next. It isn't surprising that you were moving your eyes from one place
to another, because you were trying to find details in the scene. But it
may surprise you to know that even when you are freely viewing an object
or scene without searching for anything in particular, you move your
eyes about three times per second and more than 200,000 times a day.
This rapid scanning is shown in Figure 6.9, which is a pattern of
fixations (yellow dots) separated by saccadic eye movements (lines) that
occurred as a participant viewed the picture of the fountain. Note that

Maria Wachala/Getty Images (Scanning measurements by James Brockmole)

How Does the Brain Deal With What Happens When the Eyes Move?

Figure 6.9 Scan path of a person freely viewing a picture. Fixations are
indicated by the yellow dots and eye movements by the red lines. Notice
that this person looked preferentially at some areas of the picture, but
is ignoring other areas. 128

We've seen that eye movements direct our attention to what we want to
see. But in the process of shifting the gaze from one place to another,
eye movements also do something else: They cause the image on the retina
to become smeared. Consider, for example, Figure 6.10a, which shows the
scan path as a person first fixates on the finger and then moves his
eyes to the ear. While this movement of the eyes is shifting overt
attention from the finger to the ear, the image of everything located
between the finger and ear is sweeping across the retina (Figure 6.10b).
But we don't see a blurred image. We see a stationary scene. How can
that be? The answer to this question is provided by corollary discharge
theory (Helmholtz, 1863; Subramanian, et al., 2019; Sun & Goldberg,
2016; von Holst & Mittelstaedt, 1950; Wurtz, 2018). The first step in
understanding corollary discharge theory is to consider the following
three signals associated with movement of the eyes (Figure 6.11). 1. The
motor signal (MS) occurs when a signal to move the eyes is sent from the
brain to the eye muscles. 2. A corollary discharge signal (CDS) is a
copy of the motor signal, so occurs whenever there is a motor signal. 3.
The image displacement signal (IDS) occurs when an image moves across
the retina, as happens when movement of the eye causes the image of a
stationary scene to sweep across the retina (Figure 6.11). According to
corollary discharge theory, the brain contains a structure called the
comparator. The comparator operates according to the following rule:
When only the CDS or the IDS signal reaches it, movement is perceived.
But when both signals reach the comparator, no movement is perceived.

Chapter 6  Visual Attention

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

(b)

(a) 

Bruce Goldstein

(a) 

Because both the CDS and IDS reach the comparator when the eye scans a
scene, no movement is perceived, and the scene remains stationary. It
should be noted that the comparator is not a single brain area, but
involves a number of different structures (Sommer & Wurtz, 2008). The
important thing for our purposes is not exactly where the comparator is,
but that that there is a mechanism that takes into account both
information about stimulation of the receptors and information about
movement of the eyes, which determines whether or not movement
perception will occur. According to this mechanism, what should happen
if the eyes moved but there was no corollary discharge? In that
situation, the comparator receives only one signal---the IDS---so the
scene should appear to move. You can create this situation by closing
one eye and gently pushing on the eyelid of the other eye, so the eye
moves slightly (Figure 6.12). When you do this, there is an IDS, because
the eye is moving, but there is no CDS, because no signal is being sent
to the eye muscles. Because the comparator receives just one signal, the
scene appears to move. Imagine how disturbing it would be if this kind
of movement occurred each time your eyes moved while scanning a scene.
Luckily, information provided by the CD helps keep our world stationary
as our eyes move throughout a scene (Wurtz, 2013). Although the CD
solves the problem of the smeared retinal image by alerting the brain
that the eye is moving, movement of the eyes also causes another
problem: Each eye movement causes the scene to change. First there's a
finger in the center of a scene, then moments later an ear appears, and
a moment after that, something else takes center stage. Thus, what
happens on the retina is like a series of snapshots that are somehow
processed so we see not "snapshots" but a stationary scene. Luckily, the
corollary discharge not only keeps our world stationary as our eyes
move, but it also deals with the snapshot problem by helping the brain
prepare for what is coming next. Martin Rolfs and coworkers (2011)
determined this

(b) 

Figure 6.10 (a) Fixation on the person's finger followed by eye movement
and then fixation on the person's ear. (b) As the eye moves between the
finger and the ear, different images fall on the fovea. Some of these
images are indicated by what's inside each circle. Since the eye is
moving, these images aren't still images, but smear across the retina as
the eye scans from finger to ear.

Motor

CDS

IDS

Comparator

Figure 6.11 Explanation based on corollary discharge theory for why we
don't see a smeared image when we move our eyes from one place to
another. (1) A motor signal (MS) is sent from the motor area to the eye
muscles; (2) the corollary discharge signal (CDS), which is a copy of
the motor signal, is sent to the comparator; (3) the eye moves in
response to the MS and this movement causes an image to move across the
retina, generating an image displacement signal (IDS), which travels out
the back of the eye to the comparator, where it meets the CDS. When the
IMS meets the CDS, the CDS inhibits perception of the smeared image on
the retina caused by movement of the eye.

Bruce Goldstein

MS

Figure 6.12 Why is this woman smiling? Because when she pushes on her
eyelid, so her eye moves, she sees the world jiggle. 6.3 What Happens
When We Scan a Scene by Moving Our Eyes?

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

129

different from their surroundings, whether in color, contrast, movement,
or orientation, are said to have visual salience. Visually salient
objects can attract attention, as you will see in the following
demonstration.

by measuring participants' ability to judge the slant of a line flashed
near a target where they were going to move their eyes. They found that
the participants' performance on the slant task began to increase before
the eye began moving toward the target. This result, combined with the
results of many physiological experiments, shows that attention begins
shifting toward the target just before the eye begins moving toward it,
a phenomenon called the predictive remapping of attention (Melcher,
2007; Rao et al., 2016). Although the details of this process remain to
be worked out, this remapping appears to be one of the reasons that we
see a stable, coherent scene, even though it is based on a series of
"snapshots" on the retina.

DEMONSTRATION

Attentional Capture

Each shape in Figure 6.14 contains a vertical or horizontal line. What
is the orientation of the line inside the green circle? Answer this
question before reading further.

6.4 Things That Influence Visual Scanning William James's statement,
"Attention involves withdrawing from some things in order to effectively
deal with others" leads to the question, "what causes us to direct our
attention toward things we want to deal with and away from things we
want to withdraw from?" We will answer this question by describing a
number of things that influence where people shift their attention by
moving their eyes.

Visual Salience

Figure 6.14. An example of attention capture: When participants are
instructed to find the green circle, they often look at the red diamond
first. (Theeuwes, 1992)

Ales Fevzer/CORBIS

Some things in the world draw our attention because they stand out
against their backgrounds. For example, the man in the red shirt in
Figure 6.13 is conspicuous because his shirt's color starkly contrasts
with the whites and pale blues worn by everyone else in the scene. Scene
regions that are markedly

Figure 6.13 The red shirt is highly salient because it is bright and
contrasts with its surroundings. 130

It probably wasn't too difficult to find the green circle and to
determine that it contains a horizontal line. But did you look at the
red diamond first? Most people do. The question is, why would they do
so? You were asked to find a green circle, and the red diamond is
neither green nor a circle. Regardless, people attend to the red diamond
because it is highly salient, and salient items attract people's
attention (Theeuwes, 1992). Researchers use the term attentional capture
to describe situations like this, in which properties of a stimulus grab
attention, seemingly against a person's will. Even though attentional
capture can distract us from what we want to be doing, capture is an
important means of directing attention because conspicuous stimuli like
rapid movement or loud sounds can capture our attention to warn us of
something dangerous like an animal or object moving rapidly toward us.
To investigate how visual salience influences attention in scenes that
do not contain a single salient object, researchers have developed an
approach in which they analyze characteristics such as color,
orientation, and intensity at each location in a scene and combine these
values to create a saliency map of the scene. A saliency map reveals
which regions are visually different from the rest of the scene (Itti &
Koch, 2000; Parkhurst et al., 2002; Torralba et al., 2006). Figure 6.15
shows a scene (a) and its saliency map (b) as determined by Derrick
Parkhurst and coworkers (2002). Regions of greater visual salience are

Chapter 6  Visual Attention

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

denoted by brighter regions in the saliency map. Notice how the surf in
Figure 6.15a is indicated as particularly salient in Figure 6.15b. This
is because the surf constitutes an abrupt change in color, brightness,
and texture relative to the sky, beach, and ocean. The clouds in the sky
and the island on the horizon are also salient for similar reasons. When
Parkhurst and coworkers calculated saliency maps for a number of
pictures and then measured observers' fixations as they observed the
pictures, they found that the first few fixations were more likely to
occur on high-saliency areas. After the first few fixations, however,
scanning begins to be influenced by cognitive processes that depend on
things such as an observer's interests and goals. As we will see in the
next section, interests and goals are influenced by the observer's past
experiences in observing the environment.

The Observer's Interests and Goals One way to show that where we look
isn't determined only by saliency is by checking the eye movements of
the participant

(a) Visual scene

looking at the fountain in Figure 6.9. Notice that this person never
looked at the bright blue water, even though it is very salient due to
its brightness, color, and position near the front of the scene. This
person also ignored the rocks, columns, windows, and several other
prominent architectural features. Instead, this person focused on
aspects of the fountain that he or she found more interesting, such as
the statues. It is likely that the meaning of the statues has attracted
this particular person's attention. It is important to note, however,
that just because this person spent most of his or her time looking at
the statues doesn't mean everyone would. Just as there are large
variations between people, there are variations in how people scan
scenes (Castelhano & Henderson, 2008; Noton & Stark, 1971). Thus,
another person, who might be interested in the architecture of the
buildings, might look less at the statues and more at the building's
windows and columns. Attention can also be influenced by a person's
goals. In a classic demonstration, Alfred Yarbus (1967) recorded
participants' eye movements while they were told to just look at Repin's
painting An Unexpected Visitor (Figure 6.16a), or to determine the ages
of the people (Figure 6.16b), to remember the people's clothing (Figure
6.16c), or to remember the position of the people and objects (Figure
6.16d). It is clear from the eye movement records that the patterns of
eye movements depended on the information participants were told to
remember. More recent work has shown that people's interests and goals
can actually be decoded from their eye movements (Borji & Itti, 2014).
For example, John Henderson and coworkers (2013) recorded the eye
movements of participants who either searched for a specific object in a
scene or who tried to memorize the entire scene for a later test. After
the experiment, the researchers were able to correctly guess the
participants' task on each trial simply by examining their eye
movements. Clearly, as people's intentions and tasks change, they will
change how they focus their attention on a scene.

Vision Research/Elsevier

Scene Schemas

(b) Saliency map

Figure 6.15 (a) A visual scene. (b) Saliency map of the scene determined
by analyzing the color, contrast, and orientations in the scene. Lighter
areas indicate greater salience. (Adapted from Parkhurst et al., 2002)

Attention is also influenced by scene schemas---an observer's knowledge
about what is contained in typical scenes (remember our discussion of
regularities of the environment in Chapter 5, page 105). Thus, when
Melissa Võ and John Henderson (2009) showed pictures like the ones in
Figure 6.17, observers looked longer at the printer in Figure 6.17a than
the pot in Figure 6.17b because a printer is less likely to be found in
a kitchen. The fact that people look longer at things that seem out of
place in a scene means that attention is being affected by their
knowledge of what is usually found in the scene. Another example of how
cognitive factors based on knowledge of the environment influences
scanning is an experiment by Hiroyuki Shinoda and coworkers (2001) in
which they measured observers' fixations and tested their ability to
detect traffic signs as they drove through a computer-generated
environment in a driving simulator. They found that the observers were
more likely to detect stop signs positioned at intersections than those
positioned in the middle of a block, and 6.4 Things That Influence
Visual Scanning

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

131

(a)

(c) 

Remember clothing.

(a) 
(b) 

Determine ages of people.

(d) 

Remember the positions of the people and objects.

Unexpected, 1884--88 (oil on canvas), Repin, Ilya Efimovich
(1844-1930)/Tretyakov Gallery, Moscow, Russia/Bridgeman Images; Lucs-kho

Figure 6.16 Yarbus (1967) asked participants to view the painting in (a)
and recorded their eye movements while they either (b) determined the
ages of the people, (c) had to remember the clothes worn by the people,
or (d) had to remember the positions of the people and objects in the
room. The scan paths show that subjects' eye movements are strongly
influenced by the task.

that 45 percent of the observers' fixations occurred close to
intersections. In this example, the observers are using learning about
regularities in the environment (stop signs are usually at corners) to
determine when and where to look for stop signs.

Task Demands

(b) 

Figure 6.17 Stimuli used by Vo and Henderson (2009). Observers spent
more time looking at the printer in (a) than at the pot in (b), shown
inside the yellow rectangles (which were not visible to the observers).
132

The examples in the last section demonstrate that knowledge of various
characteristics of the environment can influence how people direct their
attention. However, the last example, in which participants drove
through a computer-generated environment, was different from the rest.
The difference is that instead of looking at pictures of stationary
scenes, participants were interacting with the environment. This kind of
situation, in which people are shifting their attention from one place
to another as they are doing things, occurs when people are moving
through the environment, as in the driving example, and when people are
carrying out specific tasks. Because many tasks require shifting
attention to different places as the task unfolds, it isn't surprising
that the timing of when people look at specific places is determined by
the sequence of actions involved in the task. Consider, for example, the
pattern of eye movements in Figure 6.18, which were measured as a person
was making a peanut butter sandwich. The process of making the sandwich
begins by removing a slice of bread from the bag and putting it on the
plate. This operation is accompanied by an eye movement from the bag to
the plate.

Chapter 6  Visual Attention

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

5. What is spatial attention? Describe Posner's experiment on speeding
response to locations. Be sure you understand the precueing procedure,
covert attention, and what Posner's results demonstrated. 6. What does
feature integration theory propose? 7. Describe two types of visual
search: feature search and conjunction search. How are these two types
of searches affected by the number of distractors in a display? 8. How
do these two types of searches relate to the two stages of Treisman's
feature integration theory? Figure 6.18 Sequence of fixations of a
person making a peanut butter sandwich. The first fixation is on the
loaf of bread. (From Land & Hayhoe, 2001)

The observer then looks at the peanut butter jar just before it is
lifted and looks at the top just before it is removed. Attention then
shifts to the knife, which is picked up and used to scoop the peanut
butter and spread it on the bread (Land & Hayhoe, 2001). The key finding
of these measurements, and also of another experiment in which eye
movements were measured as a person prepared tea (Land et al., 1999), is
that the person's eye movements were determined primarily by the task.
The person fixated on few objects or areas that were irrelevant to the
task, and eye movements and fixations were closely linked to the action
the person was about to take. Furthermore, the eye movement usually
preceded a motor action by a fraction of a second, as when the person
first fixated on the peanut butter jar and then reached over to pick it
up. This is an example of the "just in time" strategy---eye movements
occur just before we need the information they will provide (Hayhoe &
Ballard, 2005; Tatler et al., 2011). The examples we have described in
connection with scanning based on cognitive factors and task demands
have something in common: They all provide evidence that scanning is
influenced by people's predictions about what is likely to happen
(Henderson, 2017). Scanning anticipates what a person is going to do
next as they make a peanut butter and jelly sandwich; scanning
anticipates that stop signs are most likely to be located at
intersections; and pausing scanning to look longer at an object occurs
when a person's expectations are violated, as when a printer
unexpectedly appears in a kitchen. TEST YOuRSELF 6.1 1. What are the two
main points that William James makes about attention? (Hint: what it is
and what it does.) What are two reasons for paying attention to some
things and ignoring others? 2. Define attention, overt attention, and
covert attention. 3. Describe Cherry's dichotic listening experiment.
What did it demonstrate? 4. What is the central feature of Broadbent's
model of attention?

9.  What is the difference between central vision and peripheral vision
    and how is this relevant to eye movements?
10. What are fixations? Saccadic eye movements?
11. Describe how the corollary discharge theory explains why we don't
    see a scene as smeared when we move our eyes.
12. Why does the scene appear to move when we push on our eyelid?
13. Describe predictive remapping of attention. Why is it necessary?
14. Describe the following factors that determine where we look: visual
    salience, observer goals, scene schemas, and scanning based on task
    demands. Describe the examples or experiments that illustrate each
    factor.

6.5 The Benefits of Attention What do we gain by attending? Based on our
description of overt attention that is associated with eye movements, we
might answer that question by stating that shifting attention by moving
our eyes enables us to see places of interest more clearly. This is
extremely important, because it places the things we're interested in
front-and-center where they are easy to see. But some researchers have
approached attention not by measuring factors that influence eye
movements, but by considering what happens during covert attention, when
attention is shifted without making eye movements, as occurred in
Posner's precueing experiments that we described at the beginning of the
chapter (p. 125). One reason Posner studied covert attention is that it
is a way of studying what is happening in the mind without the
interference of eye movements. We will now consider more recent research
on covert attention, which shows how shifting attention "in the mind"
can affect how quickly we can respond to locations and to objects, and
how we perceive objects.

Attention Speeds Responding Posner's precueing study showed how covert
attention resulted in faster responding to the cued locations. We will
now 6.5 The Benefits of Attention

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

133

Attention Influences Appearance

Cue

C

A

374 ms

C

- D

A

324 ms

B

358 ms

- B

D

Present cue...................Cue off...................Present target
(a)

(b) 

Figure 6.19 In Egley and coworkers' (1994) experiment, (a) a cue signal
appears at one place on the display, then the cue is turned off and (b)
a target is flashed at one of four possible locations, A, B, C, or D.
Numbers are reaction times in ms for positions A, B, and C, when the cue
appeared at position A. (From Egly et al., 1994)

consider some experiments that show (1) that when attention is directed
to one part of an object, the enhancing effect of that attention spreads
to other parts of the object, and (2) that attention can influence
appearance. Consider, for example, the experiment diagrammed in Figure
6.19 (Egly et al., 1994). As participants kept their eyes on the 1, one
end of the rectangle was briefly highlighted (Figure 6.19a). This was
the cue signal that indicated where a target, a dark square (Figure
6.19b), would probably appear. In this example, the cue indicates that
the target is likely to appear in position A, at the upper part of the
right rectangle. (The letters used to illustrate positions in our
description did not appear in the actual experiment.) The participants'
task was to press a button when the target was presented anywhere on the
display. The numbers indicate their average reaction times, in
milliseconds, for three target locations when the cue signal had been
presented at A. Not surprisingly, participants responded most rapidly
when the target was presented at A, where the cue had been presented.
However, the most interesting result is that participants responded more
rapidly when the target was presented at B (reaction time 5 358 ms) than
when the target was presented at C (reaction time 5 374 ms). Why does
this occur? It can't be because B is closer to A than C, because B and C
are exactly the same distance from A. Rather, B's advantage occurs
because it is located within the object that was receiving the
participant's attention. Attending at A, where the cue was presented,
causes the maximum effect at A, but the effect of this attention spread
throughout the object so some enhancement occurred at B as well. The
faster responding that occurs when enhancement spreads within an object
is called the same-object advantage (Marino & Scholl, 2005).

Does the fact that attention can result in faster reaction times show
that attention can change the appearance of an object? Not necessarily.
It is possible that the target stimulus always looks the same, but
attention enhances the observer's ability to press the button quickly.
To answer the question of whether attention affects an object's
appearance, we need to do an experiment that measures the perceptual
response to a stimulus rather than the speed of responding to the
stimulus. In a paper titled "Attention Alters Appearance," Marisa
Carrasco and coworkers (2004) showed that attention affected the
perceived contrast between the alternating light and dark bars like the
ones in Figure 6.20c, where perceived contrast refers to how different
the light and dark bars appear. The procedure for Carrasco's experiment,
shown in Figure 6.20, was as follows: (a) Participants were instructed
to keep their eyes fixed on the small fixation dot at all times. (b) A
cue dot was flashed for 67 ms either on the left or on the right. Even
though participants were told that this dot wasn't related to what
happened next, it functioned to shift their covert attention to the left
or to the right. (c) A pair of gratings, one tilted to the left and the
other tilted to the right, was flashed for 40 ms. The contrast between
the bars of the gratings was randomly varied from trial to trial, so
sometimes the contrast of the right grating was higher, sometimes the
contrast of the left grating was higher, and sometimes the two gratings
were identical. The participants indicated, by pressing a key, whether
the grating stimulus with the highest contrast was tilted to the left or
to the right. Carrasco found that when two gratings were physically
identical, participants were more likely to report the orientation of
the one that was on the same side as the flashed cue dot. Thus, even
though the two gratings were the same, the one that received attention
appeared to have more contrast (see also Liu et al., 2009). In addition
to perceived contrast, a variety of other perceptual characteristics are
affected by attention. For example, attended objects are perceived to be
bigger, faster, and more richly colored (Anton-Erxleben et al., 2007;
Fuller & Carrasco, 2006; Turatto et al., 2007), and attention increases
visual acuity---the sharpness of vision (Montagua et al., 2009). Thus,
more than 100 years after William James suggested that attention makes
an object "clear and vivid," researchers have provided experimental
evidence that attention does, in fact, enhance the appearance of an
object. (See Carrasco & Barbot, 2019.)

Figure 6.20 Procedure for Carrasco and coworkers' (2004) experiment. See
text for explanation.

Cue flashed

Fixation dot (a) Fixate

134

(b) Cue flashed

(c) Gratings flashed

Chapter 6  Visual Attention

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

From the experiments we have described, it is clear that attention can
affect both how a person responds to a stimulus and how a person
perceives a stimulus. It should be no surprise that these effects of
attention are accompanied by changes in physiological responding.

6.6 The Physiology of Attention A large number of experiments have shown
that attention affects physiological responding in a variety of ways. We
begin by considering evidence that attention increases the neural
response to an attended item.

Attention to Objects Increases Activity in Specific Areas of the Brain

Signal change (%)

In an experiment by Kathleen O'Craven and coworkers (1999), participants
saw a face and a house superimposed (Figure 6.21a). You may remember the
experiment from Chapter 5 in which a picture of a house was presented to
one eye and a picture of a face was presented to the other eye (see
Figure 5.47, page 114). In that experiment, presenting different images
to each eye created binocular rivalry, so perception alternated between
the two images. When the face was perceived, activation increased in the
fusiform face area (FFA). When the house was perceived, activation
increased in the parahippocampal place area (PPA). In O'Craven's
experiment, the superimposed face and house stimuli were presented to
both eyes, so there was no binocular rivalry. Instead of letting rivalry
select the image that is visible, O'Craven told participants to direct
their attention to one stimulus or the other. For each pair, one of the
stimuli was stationary and the other was moving slightly back and forth.
When looking at a pair, participants were told to attend to either the
moving or stationary house, the moving or stationary face, or to the
direction of movement. As they were doing this, activity in their FFA,
PPA, and MT/MST (an area specialized for movement that we will discuss
in Chapter 8) was measured. 2.5

2.5

2.0

2.0

1.5

1.5

1.0

1.0

.5

.5

0

Fa ce

0

H

ou

se

Attended stimulus

(a) Stimulus

(b) FFA Activity

The results for when participants attended to the house or the face show
that attending to the moving or stationary face caused enhanced activity
in the FFA (Figure 6.21b) and attending to the moving or stationary
house caused enhanced activity in the PPA (Figure 6.21c). In addition,
attending to the movement caused activity in the movement areas, MT/
MST, for both moving face and moving house stimuli. Thus, attention to
different types of objects influences the activity in areas of the brain
that process information about that type of object (see also Çukur et
al., 2013).

Attention to Locations Increases Activity in Specific Areas of the Brain
What happens in the brain when people shift their attention covertly to
different locations while keeping their eyes stationary? Ritobrato Datta
and Edgar DeYoe (2009; see also Chiu & Yantis, 2009) answered this
question by measuring how brain activity changed when covert attention
was focused on different locations. They measured brain activity using
fMRI as participants kept their eyes fixed on the center of the stimulus
shown in Figure 6.22a and shifted their attention to different locations
within the display. Because the eyes did not move, the visual image on
the retina did not change. Nevertheless, they found that patterns of
activity within the visual cortex changed depending on where a
participant was directing his or her attention. The colors in the
circles in Figure 6.22b indicate the area of brain that was activated
when a participant directed his attention to the locations indicated by
the letters on the stimulus in Figure 6.19a. Notice that the yellow "hot
spot," which is the place of greatest activation, is near the center
when the participant is paying attention to area A, near where he is
looking. But shifting his attention to areas B and C, while keeping the
eyes stationary, causes the increase in brain activity to move out from
the center. By collecting brain activation data for all of the locations
on the stimulus, Datta and DeYoe created "attention maps" that show how
directing attention to a specific area of space activates a specific
area of the brain. These attention maps are like the retinotopic map we
described in Chapter 4 Figure 6.21 (a) Superimposed face and house
stimulus used in O'Craven and coworkers' (1999) experiment. (b) FFA
activation when the subject attended to the face or the house. (c) PPA
activation for attention to the face or the house. (Based on data from
O'Craven et al., 1999)

H

Fa c

e

ou

se

Attended stimulus

(c) PPA Activity 6.6 The Physiology of Attention

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

135

Figure 6.22 (a) Participants in Datta and DeYoe's (2009) experiment
directed their attention to different areas of this circular display
while keeping their eyes fixed on the center of the display. (b)
Activation of the brain that occurred when the participants attended to
the areas indicated by the letters on the stimulus disc. The center of
each circle is the place on the brain that corresponds to the center of
the stimulus. The yellow "hot spot" is the area of the brain that is
maximally activated by attention.

Stimulus disc

A Attention to one area

A

B

B C

C Always looking at center of disc

(Datta & DeYoe, 2009)

(a) 

(see Figure 4.12, page 75), in which presenting objects at different
locations on the retina activates different locations on the brain.
However, in Datta and DeYoe's experiment, brain activation is changing
not because images are appearing at different places on the retina, but
because the participant is directing his or her mind to different places
in the visual field. What makes this experiment even more interesting is
that after attention maps were determined for a particular participant,
that participant was told to direct his or her attention to a "secret"
place, unknown to the experimenters. Based on the location of the
resulting yellow "hot spot," the experimenters were able to predict,
with 100 percent accuracy, the "secret" place where the participant was
attending. This is similar to the "mind reading" experiment we described
in Chapter 5, in which brain activity caused by an oriented line was
analyzed to determine what orientation the person was seeing (see Figure
5.48, page 115). In the attention experiments, brain activity caused by
where the person was attending was analyzed to determine where the
person was directing his or her mind!

Attention Shifts Receptive Fields In Chapter 4 we showed how presenting
vertical bars outside a neuron's receptive field can increase the rate
of firing to a vertical bar located inside the receptive field (see
Figure 4.33, page 86). We now describe a situation in which attention
can shift the location of a neuron's receptive field. Theo Womelsdorf
and coworkers (2006) demonstrated this by recording from neurons in a
monkey's temporal lobe. Figure 6.23a shows the location of a neuron's
receptive field when the monkey was keeping its eyes fixed on the white
dot, but was paying attention to the diamond location indicated by the
arrow. Figure 6.23b shows how the location of the receptive field
shifted when the monkey's attention shifted to the circle location
indicated by the arrow. In both of these examples, yellow indicates the
area of the retina that, when stimulated, causes the greatest response.
This shifting of the receptive field, depending on where the monkey is
attending, is an amazing result because it means that attention is
changing the organization of part of the visual system. Receptive
fields, it turns out, aren't fixed in place but can change in response
to where the monkey is paying 136

(a) 
(b) 
(c) 

Figure 6.23 Receptive field maps on the retina determined when a monkey
was looking at the fixation spot (white) but paying attention to
locations indicated by the arrows: (a) the diamond or (b) the circle.
The arrows were not included in the display seen by the monkey. The
yellow areas are areas of the receptive field that generate the largest
response. Notice that the receptive field map shifts to the right when
the monkey shifts its attention from the diamond to the circle. (From
Womelsdorf et al., 2006)

attention. This concentrates neural processing power at the place that
is important to the monkey at that moment. As we continue exploring how
the nervous system creates our perceptions, we will encounter other
examples of how the flexibility of our nervous system helps us function
within our everchanging environment.

6.7 What Happens When We Don't Attend? We have seen that paying
attention affects both responding to stimuli and perceiving them. But
what happens when we don't pay attention? One idea is that you don't
perceive things you aren't attending to. After all, if you're looking at
something over to the left, you're not going to see something else that
is far to the right. But research has shown not only that we miss things
that are out of our field of view, but that not attending can cause us
to miss things even if we are looking directly at them. One example of
this is a phenomenon called inattentional blindness. In 1998, Arien Mack
and Irvin Rock published a book titled Inattentional Blindness, in which
they described experiments

Chapter 6  Visual Attention

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Trials 1--5

Figure 6.25 Frame from Simons and Chabris's (1999) experiment.

After seeing the video, participants were asked whether they saw
anything unusual happen or whether they saw anything other than the six
players. Nearly half of the observers failed to report that they saw the
woman or the gorilla. This experiment demonstrated that when people are
attending to one sequence of events, they can fail to notice another
event, even when it is right in front of them (also see Goldstein &
Fink, 1981; Neisser & Becklen, 1975). Following in the footsteps of
inattentional blindness experiments, researchers developed another way
to demonstrate how a lack of attention can affect perception. Instead of
presenting several stimuli at the same time, they first presented one
picture, then another slightly different picture. To appreciate how this
works, try the following demonstration. DEMONSTRATION

Change Detection

When you are finished reading these instructions, look at the picture in
Figure 6.26 for just a moment, and then turn the page and see whether
you can determine what is different in Figure 6.27. Do this now.

Trial (6) (b)

Figure 6.24 Inattentional blindness experiment. (a) The cross display is
presented for five trials. One arm of the cross is slightly longer on
each trial. The participant's task is to indicate which arm (horizontal
or vertical) is longer. (b) On the sixth trial, the participants carry
out the same task, but a small square is included in the display. After
the sixth trial, participants are asked if they saw anything different
than before. (From Cartwright-Finch & Lavie, 2007; Lavie, 2010)

Bruce Goldstein

(a) 

Bruce Goldstein

that showed that participants can be unaware of clearly visible stimuli
if they aren't directing their attention to them. In an experiment based
on one of Mack and Rock's experiments, Ula Cartwright-Finch and Nilli
Lavie (2007) presented the cross stimulus shown in Figure 6.24. The
cross was presented for five trials, and the participants' task was to
indicate which arm of the briefly flashed cross was longer, the
horizontal or the vertical. This was a difficult task because the cross
was flashed rapidly, the arms were just slightly different in length,
and the arm that was longer changed from trial to trial. On the sixth
trial, a small outline of a square was added to the display (Figure
6.24b). Immediately after the sixth trial, participants were asked
whether they noticed if anything had appeared on the screen that they
had not seen before. Out of 20 participants, only 2 reported that they
had seen the square. In other words, most of the participants were
"blind" to the small square, even though it was located right next to
the cross. This demonstration of inattentional blindness used a rapidly
flashed geometric test stimulus. But similar effects occur for more
naturalistic stimuli that are visible for longer periods of time. For
example, imagine looking at a display in a department store window. When
you focus your attention on the display, you probably fail to notice the
reflections on the surface of the window. Shift your attention to the
reflections, and you become less aware of the display inside the window.
The idea that attention can affect perception of overlapping scenes was
tested in an experiment by Daniel Simons and Christopher Chabris (1999),
who created a 75-second film that showed two "teams" of three players
each. One team, dressed in white, was passing a basketball around, and
the other was "guarding" that team by following them around and putting
their arms up as in a basketball game (Figure 6.25). Participants were
told to count the number of passes, a task that focused their attention
on the team wearing white. After about 45 seconds, one of two events
occurred. Either a woman carrying an umbrella or a person in a gorilla
suit walked through the "game," an event that took 5 seconds.

Figure 6.26 Stimulus for change-blindness demonstration. See text.

6.7 What Happens When We Don't Attend?

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

137

6.8 Distraction by Smartphones Perhaps the most studied effect of
smartphone distractions involves the effect on driving. We consider this
first, and then will describe how smartphone distraction affects other
activities.

Bruce Goldstein

Smartphone Distractions While Driving

Figure 6.27 Stimulus for change-blindness demonstration. See text.

Were you able to see what was different in the second picture? People
often have trouble detecting the change even though it is obvious when
you know where to look. (See the bottom of page 146 for a hint and then
try again.) Ronald Rensink and coworkers (1997) did a similar experiment
in which they presented one picture, followed by a blank field, followed
by the same picture but with an item missing, followed by a blank field,
followed by the original picture, and so on. The pictures were
alternated in this way until observers were able to determine what was
different about them. Rensink found that the pictures had to be
alternated back and forth a number of times before the difference was
detected. This difficulty in detecting changes in scenes is called
change blindness (Rensink, 2002). Change blindness occurs regularly in
popular films, in which some aspect of the scene, which should remain
the same, changes from one shot to the next. In the Wizard of Oz (1939),
Dorothy's (Judy Garland's) hair changes length many times from short to
long and back again. In Pretty Woman (1990) Vivian (Julia Roberts) began
to reach for a croissant for breakfast that suddenly turned into a
pancake. And magically, in Harry Potter and the Sorcerer's Stone (2001)
Harry (Daniel Radcliffe) suddenly changed where he was sitting from one
shot to the next during a conversation in the Great Hall. These changes
in films, called continuity errors, have been well documented on the
Internet (search for "continuity errors in movies"). The message of the
change blindness and inattentional blindness experiments is that when we
are paying attention to one thing, we miss other things. Although change
blindness and inattentional blindness were determined in laboratory
experiments, there are many examples in real life of things that
distract us by grabbing our attention. A major perpetrator of this
distraction is the smartphone, which has been accused of being "the
prime culprit in hijacking attention" (Budd, 2017). 138

Driving presents a paradox: in many cases, we are so good at it that we
can operate on "auto-pilot," as when we are driving down a straight
highway in light traffic. However, in other cases, driving can become
very demanding, as when traffic increases or hazards suddenly present
themselves. In this latter case, distractions that result in a decrease
in attention to driving are particularly dangerous. The seriousness of
driver inattention was verified by a research project called the 100-Car
Naturalistic Driving Study (Dingus et al., 2006). In this study, video
recorders in 100 vehicles created records of both what the drivers were
doing and the view out the front and rear windows. These recordings
documented 82 crashes and 771 near crashes in more than 2 million miles
of driving. In 80 percent of the crashes and 67 percent of the near
crashes, the driver was inattentive in some way 3 seconds beforehand.
One man kept glancing down and to the right, apparently sorting through
papers in a stopand-go driving situation, until he slammed into an SUV.
A woman eating a hamburger dropped her head below the dashboard just
before she hit the car in front of her. One of the most distracting
activities was pushing buttons on a smartphone or similar device. More
than 22 percent of near crashes involved that kind of distraction, and
it is likely that this number may be higher now because of increases in
smartphone use since that study. In a laboratory experiment on the
effects of smartphones, David Strayer and William Johnston (2001) gave
participants a simulated driving task that required them to apply the
brakes as quickly as possible in response to a red light. Doing this
task while talking on a phone caused participants to miss twice as many
of the red lights as when they weren't talking on the phone (Figure
6.28a) and also increased the time it took them to apply the brakes
(Figure 6.28b). Perhaps the most important finding of this experiment is
that the same decrease in performance occurred regardless of whether
participants used a hands-free or a handheld device. Taking into account
results such as these, plus many other experiments on the effects of
phones on driving, Strayer and coworkers (2013) concluded that talking
on the phone uses mental resources that would otherwise be used for
driving the car (also see Haigney & Westerman, 2001; Lamble et al.,
1999; Spence & Read, 2003; Violanti, 1998). This conclusion that the
problem posed by phone use during driving is related to the use of
mental resources is an important one. The problem isn't driving with one
hand. It is driving with fewer mental resources available to focus on
driving.

Chapter 6  Visual Attention

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

600

Reaction time (ms)

Fraction of red lights missed

.08 .06 .04 .02 0

No cellphone

(a) 

500

450

With cellphone

No With cellphone cellphone

(b) 

Figure 6.28 Results of Strayer and Johnston's (2001) cellphone
experiment. When participants were talking on a cellphone, they (a)
missed more red lights and (b) took longer to apply the brakes. (From
Strayer & Johnston, 2001)

But even though research clearly shows that driving while talking on a
phone is dangerous, many people believe it doesn't apply to them. For
example, in response to a class assignment, one of my students wrote, "I
do not believe my driving is affected by talking on the phone ... My
generation learned to drive when cell phones were already out. I had one
before driving, so while learning to drive, I also simultaneously
learned to talk on the phone and drive." Thinking such as this may be
why 27 percent of adults report that they sometimes text while driving,
even in the face of overwhelming evidence that it is dangerous (Seiler,
2015; Wiederhold, 2016). For example, a study by the Virginia Tech
Transportation Institute found that truck drivers who send text messages
while driving were 23 times more likely to cause a crash or near crash
than truckers who were not texting (Olson et al., 2009). Because of
results such as these, which indicate that texting is even more
dangerous than talking on a phone, most states now have laws against
text-messaging while driving.

The main message here is that anything that distracts attention can
degrade driving performance. And phones aren't the only
attention-grabbing device found in cars. Most cars now feature screens
that can display the same apps that are on your phone. Some
voice-activated apps enable drivers to make movie or dinner
reservations, send and receive texts or emails, and create posts on
Facebook. An early system in Ford vehicles was called an "infotainment
system." But a study from the AAA Foundation for Traffic Safety,
Measuring Cognitive Distraction in the Automobile, indicates that
perhaps too much information and entertainment while driving isn't a
good thing. The study found that voice-activated activities were more
distracting, and therefore potentially more dangerous, than either
hands-on or handsfree phones. The study concludes that "just because a
new technology does not take the eyes off the road does not make it safe
to be used while the vehicle is in motion" (Strayer et al., 2013).
Research such as this shows that attention affects not only where we are
looking as we are driving, but also how we are thinking. But media's
effect on attention has extended far beyond driving, as indicated by
scenes like the one in Figure 6.29, which shows that you don't have to
be in a car to have your attention captured by your phone! We now
consider research which shows that phones, and the Internet in general,
can have negative effects on many aspects of our behavior.

Distractions Beyond Driving The proliferation of smartphones has ushered
in an era of unprecedented connectivity. Consumers around the globe are
now constantly connected to faraway friends, endless entertainment, and
virtually unlimited information ... Just a decade ago, this state of
constant connection would have been inconceivable; today, it is
seemingly indispensable. (Ward et al., 2017)

Page Light Studios/Shutterstock.com

Figure 6.29 City scene showing people walking and paying attention to
their smartphones.

6.8 Distraction by Smartphones

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

139

140

demonstrated this in an experiment in which participants took tests
designed to measure cognitive functions that depend on attention.
Participants were divided into three groups, based on where their
smartphone was when they were taking the tests. The "other room" group
left their phone and other belongings outside the testing room. The
"pocket/bag" group took their belongings into the testing room, but kept
their phone where they normally would, usually in their pocket or bag.
The "desk" group took only their phones into the testing room and placed
it face down on their desk. Participants in all conditions were
instructed to turn their phones to silent by turning off the ring and
vibrate. One result of the study is shown in Figure 6.30, which shows
how participants scored on a test of working memory capacity. Working
memory is a memory function that is involved in temporarily holding and
manipulating information in the mind while carrying out tasks such as
comprehension, solving problems, and reasoning (Baddeley & Hitch, 1974;
Goldstein & Brockmole, 2019). These results show that performance in the
"desk" and "pocket/bag" conditions were significantly lower than
performance in the "other room" condition. Thus, just having the phone
within reach caused a decrease in working memory capacity. Ward also
showed that having the phone on the desk caused a decrease in scores on
a test of intelligence. Based on these results, Ward and coworkers
proposed that a potentially costly side effect of the presence of

35 34 33

Working memory capacity

Many research studies have documented high usage of smartphones and the
Internet. For example, 92 percent of college students report that they
have texted, browsed the web, sent pictures, or visited social networks
during class time (Tindell & Bohlander, 2012). By checking college
students' phone bills (with their permission!), Judith Gold and
coworkers (2015) determined that they send an average of 58 text
messages a day, and Rosen and coworkers (2013) showed that during a
15-minute study session, students averaged less than 6 minutes on-task
before interrupting studying to stretch, watch TV, access websites, or
use technology such as texting or Facebook. What's particularly
remarkable is how suddenly this has come upon us. In 2007 only 4 percent
of American adults owned smartphones (Radwanick, 2012), but in 2019,
just 12 years later, 82 percent of American adults, and 96 percent
between the ages of 18 and 29, owned smartphones (Pew Research Center,
2019; Ward et al., 2017). How often do you consult your phone? If you
check your phone constantly, one explanation of your behavior involves
operant conditioning, a type of learning in which behavior is controlled
by rewards (called reinforcements) that follow behaviors (Skinner,
1938). A basic principle of operant conditioning is that the best way to
ensure that a behavior will continue is to reinforce it intermittently.
So when you check your phone for a message and it's not there, well,
there's always a chance it will be there the next time. And when it
eventually appears, you've been intermittently reinforced, which
strengthens future phoneclicking behavior. Some people's dependence on
their phone is captured in the following sticker, marketed by Ephemera,
Inc: "After a long weekend without your phone, you learn what's really
important in life. Your phone." (See Bosker, 2016, for more on how
smartphones are programmed to keep you clicking.) Constant switching
from one activity to another has been described as "continuous partial
attention" (Rose, 2010), and here is where the problem lies, because as
we saw for driving, distraction from a task impairs performance. It
isn't surprising, therefore, that people who text more tend to have
lower grades (Barks et al., 2011; Kuznekoff et al., 2015; Kuznekoff &
Titsworth, 2013; Lister-Landman et al., 2015), and in extreme cases,
some people are "addicted" to the Internet, where addiction is defined
as occurring when Internet use negatively affects a number of areas of a
person's life (for example, social, academic, emotional, and family)
(Shek et al., 2016). What's the solution? According to Steven Pinker
(2010), given that the computer and Internet are here to stay, "the
solution is not to bemoan technology, but to develop strategies of
self-control, as we do with every other temptation in life." This sounds
like good advice, but sometimes powerful temptations are difficult to
resist. One example, for some people, is chocolate. Another is checking
their phone. So perhaps a solution might be to decide to limit the
number of times you consult your phone. But to make things even more
interesting, a recent study has shown that even if you decide not to
interact with your phone, its mere presence can have negative effects on
memory and intelligence. Adrian Ward and coworkers (2017)

32 31 30 29 28 27 26 25 Desk

Pocket/Bag

Other room

Phone location

Figure 6.30 How proximity of a smartphone affects working memory
capacity. Working memory was lower when the phone was in the same
room---either on the desk or in the person's bag or pocket---compared to
when it was in the other room. (From Ward et al., 2017)

Chapter 6  Visual Attention

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

smartphones is "smartphone-induced brain drain." They suggest that one
way to prevent this brain-drain is "defined and protected periods of
separation," similar to the situation in the "other room" group who were
physically separated from their phones. This may be easier said than
done, but at a minimum it seems unwise to have the phone in plain view
on your desk, because it may be draining your attention, even though you
think you are ignoring it.

their right hemisphere behave as if the left half of their visual world
no longer exists (Driver & Vuillemier, 2001; Harvey & Rossit, 2012). One
conclusion from symptoms such as these might be that the person is blind
on the left side of the visual field. This was, in fact, an early
explanation of neglect (Bay, 1950). But an experiment by Edoardo Bisiach
and Claudio Luzzatti (1978) put an end to that explanation, by asking a
patient with neglect to describe things he saw when imagining himself
standing at one end of the Piazza del Duomo in Milan, a place with which
he had been familiar before his brain was damaged (Figure 6.32). The
patient's responses showed that he neglected the left side of his mental
image, just as he neglected the left side of his perceptions. Thus, when
he imagined himself standing at A, he neglected the left side and named
only objects to his right (small a's). When he imagined himself standing
at B, he continued to neglect the left side, again naming only objects
on his right (small b's). Thus, neglect can occur even if the person is
imagining a scene with his or her eyes closed. Other research also
showed that patients with visual neglect do see things on their left if
they are told to pay attention to what's on the left side of the
environment. The problem in neglect, therefore, seems to be a lack of
attention to what's on the left, rather than a loss of vision on the
left. A condition that often accompanies neglect, extinction, is
demonstrated in the laboratory as follows. A patient is told to look at
a "fixation cross," so she is always looking straight ahead, and when a
light is flashed to her left side, she reports seeing the light. This
indicates that she is not blind on her left side. But if two lights are
simultaneously flashed, one on the left and one on the right, she
reports that she sees a light on the right, but does not report seeing a
light on the left. Thus, lack of awareness of what is happening on the
left occurs when a competing stimulus is presented on the right.
Extinction provides insight into attentional processing because it
suggests that the unawareness of stimuli on the left is caused by
competition from the stimuli on the right, with the left ending up being
the loser. Thus, when there is a

6.9 Disorders of Attention: Spatial Neglect and Extinction We've seen
that when we are paying attention to one thing, we may miss other
things. This is a consequence of the fact that we can only focus our
attention on one place at a time. But there is a neurological condition
called spatial neglect that exaggerates this effect. Consider, for
example, the case of Burgess, a 64-year-old male who had a stroke that
caused damage to structures in his parietal lobe on the right side of
his brain. Burgess became unaware of sounds, people, and objects that
were in the left side of his visual field. When he walked down the
street, he hugged the right side of the pavement, brushing up against
walls and hedges. He didn't notice potential dangers coming from the
left, so couldn't go out on his own (Hoffman, 2012). This ignoring of
the side of space opposite the side of the brain damage has been
measured in clinical tests. As shown in Figure 6.31, spatial neglect
patients will (a) draw only the right side of an object when asked to
draw it from memory, (b) mark only target circles on the right side of a
display, and (c) place a mark far to the right when asked to mark the
center of a line. This rightward bias is also reflected in everyday
behaviors such as eating food only on the right side of a plate and
shaving or grooming only the right side of the face. In other words,
people with neglect due to damage to

(a) 
(b) 
(c) 

Figure 6.31 Test results for a spatial neglect patient. (a) A starfish
drawn from memory is missing its left side; (b) when asked to mark
target circles---the ones without a vertical stem---the patient only
marked circles on the right; (c) when asked to place a mark at the
center of each horizontal line, the mark was placed to the right of
center. (From Harvey & Rossit, 2012)

6.9 Disorders of Attention: Spatial Neglect and Extinction

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

141

+ a

a

12 percent (a) Ring on left flower on right

b b

b

a b

- 

a a

B

35 percent b b b 30 b b

a a

A

(b) Flower on left ring on right

a a a

a

- 

b

Figure 6.32 Piazza del Duomo in Milan. When Bisiach and Luzzatti's
(1978) patient imagined himself standing at A, he could name objects
indicated by the a's. When he imagined himself at B, he could name
objects indicated by the b's. (Bisiach & Luzzatti, 1978)

stimulus only on the left, the signals generated by this stimulus are
transmitted to the brain and the patients see the stimulus. However,
when a stimulus is added on the right, the same signal is still
generated on the left, but the patient doesn't "see" on the left because
attention has been distracted by the more powerful stimulus on the
right. "Seeing" or "conscious awareness" is therefore a combination of
signals sent from stimuli to the brain and attention toward these
stimuli. This happens in non-brain-damaged people as well, who are often
unaware of things happening off to the side of where they are paying
attention---but there is a difference. Even though they may miss things
that are off to the side, they are aware that there is an "off to the
side"! There's still more to learn from the phenomenon of extinction,
because it turns out that extinction can be partially eliminated for
certain types of stimuli. When a ring stimulus the patient has never
seen before is presented on the left and a flower stimulus which also
hasn't been previously seen is presented on the right, patients with
visual neglect see the ring on only 12 percent of the trials (Figure
6.33a) (Treisman & Gelade, 1980; Treisman, 1985). Extinction is
therefore high when the ring is on the left. However, when the stimuli
are switched so the flower appears on the left, perception rises to 35
percent (Figure 6.33b). Finally, when a spider is presented on the left,
it is seen on 78 percent of the trials (Figure 6.33c) (Vuilleumier &
Schwartz, 2001a). In a similar experiment, patients were more likely to
see sad or smiley faces on the left than neutral faces (Vuilleumier &
Schwartz, 2001b). Why is the patient more likely to see the spider? The
answer seems obvious: the spider attracts attention, perhaps because it
is menacing and causes an emotional response, whereas the flower shape
does not. But how do patients know that the shape on the left is a
spider or a flower? Don't they need to have seen the spider or flower?
But if they've seen them, 142

78 percent (c)

Spider on left ring on right

Figure 6.33 A series of tests to determine the degree of extinction for
different pairs of stimuli. The number below the left image indicates
the percentage of trials on which that image was identified by a patient
who usually shows neglect of objects in the left side of the visual
field. (a) Ring on left, flower on right; (b) flower on left, ring on
right; (c) spider on left, ring on right. (Vuilleumier & Schwartz, 2001,
Fig 1b)

why do they, as in the case of the ring and the flower, often fail to
report seeing them? What apparently is happening is that the flower and
spider are processed by the brain at a subconscious level to determine
their identity, and then another process determines which stimuli will
be selected for conscious vision. The identification at a subconscious
level before attention has occurred is an example of preattentive
processing, which we mentioned at the beginning of the chapter in
connection with Anne Treisman's feature integration theory of attention
(see page 126). The patient isn't aware of preattentive processing,
because it is hidden and happens within a fraction of a second. What the
patient is aware of is which stimuli are selected to receive the
attention that leads to conscious vision (Rafel, 1994; Vuilleumier &
Schwartz, 2001a, 2001b). Thus, neglect and extinction provide more
examples of the role of attention in creating our conscious awareness of
the environment.

SOMETHING TO CONSIDER:

Focusing Attention by Meditating Two people are sitting in chairs, feet
on the ground, eyes closed. What's going on in their minds? We can't, of
course, tell by looking, but we know that they are meditating. What's
that all about, and what does it have to do with attention?

Chapter 6  Visual Attention

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Meditation is an ancient practice that originated in Buddhist and Hindu
cultures, which involves different ways of engaging the mind (Basso et
al., 2019). In a common form of meditation, called focused attention
meditation, the person focuses on a specific object, which can be the
breath, a sound, a mantra (a syllable, word, or group of words), or a
visual stimulus. There are other types of meditation, as well, such as
open-monitoring meditation, in which the person observes thoughts,
emotions, and sensations as they arise, in a nonjudgmental way, and
loving-kindness meditation, in which the person generates thoughts of
love, kindness, and empathy toward others and themselves. In the
remainder of this section, we will consider focused attention
meditation. Most meditation practice in the United States is focused
attention meditation, with the two most popular meditation apps, Calm
and Headspace, emphasizing paying attention to the in and out of the
breath. The fact that these apps are multimillion dollar businesses,
each with over 1 million paying subscribers, testifies to the growing
popularity of meditation, as does the fact that the percentage of adults
in the United States who meditated in the last 12 months increased from
4.1 percent in 2012 to 14.2 percent in 2017, which translates into 35
million adults in 2017 (Clarke et al., 2018). Let's return to our
meditators, who we left sitting with their eyes closed. Perhaps the best
way to begin describing their experience is to consider what was going
on in their minds before they sat down to meditate. One of the
characteristics of the mind is that it is very active. Engaging in
specific tasks like studying or solving problems involves task-oriented
attention. But sometimes this task-oriented attention is interrupted by
thoughts that have nothing to do with the task. You may have experienced
this if, as you were studying, your mind drifted off to contemplate
something that is going to happen later, or something that happened
earlier in the day. This type of nontask-oriented mind activity is
called day dreaming or mind wandering. Matthew Killingsworth and Daniel
Gilbert (2010) determined the prevalence of mind wandering by using a
technique called experience sampling, based on an iPhone app, which
beeped at random times as participants went about their daily lives.
When they heard the beep, participants reported that they were mind
wandering 47 percent of the time. This scientifically determined result
would probably not be surprising to

Buddhist meditators, who coined the term "monkey mind" to refer to a
mind that is constantly active, often to the detriment of focusing
attention on a task or being able to relax without being bothered by all
the thoughts created by the pesky monkey mind. So when our meditators
sit down to meditate, their task is to turn off the monkey mind! To
accomplish this, they focus their attention on the in and out of their
breathing, so their consciousness is centered on "breathing in" and
"breathing out." But invariably the monkey mind intrudes, and when the
person realizes that they are thinking about something other than their
breath they acknowledge the thought, without becoming involved in it,
and return their attention to the breath. Meditation, therefore,
involves a cycle of focused attention, an interruption by mind
wandering, awareness of the interruption, then focusing attention back
on the breath. What this sequence demonstrates is that attention can be
used not only to focus on a particular object, place, or activity, as we
have been discussing in most of this chapter, but can also be used to
"clear the mind" by turning off that sometimes annoying monkey that
keeps chattering in our head. As meditators become more practiced, they
are able to spend more time in the focused attention part of the cycle
(Hasenkamp et al., 2012). There's an old Zen story about 15th century
Zen Master Ikkyu, who, when asked for the source of the highest wisdom
answered, "Attention, attention, attention!" (Austin, 2009). What does
this mean? It could be applied to the cycle of focusing and shifting
attention we have just described. Another interpretation is that we
could substitute the word "awareness" for attention (Beck, 1993).
Philosophical discussions about the importance of attention in
meditation and life in general aside, there is a great deal of
scientific evidence that meditation has many beneficial effects,
including pain relief (Zeidan & Vago, 2016), stress reduction (Goyal et
al., 2014), improving cognitive functions such as memory (Basso et al.,
2019), and, not surprisingly, improving the ability to focus attention
(Moor & Malinowski, 2009; Semple, 2010). Other experiments have shown
not only that meditation affects behavior, but that it also affects
activity in areas of the brain associated with regulation of thought and
action (Fox et al., 2016) and causes reorganization of neural networks
that extend over large areas of the brain (Tang et al., 2017).

DEVELOPMENTAL DIMENSION Infant Attention and Learning Object Names How
do infants learn the names of objects? The answer to that question is
complicated, but an important part of the answer is "by paying attention
while interacting with an adult---usually a parent." Another way of
stating this is that infants typically learn words with a social
partner, who is most often one of their parents. There are two crucial
aspects of this interaction. First, the

child directs their attention to a particular object. Second, as the
child is attending, the parent names the object. Until recently, it has
been difficult to study this interaction between attention and naming
because of difficulties in measuring exactly where an infant is
directing their attention. But this problem has been solved by the
development of head-mounted eye tracking. Something to Consider:
Focusing Attention by Meditating

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

143

Head-Mounted Eye Tracking

Eye tracking has been measured in adults by having them keep their heads
steady with the aid of a chin rest, and then tracking their eye
movements as they scan a picture of a scene (Figure 6.9). This technique
has yielded valuable information about attention, but is both an
artificial situation and not feasible for infants. Head-mounted eye
tracking solves the problem of artificiality and suitability for infants
by fitting a perceiver with two devices: (1) a head-mounted scene
camera, which indicates the orientation of the perceiver's head and
their general field of view, and (2) an eye camera, which indicates the
precise location where the person is looking within that field of view
(Borjon et al., 2018). Figure 6.34 shows an infant and parent wearing
headmounted eye trackers while playing with toys. Figure 6.35 shows
attention records recorded as an infant and parent looked at three
different toys, with purple indicating looking at the partner's face and
the other colors indicating attention to each toy. The infant's
fixations are on the top record and the parent's records are on the
bottom record. The purple in the bottom record indicate that the parent
glances often at the infant's face. In contrast, the infant's fixations
are focused on the toys.

We can distinguish two different types of infant attention. The black
arrows above the infant record indicate the beginnings of periods of
sustained attention, which is attention to an object lasting 3 seconds
or more. The red arrows below the record indicate periods of joint
attention, which are periods of at least 0.5 seconds when infant and
parent are looking at the same object. Courtesy of Chen Yu, Department
of Psychology, University of Texas at Austin

METHOD

Figure 6.34 Infant and parent wearing head-mounted tracking devices that
measure where they are looking while playing with toys. The parent is
also wearing a microphone to determine when she is speaking. face

three objects

infant fixations parent fixations

Figure 6.35 How infants and parents directed their attention during toy
play. See text for an explanation of this figure. (From Yu et al., 2018)

Chen Yu and coworkers (2018) used head-mounted eye tracking to measure
where 9-month-old infants and a parent were directing their attention as
they were playing with toys, and the parent's speech was recorded, in
order to determine where the infant was looking and when the parent
named a toy. By considering both the looking data and the speech data,
Yu identified high-quality naming events---instances in which the parent
named the object while the infant was looking at it. For each infant,
they multiplied the quality of naming---the proportion of naming that
occurred when the infant was looking at the object---times the quantity
of naming---the number of times the parent named an object. Figure 6.36
plots the infant vocabulary at 12 months versus the quality 3 quantity
measure determined at 9 months, with each data point representing an
individual infant--parent pair. The wide range in the quality 3 quantity
measure reflects the fact that there were large individual differences
in how often the parents named objects, with frequencies ranging from
4.4 to 16.4 naming instances 144

per minute. The relationship between naming and later vocabulary is
especially important, because larger early vocabulary is associated with
better future language ability and school achievement (Hoff, 2013;
Murphy et al., 2014). The relationship between vocabulary at 12 months
and naming at 9 months shown in Figure 6.36 is based on naming that
occurred during infant sustained attention---when the infant's attention
was focused on an object for 3 seconds or longer. However, when Yu
considered naming that occurred when there was no sustained attention,
but only joint attention (that is, when the infant and adult were
looking at an object at the same time, but the infant's gaze lasted less
than 3 seconds), they found that naming did not predict later
vocabulary. Thus, the crucial condition for infant name learning is
focusing attention on an object for a sustained period of time, and then
hearing the object's name. If word learning doesn't occur when there is
only joint attention, does joint attention serve a purpose? Chen Yu and
Linda Smith (2016) studied this question by measuring where

Chapter 6  Visual Attention

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

200

Vocabulary at 12 months

11- to 13-month-old infants and their parents were looking as they
played with toys. Yu and Smith found that 65 percent of instances of
sustained attention to a toy occurred along with joint attention, that
infants' sustained attention was longer when joint attention also
occurred, and that longer periods of joint attention were associated
with longer sustained attention. The message of these results is that
when the parent is demonstrating interest in an object by looking at it,
they transmit this interest to the infant by doing other things like
talking about the object and touching it, and this encourages the infant
to look longer at the object (Suarez-Rivera et al., 2019). At the
beginning of our discussion we stated that infants typically learn with
a social partner, usually a parent. But infants rarely look at the
parent's face during toy play (Figure 6.35), because what's important to
the infant are the toys. Instead, infants learn an object's name (1) by
following the cue of the parent's attention, which is indicated by
talking about an object and touching it, and (2) by hearing the object's
name while their attention is focused on the object.

150

100

50

0

0

1 2 3 4 Naming × quantity measure at 9 months

5

Figure 6.36 The relationship between infant vocabulary at 12 months and
the quality of naming measured in the Yu et al. (2018) experiment. See
text for details.

TEST YOuRSELF 6.2 1. Describe Eagly's experiment that provided evidence
for the same-object advantage.

7.  What is the evidence that driving while talking on a smartphone or
    while texting is a bad idea?

8.  Describe Carrasco's experiment that showed an object's appearance
    can be changed by attention.

9.  What is the evidence that high usage of smartphones and the Internet
    can, under some conditions, have negative effects on performance?

10. Describe O'Craven's experiment in which people observed superimposed
    face and house stimuli. What did this experiment indicate about the
    effect of attention on the responding of specific brain structures?

11. Describe Datta and DeYoe's experiment on how attending to different
    locations activates the brain. What is an attention map? What was
    the point of the "secret place" experiment? Compare this experiment
    to the "mind reading" experiments described at the end of Chapter 5.

12. Describe Womesdorf and coworkers' experiment in which they recorded
    from neurons in a monkey's temporal lobe. How did they show how
    receptive field location is affected by attention?

13. Describe the following two situations that illustrate how not
    attending can result in not perceiving: (1) inattentional blindness
    and (2) change detection.

14. Describe the experiment that showed that just having your smartphone
    nearby can affect scores on tests of memory and intelligence.

15. What is spatial neglect? What causes it? Describe the experiment
    that showed that neglect to stimuli on the left isn't due to being
    blind on the left.

16. What is extinction? What does the "spider" experiment demonstrate
    about conscious and unconscious attentional processing?

17. What is the connection between meditation and attention?

18. Describe how head-mounted eye tracking was used by Yu and
    coworkers (2018) to measure the attention of infants and their
    parents as they were playing with toys. What is the conclusion from
    this research?

THINK ABOUT IT 1. If salience is determined by characteristics of a
scene such as contrast, color, and orientation, why might it be correct
to say that paying attention to an object can increase its salience?
(p. 134) 2. How is the idea of regularities of the environment that we
introduced in Chapter 5 (see page 105) related to the cognitive factors
that determine where people look? (p. 131)

3.  Can you think of situations from your experience that are similar to
    the change detection experiments in that you missed seeing an object
    that became easy to see once you knew it was there? What do you
    think was behind your initial failure to see this object? (p. 137)
    Hint for change detection demonstration on page 138: Pay attention
    to the sign near the lower left portion of the picture. Think About
    It

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

145

KEY TERMS Attention (p. 124) Attentional capture (p. 130) Binding
(p. 126) Change blindness (p. 138) Cocktail party effect (p. 124)
Comparator (p. 128) Conjunction search (p. 126) Continuity error
(p. 138) Corollary discharge signal (CDS) (p. 128) Corollary discharge
theory (p. 128) Covert attention (p. 124) Dichotic listening (p. 124)
Experience sampling (p. 143) Extinction (p. 141) Feature integration
theory (FIT) (p. 126)

146

Feature search (p. 126) Fixation (p. 128) Focused attention meditation
(p. 143) Focused attention stage (p. 126) Head-mounted eye tracking
(p. 143) Illusory conjunction (p. 126) Image displacement signal (IDS)
(p. 128) Inattentional blindness (p. 136) Meditation (p. 143) Mind
wandering (p. 143) Motor signal (MS) (p. 128) Operant conditioning
(p. 140) Overt attention (p. 124) Perceived contrast (p. 134)

Preattentive processing (p. 142) Preattentive stage (p. 126) Precueing
(p. 125) Predictive remapping of attention (p. 130) Saccadic eye
movement (p. 128) Saliency map (p. 130) Same-object advantage (p. 134)
Scene schemas (p. 131) Selective attention (p. 124) Shadowing (p. 124)
Spatial attention (p. 125) Spatial neglect (p. 141) Visual salience
(p. 130) Visual search (p. 126)

Chapter 6  Visual Attention

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

These mosaics showcones in an area on the edge of the favea from twelve
different people with normal color vision. The images are false colored
so blue, andMaroney red represent How didgreen McKayla of thethe U.S.
short-, medium-, long-wavelength gymnastics team,and vaulting at the
2012 cones respectively. (The true colors London Olympics, get into this
posiare purple, and abluish-purple.) tion,yellow, and then execute
landing just Notice thelater? enormous variability in thea moments The
answer involves relative proportions of mediumand close connection
between perception long-wavelength even in and action, whichcones, also
occurs forthese ev"normal" observers. eryday actions such as walking
across campus or reaching across a table to pick up a cup of coffee.
EMPICS Sport - EMPICS/Getty Images

Learning Objectives After studying this chapter, you will be able to ...
■■ Understand the ecological approach to perception. ■■ Describe the
information people use to find their way when

walking and driving. ■■ Understand how the brain's "GPS" system creates
cortical maps

that help animals and people find their way. ■■ Describe how carrying
out simple physical actions depends on

interactions between the sensory and motor components of the nervous
system, combined with prediction.

■■ Understand the physiology behind our ability to understand

other people's actions. ■■ Understand what is behind the idea that the
purpose of percep-

tion is to enable us to interact with the environment. ■■ Understand
what it means to say that "prediction is everywhere." ■■ Describe what
an infant affordance is and how research has

studied this phenomenon

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Ch HAPTER a p t e r 47

Taking Action

Chapter Contents 7.1 The Ecological Approach to Perception

7.3 Finding Your Way Through the Environment

7.5 Observing Other People's Actions

The Moving Observer Creates Information in the Environment Reacting to
Information Created by Movement The Senses Work Together

The Importance of Landmarks Cognitive Maps: The Brain's "GPS" Individual
Differences in Wayfinding

Mirroring Others' Actions in the Brain Predicting People's Intentions

Demonstration: Keeping Your Balance

Affordances: What Objects Are Used For

7.2 Staying on Course: Walking and Driving Walking Driving a Car

TEST YOURSELF 7.1

7.6 Action-Based Accounts of Perception

7.4 Interacting With Objects: Reaching, Grasping, and Lifting

Is Everywhere

Reaching and Grasping Lifting the Bottle Adjusting the Grip

Some Questions We Will Consider: ■■ What is the connection between
perceiving and moving

through the environment? (p. 149) ■■ How do we find our way from one
place to another? (p. 154) ■■ How do sensory and motor functions
interact as we reach

for a bottle of ketchup? (p. 160) ■■ How do neurons in the brain respond
when a person per-

forms an action and when the person watches someone else carry out the
same action? (p. 164)

W

hat does "action" have to do with perception? One perspective on this
question is provided by the sea squirt, a tadpole-like creature with a
spinal cord connected to a primitive brain, an eye, and a tail, which
helps it find its way as it swims through the water (Figure 7.1a).
However, early in its life, the sea squirt gives up its traveling ways
and finds a place like a rock, the ocean floor, or the hull of a ship to
attach itself (Figure 7.1b). Once the sea squirt finds the place where
it will remain for the rest of its life, it has no more use for its eye,
brain, or flipping tail, and so absorbs its brain and eye into its body
(Beilock, 2012). The message of the sea squirt is that once it becomes
totally stationary, it doesn't have any use for the perceptual
capacities

SOMETHING TO CONSIDER: Prediction DEVELOPMENTAL DIMENSION: Infant

Affordances

TEST YOURSELF 7.2 THINK ABOUT IT

provided by the brain. We can appreciate what this has to do with human
perception by noting that in most of the early research on perception,
human participants were much like sea squirts---attached firmly to their
chairs, responding to stimuli or scenes on a computer screen. Drawing an
analogy between brainless sea squirts and participants in perception
experiments might be going a bit far, but the fact is that people, in
contrast to mature sea squirts, are in almost constant motion while
awake and one of the purposes of their brains is to enable them to act
within the environment. In fact, Paul Cisek and John Kalaska (2010)
state that the primary purpose of the brain is "to endow organisms with
the ability to adaptively interact with the environment." So to
understand perception we need to take a step beyond our discussion in
the previous chapter, when we described how people direct their
attention to specific objects or areas in the environment, and broaden
our perspective to consider the interaction between perception and our
ability to interact with the environment. So what does this interaction
look like? Let's consider the situation when you have just left class
and are heading to the student union for lunch. That isn't much of a
problem, because you know your way around campus. But that's something
you learned as you created the campus map in your head. So now, having
consulted your mental map, how do you stay 149

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Wim Van Egmond/Science Source

flawed because placing participants in small testing rooms to look at
simple stimuli ignores what a person perceives when he or she performs
natural, real-world tasks like walking down a street or drinking a glass
of water. Perception, Gibson argued, evolved so that we can move within,
and act upon, the world; therefore, he thought a better approach was to
study perception in situations where people move through and interact
with the environment. Gibson's approach, which focused on perception in
natural contexts, is called the ecological approach to perception
(Gibson, 1950, 1962, 1979). One goal of the ecological approach is to
determine how movement creates perceptual information that helps people
move within the environment.

(a) Top: Sea squirt tadpole

(b) Adult sea squirts attached to a rock

Figure 7.1 (a) A swimming sea squirt. Its spinal cord is attached to a
primitive brain, and it has eyes. (b) A number of mature sea squirts
attached to a rock. The spinal cord, brain, and eyes are gone.

on course during your trip? This may seem like a simple question,
because you do this without thinking when in a familiar environment.
But, as we will see, perception and memory play an important role in
keeping us on course. Another interaction with the environment occurs as
you are eating lunch. You reach across the table, pick up your drink,
and raise it to your lips. Another thing you do without much thinking or
effort, but which involves complex interactions between perception and
taking action. This chapter looks at how perception operates as we move
through and interact with the environment. To begin, we go back in
history to consider the ideas of J. J. Gibson, who championed the
ecological approach to perception.

7.1 The Ecological Approach to Perception Through most of the 20th
century, the dominant way perception research was carried out was by
having stationary observers look at static stimuli in the laboratory.
However, in the 1970s and 1980s, one group of psychologists, led by J.
J. Gibson, argued that this traditional way of studying perception was
150

To understand what it means to say that movement creates perceptual
information, imagine that you are driving down an empty street. No other
cars or people are visible, so everything around you---buildings, trees,
traffic signals---is stationary. Even though the objects around you
aren't moving, your movement relative to the objects causes you to see
the houses and trees moving past when you look out of the side window.
And when you look at the road ahead, you see the road moving toward you.
As your car hurtles forward when crossing a bridge, everything around
you---the sides and top of the bridge, the road below---moves past you
in a direction opposite to the direction you are moving (Figure 7.2).
The movement described above, in which movement of an observer creates
movement of objects and the scene relative to the observer, is called
optic flow. Optic flow has two important characteristics: 1. Optic flow
is more rapid near the moving observer, as shown in Figure 7.2 by longer
arrows indicating more rapid flow. The different speed of flow---fast
near the observer and slower farther away---is called the gradient

Barbara Goldstein

imageBROKER/Alamy Stock Photo

The Moving Observer Creates Information in the Environment

Figure 7.2 The side and top of the bridge and the road below appear to
move toward a car that is moving forward. This movement is called optic
flow.

Chapter 7  Taking Action

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

of flow. The gradient of flow provides information about how fast the
observer is moving. According to Gibson, the observer uses the
information provided by the gradient of flow to determine his or her
speed of movement. 2. There is no flow at the destination toward which
the observer is moving. The absence of flow at the destination point is
called the focus of expansion (FOE). In Figure 7.2 the FOE, marked by
the small white dot, is at the end of the bridge; it indicates where the
car will end up if its course is not changed. Another important concept
of the ecological approach is the idea of invariant
information---information that remains constant regardless of what the
observer is doing or how the observer is moving. Optic flow provides
invariant information because the same flow information is present each
time the observer is moving through the environment in a particular way.
For example, the FOE always occurs at the point toward which the
observer is moving. If an observer changes direction, objects in the
scene may change, but there is still a FOE. Thus, even when specific
aspects of a scene change, optic flow and the FOE continue to provide
information about how fast a person is moving and where he or she is
heading. When we consider depth perception in Chapter 10, we will see
that Gibson proposed other sources of invariant information, which
indicate an object's size and its distance from the observer.

Reacting to Information Created by Movement After identifying
information created by the moving observer, the next step is to
determine whether people use this information. Research on whether
people use optic flow information has asked observers to make judgments
regarding where they are heading based on computer-generated displays of
moving dots that create optic flow stimuli. The observer's task is to
judge, based on optic flow stimuli, where he or she would be heading
relative to a reference point. Examples of these stimuli are depicted in
Figures 7.3a and 7.3b. In each figure, the lines represent the movement
trajectories of individual dots. Longer lines indicate faster movement
(as in Figure 7.2). Depending on the trajectory and speed of the dots,
different flow patterns can be created. The flow in Figure 7.3a
indicates movement directly toward the vertical line on the horizon; the
flow in Figure 7.3b indicates movement to the right of the vertical
line. Observers viewing stimuli such as this can judge where they are
heading relative to the vertical line to within about 0.5 to 1 degree
(Warren, 1995, 2004; also see Fortenbaugh et al., 2006; Li et al.,
2006). Figure 7.4 indicates how movement creates information which, in
turn, is used to guide further movement. For example, when a person is
driving down the street, movement of the car provides optic flow
information, and the observer then uses this flow information to help
steer the car. A different example of movement that creates information
that is then used to guide further movement is provided by
somersaulting.

(a) 
(b) 

Figure 7.3 (a) Optic flow generated by a person moving straight ahead
toward the vertical line on the horizon. The lengths of the lines
indicate the person's speed. (b) Optic flow generated by a person moving
in a curved path that is headed to the right of the vertical line. (From
Warren, 1995) Car moving Movement

Provides information for guiding further movement

Creates flow

Flow Object moving relative to car

Figure 7.4 The relationship between movement and flow is reciprocal,
with movement causing flow and flow guiding movement. This is the basic
principle behind much of our interaction with the environment.

We can appreciate the problem facing a gymnast who wants to execute an
airborne backward somersault (or backflip) by realizing that, within 600
ms, the gymnast must execute the somersault and then end in exactly the
correct body configuration precisely at the moment that he or she hits
the ground (Figure 7.5). One way this could be accomplished is to learn
to run a predetermined sequence of motions within a specific 7.1 The
Ecological Approach to Perception

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

151

Figure 7.5 "Snapshots" of a somersault, or backflip, starting on the
left and finishing on the right. (From Bardy & Laurent, 1998)

period of time. In this case, performance should be the same with eyes
open or closed. However, Benoit Bardy and Makel Laurent (1998) found
that expert gymnasts performed somersaults more poorly with their eyes
closed. Films showed that when their eyes were open, the gymnasts
appeared to be making in-the-air corrections to their trajectory. For
example, a gymnast who initiated the extension of his or her body a
little too late compensated by performing the rest of the movement more
rapidly. Thus, somersaulting, like driving a car, involves using
information created by movement to guide further movement.

The Senses Work Together Another of Gibson's ideas was that the senses
do not work in isolation. He felt that rather than considering vision,
hearing, touch, smell, and taste as separated senses, we should consider
how each one provides information for the same behaviors. One example of
how a behavior originally thought to be the exclusive responsibility of
one sense is also served by another one is the sense of balance. Your
ability to stand up straight, and to keep your balance while standing
still or walking, depends on systems that enable you to sense the
movement and position of your body relative to gravity. These systems
include the vestibular canals of your inner ear and receptors in the
joints and muscles. However, Gibson (and others) noted that information
provided by vision also plays a role in keeping our balance, a fact we
can use to emphasize the way the senses work together. One way to
illustrate the role of vision in balance is to consider what happens
when visual information isn't available, as in the following
demonstration. DEMONSTRATION

Keeping Your Balance

Keeping your balance is something you probably take for granted. Stand
up. Raise one foot from the ground and stay balanced on the other. Then
close your eyes and notice what happens.

Did staying balanced become more difficult when you closed your eyes?
This occurs because vision provides a frame 152

of reference that helps the muscles constantly make adjustments to help
maintain balance (Aartolahti et al., 2013; Hallemans et al., 2010; Lord
& Menz, 2000). The importance of a visual frame of reference for balance
has also been examined by considering what happens to a person when his
or her visual and vestibular senses provide conflicting information
regarding posture. For example, David Lee and Eric Aronson (1974) placed
13- to 16-month-old toddlers in a "swinging room" (Figure 7.6). In this
room, the floor was stationary, but the walls and ceiling could swing
toward and away from the toddler. Figure 7.6a shows the room swaying
toward the toddler. This movement of the wall creates the optic flow
pattern on the right. Notice that this pattern is similar to the optic
flow that occurs when moving forward, as when driving across the bridge
in Figure 7.2. The optic flow pattern that the toddler observes creates
the impression that he or she is swaying forward. After all, the only
natural circumstance in which the entire world suddenly moves toward you
is a situation in which you are moving (or falling) forward. This
perception causes the toddler to sway back to compensate (Figure 7.6b).
When the room moves back, as in Figure 7.6c, the optic flow pattern
creates the impression of swaying backward, so the toddler sways forward
to compensate. In Lee and Aronson's experiment, although a few of the
toddlers were unaffected by the sway, 26 percent swayed, 23 percent
staggered, and 33 percent fell down, even though the floor remained
stationary throughout the entire experiment! Even adults were affected
by the swinging room. Lee describes their behavior as follows:
"oscillating the experimental room through as little as 6 mm caused
adult participants to sway approximately in phase with this movement.
The participants were like puppets visually hooked to their surroundings
and were unaware of the real cause of their disturbance" (p. 173).
Adults who didn't brace themselves could, like the toddlers, be knocked
over by their perception of the moving room. The swinging room
experiments therefore show that vision can override the traditional
sources of balance information provided by the inner ear and the
receptors in the muscles and joints (also see Fox, 1990; Stoffregen et
al., 1999; Warren et al., 1996).

Affordances: What Objects Are Used For Gibson's emphasis on
understanding perception in natural environments extended to how people
interact with objects. In connection with this, Gibson introduced the
concept of affordances---information that indicates how an object can be
used. In Gibson's (1979) words, "The affordances of the environment are
what it offers the animal, what it provides for or furnishes." A chair,
or anything that is sit-on-able, affords sitting; an object of the right
size and shape to be grabbed by a person's hand affords grasping; and so
on. What this means is that perception of an object includes not only
its physical properties, such as shape, size, color, and orientation,
that enable us to recognize the object, but also

Chapter 7  Taking Action

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Figure 7.6 Lee and Aronson's swinging room. (a) Moving the room toward
the observer creates an optic flow pattern associated with moving
forward, so (b) the observer sways backward to compensate. (c) As the
room moves away from the observer, flow corresponds to moving backward,
so the person leans forward to compensate and may even lose his or her
balance. (Based on Lee & Aronson, 1974)

(a) Room swings toward person.

Floor remains stationary

Flow when wall is moving toward person

(b) Person sways back to compensate.

(c) When room swings away, person sways forward to compensate.

Flow when wall is moving away from person

information about how the object is or could be used. For example, when
you look at a cup, you might see that it is "a round white coffee cup,
about 5 inches high, with a handle," but your perceptual system would
also respond with information indicating that it "can be picked up,"
"can be filled with liquid," or even "can be thrown." Affordances thus
go beyond simply recognizing the cup; they guide our interactions with
it. Another way of saying this is that "potential for action" is part of
our perception of an object. Gibson's emphasis on (1) studying the
acting observer, (2) identifying invariant information in the
environment that observers use for perception, (3) considering the
senses as working together, and (4) focusing on object affordances was
revolutionary for its time. But even though perception researchers were
aware of Gibson's ideas, most research continued in the traditional
way---testing stationary participants looking at stimuli in laboratory
settings. Of course, there is nothing wrong with testing stationary
observers in the laboratory, and much of the research described in this

book takes this approach. However, Gibson's idea that perception should
also be studied as it is often experienced, by observers who are moving
and in more naturalistic settings, finally began to take hold in the
1980s, and today perception in naturalistic settings is one of the major
themes of perception research. One modern approach to affordances has
looked at the behavior of people with brain damage. Glyn Humphreys and
Jane Riddoch (2001) studied affordances by testing patient M.P., who had
damage to his temporal lobe that impaired his ability to name objects.
M.P. was given a cue, either (1) the name of an object ("cup") or (2) an
indication of the object's function ("an item you could drink from"). He
was then shown 10 different objects and was told to press a key as soon
as he found an object that matched the cue. M.P. identified the object
more accurately and rapidly when given the cue that referred to the
object's function. Humphreys and Riddoch concluded from this result that
M.P. was using his knowledge of an object's affordances to help him
identify it. 7.1 The Ecological Approach to Perception

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

153

400 msec

Figure 7.7 EEG response to tools (red) and non-tools (blue), showing
that the response to tools is larger between 210 and 270 msecs after
they are presented. (From Proverbio, 2011)

Another modern approach has recorded the brain's response to objects.
Alice Proverbio and coworkers (2011) recorded a person's
electroencephalogram (EEG), which is recorded with electrodes on the
scalp that pick up the response of the thousands of neurons under the
electrodes. As they were recording the EEG, the person looked at 150
pictures of manipulable tools, 150 pictures of non-tool objects, and 25
pictures of plants. Their task was to respond to the plants by pressing
a key and to ignore the other pictures. Figure 7.7 compares the response
to the tools (red) and non-tools (blue), and shows that between 210 and
270 msec after presentation, the tools generated a larger response than
the non-tools. They call this response an action affordance because it
involves both the object's affordance (what it is for, for example,
"pounding" for a hammer) and the action associated with it (the grip
necessary to hold the hammer and the movements when pounding in a nail).
The remainder of this chapter focuses on research that considers the
following situations in which perception and action occur together in
the environment: (1) walking or driving through the environment; (2)
finding one's way from one location to another; (3) reaching out and
grasping objects; and (4) watching other people take action.

7.2 Staying on Course: Walking and Driving Following in Gibson's
footsteps, a number of researchers have considered the types of
information that people use when they are walking or driving. Perceptual
information we have already discussed such as optic flow is important,
but other sources of information come into play as well.

Walking How does a person stay on course as he or she is walking toward
a specific location? We have already discussed how optic flow can
provide invariant information regarding a person's trajectory and speed,
but other information can 154

Moving off course to the right

Walking toward tree (a)

(b) 

Correcting course back toward tree (c)

Arriving at tree

Bruce Goldstein

200 msec

(d) 

Figure 7.8 (a) As long as a person is moving toward the tree, it remains
in the center of the person's field of view. (b) When the person walks
off course, the tree drifts to the side. (c) When the person corrects
the course, the tree moves back to the center of the field of view,
until (d) the person arrives at the tree.

be used as well. For example, when using the visual direction strategy,
people keep their body pointed toward their goal. This is shown in
Figure 7.8, in which the goal is a tree (Figure 7.8a). Walking off
course causes the tree to drift to the side (Figure 7.8b), so a course
correction is needed to bring the tree back to the center (Figures 7.8c
and 7.8d) (Fajen & Warren, 2003; Rushton et al., 1998). Another
indication that optic flow information is not always necessary for
navigation is that we can find our way even when flow information is
minimal, such as at night or in a snowstorm (Harris & Rogers, 1999).
Jack Loomis and coworkers (1992; Philbeck et al., 1997) have
demonstrated this by eliminating optic flow altogether, using a "blind
walking" procedure in which people observe a target object located up to
12 meters away, then walk to the target with their eyes closed. These
experiments show that people are able to walk directly toward the target
and stop within a fraction of a meter of it. In fact, people can do this
even when they are asked to walk off in the wrong direction first and
then make a turn and walk to the target, all while keeping their eyes

Chapter 7  Taking Action

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Target

2 Turning points

Judged position of target

1

Start

Figure 7.9 The results of a "blind walking" experiment (Philbeck et al.,
1997). Participants looked at the target, which was 6 meters from the
starting point, then closed their eyes and begin walking to the left.
They turned either at point 1 or point 2, keeping their eyes closed the
whole time, and continued walking until they thought they had reached
the target.

closed. Some records from these "angled" walks are shown in Figure 7.9,
which shows the paths taken when a person first walked to the left from
the starting position and then was told to turn either at point 1 or 2
and walk to a target that was 6 meters away. The fact that the person
generally stopped close to the target shows that we are able to navigate
short distances accurately in the absence of any visual stimulation at
all (also see Sun et al., 2004). Participants in the blind walking
experiment accomplished this feat by mentally combining knowledge of
their own movements (e.g., muscle movements can give the walker a sense
of his or her speed as well as shifts in direction) with their memory
for the position of the target throughout their walk. The process by
which people and animals keep track of their position within a
surrounding environment while they move is called spatial updating (see
Wang, 2003). But just because people can walk toward objects and
locations without optic flow information doesn't mean that they don't
use such information to help them when it is available. Optic flow
provides important information about direction and speed when walking
(Durgin & Gigone, 2007), and this information can be combined with the
visual direction strategy and spatial updating processes to guide
walking behaviors (Turano et al., 2005; Warren et al., 2001).

Focus of expansion

(a) 
(b) 

Driving a Car Another common activity that requires people to keep track
of their movement through the environment is driving. To study
information people use to stay on course when driving, Michael Land and
David Lee (1994) fitted an automobile in the United Kingdom with
instruments to record the angle of the steering wheel and the car's
speed, and measured where the driver was looking with a video eye
tracker. As we noted earlier, according to Gibson, the focus of
expansion (FOE) provides information about the place toward which a
moving observer is headed. However, Land and Lee found that although
drivers look straight ahead while driving, they tend to look at a spot
closer to the front of the car rather than directly at the FOE, which is
further down the road (Figure 7.10a). Land and Lee also found that
drivers don't use the FOE on curved roads, because the FOE keeps
changing as the car rounds the curve, making the FOE a poor indicator of
how the car should be steered. When going around a curve, drivers don't
look directly at the road, but instead look at the tangent point of the
curve on the side of the road, as shown in Figure 7.10b. This allows
drivers to constantly note the position of the car relative to the lines
at the side of the road. By maintaining a constant distance between the
car and the lines on the road, a driver can keep the car headed in the
right direction (see Kandel et al., 2009; Land & Horwood, 1995; Macuga
et al., 2019; Rushton & Salvucci, 2001; Wilkie & Wann, 2003).

7.3 Finding Your Way Through the Environment In the last section we
considered information in the immediate environment that helps walkers
and drivers stay on course as they walk toward a goal or drive along a
road. But we often travel to more distant destinations that aren't
visible from our starting point, such as when we walk across campus from
one class to another or drive to a destination several miles away. This
kind of navigation, in which we take a route that usually involves
making turns, is called wayfinding.

Figure 7.10 Results of Land and Lee's (1994) experiment. Because this
study was conducted in the United Kingdom, subjects were driving on the
left side of the road. The ellipses indicate the place where the drivers
were most likely to look while driving down (a) a straight road and (b)
a curve to the left.

7.3 Finding Your Way Through the Environment

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

155

The Importance of Landmarks One important source of information for
wayfinding is landmarks---objects on the route that serve as cues to
indicate where to turn. Sahar Hamid and coworkers (2010) studied how
participants used landmarks as they learned to navigate through a
maze-like environment displayed on a computer screen in which pictures
of common objects served as landmarks. Participants first navigated
through the maze until they learned its layout (training phase) and then
were told to travel from one location in the maze to the other (testing
phase). During both the training and testing phases, participants' eye
movements were measured using a head-mounted eye tracker like the one in
Chapter 6 in which eye movements were measured as a person made a peanut
butter and jelly sandwich (see page 132). This maze contained both
decision-point landmarks---objects at corners where the participant had
to decide which direction to turn---and non-decision-point
landmarks---objects located in the middle of corridors that provided no
critical information about how to navigate. Measurement of the
participants' eye movements as they were navigating the maze showed that
they spent more time looking at landmarks at decision points, corners
where it was necessary to determine which way to turn, than landmarks in
the middle of corridors. When maze performance was tested with half of
the landmarks removed, removing landmarks that had been viewed less
(which were likely to be in the middle of the corridors) had little
effect on performance (Figure 7.11a). However, removing landmarks that
observers had looked at longer caused a substantial drop in performance
(Figure 7.11b). It makes sense that landmarks that are

looked at the most would be the ones that are used to guide navigation,
and it has also been found that decision-point landmarks are also more
likely to be remembered (Miller & Carlson, 2011; Schinazi & Epstein,
2010). Hamid's study used behavioral measures to study how landmarks
influence wayfinding. But what is happening in the brain? Gabriele
Janzen and Miranda van Turennout (2004) studied this question by having
participants view a film sequence that moved through a
computer-simulated museum (Figure 7.12). Decision-point objects marked
places where it was necessary to make a turn (Figure 7.12a).
Non-decisionpoint objects were located at places where a decision was
not required (Figure 7.12b). After studying the museum's layout in the
film, participants were given a recognition test while in an fMRI
scanner. Figure 7.12c indicates activity in an area of the brain known
to be associated with navigation called the parahippocampal gyrus (see
Figure 4.31, page 85). The left pair of bars indicates that for objects
that were remembered, activation was greater for decision-point objects
than for non-decision-point objects. But the most interesting

(a) Toy at decision point

All landmarks present

(b) Toy at non-decision point

Non-decision points

Half of landmarks removed

Decision points 3.5

1.0

3 Brain activation

Maze performance

0.8 0.6 0.4

2.5 2 1.5 1 0.5

0.2

0 0 (a) Least fixated landmarks

(b) Most fixated landmarks

Figure 7.11 Effect of removing landmarks on maze performance. Red = all
landmarks are present; blue = half have been removed. (a) Removing half
of the least fixated landmarks has no effect on performance. (b)
Removing half of the most fixated landmarks causes a decrease in
performance. (From Hamid et al., 2010) 156

Remembered

Forgotten

(c) 

Figure 7.12 (a & b) Two locations in the "virtual museum" viewed by
Janzen and van Turennout's (2004) observers. (c) Brain activation during
the recognition test for objects that had been located at decision
points (red bars) and non-decision points (blue bars). Notice that brain
activation was greater for decision-point objects even if they weren't
remembered. (Adapted from Janzen & van Turennout, 2004)

Chapter 7  Taking Action

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

result, indicated by the right pair of bars, is that the advantage for
decision-point objects also occurred for objects that were not
remembered during the recognition test. Janzen and van Turennout
concluded that the brain automatically distinguishes objects that are
used as landmarks to guide navigation. The brain therefore responds not
just to the object but also to how relevant that object is for guiding
navigation. This means that the next time you are trying to find your
way along a route that you have traveled before but aren't totally
confident about, activity in your parahippocampal gyrus may
automatically be "highlighting" landmarks that indicate when you should
continue going straight, turn right, or turn left, even when you may not
remember having seen these landmarks before (see also Janzen, 2006;
Janzen et al., 2008). In addition to evidence that the brain contains
neurons that keep track of landmarks, there is also evidence that the
brain creates a map of the environment.

the time, rewarding the rat with food every time it turns right should
strengthen the "turn right" response, and so increase the chances that
the rat will turn right to obtain food in future. However, after taking
precautions to be sure the rat couldn't determine the location of the
food based on smell, Tolman placed the rat at C, and something
interesting happened. The rat turned left at the intersection to reach
the food at B (Figure 7.13c). This result is important because it shows
that the rat did not merely learn a sequence of moves to a get to the
food during training. Instead, the rat was able to use its cognitive map
of the spatial layout of the maze to locate the food (Tolman, 1948).
More than 30 years after Tolman's experiments, John O'Keefe recorded the
activity of individual neurons in a rat's hippocampus (see Figure 4.31),
and found neurons that fired when the rat was in a specific place within
the box and that different neurons preferred different locations
(O'Keefe & Dostrovsky, 1971; O'Keefe & Nadel, 1978). A record similar to
those determined by O'Keefe is shown in Figure 7.14a. The gray lines
show the path taken by a rat as it wandered around a recording box.
Overlaid on top of this path are the locations where four different
neurons fired. In this example, the "purple neuron" only fired when the
animal was in the upper right portion of the box, and the "red neuron"
only fired when the rat was in the lower left corner. These neurons have
come to be called place cells because they only fire when an animal is
in a certain place in the environment. The area of the environment
within which a place cell fires is called its place field. The discovery
of place cells was an important first step in determining how the
brain's "GPS system" works. Further research identified neurons called
grid cells in an area near the hippocampus called the entorhinal cortex
(see Figure 4.31, page 85) which are arranged in regular, gridlike
patterns like the three types of grid cells (denoted by orange, blue,
and green dots) shown in Figure 7.14b (Fyhn et al., 2008; Hafting et
al., 2005). One possible function of grid cells is to provide
information about the direction of movement (Moser, Moser, et al., 2014;

Cognitive Maps: The Brain's "GPS" Have you ever had the experience of
being unsure of where you are, such as emerging from a subway station
and not knowing which direction you are facing, or losing track of where
you are in a walk in the woods? Joshua Julian and coworkers (2018)
suggest that the experience of being lost underscores the fact that we
are spatially oriented most of the time---but not always. The idea that
we usually know where we are in space has given rise to the idea that we
have a map in our heads, called a cognitive map, that helps us keep
track of where we are. Early research on cognitive maps was carried out
by Edward Tolman, who was studying how rats learned to run through mazes
to find rewards. In one of his experiments, Tolman (1938) placed a rat
in a maze like the one in Figure 7.13. Initially, the rat explored the
maze, running up and down each of the alleys (Figure 7.13a). After this
initial period of exploration, the rat was placed at A and food was
placed at B, and the rat quickly learned to turn right at the
intersection to obtain the food (Figure 7.13b). According to simple
learning theories of

C

D

C

C

B D

B D

B

Food

Food

A

A

A

(a) Explore maze

(b) Turn right for food

(c) Turn left for food

Figure 7.13 Maze used by Tolman. (a) The rat initially explores the
maze. (b) The rat learns to turn right to obtain the food at B when it
starts at A. (c) When placed at C, the rat turns left to reach the food
at B. In this experiment, precautions were taken to prevent the rat from
knowing where the food was based on cues such as smell.

7.3 Finding Your Way Through the Environment

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

157

Figure 7.14 A record similar to those O'Keefe produced by recording from
neurons in a rat's hippocampus as it walked inside a box. (a) The path
taken by a rat in a box is outlined in gray. The positions within the
box where four place cells fired are highlighted by red, blue, purple,
and green dots. (b) The positions within the box where three grid cells
fired are denoted by orange, blue, and green dots. See text for details.

(a) 
(b) 

Moser, Roudi, et al., 2014). For example, movement along the pink arrow
would lead to responses in the "orange cell," then the "blue cell," and
then the "green cell." Movement in other directions would result in
different patterns of firing across grid cells. Thus, grid cells may be
able to code distance and direction information as an animal moves, and
place cells and grid cells probably work together, because they are
connected with each other. There is much left to learn about these cells
and their interconnections, but these discoveries are already recognized
as being so important that John O'Keefe, May-Britt Moser, and Edvard
Moser were jointly awarded the 2014 Nobel Prize in Physiology or
Medicine for their discovery of place and grid cells. What makes these
place and grid cells especially important is that recent experiments
suggest that similar cells may also exist in humans. Joshua Jacobs and
coworkers (2013) found neurons in humans similar to the rat grid cells
by recording from single neurons in patients, like those described in
Chapter 4 (p. 85), who were being prepared for surgery to treat severe
epilepsy. Figure 7.15 shows the results for a neuron in one patient's
entorhinal cortex. The red areas, which indicate high firing frequency,
form a grid pattern similar to what occurs in the rat. Although the
human patterns are "noisier" than the rats', the results from 10
different patients led Jacobs to conclude that these neurons, like the
rat grid cells, help humans create maps of the environment. Cells
similar to rat place cells have also been discovered in humans (Ekstrom
et al., 2003). So the next time you have to navigate along a route, give
credit both to your knowledge of landmarks and to neurons that are
signaling where you are and where you are going.

Individual Differences in Wayfinding Just as different people have
different mental abilities, wayfinding ability varies from one
individual to another. One difference in wayfinding can be traced to
experience. People who have practiced getting from one place to another
in a particular environment are often good at finding their way. 158

Figure 7.15 Colors indicate firing of a neuron in a participant's
entorhinal cortex at locations in the area that the participant was
visiting in the virtual environment. Red indicates the locations
associated with a high firing rate. Note that they are arranged in a
hexagonal layout, similar to what was observed in earlier experiments on
rats. (From Jacobs et al., 2013)

This effect of practice was linked to physiology in an experiment by
Eleanor Maguire and coworkers (2006) who studied two groups of
participants: (1) London bus drivers, who have learned specific routes
through the city, and (2) London taxi drivers, who have to travel to
many different places throughout the city. Figure 7.16a shows the
results of an experiment in which bus drivers and taxi drivers were
asked to identify pictures of London landmarks. Taxi drivers scored
higher than bus drivers, as we might expect from their more widespread
exposure to London. And when the bus and taxi drivers' brains were then
scanned, Maguire found that the taxi drivers had greater volume in the
back of their hippocampus (posterior) (Figure 7.16b) and less volume in
the front (anterior). This result is similar to the results of the
experiencedependent plasticity experiments described in Chapter 4.
Remember that kittens reared in an environment of vertical stripes had
more neurons in their cortex that responded to vertical stripes (see
Figure 4.11, page 74). Similarly, taxi drivers who have extensive
experience navigating have a larger

Chapter 7  Taking Action

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Landmarks correctly recognized

42

(a) 

along a route (perception), paying attention to specific objects
(attention), using information stored from past trips through the
environment (memory), and combining all this information to create maps
that help us relate what we are perceiving to where we are now and where
we are planning to go next.

40.8

40 38 36

35.2

34

TEST YOuRSELF 7.1

32

1.  What is the moral of the story about sea squirts? Taxi drivers

Bus drivers

2.  What was J. J. Gibson's motivation for establishing the ecological
    approach to perception?
3.  What is optic flow? What are two characteristics of optic flow?
    Describe the experiment that considered whether people can use optic
    flow to determine their heading.
4.  What is invariant information? How is invariance related to optic
    flow?
5.  What is observer-produced information? Describe its role in
    somersaulting and why there is a difference between novices and
    experts when they close their eyes.
6.  What is the point of the "keeping your balance" demonstration?
7.  Describe the swinging room experiments. What principles do they
    illustrate?

<!-- -->

(b) 

Figure 7.16 (a) Performance on a landmark test by London taxi drivers
and bus drivers. A perfect score is 48. (b) Cross section of the brain.
Yellow indicates greater hippocampus volume in London taxi drivers
compared to London bus drivers, as determined by magnetic resonance
imaging. (From Maguire et al., 2006)

posterior hippocampus. Importantly, drivers with the largest posterior
hippocampus were the ones with the most years of experience. This final
result provides strong support for experience-dependent plasticity (more
experience creates a larger hippocampus) and rules out the possibility
that it is simply that people with a larger hippocampus are more likely
to become taxi drivers than bus drivers. Is there any evidence for
hippocampus-related navigation differences among non-taxi-drivers? Iva
Brunec and coworkers (2019) answered this question by giving a
questionnaire to a group of young adults to measure their reliance on
mapbased strategies when navigating. For example, they were asked, "When
planning a route, do you picture a map of your route?" People who scored
higher on using mapping strategies performed better on a navigation
test, and also had larger posterior hippocampus and smaller anterior,
just like the London taxi drivers. When taken together, the important
message of all of these studies is that wayfinding is multifaceted. It
depends on numerous sources of information and is distributed throughout
many structures in the brain. This isn't surprising when we consider
that wayfinding involves seeing and recognizing objects

8.  What is an affordance? Describe the results of the experiments on
    patient M.P. that illustrate the possible operation of affordances.
9.  Describe the experiment that compared the EEG response to
    manipulable tools and other objects. What is an action affordance?
10. What does research on walking and driving a car tell us about how
    optic flow may (or may not) be used in navigation? What are some
    other sources of information for navigation?
11. What is wayfinding? Describe Hamid's research on looking at
    landmarks.
12. Describe Janzen and van Turennout's experiment in which they
    measured people's brain activity when remembering objects they had
    seen while navigating through a computer simulated museum. What did
    Janzen and van Turennout conclude about the brain and navigation?
13. What did Tolman's rat maze experiment demonstrate?
14. Describe the rat experiments that discovered place cells and grid
    cells. How might these cells help rats navigate?
15. Describe Jacobs and coworkers' experiment that provided evidence for
    human grid cells.
16. Describe the taxi-driver experiment and the experiment on non-taxi
    drivers. What did each experiment reveal about the physiology of
    individual differences in wayfinding?
17. What does it mean to say that wayfinding is "multifaceted"? How does
    wayfinding reveal interactions between perception, attention,
    memory, and action?

7.3 Finding Your Way Through the Environment

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

159

7.4 Interacting With Objects: Reaching, Grasping, and Lifting

Reaching and Grasping Reaching for the bottle is the first step. One way
to understand reaching and grasping is to look at what's going on in the
brain.

Brain Areas for Reaching and Grasping An important breakthrough in
studying the physiology of reaching and grasping came with the discovery
of the ventral (what) and dorsal (where/how/action) pathways described
in Chapter 4 (see Figure 4.23, page 80). Remember that D.F., who had
damage to her ventral pathway, had difficulty recognizing objects or
judging their orientation, but she could "mail" an object by placing it
through an oriented opening. The idea that there is one processing
stream for perceiving objects and another for acting on them helps us
understand what is happening when a person reaches for the ketchup
bottle. The first step, identifying the bottle among the other things on
the table, involves the ventral (what) pathway. The next step, reaching
for the bottle, involves the dorsal (action) pathway. As reaching
progresses, the location of the bottle and its shape are perceived using
the ventral pathway and positioning the fingers to grasp the bottle
involves the dorsal pathway. Thus, reaching for and grasping the bottle
involves continuously perceiving the shape and position of the bottle,
shaping the hand and fingers relative to the bottle, and calibrating
actions in order to grasp the bottle (Goodale, 2011). But this
interaction between the dorsal and ventral pathways isn't the whole
story. Specific areas of the brain are involved in reaching and
grasping. One of the most important areas of the brain for reaching and
grasping is the parietal lobe. The area in the monkey parietal lobe
involved in reaching is called the parietal reach region (PRR) (Figure
7.18).

So far, we have been describing how we move around in the
environment---driving a car, walking, navigating from one place to
another. But another aspect of action is interacting with objects in the
environment. To discuss how people interact with objects, we will
consider the sequence of events in Figure 7.17, in which the following
events occur, culminating in an extremely important
behavior---depositing a dollop of ketchup on a burger! (a) Reaching
toward the bottle (b) Grasping the bottle (c) Lifting and tilting the
bottle (d) Using the other hand to hit the bottle to deposit the ketchup
on the burger We usually accomplish the sequence in Figure 7.17 rapidly
and without thinking. But as we will see, getting the ketchup onto the
burger involves multiple senses, commands sent from the motor area of
the brain to create movement, and predictive mechanisms that involve
corollary discharge signals like the ones we described in Chapter 6
(p. 128), which help create accurate reaching and lifting and which help
adjust the grip so the ketchup bottle is gripped firmly. We begin with
reaching and grasping.

(a) 
(b) 

Parietal reach region (PRR) Premotor (mirror neurons) (d)

Bruce Goldstein

(c) 

Figure 7.17 Steps leading to depositing a dollop of ketchup on a
hamburger. The person (a) reaches for the bottle with her right hand,
(b) grasps the bottle, (c) lifts the bottle, and (d) delivers a "hit" to
the bottle that has been rotated so it is over the hamburger.

160

Pre-frontal cortex (PFC)

Middle temporal (MT) area Medial superior temporal (MST) area

Figure 7.18 Monkey cortex showing location of the parietal reach region
(PRR) and the area of premotor cortex where mirror neurons were found.
In addition, two areas involved in motion perception, the middle
temporal (MT) area and the medial superior temporal (MST) area, are
shown. These areas will be discussed in Chapter 8.

Chapter 7  Taking Action

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Looks at fixation light in dark

Lights on, sees object

Lights out, can't see object

Reaches in dark, then grasps object

Figure 7.19 The monkey's task in Fattori and coworkers' (2010)
experiment. The monkey always looked at the small light above the
sphere. The monkey sees the object to be grasped when the lights go on,
then reaches for and grasps the object once the lights go off and the
fixation light changes color. (From Fattori et al., 2010)

This region contains neurons that control not only reaching but also
grasping (Connelly et al., 2003; Vingerhoets, 2014). Evidence suggests
that there are a number of regions like the monkey's parietal reach
regions in the human parietal lobe (Filimon et al., 2009). Recording
from single neurons in a monkey's parietal lobe has revealed neurons in
an area next to the parietal reach region that respond to specific types
of hand grips. This was determined by Patricia Fattori and coworkers
(2010) using the procedure shown in Figure 7.19: (1) The monkey observed
a small fixation light in the dark; (2) lights were turned on for
Whole-hand prehension

Primitive precision grip

Advanced precision grip

half a second to reveal the object to be grasped; (3) the lights went
out and then, (4) after a brief pause, the fixation light changed color,
signaling that the monkey should reach for the object. The key part of
this sequence occurred when the monkey reached for the object in the
dark. The monkey knew what the object was from seeing it when the lights
were on (a round ball in this example), and so while reaching for it in
the dark, shaped its grip to match the object. A number of different
objects were used, as shown in Figure 7.20a, each of which required a
different grip.

Finger prehension

(a) 
(b) 

0 Neuron A

1

0

1

0

1

0

1

0

1

0

1

0

1

0

1

1

0

1

0

1

0

1

Figure 7.20 Results of Fattori and coworkers' (2010) experiment showing
how three different neurons respond to reaching and grasping of four
different objects. (a) Four objects. The type of grasping movement
associated with each object is indicated above the object. (b) Response
of neuron A to grasping each object. This neuron responds best to
whole-hand prehension. (c) Response of neuron B, which responds best to
advanced precision grip. (d) Response of neuron C, which responds to all
four types of grasping.

(c) 

Neuron B (d)

0 Neuron C

Horizontal axis = Time in seconds Vertical axis = Rate of nerve firing

7.4 Interacting With Objects: Reaching, Grasping, and Lifting

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

161

The key result of the experiment is that there are neurons that respond
best to specific grips. For example, neuron A (Figure 7.20b) responds
best to "whole-hand prehension," whereas neuron B (Figure 7.20c)
responds best to "advanced precision grip." There are also neurons, like
C (Figure 7.20d), that respond to a number of different grips. Remember
that these neurons were firing as the monkey was reaching for the object
in the dark, so the firing was caused not by visual stimulation, but by
the monkey's prediction of what the object's shape would be when it was
grasped. In a follow-up experiment on the same monkeys, Fattori and
coworkers (2012) discovered neurons that responded not only when a
monkey was preparing to grasp a specific object, but also when the
monkey viewed that specific object. An example of this type of neuron,
which Fattori calls visuomotor grip cells, is a neuron that initially
responds when the monkey sees a specific object and then also responds
as the monkey is forming its hand to grasp the same object. This type of
neuron is therefore involved in both perception (identifying the object
and/or its affordances by seeing) and action (reaching for the object
and gripping it with the hand) (also see Breveglieri et al., 2018).

Proprioception We've seen that there are brain areas and neurons that
are involved in guiding the hand on its way to grasp an object. But
there's also another mechanism that helps keep the hand on course. That
mechanism is proprioception--- the ability to sense body position and
movement. Proprioception depends on neurons located throughout the body,
such as the ones shown in Figure 7.21 for the human limb. Proprioceptive
receptors in the elbow joint, muscle spindle, and tendon sense the
position and movement of the arm. We take proprioception for granted,
because it operates without our conscious involvement, but when the
sense of proprioception is lost the results are disastrous, as
illustrated by the case of Ian Waterman, who at the age of 19, suffered
complications accompanying the flu that damaged sensory neu-

(a) 

Muscle spindle

rons from his neck down. Ian was unable to sense the position of his
limbs and also lost the sense of touch on his body. Thus, although Ian
could still contract his muscles, he was unable to coordinate his
movements. After years of training he was eventually able to carry out
actions, but to do this he had to depend solely on his sense of vision
to monitor the position and movement of his limbs. Ian's need to
constantly visually monitor his movements is quite different than what
happens if proprioception is available, because as you reach for
something, you have two sources of information in addition to visual
information: (1) proprioceptive information, which also provides
information about the position of your hand and arm, and (2) corollary
discharge (CD) signals. Remember from Chapter 6 that when signals are
sent from the motor area to move the eye muscles, a CD signal provides
advance information about how the eyes are going to move. For reaching
and grasping, when signals are sent from the motor area to move the arm
and hand, a CD signal provides advance information about the movement
that helps keep the arm and hand movements on course toward their goal
(Tuthill & Azim, 2018; Wolpert & Flanagan, 2001). Figure 7.22 shows what
happens to reaching if the CD isn't available. In this experiment, the
participant reaches to the right until they hear a tone, which signals
that they should reach to the left for the target. Participants could do
this accurately, as indicated by the blue line. But if the CD signals
are disrupted by electrical stimulation of the cerebellum, a structure
important for controlling motor functioning, the reach misses the
target, as indicated by the red line (Miall et al., 2007; Shadmehr et
al., 2010). Table 7.1 summarizes the three sources of information that
help guide our hand toward a target.

Lifting the Bottle Once the bottle in Figure 7.17 is grasped, it is
lifted, and the person makes a prediction to determine the force of the
lift. This prediction takes into account the size of the bottle, how
full it is, and past lifting experiences with similar objects. Thus,
different predictions occur if the bottle is full or almost empty, and
if the prediction is accurate, the bottle will be lifted with just the
right force. However, what if the person thinks the bottle is full, but
it turns out to be almost empty? In this situation the person uses too
much force, so the lift is too high. Table 7.1 Signals That Help Guide
Reaching

Joint receptors

Golgi tendon organ

Figure 7.21 Proprioceptive neurons are located in the elbow joint,
tendon, and muscle spindle of the arm. (Based on Tuthill & Azim, 2018)

162

Signal

Purpose

Visual

Monitor hand position

Proprioceptive

Sense hand/arm position

Corollary discharge

Provide information from motor signals about where hand/arm is going to
move

Chapter 7  Taking Action

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

(a)

Figure 7.22 (a) The participant moves the arm to the right and then,
responding to a tone, reaches for the target. (b) Blue line = accurate
reaching. Red line = how reach is thrown off course when CD signal is
disrupted by electrical stimulation. (From Shadmehr, et al., 2010)

(b) Virtual target

Half-silvered mirror

Reaching movement Lateral movement TMS

Target

This effect of erroneously predicting weight is demonstrated by the
size-weight illusion, in which a person is presented with two weights.
The weight on the left is small and the weight on the right is large,
but their weights are exactly the same (Figure 7.23). The person is told
to grasp the handles on top of each weight and, when he hears a signal,
to lift them simultaneously. What happens next surprises him, because he
lifts the large weight on the right much higher than the small weight on
the left and says that the larger weight feels lighter. (Remember that
both weights are actually the same weight.) Thus, the sizeweight
illusion, which was first described by French physician Augustin
Charpientier in 1891, shows that when observing two

Control

differently sized objects, we predict that the larger one will be
heavier, so we exert more force to lift it, causing it to be lifted
higher and, surprisingly, to feel lighter (Buckingham, 2014).

Adjusting the Grip Once the bottle has been lifted and rotated, so it is
poised above the burger, we are ready to dispense the ketchup. The
ketchup, however, is not cooperating, so a swift hit delivered by the
other hand (Figure 7.24) is needed to get it to leave the bottle. It is
important, of course, to grip the bottle firmly

Figure 7.23 This person, who is getting ready to simultaneously lift the
two weights, is about to experience the size-weight illusion, because
both weights are the same weight, even though they differ in size.

1 kg

1 kg

7.4 Interacting With Objects: Reaching, Grasping, and Lifting

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

163

up a pen to write involves a different configuration of the fingers and
force of grip than picking up the ketchup bottle or picking up a pen to
move it to another place on your desk. Considering the number of actions
you carry out every day, there is a lot of prediction going on. Luckily
you usually don't have to think about it because your brain takes care
of it for you.

Predictor Corollary discharge Predicted hit

Motor command

7.5 Observing Other People's Actions

(a) Force Grip

Hit force Time

Mirroring Others' Actions in the Brain

(b) Lag

Force

{ Grip

Hit force Time Figure 7.24 (a) A person getting ready to hit the bottom
of the ketchup bottle. The motor command being sent to the hitting
(left) hand is accompanied by a corollary discharge which predicts the
force of the upcoming hit. This prediction causes the motor command sent
to the right hand (not shown) to grip the bottle with a force (green)
that closely matches the force and timing of the hit (red), so the
bottle is held tight when the hit is delivered. (b) When someone else
hits the bottle, there is no prediction of the force of the upcoming
hit, so the grip force lags behind the hit force and must be increased
to prevent slippage. (From Wolpert & Flanagan, 2001)

so the hit won't cause the bottle to slip. Figure 7.24a shows that when
the person delivers the hit, the force of the grip (green) increases
exactly when the hit is delivered (red). However, Figure 7.24b shows
that when someone else delivers the hit, the increase in grip lags
behind the hit. Why is the grip adjusted quickly and accurately in (a)?
Because a corollary discharge provides information about the timing and
force of the hit. When someone else delivers the hit, there is no CD, so
there is no prediction of what is going to happen. Simple actions that
we carry out every day therefore depend on constant interactions between
sensory and motor components of the nervous system, and constant
prediction of what's going to happen next: how far to reach, how to
adjust your hand to grasp things properly, how hard to grip, all of
which depend on the objects involved and the upcoming task. Thus,
picking 164

We not only take action ourselves, but we regularly watch other people
take action. This "watching others act" is most obvious when we watch
other people's actions on TV or in a movie, but it also occurs any time
we are around someone else who is doing something. One of the most
exciting outcomes of research studying the link between perception and
action was the discovery of neurons in the premotor cortex (Figure 7.18)
called mirror neurons.

In the early 1990s, a research team led by Giacomo Rizzolatti was
investigating how neurons in the monkey's premotor cortex fired as the
monkey performed actions like picking up a toy or a piece of food. Their
goal was to determine how neurons fired as the monkey carried out
specific actions. But as sometimes happens in science, they observed
something they didn't expect. When one of the experimenters picked up a
piece of food while the monkey was watching, neurons in the monkey's
cortex fired. What was so unexpected was that the neurons that fired to
observing the experimenter pick up the food were the same ones that had
fired earlier when the monkey had itself picked up the food (Gallese et
al., 1996). This initial observation, followed by many additional
experiments, led to the discovery of mirror neurons--- neurons that
respond both when a monkey observes someone else grasping an object such
as food on a tray (Figure 7.25a) and when the monkey itself grasps the
food (Figure 7.25b; Rizzolatti et al., 2006). They are called mirror
neurons because the neurons' response to watching the experimenter grasp
an object is similar to the response that would occur if the monkey were
performing the same action. Just looking at the food causes no response,
and watching the experimenter grasp the food with a pair of pliers, as
in Figure 7.25c, causes only a small response (Gallese et al., 1996;
Rizzolatti et al., 2000). This last result indicates that mirror neurons
can be specialized to respond to only one type of action, such as
grasping or placing an object somewhere. Simply finding a neuron that
responds when an animal observes a particular action doesn't tell us why
the neuron is firing, however. For example, we could ask if the mirror
neurons in Rizzolatti's study were responding to the anticipation of
receiving food rather than to the experimenter's specific actions. It
turns out that this cannot be a reasonable explanation

Chapter 7  Taking Action

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Firing rate

Figure 7.25 Response of a mirror neuron. (a) Response to watching the
experimenter grasp food on the tray. (b) Response when the monkey grasps
the food. (c) Response to watching the experimenter pick up food with a
pair of pliers. (From Rizzolatti et al., 2000)

100

Sees experimenter break peanut and hears sound

0

(a) 

Firing rate

because the type of object made little difference. The neurons responded
just as well when the monkey observed the experimenter pick up an object
that was not food. But could the mirror neurons simply be responding to
the pattern of motion? The fact that the neuron does not respond when
watching the experimenter pick up the food with pliers argues against
this idea. Further evidence that mirror neurons are doing more than just
responding to a particular pattern of motion is the discovery of neurons
that respond to sounds that are associated with actions. These neurons
in the premotor cortex, called audiovisual mirror neurons, respond when
a monkey performs a hand action and when it hears the sound associated
with this action (Kohler et al., 2002). For example, the results in
Figure 7.26 show the response of a neuron that fires (a) when the monkey
sees and hears the experimenter break a peanut, (b) when the monkey just
sees the experimenter break the peanut, (c) when the monkey just hears
the sound of the breaking peanut, and (d) when the monkey breaks the
peanut. What this means is that just hearing a peanut breaking or just
seeing a peanut being broken causes activity that is also associated
with the perceiver's action of breaking a peanut. These neurons are
responding, therefore, to what is "happening"---breaking a
peanut---rather than to a specific pattern of movement. At this point
you might be asking whether mirror neurons are also present in the human
brain. After all, we've only been talking about monkey brains so far.
Some research with humans does suggest that our brains also contain
mirror neurons. For example, researchers using electrodes to record the
brain activity in people with epilepsy in order to determine which part
of their brains was generating their seizures have recorded activity
from neurons with the same mirror properties as those identified in
monkeys (Mukamel et al., 2010). Additional work using fMRI in
neurologically normal individuals has further suggested that these
neurons are distributed throughout the frontal, parietal, and temporal
lobes (Figure 7.27) in a network that has been broadly called the mirror
neuron system (Caspers et al., 2010; Cattaneo & Rizzolatti, 2009;
Grosbras et al., 2012; Molenberghs et al., 2012). However, a great deal
more research is needed to determine if and how this mirror neuron
system supports perception and action in humans. In the next section, we
highlight some of the work that seems promising in identifying the role
mirror neurons play in human perception and performance.

Firing rate

(c) 

100

Sees experimenter break peanut

0

(b) 

Firing rate

(b) 

100

Hears sound

0

(c) 

Firing rate

(a) 

100

Monkey breaks peanut

0

(d) 

Figure 7.26 Response of an audiovisual mirror neuron to four different
stimuli. (From Kohler et al., 2002)

Predicting People's Intentions Some researchers have proposed that there
are mirror neurons that respond not just to what is happening but to why
something is happening, or more specifically, to the intention behind
what is happening. To understand what this means, let's visit a coffee
shop and observe a person reaching for her coffee cup. Why, we might
wonder, is she reaching for the cup? One obvious answer is that she
intends to drink some coffee, although 7.5 Observing Other People's
Actions

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

165

Frontal Lobe

Parietal Lobe

Temporal Lobe

Figure 7.27 Cortical areas in the human brain associated with the mirror
neuron system. Colors indicate the type of actions processed in each
region: From top down: Purple, reaching; Blue, upper limb movements;
Orange, tool use; Green, movements not directed toward objects; Dark
blue, upper limb movements. (Adapted from Cattaneo & Rizzolatti, 2009)

if we notice that the cup is empty, we might instead decide that she is
going to take the cup back to the counter to get a refill, or if we know
that she never drinks more than one cup, we might decide that she is
going to place the cup in the used cup bin. Thus, there are a number of
different intentions that may be associated with the same action. What
is the evidence that the response of mirror neurons can be influenced by
different intentions? Mario Iacoboni and coworkers (2005) did an
experiment in which they measured participants' brain activity as they
watched short film clips represented by the stills in Figure 7.28.
Stills for the two Intention films, on the right, show a hand reaching
in to pick up a cup. But there is an important difference between the
two scenes. In the top panel, the table is neatly set up, the food is

Control film: Context

Control film: Action

Intention film

Before tea

Drinking

After tea

Cleaning up

Bruce Goldstein

Figure 7.28 Images from the Context, Action, and Intention film clips
viewed by Iacoboni and coworkers' (2005) participants. Each column
corresponds to one of the experimental conditions. In the Context
condition there were two clips, before tea (everything in its place) and
after tea (a mess). In the Action condition the two types of grips
(whole hand and using the handle) were shown an equal number of times.
In the Intention condition, the "drinking" context was the same as
"before tea" but with the hand added. The "cleaning up" context
corresponded to "after tea." The two types of hand grips (whole hand and
using the handle) were shown an equal number of times during the
"drinking" and "cleaning" clips.

untouched, and the cup is full of tea. In the bottom panel, the table is
a mess, the food has been eaten, and the cup appears to be empty.
Iacoboni hypothesized that viewing the top film would lead the viewer to
infer that the person was picking up the cup to drink from it and
viewing the bottom film would lead the viewer to infer that the person
was picking up the cup to clean up. Iacoboni's participants also viewed
the control films shown in the other panels. The Context control film
showed the table setting, and the Action control film showed the hand
reaching in to pick up an isolated cup. The reason these two types of
films were presented was that they contained the visual elements of the
intention films, but didn't suggest a particular intention. Figure 7.29
shows the brain response for the intention films and the control films.
The key result is that there was a significant difference between the
response to the drinking and cleaning up films, but there was no
difference between the before tea and after tea context films. Based on
the difference in activity between the two Intention conditions,
Iacoboni concluded that the mirror neuron area is involved with
understanding the intentions behind the actions shown in the films. He
reasoned that if the mirror neurons were just signaling the action of
picking up the cup, then a similar response would occur regardless of
whether a context surrounding the cup was present. Mirror neurons,
according to Iacoboni, code the "why" of actions and respond differently
to different intentions. If mirror neurons do, in fact, signal
intentions, how do they do it? One possibility is that the response of
these neurons is determined by the chain of motor activities that could
be expected to happen in a particular context (Fogassi et al., 2005;
Gallese, 2007). For example, when a person picks up a cup with the
intention of drinking, the next expected actions would be to bring the
cup to the mouth and then to drink some coffee or tea. However, if the
intention is to clean up, the expected action might be to carry the cup
over to the sink. According to this

(From Iacoboni et al., 2005)

166

Chapter 7  Taking Action

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Figure 7.29 Iacoboni and coworkers' (2005) results, showing the brain
response for the Action, Drinking, and Cleaning conditions. The
important result is that for the intention condition, the drinking
response is significantly larger than the cleaning response. However,
there is no difference between drinking and cleaning for the context
condition.

0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 --0.1 --0.2 --0.3 Action

Context: drinking

Context: cleaning

Intention: drinking

idea, mirror neurons that respond to different intentions are responding
to the action that is happening plus the sequence of actions that is
most likely to follow, given the context. The exact functions of mirror
neurons in humans are still being actively researched (Caggiano et al.,
2009; Rizzolati & Singaglia, 2016). In addition to proposing that mirror
neurons signal what is happening as well as the intentions behind
various actions, researchers have also proposed that mirror neurons help
us understand (1) communications based on facial expressions (Buccino et
al., 2004; Ferrari et al., 2003); (2) emotional expressions (Dapretto et
al., 2006); (3) gestures, body movements, and mood (Gallese, 2007;
Geiger et al., 2019); (4) the meanings of sentences (Gallese, 2007); and
(5) differences between ourselves and others (Uddin et al., 2007). As
might be expected from this list, it has also been proposed that mirror
neurons also play an important role in guiding social interactions
(Rizzolatti & Sinigaglia, 2010; Yoshida et al., 2011) and that disorders
characterized by impaired social interactions such as autism spectrum
disorder may be associated with abnormal functioning of the mirror
neuron system (Oberman et al., 2005, 2008; Williams et al., 2001). As
with any newly discovered phenomenon, the function of mirror neurons is
a topic of debate among researchers. Some researchers propose that
mirror neurons play important roles in human behavior (as noted above),
and others take a more cautious view (Cook et al., 2014; Hickock, 2009).
One of the criticisms of the idea that mirror neurons determine people's
intentions is that the mirror neuron response occurs so rapidly after
observing an action that there's not enough time to understand the
action. It has, therefore, been suggested that mirror neurons could help
detect and recognize an action but that then a slower process may be
needed to achieve an understanding of a person's intentions behind the
action (Lemon, 2015). Consider that when feature detectors that respond
to oriented moving lines were discovered in the 1960s, some researchers
proposed that these feature detectors could explain

Intention: cleaning

how we perceive objects. With the information available at the time,
this was a reasonable proposal. However, later, when neurons that
respond to faces, places, and bodies were discovered, researchers
revised their initial proposals to take these new findings into account.
In all likelihood, a similar process will occur for mirror neurons. Some
of the proposed functions will be confirmed, but others may need to be
revised.

7.6 Action-Based Accounts of Perception The traditional approach to
perception has focused on how the environment is represented in the
nervous system and in the perceiver's mind. According to this idea, the
purpose of visual perception is to create a representation in the mind
of whatever we are looking at. Thus, if you look at a scene and see
buildings, trees, grass, and some people, your perception of the
buildings, trees, grass, and people is representing what is "out there,"
and so accomplishes vision's purpose of representing the environment.
But as you might have suspected after reading this chapter, many
researchers believe that the purpose of vision is not to create a
representation of what is out there but to guide our actions that are
crucial for survival (Brockmole et al., 2013; Goodale, 2014; Witt,
2011a). The idea that action is crucial for survival has been described
by Melvyn Goodale (2011) as follows: "Many researchers now understand
that brains evolved not to enable us to think, but to enable us to move
and interact with the world. Ultimately, all thinking (and by extension,
all perception) is in the service of action" (p. 1583). According to
this idea, perception may provide valuable information about the
environment, but taking a step beyond perception and acting on this 7.6
Action-Based Accounts of Perception

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

167

information enables us to survive so we can perceive another day (Milner
& Goodale, 2006). The idea that the purpose of perception is to enable
us to interact with the environment has been taken a step further by
researchers who have turned the equation around from "action depends on
perception" to "perception depends on action." The action-specific
perception hypothesis (Witt, 2011a) states that people perceive their
environment in terms of their ability to act on it. This hypothesis has
been largely based on the results of experiments involving sports. For
example, Jessica Witt and Dennis Proffitt (2005) presented a series of
circles to softball players just after they had finished a game, and
asked them to pick the circle that best corresponded to the size of a
softball. When they compared the players' estimates to their batting
averages from the just-completed game, they found that batters who hit
well perceived the ball to be bigger than batters who were less
successful. Other experiments have shown that tennis players who have
recently won report that the net is lower (Witt & Sugovic, 2010) and
participants who were most successful at kicking football field goals
estimated the goal posts to be farther apart (Witt & Dorsch, 2009). The
field goal experiment is especially interesting because the effect
occurred only after they had attempted 10 field goals. Before they
began, the estimates of the poor kickers and the good kickers were the
same. These sports examples all involved making judgments after doing
either well or poorly. This supports the idea that perception can be
affected by performance. What about situations in which the person
hasn't carried out any action but has an expectation about how difficult
it would be to perform that action? For example, what if people who were
physically fit and people who were not physically fit were asked to
estimate distances? To answer this question, Jessica Witt and coworkers
(2009) asked people with chronic back and/or leg pain to estimate their
distance from various objects placed in a long hallway. Compared to
people without pain, the chronic pain group consistently overestimated
their distance from objects. The reason for this, according to Witt, is
that over time people's general fitness level affects their perception
of how difficult it will be to carry out various sorts of physical
activity, and this in turn affects their perception of the activity.
Thus, people with pain that makes walking difficult will perceive an
object as being farther away, even if they are just looking at the
object. Also, older people, who generally have less physical ability
than younger people, estimate distances to be farther compared to
younger adults (Sugovic & Witt, 2013). Some researchers, however,
question whether the perceptual judgments measured in some of the
experiments we have described are actually measuring perception.
Participants in these experiments might, they suggest, be affected by
"judgment bias," caused by their expectations about what they think
would happen in a particular situation. For example, participants'
expectation that objects could appear farther when a person has
difficulty walking might cause them to say the object appears farther,
even though their perception of 168

distance was actually not affected (Durgin et al., 2009, 2012; Loomis &
Philbeck, 2008; Woods et al., 2009). This explanation highlights a basic
problem in measuring perception in general: Our measurement of
perception is based on people's responses, and there is no guarantee
that these responses accurately reflect what a person is perceiving.
Thus, as pointed out above, there may be some instances in which
participants' responses reflect not what they are perceiving, but what
they think they should be perceiving. For this reason, researchers have
tried very hard to conduct experiments in which the effects of action
have been demonstrated even when there are no obvious expectations or
task demands (Witt, 2011a, 2011b; Witt et al., 2010). A reasonable
conclusion, taking a large number of experiments into account, is that
in some experiments participants' judgments may be affected by their
expectations, but in other experiments their judgments may reflect a
real relationship between their ability to act and their perception.

SOMETHING TO CONSIDER:

Prediction is Everywhere We first introduced the term "prediction" in
Chapter 5, "Perceiving Objects and Scenes," when we discussed how the
Gestalt laws of organization predict what we will perceive in certain
situations. But the history of prediction in perception really began
with Helmholtz's theory of unconscious inference (p. 108). Helmholtz
didn't use the word "prediction" when he proposed his theory, but think
about what he proposed: We perceive the object that is most likely to
have caused the image on the retina. In other words, perception of an
object is based on a prediction about what's probably out there.
Prediction also appeared in Chapter 6, "Visual Attention," when we saw
that people often direct their gaze to where they expect something is
going to be (p. 131). Chapter 6 also introduced how the corollary
discharge indicates where the eye is going to move next (p. 128). What
makes this a prediction is that the CD occurs before the movement
happens. The CD is saying, basically, "here's what's coming next." And
luckily for us, that message helps keep our perception of the world
stable, even as the moving eye creates a smeared image on the retina.
These examples of prediction from Chapters 5 and 6 have two things in
common: (1) we are not usually conscious of their operation, and (2)
they operate extremely rapidly---on a scale of fractions of a second.
This rapid operation makes sense, because we see objects almost
instantaneously, and because the eyes move very rapidly. We also
encountered prediction a number of times in this chapter, especially as
we discussed dispensing the ketchup on that burger. The CD came into
play again, as one of the sources of information that helps guide our
reach and that helps calibrate our grip so it is strong enough to keep
the bottle from slipping when we hit it.

Chapter 7  Taking Action

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Prediction is also involved in some of the other things we discussed in
this chapter, especially predicting other people's intentions, and how
predictions about how to interact with the environment can be based on
how difficult we think it will be to carry out a particular action.
Compared to the object and eye movement predictions described in
Chapters 5 and 6, this chapter's action predictions occur on a longer
time scale---seconds, rather than fractions of a second. Also, we can be
conscious of making some predictions, such as when we're trying to
predict how heavy an object is that we have to lift or when we are
trying to determine someone else's intentions. In the next chapter, we
will consider how prediction (and our friend the CD) plays a role in
perceiving motion, and later, we will see that prediction comes into
play in the following chapters:

find the music jarring, or more interesting, depending on how much the
music has deviated from what we were expecting. ■■ Chapter 14,
"Perceiving Speech." Prediction is involved in our ability to perceive
words, sentences, and stories. Prediction also influences how we
perceive our own voices as we are speaking. ■■ Chapter 15, "Cutaneous
Senses." Prediction influences our experience when we are touched, so
touching yourself and being touched by someone else can feel different.
This is one reason you can't tickle yourself! ■■ Chapter 16, "Chemical
Senses." The fact that the experience of taste is influenced by what we
were expecting to taste is demonstrated by strong reactions people
experience when they think they are going to taste one thing, but
something else happens instead.

Chapter 10, "Perceiving Depth and Size." Our perception of size is
determined by our perception of its depth. ■■ Chapter 13, "Perceiving
Music." Prediction is a central feature of music perception, because
people expect a musical composition to unfold in a specific way.
Sometimes, when unexpected notes or phrases occur, we can

From these examples, it is clear that prediction is a general phenomenon
that occurs across many sensory modalities. One reason for this is
adaptive: Knowing what is coming next can enhance survival. As we will
see in the chapters that follow, prediction can not only let us know
what's coming, but can shape the very nature of our perceptions.

■■

DEVELOPMENTAL DIMENSION Infant Affordances An infant exploring the
environment by crawling (Figure 7.30a) sees the world from a low vantage
point. But when the infant becomes older and is able to stand and walk,
his view of the world changes (Figure 7.30b). These two situations have
recently been studied by considering infant affordances. Remember that
affordances were described by J. J. Gibson as what it offers an animal,
what it provides or furnishes (p. 152). As we consider infant
affordances, we will be considering what the environment offers infants
in their quest to locomote through the environment, first by crawling,
then by walking. Very young infants can't do much, so the range of
possible affordances is

relatively narrow. But as physical and motor development create new ways
of interacting with the environment, new affordances for action open up.
Infants learn to crawl at about 8 months (range = 6--11 months) and to
walk at about 12 months (range = 9--15 months) (Martorell et al., 2006).
Compared with crawling, walking infants move more, go farther, travel
faster, carry objects more frequently, and perhaps most important, enjoy
an expanded field of view of the world (Adolph & Tamis-LeMonda, 2014).
Figure 7.31 shows how crawling infants mostly see the ground in front of
their hands, but when infants stand

(a) 
(b) 

Joanna Oseman

Figure 7.30 The infant's view of the world changes when the infant
progresses from crawling to standing.

Continued Something to Consider: Prediction Is Everywhere

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

169

Figure 7.31 Infant crawling versus walking viewpoints. (Adolph, 2019)

on two feet and walk, the whole room swoops into view, creating new
possibilities for seeing and doing (Adolph & Hoch, 2019; Kretch et al.,
2014). Thus, as the infants develop new ways of locomoting, they gain
new affordances for getting from one place to another. Kari Kretch and
Karen Adolph (2013) used the adjustable drop-off shown in Figure 7.32 to
test crawling and walking infants' ability to perceive affordances for
locomotion. All of the babies were the same age (12 months), but half
were experienced crawlers (mean = 18 weeks of crawling experience),

and half had just begun walking (mean = 5 weeks of walking experience).
The apparatus was adjusted in 1-cm increments to create drop-offs
ranging from small (a navigable 1-cm "step") to large (an impossibly
high 90 cm "cliff "). Infants began each trial facing the
precipice---crawlers on hands and knees, and walkers upright. Caregivers
stood at the bottom of the dropoff and encouraged infants to crawl or
walk using toys and snacks as incentives. A highly trained experimenter
followed alongside infants to catch them if they misperceived the
affordances and fell.

Figure 7.32 The adjustable drop-off apparatus used by Kretch and Adolph
(2013). Infants were tested on drop-offs ranging from 1 cm to 90 cm. The
90-cm cliff is shown here. (Kretch & Adolph, 2013) 170

Chapter 7  Taking Action

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Every infant crawled or walked over the small, 1-cm dropoff. But
experienced crawlers and novice walkers behaved very differently on
larger drop-offs. The experienced crawlers only attempted to crawl over
drop-offs within their ability. When drop-offs were beyond their
ability, they stopped at the edge or scooted down backward feet first.
None attempted to crawl over the 90-cm cliff. The novice walkers were a
completely different story. They walked blithely over small and large
dropoffs alike on trial after trial, regardless of the affordances. They
even attempted to walk over the 90-cm drop-off on 50 percent of trials
(so the experimenter had to catch them in mid-air). What do these
results mean? The novice walkers averaged only 5 weeks of walking
experience, but before that, they had been experienced crawlers with
about 16 weeks of crawling experience just like the experienced crawlers
who had perceived affordances accurately and refused to crawl over a
cliff-sized precipice. It would seem that all those weeks of crawling
experience should teach infants to avoid locomotion over large
drop-offs---regardless of whether they're crawling or walking. But it
does not. After crawlers get up and begin to walk, they must relearn how
to perceive affordances for their new upright posture, which creates a
new, higher vantage point. And sure enough, at 18 months of age, after
the once-novice walkers had acquired about 23 weeks of walking
experience, they perceived affordances accurately again. Just like
experienced crawlers, experienced walkers only attempted to walk over
drop-offs within their ability. They stopped at the edge of the 90-cm
cliff on 100 percent of trials. Thus, crawling experience teaches
infants to perceive affordances for crawling, and walking experience
teaches infants to perceive affordances for walking. The
body--environment relations for crawling and walking are completely
different, the information-gathering behaviors and vantage point are
different, so learning from the earlier developing skill does not
transfer to the later developing skill. Another aspect of infant
affordances in addition to experience is planning. Infants must perceive
what's coming up before it happens, so they can adjust their behavior
accordingly. Simone Gil and coworkers (2009) demonstrated that infants
learned to plan their actions ahead of time when walking down slopes
that varied in steepness, from 0 degrees to 50 degrees in 2-degree
increments (Figure 7.33a). In infants' first weeks of walking they
"march straight over the brink of impossibly steep slopes, requiring
rescue by an experimenter" (p. 1). But with each week of walking
experience, infants become better able to distinguish the affordances of
shallow versus steep slopes and to adjust their walking behavior
accordingly. Figures 7.33b and 7.33c show the footprints of an
experienced infant approaching a shallow slope and a steep slope.

The infant took long, evenly spaced steps while walking down the shallow
6-degree slope, but took bunched-up steps before she stepped over the
steep 24-degree slope. These bunched-up steps show that the infant had
predicted what was coming up and then took appropriate action to deal
with it. So just as adults adjust their hand while reaching for a bottle
to match the bottle's shape in anticipation of grasping it, infants
modify their steps while approaching a slope to match the degree of
slant in anticipation of walking on the slanted surface. The message
from these experiments is clear: The next time you see a baby crawling
or walking, remember they're not just moving their arms and legs.
They're acquiring new ways to interact with the environment, and they're
learning to perceive and exploit affordances for action.

(a) 
(b) 

6° slope

(c) 

24° slope

Figure 7.33 (a) An infant deciding whether to descend a slope. (b)
Footprints indicating walking pattern for a 6-degree slope. (c)
Footprints for the steeper 24-degree slope. (From Adolph, 2019)

Something to Consider: Prediction Is Everywhere

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

171

TEST YOuRSELF 7.2 1. Why is a scene perceived as stationary, even though
its image moves across the retina during an eye movement? 2. What is the
corollary discharge? 3. How does the idea of what (ventral) and how
(dorsal) streams help us describe an action such as reaching for a
ketchup bottle? 4. What is the parietal reach region? Describe Fattori's
experiments on "grasping neurons." 5. What is proprioception? What
happened to Ian Waterman?

11. Is there any evidence that there are mirror neurons in the human
    brain?

12. Describe Iacoboni's experiment that suggested there are mirror
    neurons that respond to intentions.

13. What is a possible mechanism that might be involved in mirror
    neurons that respond to intentions?

14. What are some of the proposed functions of mirror neurons? What is
    the scientific status of these proposals?

15. How does disrupting the corollary discharge affect reaching
    behavior?

16. Describe the action-based account of perception. In your discussion,
    indicate (a) why some researchers think the brain evolved to enable
    us to take action and (b) how experiments have demonstrated a link
    between perception and "ability to act."

17. Describe the size-weight illusion. What does it tell us about how
    expectations affect lifting?

18. What is the evidence behind the statement that "prediction is
    everywhere"?

19. Describe what happens to the person's grip on the ketchup bottle
    when (a) they hit the bottle with their other hand, and (b) when
    someone else hits the bottle. Why is there a difference?

20. What is an example of an affordance, as applied to young infants?
    What is the evidence that infants develop affordances? What do
    infant stepping patterns tell us about their affordances?

21. What are three sources of information for the position of the hand
    and arm during reaching?

22. What are mirror neurons? What is the evidence that mirror neurons
    aren't just responding to a specific pattern of motion?

THINK ABOUT IT 1. We have seen that gymnasts appear to take visual
information into account as they are in the act of executing a
somersault. In the sport of synchronized diving, two people execute a
dive simultaneously from two side-by-side diving boards. They are judged
based on how well they execute the dive and how well the two divers are
synchronized with each other. What environmental stimuli do you think
synchronized divers need to take into account in order to be successful?
2. Can you identify specific environmental information that you use to
help you carry out actions in the environment? This question is often
particularly relevant to athletes.

3.  It is a common observation that people tend to slow down as they are
    driving through long tunnels. Explain the possible role of optic
    flow in this situation.
4.  If mirror neurons do signal intentions, what does that say about the
    role of top-down and bottom-up processing in determining the
    response of mirror neurons?
5.  How do you think the response of your mirror neurons might be
    affected by how well you know a person whose actions you were
    observing?
6.  How does your experience in interacting with the environment
    (climbing hills, playing sports) correspond or not correspond to the
    findings of the "potential for action" experiments described in the
    Something to Consider section?

KEY TERMS Action affordance (p. 154) Action-specific perception
hypothesis (p. 168) Affordances (p. 152) Audiovisual mirror neurons
(p. 165) Cognitive map (p. 157) Ecological approach to perception
(p. 150) Focus of expansion (FOE) (p. 151) 172

Gradient of flow (p. 150) Grid cells (p. 157) Invariant information
(p. 151) Landmark (p. 156) Mirror neurons (p. 164) Mirror neuron system
(p. 165) Optic flow (p. 150) Parietal reach region (PRR) (p. 160) Place
cells (p. 157)

Place field (p. 157) Proprioception (p. 162) Size-weight illusion
(p. 162) Spatial updating (p. 155) Visual direction strategy (p. 154)
Visuomotor grip cells (p. 162) Wayfinding (p. 155)

Chapter 7  Taking Action

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

We perceive motion when images move across our retina, as would occur if
these birds flew across our field of view; when we move our eyes to
follow the birds' movements; and when we are influenced by knowledge we
have gained from perceiving motion in the past. Ashley Cooper/Getty
Images

Learning Objectives After studying this chapter, you will be able to ...
■■ Describe five different functions of motion perception.

■■ Describe why we need to go beyond considering the responses

and what research has revealed about the relation between them.

of single neurons to understand the physiology of motion perception.

■■ Describe how we perceive motion both when we move our eyes

■■ Understand how perceiving movement of the body has been

to follow a moving object and when we keep our eyes steady as an object
moves across our field of view.

■■ Describe what it means to say that we can perceive motion in

■■ Understand the difference between real motion and illusory motion

■■ Understand the multiple neural mechanisms that explain mo-

tion perception.

studied both behaviorally and physiologically. still pictures. ■■
Describe how infants perceive biological motion.

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

C h a pter 8

Perceiving Motion

Chapter Contents 8.1 Functions of Motion Perception 8.6 Single-Neuron
Responses to Motion Detecting Things

8.8 Motion and the Human Body

Perceiving Objects Perceiving Events Social Perception Taking Action

Experiments Using Moving Dot Displays Lesioning the MT Cortex
Deactivating the MT Cortex

8.2 Studying Motion Perception

Method: Transcranial Magnetic

8.9 Motion Responses to Still Pictures

Stimulating the MT Cortex

SOMETHING TO CONSIDER: Motion, Motion, and More Motion

When Do We Perceive Motion? Comparing Real and Apparent Motion Two
Real-Life Situations We Want to Explain

8.3 The Ecological Approach to Motion Perception

Apparent Motion of the Body Biological Motion Studied by PointLight
Walkers

Stimulation (TMS)

Method: Microstimulation

DEVELOPMENTAL DIMENSION: Infants

Perceive Biological Motion

8.7 Beyond Single-Neuron Responses to Motion

TEST YOURSELF 8.2

The Aperture Problem

8.4 The Corollary Discharge and Motion Perception

Demonstration: Movement of a Bar

TEST YOURSELF 8.1

Solutions to the Aperture Problem

THINK ABOUT IT

Across an Aperture

8.5 The Reichardt Detector

Some Questions We Will Consider: ■■ Why do some animals freeze in place
when they sense

danger? (p. 176) ■■ How do ﬁlms create movement from still pictures?

(p. 179) ■■ What's special about movement of human and animal

bodies? (p. 188)

P

erhaps the most dramatic way to illustrate the importance of motion
perception to daily life (and survival) comes from case studies of
people who, through disease or trauma, suffer from damage to parts of
the brain responsible for perceiving and understanding movement. When
this happens, a person is said to suffer from a condition called
akinetopsia or "motion blindness," where motion is either very difficult
or impossible to perceive. The most famous and

well-studied case of akinetopsia is a 43-year-old woman known as L.M.
(Zihl et al., 1983, 1991). Without the ability to perceive motion
following a stroke, L.M. was unable to successfully complete activities
as simple as pouring a cup of tea. As she put it, "the fluid appeared to
be frozen, like a glacier," and without the ability to perceive the tea
rising in the cup, she had trouble knowing when to stop pouring. Her
condition caused other, more serious problems as well. It was difﬁcult
for her to follow dialogue because she couldn't see the motions of a
speaker's face and mouth, and people suddenly appeared or disappeared
because she couldn't see them approaching or leaving. Crossing the
street presented serious problems because at ﬁrst a car might seem far
away, but then suddenly, without warning, it would appear very near.
Thus, her disability was not just a social inconvenience but enough of a
threat to the woman's well-being that she rarely ventured outside into
the world of moving---and sometimes dangerous---objects.

175

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

8.1 Functions of Motion Perception

Perceiving Objects

The experience of L.M. and the few other people with akinetopsia makes
it clear that being unable to perceive motion is a great handicap. But
looking closely at what motion perception does for us reveals a long
list of functions.

Detecting Things

Perceiving Events As we observe what's going on around us, we typically
observe ongoing behavior as a sequence of events. For example, consider
events that might occur in a coffee shop. You observe a man enter the
shop, stop at the counter, have a brief conversation with the barista,
who in turn leaves and returns with coffee in a paper cup. The customer
pushes down on the lid to make sure it is secure, pays for the coffee,
drops a tip into the tip jar, turns around, and walks out the door.

(a) 

Stephen Frink/Getty Images

Ashley Cooper/Getty Images

Detection is at the head of the list because of its importance for
survival. We need to detect things that might be dangerous in order to
avoid them. Imagine, for example, that you are reading a book under your
favorite tree on campus when a stray baseball flies in your general
direction. Without thinking, your natural response is to look up from
your book and quickly move out of the ball's path. This is an example of
attentional capture, discussed in Chapter 6 (p. 130), in which our
attention is automatically drawn to salient objects. Motion is a very
salient aspect of the environment, so it attracts our attention
(Franconeri & Simons, 2003). Movement perception is extremely important
for animals that hunt, because movement reveals prey which, when
stationary, may be difficult to see because of camouflage (Figure 8.1),
but which become visible if they move. Movement serves a similar purpose
for the prey, who use movement to detect predators as they approach. Or
examining a less life-or-death function of movement perception, consider
the problem of trying to find your friend among a sea of faces in the
stadium. Looking up at the crowd, you have no idea where to look, but
suddenly you see a person waving and recognize that it is your friend.
The detection function of movement perception comes to the rescue!

Movement helps us perceive objects in a number of ways. Remembering our
discussion from Chapter 5 (p. 91) about how even clearly visible objects
may be ambiguous, you can appreciate how motion of an object can reveal
characteristics that might not be obvious from a single, stationary view
(Figure 8.2a). Movement of an observer around an object can have a
similar effect: Viewing the "horse" in Figure 8.2b from different
perspectives reveals that its shape is not exactly what you may have
expected based on your initial view. Thus, our own motion relative to
objects is constantly adding to the information we have about those
objects, and, most relevant to this chapter, we receive similar
information when objects move relative to us. Observers perceive shapes
more rapidly and accurately when an object is moving (Wexler et al.,
2001). Movement also serves an organizing function, which groups smaller
elements into larger units. The motion of individual birds becomes
perceived as the larger unit of the flock, in which the birds are flying
in synchrony with each other. When a person or animal moves, movement of
individual units--- arms, legs, and body---become coordinated with each
other to create a special type of movement called biological movement,
which we will discuss later in the chapter.

(b) 

Figure 8.1 Even perfectly camouflaged animals like this (a) leaf-tail
gecko and (b) pygmy sea horse would be instantly revealed by their
movement. 176

Chapter 8  Perceiving Motion

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Figure 8.2 (a) The shape and features of this car are revealed when
different aspects of it become visible as it moves. (b) Moving around
this "horse" reveals its true shape.

Bruce Goldstein

(a) 
(b) 

This description, which represents only a small fraction of what is
happening in the coffee shop, is a sequence of events unfolding in time.
And just as we can segment a static scene into individual objects, we
can segment ongoing behavior into a sequence of events, where an event
is defined as a segment of time at a particular location that is
perceived by observers to have a beginning and an end (Zacks & Tversky,
2001; Zacks et al., 2009). In our coffee shop scenario, placing an order
with the coffee barista is an event, reaching out to accept the cup of
coffee is an event, dropping change in the tip jar is an event, and so
on. The point in time when each of these events ends and the next one
begins is called an event boundary. The connection of events to motion
perception becomes obvious when we consider that event boundaries are
often associated with changes in the nature of motion. One pattern of
motion occurs when placing the order, another when reaching out for the
coffee cup, and so on. Jeffrey Zacks and coworkers (2009) measured the
connection between events and motion perception by having participants
watch films of common activities such as paying bills or washing dishes
and asking them to press a button when they believe one unit of
meaningful activity ended and another began (Newtson & Engquist, 1976;
Zacks et al., 2001). When Zacks compared event boundaries to the actor's
body movements measured with a motion tracking system, he found that
event boundaries were more likely to occur when there was a change in
the speed or acceleration of the actor's hands. From the results of this
and other experiments, Zacks concluded that the perception of movement
plays an important role in separating activities into meaningful events.

Social Perception Interactions with other people involve movement at
many levels. L.M.'s akinetopsia made it difficult for her to interact
with people, because she couldn't tell who was talking by seeing their
lips moving. On a larger scale, we use movement cues to determine a
person's intentions. If you see someone across the street waving an arm,
are they hailing a taxi or swatting

at a fly? An experiment by Atesh Koul and coworkers (2019) showed that
the speed and timing of the movement can help answer this type of
question. In their experiment, they had participants observe a hand
reaching for a bottle, with the intention of either drinking or pouring
from it, and were asked to indicate, "pour" or "drink." When the
experimenter compared motion information such as the velocity and
trajectory of the hand, and the nature of the grip, to the participant's
judgments, they found that participants were using this information to
decide why the hand was reaching for the cup (also see Cavallo et al.,
2016). Other experiments have shown that the characteristics of movement
can be used to interpret emotions (Melzer et al., 2019). The link
between motion intention and emotion is so powerful that it can give
human characteristics to geometrical objects. This was demonstrated in a
famous experiment by Fritz Heider and Marianne Simmel (1944), who showed
a 2½-minute animated film to participants and asked them to describe
what was happening in the movie. The movie consisted of a "house" and
three "characters"---a small circle, a small triangle, and a large
triangle. These three geometric objects moved around both inside and
outside the house, and sometimes interacted with each other (Figure
8.3). Although the characters in the film were geometric objects, the
participants created stories to explain the objects' actions, often
giving them humanlike characteristics and personalities. For example,
one account described the small triangle and circle as a couple who were
trying to be alone in the house when the big triangle ("a bully")
entered the house and interrupted them. The small triangle didn't
appreciate this intrusion and attacked the big triangle. In other
studies, researchers have shown how such simple motion displays can
evoke interpretations of desire, coaxing, chasing, fighting, mocking,
fearfulness, and seduction (Abell et al., 2000; Barrett et al., 2005;
Castelli et al., 2000; Csibra, 2008; Gao et al., 2009). Who would have
thought the world of geometric objects could be so exciting? Although
assigning social motives to moving geometrical objects makes a great
story, the real story occurs as we interact 8.1 Functions of Motion
Perception

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

177

Figure 8.3 Still images from a film like one used by Heider and Simmel
(1944). The objects moved in various ways, going in and out of the
"house" and sometimes interacting with each other. The nature of the
movements led participants to make up stories that often described the
objects as having feelings, motivations, and personalities.

with people in social situations. Many social cues are available in
person-to-person interactions, including facial expressions, language,
tone of voice, eye contact, and posture, but research has shown that
movement can provide social information even when these other cues
aren't available. For example, Laurie Centelles and coworkers (2013)
used a way of presenting human motion called point-light walkers, which
are created by placing small lights on people's joints and then filming
the patterns created by these lights when people move (Johansson, 1973,
1975; Figure 8.4). When a person wearing the lights moves, observers see
a "person moving" without any of the other cues that can occur in social
situations. The observers in Centelles' experiment viewed the stimulus
created by two people wearing lights under two conditions: (1) social
interaction: the people were interacting in various ways and (2)
non-social interaction: the people were near each other but were acting
independently. The observers were able to indicate whether the two
people were interacting with each other or were acting independently.
Interestingly, a group of observers with autism spectrum disorder, which
is characterized by having difficulty with real-life social
interactions, were not as good as the other observers at telling the
difference between the social and non-social conditions. Many other
studies have shown that movement provides information that facilitates
social interactions (Barrett et al., 2005; Koppensteiner, 2013).

Finally, let's return to picking up the ketchup bottle we discussed in
Chapter 7 (see Figure 7.17 ). Each component of our action---reaching
for the bottle, grasping it, lifting it--- generates motion that we must
keep track of in order to get the ketchup onto the burger. From the
discussion above, it is clear that perceiving motion is involved in our
lives at many levels: detecting stimuli, perceiving objects,
understanding events, interacting socially with others, and carrying out
physical actions ranging from walking down the sidewalk, to watching
sports, to reaching for a bottle of ketchup. We now consider how
researchers have gone about studying motion perception.

Taking Action Our discussion in Chapter 7, "Taking Action," was full of
move­ ment. Navigating ourselves through the environment and walking down
a crowded sidewalk are examples of how our own movement depends on our
perception of movement. We perceive the stationary scene moving past us
as we walk down the sidewalk. We pay attention to other people's
movements to avoid colliding with them. Movement perception is also
crucially involved in sports--- both watching, as you follow a
double-play unfold in baseball or watch the trajectory of a long pass in
football, or as you participate yourself. 178

Figure 8.4 Point light walker stimulus created by placing lights on a
person and having them walk in the dark. In the experimental situation,
only the lights and their motion are visible.

Chapter 8  Perceiving Motion

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

8.2 Studying Motion Perception A central question in the study of motion
perception is, when do we perceive motion?

When Do We Perceive Motion? The answer to this question may seem
obvious: We perceive motion when something moves across our ﬁeld of
view, which is an example of real motion. Perceiving a car driving by,
people walking, or a bug scurrying across a tabletop are all examples of
the perception of real motion. However, there are three types of
illusory motion---the perception of the motion of stimuli that aren't
actually moving. Apparent motion is the most famous and most studied
type of illusory motion. We introduced apparent motion in Chapter 5 when
we told the story of Max Wertheimer's observation that when two stimuli
in slightly different locations are alternated with the correct timing,
an observer perceives one stimulus moving back and forth smoothly
between the two locations (Figure 8.5a; also see Figure 5.12).

Dark

Flash

Dennis Barnes/Britain On View/Getty Images

Flash

Bruce Goldstein

(a) 

This perception is called apparent motion because there is no actual (or
real) motion between the stimuli. This is the basis for the motion we
perceive in movies, on television, and in moving signs that are used for
advertising and entertainment (Figure 8.5b). Induced motion occurs when
motion of one object (usually a large one) causes a nearby stationary
object (usually smaller) to appear to move. For example, the moon
usually appears stationary in the sky. However, if clouds are moving
past the moon on a windy night, the moon may appear to be racing through
the clouds. In this case, movement of the larger object (clouds covering
a large area) makes the smaller, but actually stationary, moon appear to
be moving. Motion aftereffects occur when viewing a moving stimulus
causes a stationary stimulus to appear to move (Glasser et al., 2011).
One example of a motion aftereffect is the waterfall illusion (Addams,
1834) (Figure 8.6). If you look at a waterfall for 30 to 60 seconds (be
sure it ﬁlls up only part of

(b) 

Figure 8.5 Apparent motion (a) between two lights, which appear to move
back and forth when they are rapidly flashed one after the other; (b) on
a moving sign. Our perception of words moving across a lighted display
is so compelling that it is often difficult to realize that signs like
this are simply lights flashing on and off.

Figure 8.6 An image of the Falls of Foyers near Loch Ness in Scotland
where Robert Addams (1834) first experienced the waterfall illusion.
Looking at the downward motion of the waterfall for 30 to 60 seconds can
cause a person to then perceive stationary objects such as the rocks and
trees that are off to the side as moving upward. 8.2 Studying Motion
Perception

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

179

your ﬁeld of view) and then look off to the side at part of the scene
that is stationary, you will see everything you are looking at---rocks,
trees, grass---appears to move upward for a few seconds. If you're short
on waterfalls, next time you are at the movies, you may be able to
induce this illusion by carefully watching the rolling credits at the
end of the movie and then looking off to the side. This works best if
you sit toward the rear of the theater. Researchers studying motion
perception have investigated all the types of perceived motion described
above---and a number of others as well (Blaser & Sperling, 2008;
Cavanaugh, 2011). Our purpose, however, is not to understand every type
of motion perception but to understand some of the principles governing
motion perception in general. To do this, we will focus on real and
apparent motion.

Comparing Real and Apparent Motion For many years, researchers treated
the apparent motion created by ﬂashing stationary objects or pictures
and the real motion created by actual motion through space as though
they were separate phenomena, governed by different mechanisms. However,
there is ample evidence that these two types of motion have much in
common. For example, Axel Larsen and coworkers (2006) presented three
types of displays to a person in an fMRI scanner: (1) a control
condition, in which two squares in slightly different positions were
ﬂashed simultaneously (Figure 8.7a); (2) a real motion display, in which
a small square moved back and forth (Figure 8.7b); and (3) an apparent
motion display, in which squares were ﬂashed one after another so that
they appeared to move back and forth (Figure 8.7c). Larsen's results are
shown below the dot displays. The blue-colored area in Figure 8.7a is
the area of visual cortex activated by the control squares, which are
perceived as two squares simultaneously ﬂashing on and off with no
motion between them. Each square activates a separate area of the
cortex. In Figure 8.7b, the red indicates the area of cortex activated
by real movement of the square. In Figure 8.7c, the

yellow indicates the area of cortex activated by the apparent motion
display. Notice that the activation associated with apparent motion is
similar to the activation for the real motion display. Two ﬂashed
squares that result in apparent motion activate the area of the brain
representing the space between the positions of the ﬂashing squares even
though no stimulus is presented there. Because of the similarities
between the neural responses to real and apparent motion, researchers
study both types of motion together and concentrate on discovering
general mechanisms that apply to both. In this chapter, we will follow
this approach as we look for general mechanisms of motion perception. We
begin by describing two real-life situations in which we perceive
motion.

Two Real-Life Situations We Want to Explain Figure 8.8a shows a
situation in which Jeremy is walking from

left to right and Maria is following Jeremy's motion with her eyes. In
this case, Jeremy's image remains stationary on Maria's retinas, yet
Maria perceives Jeremy as moving. This means that motion perception
can't be explained just by the motion of an image across the retina.
Figure 8.8b shows a situation in which Maria looks straight ahead as
Jeremy walks by. Because she doesn't move her eyes, Jeremy's image
sweeps across her retina. Explaining motion perception in this case
seems straightforward because as Jeremy's image moves across Maria's
retina, it stimulates a series of receptors one after another, and this
stimulation signals Jeremy's motion. In the sections that follow, we
will consider a number of different approaches to explaining motion
perception when (1) the eye is moving to follow an object as it moves
(Figure 8.8a), and (2) the eye is stationary as an object moves across
the visual field (Figure 8.8b). We begin by considering an approach
based on J. J. Gibson's ecological approach to perception that we
described in Chapter 7 (p. 150).

From Larsen et al., 2006

Figure 8.7 Three conditions in Larsen's (2006) experiment: (a) control
condition, in which two squares in slightly different positions were
ﬂashed simultaneously; (b) real motion, in which a small square moved
back and forth; (c) apparent motion, in which squares were ﬂashed one
after another so that they appeared to move back and forth. Stimuli are
shown on top and the resulting brain activation is shown below. In (c),
the brain is activated in the space that represents the area between the
two dots, where movement was perceived but no stimulus was present.

(a) Control

180

(b) Real

(c) Apparent

Chapter 8  Perceiving Motion

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

(a) Jeremy walks past Maria; Maria follows him with her eyes (creates
local disturbance in optic array)

(b) Jeremy walks past Maria; Maria's eyes are stationary (creates local
    disturbance in optic array)

(c) Scans scene by moving her eyes from left to right (creates global
    optic flow)

Figure 8.8 Three motion situations. (a) Maria follows Jeremy's movement
with her eyes. (b) Maria is stationary and looks straight ahead as
Jeremy walks past. (c) Maria moves her eyes from left to right.

8.3 The Ecological Approach to Motion Perception Gibson's approach
(1950, 1966, 1979), which we introduced in Chapter 7, involves looking
for information in the environment that is useful for perception (see
page 150). This information for perception, according to Gibson, is
located not on the retina but "out there" in the environment. He thought
about information in the environment in terms of the optic array---the
structure created by the surfaces, textures, and contours of the
environment---and he focused on how movement of the observer causes
changes in the optic array. Let's see how this works by returning to
Jeremy and Maria in Figure 8.8. In Figure 8.8a, when Jeremy walks from
left to right and Maria follows him with her eyes, portions of the optic
array become covered as he walks by and then are uncovered as he moves
on. This result is called a local disturbance in the optic

array. This local disturbance in the optic array, which occurs when
Jeremy moves relative to the environment, covering and uncovering the
stationary background, causes Maria to perceive Jeremy's movement, even
though his image is stationary on her retina. In Figure 8.8b, when Maria
keeps her eyes still as Jeremy walks past, Jeremy's image is moving
across her retina, but as far as Gibson is concerned, the crucial
information for movement is the same local disturbance in the optic
array that occurred when Maria was keeping her eyes still. Whether
Maria's eyes are moving or still, the local disturbance out there in the
environment signals that Jeremy is moving. Gibson's approach explains
not only why Maria perceives movement in the situations in Figures 8.8a
and 8.8b, but also why she doesn't perceive movement when she moves her
eyes across the stationary scene. The reason is that as Maria moves her
eyes from left to right, everything around her---the walls, the window,
the trash can, the clock, and the furniture--- moves to the left of her
field of view (Figure 8.8c). A similar situation would occur if Maria
were to walk through the scene. The fact that everything moves at once
in response to movement of the observer's eyes or body is called global
optic ﬂow; this signals that the environment is stationary and that the
observer is moving, either by moving their body or by scanning with
their eyes, as in this example. Thus, according to Gibson, motion is
perceived when one part of the visual scene moves relative to the rest
of scene, and no motion is perceived when the entire field moves, or
remains stationary. While this is a reasonable explanation, we will see
in the next section that we also need to consider other sources of
information to fully understand how we perceive motion in the
environment.

8.4 The Corollary Discharge and Motion Perception Gibson's approach
focuses on information that is "out there" in the environment. Another
approach to explaining the movement situations in Figure 8.8 is to
consider the neural signals that travel from the eye to the brain. This
brings us back to the corollary discharge signal, which we introduced in
Chapter 6 (p. 128) to explain why we don't see the scene blur when we
move our eyes from place to place when scanning a scene. We now consider
how the corollary discharge comes into play as we perceive movement. As
we noted in Chapter 6, corollary discharge theory distinguishes three
signals: (1) the image displacement signal, which occurs when an image
moves across the retina, (2) the motor signal, which is sent from the
motor area to the eye muscles to cause the eye to move, and (3) the
corollary discharge signal, which is a copy of the motor signal.
According to corollary discharge theory, movement will be perceived if a
brain structure called the comparator (actually a number of brain
structures) receives just one signal---either

8.4 The Corollary Discharge and Motion Perception

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

181

the image displacement signal or the corollary discharge signal. It also
states that no movement will be perceived if the comparator receives
both signals at the same time. Keeping that in mind, let's consider how
corollary discharge theory would explain the perception that occurs in
each of the situations in Figure 8.8. Figure 8.9a shows the signals that
occur when Maria is following Jeremy with her eyes. There is a CD
signal, because Maria is moving her eyes. There is, however, no image
displacement signal, because Jeremy's image stays in the same place on
Maria's retina. The comparator, therefore, receives just one signal, so
Maria perceives Jeremy to be moving. Figure 8.9b shows that if Maria
keeps her eyes stationary as Jeremy walks across her field of view,
there is an image movement signal, because Jeremy's image is moving
across Maria's retina, but there is no CD signal, because Maria's eyes
are not moving. Because only one signal reaches the comparator, movement
is perceived. Figure 8.9c shows that if Maria scans the room, there is a
CD signal because her eyes are moving and an image movement signal
because the scene is moving across her retinas. Because both signals
reach the comparator, no movement is perceived. (Something to think
about: How would this way Perceive motion

of thinking about the CD apply to the situation described on page 129 of
Chapter 6, which explains why people don't see a smeared image as the
eye moves from the finger to the ear) This situation has also been
approached physiologically in another way, by focusing on how the moving
image stimulates one retinal receptor after the other. We will describe
this approach by first considering a neural circuit called the Reichardt
detector. TEST YOuRSELF 8.1 1. Describe five different functions of
motion perception. 2. What is an event? What is the evidence that motion
helps determine the location of event boundaries? What is the relation
between events and our ability to predict what is going to happen next?
3. Describe four different situations that can result in motion
perception. Which of these situations involves real motion, and which
involve illusions of motion? 4. What is the evidence for similar neural
responding to real motion and apparent motion? 5. Describe Gibson's
ecological approach to motion perception. What is the advantage of this
approach? (Explain how the ecological approach explains the situations
in Figure 8.8.) 6. Describe how corollary discharge theory explains
movement perception observed in Figures 8.8a and 8.8b.

CDS (a) Eye follows moving stimulus.

Perceive motion

IMS (b) Eye is stationary; stimulus is moving.

No motion

IMS

CDS

(c) Eye moves across stationary scene.

Figure 8.9 According to the corollary discharge model, (a) when the CDS
reaches the comparator alone, motion is perceived. (b) When the IMS
reaches the brain alone, motion is also perceived. (c) If both the CDS
and IMS reach the comparator together, they cancel each other, so motion
is not perceived. 182

8.5 The Reichardt Detector We are now going to explain motion perception
that occurs in Figure 8.9b, when movement is viewed by a stationary eye,
by considering the neural circuit in Figure 8.10, proposed by Werner
Reichardt (1961, 1987), which is called the Reichardt detector. The
Reichardt detector circuit consists of two neurons, A and B, which send
their signals to an output unit that compares the signals it receives
from neurons A and B. The key to the operation of this circuit is the
delay unit that slows down the signals from A as they travel toward the
output unit. In addition, the output unit has an important property: It
multiplies the responses from A and B to create the movement signal that
results in the perception of motion. Let's now consider how this circuit
responds as Jeremy, whose position is indicated by the red dot, moves
from left to right. Figure 8.10a shows that Jeremy, approaching from the
left, first activates neuron A. This is represented by the "spikes"
shown in record 1. This response starts traveling toward the output
unit, but is slowed by the delay unit. During this delay, Jeremy
continues moving and stimulates neuron B (Figure 8.10b), which also
sends a signal down to the output unit (record 2). If the timing is
right, the delayed signal from A (record 3) reaches the output unit just
when the signal from B (record 2) arrives. Because the output unit
multiplies

Chapter 8  Perceiving Motion

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Figure 8.10 The Reichardt detector. Activated structures are indicated
by red. The output unit creates a signal only if the signals from A and
B reach it simultaneously. Top: Movement to the right results in a
signal from the output unit. Bottom: Movement to the left results in no
signal. See text for details.

Direction of motion (a) Time 1

(b) Time 2 A

B

A

B

(1) Delay unit
(2) 

Delay unit

Output unit

(2) Output unit
(3) 

No signal

Direction of motion (e) Time 3 A

(d) Time 2 B

(e) Time 1 B

A

A

B

(6) Delay unit

Delay unit

(7) 

Delay unit

(5) 
(6) Output unit

Output unit

Output unit

No signal

No signal

No signal

the responses from A and B, a large movement signal results (record 4).
Thus, when Jeremy moves from left to right at the right speed, a
movement signal occurs and Maria perceives Jeremy's movement. An
important property of the circuit diagrammed in Figure 8.10 is that it
creates a movement signal in response to movement from left to right,
but does not create a signal for movement from right to left. We can see
why this is so by considering what happens when Jeremy walks from right
to left. Approaching from the right (Figure 8.10c), Jeremy first
activates neuron B, which sends its signals directly to the output unit
(record 5). Jeremy continues moving and activates neuron A (Figure
8.10d), which generates a signal (record 6). At this point, the response
from B has become smaller because it is no longer being stimulated
(record 7), and by the time the response from A passes through the delay
unit and reaches the output unit, the response from B has dropped to
zero (Figure 8.10e). When the output unit multiplies the delayed signal
from neuron A and the zero signal from neuron B, the result is zero, so
no movement signal is generated. More complicated versions of this
circuit, which have been discovered in amphibians, rodents, primates,
and humans (Borst & Egelhaaf, 1989), create directionally sensitive

neurons, which fire only to a particular direction of motion. The visual
system contains many circuits like this, each tuned to a different
direction of motion; working together, they can create signals that
indicate the direction of movement across the visual field.

8.6 Single-Neuron Responses to Motion The Reichardt detector is a neural
circuit that creates a neuron that responds to movement in a specific
direction. Such directionally-selective neurons were recorded from
neurons in the rabbit's retina by Horace Barlow and coworkers (1964) and
from neurons in the cat's visual cortex by David Hubel and Thorsten
Wiesel (1959, 1965). Hubel and Wiesel's motion detecting neurons are the
complex cells described in Chapter 4, which respond to movement in a
particular direction (p. 70). While the visual cortex is therefore
important for motion perception, it is only the first in a series of
many brain regions that are involved (Cheong et al., 2012; Gilaie-Dotan,
et al., 2013). We will focus on the middle temporal (MT) area (see
Figure 7.18, page 160), which contains many directionally 8.6
Single-Neuron Responses to Motion

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

183

No correlation Coherence = 0%

50% correlation Coherence = 50%

(a) 

selective neurons. Evidence that the MT cortex is specialized for
processing information about motion comes from experiments that have
used moving dot displays in which the direction of motion of individual
dots can be varied.

Experiments Using Moving Dot Displays Figure 8.11a represents a display
in which an array of dots

are moving in random directions. William Newsome and coworkers (1995)
used the term coherence to indicate the degree to which the dots move in
the same direction. When the dots are all moving in random directions,
coherence is 0 percent. Figure 8.11b represents a coherence of 50
percent, as indicated by the darkened dots, which means that at any
point in time half of the dots are moving in the same direction. Figure
8.11c represents 100 percent coherence, which means that all of the dots
are moving in the same direction. Newsome and coworkers used these
moving dot stimuli to determine the relationship between (1) a monkey's
ability to judge the direction in which dots were moving and (2) the
response of a neuron in the monkey's MT cortex. They found that as the
dots' coherence increased, two things happened: (1) the monkey judged
the direction of motion more accurately, and (2) the MT neuron ﬁred more
vigorously. The mon-

(b) 
(c) 

key's behavior and the ﬁring of the MT neurons were so closely related
that the researchers could predict one from the other. For example, when
the dots' coherence was 0.8 percent, the monkey was not able to judge
the direction of the dots' motion and the neuron's response did not
differ appreciably from its baseline ﬁring rate. But increasing the
coherence increased the monkey's ability to judge the direction of
motion, and by 12.8 percent coherence---so, out of 200 moving dots,
about 25 were moving in the same direction---the monkey judged the
correct direction of movement on virtually every trial, and the MT
neuron always ﬁred faster than its baseline rate. Newsome's experiment
demonstrates a relationship between the monkey's perception of motion
and neural firing in its MT cortex. What is especially striking about
Newsome's experiment is that he measured perception and neural activity
in the same monkeys. Returning to the perceptual process introduced in
Chapter 1 (Figure 1.13, page 12), which is shown in Figure 8.12, we can
appreciate that what Newsome has done is to measure relationship C: the
physiology--perception relationship. This measurement of physiology and
perception in the same organism completes our perceptual process
triangle, which also includes relationship A: stimulus--perception---
the connection between how stimuli are moving and what we perceive; and
relationship B: stimulus--physiology---the

CEPTION PER

Newsome: Firing of MT cortex neuron and perception of moving C dots are
related

A

Flashing two dots with the right timing can result in apparent motion

MU

LUS

SIO PHY

Figure 8.12 The perceptual process from Chapter 1 (p. 11). Newsome
measured relationship C: the physiology--perception relationship, by
simultaneously recording from neurons and measuring the monkey's
behavioral response. Other research we have discussed has measured
relationship A: the stimulus--perception relationship (for example, when
flashing two dots creates apparent motion) and relationship B: the
stimulus--physiology relationship (for example, when a moving bar causes
a cortical neuron to fire).

100% correlation Coherence = 100%

LO

G

TI

Figure 8.11 Moving dot displays used by Britten and coworkers (1992).
These pictures represent moving dot displays that were created by a
computer. Each dot survives for a brief interval (20--30 msec), after
which it disappears and is replaced by another randomly placed dot.
Coherence is the percentage of dots moving in the same direction at any
point in time. (a) Coherence = 0 percent. (b) Coherence = 50 percent.
(c) Coherence = 100 percent. (Adapted from Britten et al., 1992)

S

Y

B Moving bar activates cortical neurons

184

Chapter 8  Perceiving Motion

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

connection between how stimuli are moving and neural firing. While all
three relationships are important for understanding motion perception,
Newsome's demonstration is notable because of the difficulty of
simultaneously measuring perception and physiology. This relationship
has also been demonstrated (1) by lesioning (destroying) or deactivating
some or all of the MT cortex and (2) by electrically stimulating neurons
in the MT cortex.

Lesioning the MT Cortex A monkey with an intact MT cortex can begin
detecting the direction dots are moving when coherence is as low as 1 or
2 percent. However, after the MT is lesioned, the coherence must be 10
to 20 percent before monkeys can begin detecting the direction of motion
(Newsome & Paré, 1988; also see Movshon & Newsome, 1992; Newsome et al.,
1995; Pasternak & Merigan, 1994).

Deactivating the MT Cortex Further evidence linking neurons in MT cortex
to motion perception has been determined from experiments on human
participants using a method called transcranial magnetic stimulation
(TMS) that temporarily disrupts the normal functioning of neurons.
METHOD

Transcranial Magnetic Stimulation (TMS)

One way to investigate whether an area of the brain is involved in
determining a particular function is to remove that part of the brain,
as noted above for the MT cortex in monkeys. It is possible to
temporarily disrupt the functioning of a particular area in humans by
applying a strong magnetic ﬁeld using a stimulating coil placed over the
person's skull (Figure 8.13).

Figure 8.13 TMS coil positioned to present a magnetic field to the back
of a person's head.

A series of electromagnetic pulses presented to a particular area of the
brain for a few seconds interferes with brain functioning in that area
for seconds or minutes. If a particular behavior is disrupted by the
pulses, researchers conclude that the disrupted area of the brain is
involved in that behavior.

When researchers applied TMS to the MT cortex, participants had
difficulty determining the direction in which a random pattern of dots
was moving (Beckers & Homberg, 1992). Although the effect was temporary,
these participants experienced a form of akinetopsia much like patient
L.M., discussed earlier in this chapter.

Stimulating the MT Cortex The link between the MT cortex and motion
perception has been studied not only by disrupting normal neural
activity, but also by enhancing it using a technique called
microstimulation. METHOD

Microstimulation

Microstimulation is achieved by lowering a small wire electrode into the
cortex and passing a weak electrical charge through the tip of the
electrode. This weak shock stimulates neurons that are near the
electrode tip and causes them to ﬁre, just as they would if they were
being stimulated by chemical neurotransmitters released from other
neurons. Thus, after locating neurons that normally respond to certain
stimuli using methods such as single-cell recording (p. 22),
microstimulation techniques can be used to stimulate those neurons even
when these stimuli are absent from the animal's field of view.

Kenneth Britten and coworkers (1992) used this procedure in an
experiment in which a monkey was looking at dots moving in a particular
direction while indicating the direction of motion it was perceiving.
For example, Figure 8.14a shows that, under normal conditions, as a
monkey observed dots moving to the right, it reported that the dots were
indeed moving to the right. Figure 8.14b, however, shows how the monkey
responded when the researchers stimulated neurons that are activated by
downward motion. Instead of perceiving rightward motion, the monkey
began responding as though the dots were moving downward and to the
right. The fact that stimulating the MT neurons shifted the monkey's
perception of the direction of movement provides more evidence linking
MT neurons and motion perception. In addition to the MT cortex, another
area highly involved in motion perception is the nearby medial superior
temporal (MST) area. The MST area is involved in eye movements, so it is
particularly important in localizing a moving object in space. For
example, a monkey's ability to reach for a moving object is adversely
affected by both microstimulation and lesioning of the MST cortex (Ilg,
2008). 8.6 Single-Neuron Responses to Motion

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

185

8.7 Beyond Single-Neuron Responses to Motion

Perception

(a) No stimulation

Perception

(b) Stimulation

Figure 8.14 (a) A monkey judges the motion of dots moving horizontally
to the right. (b) When a column of neurons that are activated by
downward motion is stimulated, the monkey judges the same motion as
being downward and to the right.

We've described a number of research studies which looked at how single
neurons fire to movement. As important as these studies are, just
showing that a particular neuron responds to motion does not explain how
we perceive motion in real life. We can appreciate why this is so by
considering how motion signaled by single neurons is ambiguous and can
differ from what we perceive (Park & Tadin, 2018). Consider, for
example, how a directionally selective neuron would respond to movement
of a vertically oriented pole like the one being carried by the woman in
Figure 8.15a. We are going to focus on the pole, which is essentially a
vertical bar. The ellipse represents the area of the receptive ﬁeld of a
neuron in the cortex that responds when a vertical bar moves to the
right across the neuron's receptive ﬁeld. Figure 8.15a shows the pole
entering the receptive ﬁeld on the left. As the pole moves to the right,
it moves across the receptive ﬁeld in the direction indicated by the red
arrow, and the neuron ﬁres. But what happens if the woman climbs some
steps? Figure 8.15b shows that as she walks up the steps, she and the
pole are now moving up and to the right (blue arrows). We know this
because we can see the woman and the ﬂag moving up. But the neuron,
which only sees movement through the narrow view of its receptive ﬁeld,
only receives information about the rightward movement (red arrows).
This is called the aperture problem, because the neuron's receptive
field is functioning like an aperture, which reveals only a small
portion of the scene.

Figure 8.15 The aperture problem. (a) The pole's overall motion is
horizontally to the right (blue arrows). The ellipse represents the area
in an observer's field of view that corresponds to the receptive field
of a cortical neuron on the observer's retina. The pole's motion across
the receptive field is also horizontal to the right (red arrows). (b)
When the woman walks up the steps, the pole's overall motion is up and
to the right (blue arrows). However, the pole's motion across the
receptive field is horizontal to the right (red arrows), as in (a).
Thus, the receptive field "sees" the same motion for motion that is
horizontal and motion that is up and to the right.

(a) 

186

(b) 

Chapter 8  Perceiving Motion

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

The Aperture Problem

Solutions to the Aperture Problem

Do the following demonstration to see why the neuron only receives
information about rightward movement of the bar.

There are at least two solutions to the aperture problem (Bruno &
Bertamini, 2015). The first was highlighted by one of my students who
tried the pencil demonstration in Figure 8.16. He noticed that when he
followed the directions for the demonstration, the edge of the pencil
did appear to be moving horizontally across the aperture, whether the
pencil was moving horizontally or up at an angle. However, he noted that
when he moved the pencil so that he could see its tip moving through the
aperture, as in Figure 8.17, he could tell that the pencil was moving
up. Thus, a neuron could use information about the end of a moving
object (such as the tip of the pencil) to determine its direction of
motion. As it turns out, neurons that could signal this information,
because they respond to the ends of moving objects, have been found in
the striate cortex (Pack et al., 2003). The second solution is to pool,
or combine, responses from a number of neurons. Evidence for pooling
comes from studies in which the activity of neurons in the monkey's MT
cortex is recorded while the monkey looks at moving oriented lines like
the pole or our pencil. For example, Christopher Pack and Richard Born
(2001) found that the MT neurons' initial response to the stimulus,
about 70 msec after the stimulus was presented, was determined by the
orientation of the bar. Thus the neurons responded in the same way to a
vertical bar moving horizontally to the right and a vertical bar moving
up and to the right (red arrows in Figure 8.15). However, 140 msec after
presentation of the moving bars, the neurons began responding to the
actual direction in which the bars were moving (blue arrows in Figure
8.15). Apparently, MT neurons receive signals from a number of neurons
in the striate cortex and then combine these signals to determine the
actual direction of motion. What all of this means is that the "simple"
situation of an object moving across the visual ﬁeld as an observer
looks

DEMONSTRATION

Aperture

Movement of a Bar Across an

Make a small aperture, about 1 inch in diameter, by creating a circle
with the ﬁngers of your left hand, as shown in Figure 8.16 (or you can
create a circle by cutting a hole in a piece of paper). Then orient a
pencil vertically, and move the pencil from left to right behind the
circle, as shown by the blue arrows in Figure 8.16a. As you do this,
focus on the direction that the front edge of the pencil appears to be
moving across the aperture. Now, again holding the pencil vertically,
position the pencil below the circle, as shown in Figure 8.16b, and move
it up behind the aperture at a 45-degree angle (being careful to keep
its orientation vertical). Again, notice the direction in which the
front edge of the pencil appears to be moving across the aperture.

(a) 
(b) 

Figure 8.16 Moving a pencil behind an aperture in the "Movement of a Bar
Across an Aperture" demonstration.

If you were able to focus only on what was happening inside the
aperture, you probably noticed that the direction that the front edge of
the pencil was moving appeared the same whether the pencil was moving
(a) horizontally to the right or (b) up and to the right. In both cases,
the front edge of the pencil moves across the aperture horizontally, as
indicated by the red arrow. Another way to state this is that the
movement of an edge across an aperture occurs perpendicular to the
direction in which the edge is oriented. Because the pencil in our
demonstration was oriented vertically, motion through the aperture was
horizontal. Because the motion of the edge was the same in both
situations, a single directionally selective neuron would ﬁre similarly
in (a) and (b), so based just on the activity of this neuron, it isn't
possible to tell whether the pencil is moving horizontally to the right
or upward at an angle.

Figure 8.17 The circle represents a neuron's receptive field. When the
pencil is moved up and to the right, as shown, movement of the tip of
the pencil provides information indicating that the pencil is moving up
and to the right. 8.7 Beyond Single-Neuron Responses to Motion

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

187

straight ahead is not so simple because of the aperture problem. The
visual system apparently solves this problem (1) by using information
from neurons in the striate cortex that respond to the movement of the
ends of objects and (2) by using information from neurons in the MT
cortex that pool the responses of a number of directionally selective
neurons (also see Rust et al., 2006; Smith et al., 2005; Zhang &
Britten, 2006).

pictures are alternated very rapidly (five or more times a second), even
though motion through the head is physically impossible. While the
straight-line motion of the hand through the head is an interesting
result, the most important result occurred when the rate of alternation
was slowed. When the pictures were alternated less than five times per
second, observers began perceiving the motion shown in Figure 8.18c: the
hand appeared to move around the woman's head. These results are
interesting for two reasons: (1) They show that the visual system needs
time to process information in order to perceive the movement of complex
meaningful stimuli. (2) They suggest that there may be something special
about the meaning of the stimulus---in this case, the human body---that
influences the way movement is perceived. To test the idea that the
human body is special, Shiffrar and coworkers showed that when objects
such as boards are used as stimuli, the likelihood of perceiving
movement along the longer path does not increase at lower rates of
alternation, as it does for pictures of humans (Chatterjee et al.,
1996). What is happening in the cortex when observers view apparent
motion generated by pictures like the ones in Figure 8.18? To find out,
Jennifer Stevens and coworkers (2000) measured brain activation using
brain imaging. They found that both movement through the head and
movement around the head activated areas in the parietal cortex
associated with movement. However, when the observers saw movement as
occurring around the head, the motor cortex was activated as well. Thus,
the motor cortex is activated when the perceived movements are humanly
possible but isn't activated when the perceived movements are not
possible. This connection between the brain area associated with
perceiving movement and the motor area reflects the close connection
between perception and taking action that we discussed in Chapter 7.

8.8 Motion and the Human Body Experiments using dots and lines as
stimuli have taught us a great deal about the mechanisms of motion
perception, but what about the more complex stimuli created by moving
humans and animals that are so prevalent in our environment? We will now
consider two examples of the ways in which researchers have studied how
we perceive movement of the human body.

Apparent Motion of the Body Earlier in this chapter we described
apparent motion as the perception of motion that occurs when two stimuli
that are in slightly different locations are presented one after the
other. Even though these stimuli are stationary, movement is perceived
back and forth between them if they are alternated with the correct
timing. Generally, this movement follows a principle called the shortest
path constraint---apparent movement tends to occur along the shortest
path between two stimuli. Maggie Shiffrar and Jennifer Freyd (1990,
1993) had observers view photographs like the ones in Figure 8.18a, with
the photographs alternating rapidly. Notice that in the first picture,
the woman's hand is in front of her head, and in the second, it is
behind her head. According to the shortest path constraint, motion
should be perceived in a straight line between the hands in the
alternating photos, which means observers would see the woman's hand as
moving through her head, as shown in Figure 8.18b. This is, in fact,
exactly what happens when the

Biological Motion Studied by Point-Light Walkers An approach to studying
motion of the human body that we introduced at the beginning of the
chapter involves point-light walkers, which are created by placing small
lights on people's

(a) 

Path through head (b)

Path around head

Bruce Goldstein

Two possible perceptions (as seen from above)

Bruce Goldstein

Apparent motion stimulus (pictures alternate)

(c) 

Figure 8.18 The two pictures in (a) are photographs similar to those
used in Shiffrar and Freyd's (1993) experiment. The pictures were
alternated either rapidly or more slowly. (b) When alternated rapidly,
observers perceived the hand as moving through the head. (c) When
alternated more slowly, the hand was seen as moving around the head. 188

Chapter 8  Perceiving Motion

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

joints and then filming the patterns created by these lights when people
move (see Figure 8.4). Research using point-light walkers shows that
motion of the body creates perceptual organization by causing the
movements of the individual dots to become organized into "a person
moving." When the person wearing the lights is stationary, the lights
look like a meaningless pattern. However, as soon as the person starts
walking, with arms and legs swinging back and forth and feet moving in
ﬂattened arcs, ﬁrst one leaving the ground and touching down, and then
the other, the motion of the lights is immediately perceived as being
caused by a walking person. This self-produced motion of a person or
other living organism is called biological motion. One reason we are
particularly good at perceptually organizing the complex motion of an
array of moving dots into the perception of a walking person is that we
see biological motion all the time. Every time you see a person walking,
running, or behaving in any way that involves movement, you are seeing
biological motion. Our ability to easily perceive biological motion in
moving points of light led some researchers to suspect that there may be
a specific area in the brain that responds to biological motion, just as
there are areas such as the extrastriate body area (EBA) and fusiform
face area (FFA) that are specialized to respond to bodies and faces,
respectively (see page 111 and Figures 5.41, 5.42, and 5.43). Emily
Grossman and Randolph Blake (2001) provided evidence supporting the idea
of a specialized area in the brain for biological motion by measuring
observers' brain activity as they viewed the moving dots created by a
point-light walker (Figure 8.19a) and as they viewed dots that moved
similarly to the point-light walker dots, but were scrambled so they did
not result in the impression of a person walking (Figure 8.19b). They
found that a small area in the superior temporal sulcus (STS) (see
Figure 5.42) was more active when viewing biological motion than viewing
scrambled motion in all eight of their observers.

(a) Biological

(b) Scrambled

Time

Figure 8.19 Frames from the stimuli used by Grossman and Blake (2001).
(a) Sequence from the point-light walker stimulus. (b) Sequence from the
scrambled point-light stimulus.

In later experiments, researchers determined that other brain areas are
also involved in the perception of biological motion. For example, both
the FFA (Grossman & Blake, 2002) and the portions of the PFC that
contain mirror neurons (see Figure 7.18) (Saygin et al., 2004) are
activated more by biological motion than by scrambled motion. Based on
these results, researchers have concluded that there is a network of
areas that together are specialized for the perception of biological
motion (also see Grosbras et al., 2012; Grossman et al., 2000; Pelphrey
et al., 2003, 2005; Saygin, 2007, 2012). See Table 8.1 for a summary of
the structures involved in motion perception that we have discussed in
this chapter.

Table 8.1 Brain Regions Involved in the Perception of Motion BRAIN
REGION

FUNCTIONS RELATED TO MOTION

Striate Cortex (V1)

Direction of motion across small receptive fields

Middle Temporal (MT) Area

Direction and speed of object motion

Medial Superior Temporal (MST) Area

Processing optic flow; locating moving objects; reaching for moving
objects

Superior Temporal Sulcus (STS)

Perception of motion related to animals and people (biological motion)

EXAMPLE

8.8 Motion and the Human Body

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

189

(a)

(b) 
(c) 
(d) 

Figure 8.20 (a) Biological motion stimulus. (b) Scrambled stimulus. (c)
Biological motion stimulus with noise added. The dots corresponding to
the walker are indicated by lines (which were not seen by the observer).
(d) How the stimulus appeared to the observer. (From Grossman et al.,
2005) 190

motion. From this result, Grossman concluded that normal functioning of
the "biological motion" area, STS, is necessary for perceiving
biological motion. This conclusion is also supported by studies showing
that people who have suffered damage to this area have trouble
perceiving biological motion (Battelli et al., 2003). The ability to
discriminate biological motion from randomly moving dots has also been
shown to be adversely affected when transcranial magnetic stimulation is
applied to other regions involved in the perception of biological
motion, such as the prefrontal cortex (PFC, see Figure 7.18) (van
Kemenade et al., 2012). What all of this means is that biological motion
is more than just "motion"; it is a special type of motion that is
served by specialized areas of the brain.

8.9 Motion Responses to Still Pictures Consider the picture in Figure
8.21, which most people perceive as a "freeze frame" of an
action---skiing---that involves motion. It is not hard to imagine the
person moving to a different location immediately after this picture was
taken. A situation such as this, in which a still picture depicts an
action involving motion, is called implied motion. Despite the lack of
either real motion or apparent motion in this situation, a variety of
experiments have shown that the perception of implied motion depends on
many of the mechanisms we have introduced in this chapter. Jennifer
Freyd (1983) conducted an experiment involving implied motion by brieﬂy
showing observers pictures that depicted a situation involving motion,
such as a person jumping off a low wall (Figure 8.22a). Freyd predicted
that participants looking at this picture would "unfreeze" the implied
motion depicted in the picture and anticipate the motion that was about
to happen. If this occurred, observers might "remember" the picture as
depicting a situation that occurred slightly later in time. For the
picture of the person jumping off the wall, that would mean the
observers might remember the person as being closer to the ground (as in
Figure 8.22b) than he was in the initial picture.

Ales Fevzer/Encyclopedia/Corbis

Earlier in the chapter we described how Newsome used a number of
different methods to show that the MT cortex is specialized for the
perception of motion (p. 18). In addition to showing that the MT cortex
is activated by motion, he also showed that perception of motion is
decreased by lesioning the MT cortex and is inﬂuenced by stimulating
neurons in the MT cortex. Just as Newsome showed that disrupting
operation of the MT cortex decreases a monkey's ability to perceive the
direction of moving dots, Emily Grossman and coworkers (2005) showed
that using transcranial magnetic stimulation (TMS) to disrupt the
operation of the STS in humans decreases the ability to perceive
biological motion (see "Method: Transcranial Magnetic Stimulation," page
185). The observers in Grossman's (2005) experiment viewed point-light
stimuli for activities such as walking, kicking, and throwing (Figure
8.20a), and they also viewed scrambled point-light displays (Figure
8.20b). Their task was to determine whether a display was biological
motion or scrambled motion. This is normally an extremely easy task, but
Grossman made it more difﬁcult by adding extra dots to create "noise"
(Figures 8.20c and 8.20d). The amount of noise was adjusted for each
observer so that they could distinguish between biological and scrambled
motion with 71 percent accuracy. The key result of this experiment was
that presenting transcranial magnetic stimulation to the area of the STS
that is activated by biological motion caused a signiﬁcant decrease in
the observers' ability to perceive biological motion. Such magnetic
stimulation of other motion-sensitive areas, such as the MT cortex, had
no effect on the perception of biological

Figure 8.21 A picture that creates implied motion.

Chapter 8  Perceiving Motion

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Figure 8.22 Stimuli like those used by Freyd (1983). See text for
details.

(c) Backward in time

To test this idea, Freyd showed participants a picture of a person in
midair, like Figure 8.22a, and then after a pause, she showed her
observers either (1) the same picture; (2) a picture slightly forward in
time (the person who had jumped off the wall was closer to the ground,
as in Figure 8.27b); or (3) a picture slightly backward in time (the
person was farther from the ground, as in Figure 8.22c). The observers'
task was to indicate, as quickly as possible, whether the second picture
was the same as or different from the ﬁrst picture. When Freyd compared
the time it took for participants to decide if the "time-forward" and
"time-backward" pictures were different from the first picture they had
seen, she found that participants took longer to decide if the
time-forward picture was the same or different. She concluded from this
that the time-forward judgment was more difficult because her
participants had anticipated the downward motion that was about to
happen and so confused the time-forward picture with what they had
actually seen. The idea that the motion depicted in a picture tends to
continue in the observer's mind is called representational momentum
(David & Senior, 2000; Freyd, 1983). Representational momentum is an
example of experience inﬂuencing perception because it depends on our
knowledge of the way situations involving motion typically unfold. If
implied motion causes an object to continue moving in a person's mind,
then it would seem reasonable that this continued motion might be
reﬂected by activity in the brain. When Zoe Kourtzi and Nancy Kanwisher
(2000) measured the fMRI response in the MT and MST cortex to pictures
like the ones in Figure 8.23, they found that the area of the brain that
responds to actual motion also responds to pictures of motion, and that
implied-motion (IM) pictures caused a greater response than
no-implied-motion (no-IM) pictures, at rest (R) pictures, or house (H)
pictures. Thus, activity occurs in the brain that corresponds to the
continued motion that implied-

motion pictures create in a person's mind (also see Lorteije et al.,
2006; Senior et al., 2000). Building on the idea that the brain responds
to implied motion, Jonathan Winawer and coworkers (2008) wondered
whether still pictures that implied motion, like the one in Figure 8.21,
would elicit a motion aftereffect (see page 179). To test this, they
conducted a psychophysical experiment in which they asked whether
viewing still pictures showing implied motion in a particular direction
can cause a motion aftereffect in the opposite direction. We described
one type of motion aftereffect at the beginning of the chapter by noting
that after viewing the downward movement of a waterfall, nearby
stationary objects appear to move upward. There is

Implied No implied motion (IM) motion (no-IM)

At rest (R)

House (H)

R

H

Stimuli

(b) Forward in time

\% Signal change

(a) First picture

2.0

1.0

0

IM

No-IM

Figure 8.23 Examples of pictures used by Kourtzi and Kanwisher (2000) to
depict implied motion (IM), no implied motion (no-IM), "at rest" (R),
and a house (H). The height of the bar below each picture indicates the
average fMRI response of the MT cortex to that type of picture. (From
Kourtzi & Kanwisher, 2000) 8.9 Motion Responses to Still Pictures

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

191

evidence that this occurs because prolonged viewing of the waterfall's
downward motion decreases the activity of neu­ rons that respond to
downward motion, so more upward mo­ tion neuronal activity remains
(Barlow & Hill, 1963; Mather et al., 1998). To determine whether implied
motion stimuli would have the same effect, Winawer had his participants
observe a series of pictures showing implied motion. For a particular
trial, participants saw either a series of pictures that all sho­ wed
movement to the right or a series of pictures that all showed movement
to the left. After adapting to this series of pictures for 60 seconds,
the participants' task was to indicate the direction of movement of
arrays of moving dots like the ones we described earlier (see Figure
8.11). The key result of this experiment was that before observing the
implied-motion stimuli, participants were equally likely to perceive dot
stimuli with zero coherence (all the dots moving in random directions)
as moving to the left or to the right. How­ ever, after viewing
photographs showing rightward implied motion, participants were more
likely to see the dots as moving to the left. After viewing leftward
implied motion, participants were more likely to see the randomly moving
dots as moving to the right. Because this is the same result that would
occur for adapting to real movement to the left or right, Winawer con­
cluded that viewing implied motion in pictures decreases the activity of
neurons selective to that direction of motion.

SOMETHING TO CONSIDER:

Motion, Motion, and More Motion

Although our topic is far from real estate, we can, looking back on the
last three chapters, ask a question with a simi­ lar three-part answer:
"What kept happening, throughout Chapters 6, 7, and 8?" The answer:
Motion, motion, and then more motion. Chapter 6, "Visual Attention,"
isn't all about motion, but there was a lot of movement, nonetheless,
because the eyes are in constant motion as we scan a scene, and move­
ment is one of the main aspects of the environment that at­ tracts
attention. Chapter 7, "Taking Action," was about all kinds of movement:
walking, driving, moving through the environment, reaching, grasping,
watching other people move, and considering how infants move. Chapter 7,
it is ac­ curate to say, is about movement of the body. And finally, this
chapter offered a change of perspective, as we shifted from doing
movement to perceiving movement and taking advantage of its many
functions. There's an important message here. Movement, in all its
forms, is essential for survival. It helps us know where we are, avoid
potential dangers, act in many different ways in and on the environment,
and gain access to a wealth of information about the environment.
Because movement is so important, it is not surprising that it took
three chapters to describe it, and although we will be taking a short
break from movement as we discuss color vision in Chapter 9, we will
encounter move­ ment again in Chapter 10, as we show how movement helps
us perceive depth, in Chapter 12, as we consider how we per­ ceive moving
sounds, and in Chapter 15, as we consider how we perceive motion across
our skin. Motion, as it turns out, is one of the central phenomena in
our lives and, therefore, in perception as well.

There's a well-known question about real estate that asks, "What are the
three things that determine the value of a house?" The answer: Location,
location, and location.

DEVELOPMENTAL DIMENSION Infants Perceive Biological Motion Many accounts
of biological motion perception argue that our own experiences with
people and animals are critical for developing the ability to perceive
biological motion. Evidence for this claim comes, in part, from
developmental studies that have shown that a child's ability to
recognize biological mo­ tion in point-light displays improves as he or
she gets older (Freire et al., 2006; Hadad et al., 2011). In fact, some
studies suggest that adultlike levels of performance on point-light

192

tasks are not achieved until early adolescence (Hadad et al., 2011). But
even though it may take years to reach adult levels of performance, some
research suggests that the ability to dis­ tinguish biological from
nonbiological motion may be pres­ ent at birth. One line of evidence
suggesting that the perception of biological motion may not depend on
visual experi­ ence comes from animal studies. For example, Giorgio

Chapter 8  Perceiving Motion

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Vallortigara and his coworkers (2005) found that if newborn chicks with
no prior visual experience are presented with two movement displays (1)
the "walking hen," point light display which shows dots that would be
created by a moving hen and (2) the same number of dots moving randomly
(Figure 8.24a). When these displays were shown at opposite ends of a
platform (Figure 8.24b) the chicks spent most of their time near the
biological movement display. This indicates that the chicks were able to
identify--- and, in fact, preferred---the biological motion displays
despite not having had any prior visual experience. In order for this to
happen, Vallortigara argued, chicks must possess perceptual mechanisms
tuned to biological motion prior to hatching. Intrigued by
Vallortigara's experiments with newly hatched chicks, Francesca Simion
and her coworkers (2008) wondered whether similar biological motion
detector mechanisms might also be present in newborn humans. To find
out, the researchers conducted a version of the chick study with 1- and
2-dayold newborns using the preferential looking procedure (see Chapter
3, page 60). Simion conducted her experiment in the maternity ward of a
hospital with full-term newborns. Infants in the study sat on an adult's
lap while they were shown two movies simultaneously on side-by-side
computer monitors (Figure 8.24c). On one screen, infants saw 14
point-lights moving in random directions. On the other screen, they were
shown a movie where the 14 moving point-lights depicted the same walking
hen used by Vallortigara in his experiment with chicks. Simion used the
hen-walking animation because she could not ethically deprive the
newborns of any visual experience prior to their participation in her
study. So, the newborns may have obtained some very limited experience
with human motion prior to the experiment, but it was very unlikely that
they had seen any hens wandering about the hospital.

(a) 
(b) 

The researchers wanted to know if these newborns, like newly hatched
chicks, would prefer the biological motion display to the random
point-light display. They therefore compared the amount of time the
infants spent looking at each movie. They discovered that the infants
spent 58 percent of their time looking at the point-light hen, which was
statistically greater than the time spent looking at the random
pointlight display. Thus, Simion and her colleagues concluded that, like
chicks, humans are born with an ability to detect biological motion.
From their results, both Vallortigara and Simion argued that the ability
to perceive biological motion occurs independent of experience. However,
there is also evidence that the perception of biological motion changes
with age. It would be logical to assume that if newborns are sensitive
to biological motion, this ability would then improve as they experience
more biological motion. However, research on older infants shows that
response to biological motion decreases to zero at 1 or 2 months of age,
and then returns by 3 months and increases over the next two years of
life and beyond (Sifre et al., 2018). What's going on? One idea is that
two different mechanisms are involved. At birth a reflex-like mechanism
is sensitive to biological motion. This is useful to the newborn because
it helps them react to caregivers. By 2 months this mechanism no longer
functions, but a second mechanism begins emerging at around 3 months. An
important property of this mechanism is that its performance improves as
infants accumulate more experience observing biological motion. This
helps the infant relate to caregivers on a more complex level as they
transition from stationary observers of people moving relative to them,
to active observers, whose crawling and walking help them develop social
skills as they interact with other biologicallymoving beings (see
Developmental Dimension: Infant Affordances, Chapter 7, page 169).

(c) 

Figure 8.24 (a) Top: placement of point-lights on an adult hen. Bottom:
still images from an animated movie depicting a walking hen and also
random dots. (b) The testing apparatus used by Vallortigara et
al. (2005) to measure chicks' reactions to biological motion stimuli.
Stimuli were shown on the monitors at each end of a platform. A chick's
preference for one stimulus over the other is revealed by the amount of
time the chick spends at each end of the platform. (c) The testing
apparatus used by Simion et al. (2008) to measure newborn reactions to
biological motion. The newborn's preference for one stimulus over the
other was revealed by the amount of time the newborn spent looking at
each stimulus.

Something to Consider: Motion, Motion, and More Motion

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

193

TEST YOuRSELF 8.2 1. Describe the operation of the neural circuit that
creates the Reichardt detector. Be sure you understand why the circuit
leads to firing to movement in one direction, but no firing to movement
in the opposite direction. 2. What is the evidence that the MT cortex is
specialized for processing movement? Describe the series of experiments
that used moving dots as stimuli and (a) recorded from neurons in the MT
cortex, (b) lesioned the MT cortex, and (c) stimulated neurons in the MT
cortex. What do the results of these experiments enable us to conclude
about the role of the MT cortex in motion perception? 3. Describe the
aperture problem---why the response of individual directionally
selective neurons does not provide sufﬁcient information to indicate the
direction of motion. Also describe two ways that the brain might solve
the aperture problem. 4. Describe experiments on apparent motion of a
person's arm. How do the results differ for slow and fast

presentations of the stimuli? How is the brain activated by slow and
fast presentations? 5. What is biological motion, and how has it been
studied using point-light displays? 6. Describe the experiments that
have shown that an area in the superior temporal sulcus (STS) is
specialized for perceiving biological motion. 7. What is implied motion?
Representational momentum? Describe behavioral evidence demonstrating
representational momentum, physiological experiments that investigated
how the brain responds to implied motion stimuli, and the experiment
that used photographs to generate a motion aftereffect. 8. Describe how
experiments with young animals and infants have been used to determine
the origins of biological motion perception. What is the evidence that
there may be two mechanisms of early biological motion perception?

THINK ABOUT IT 1. We described the role of the Reichardt detector in the
perception of real motion that occurs when we see things that are
physically moving, such as cars on the road and people on the sidewalk.
Explain how the detector illustrated in Figure 8.10 could also be used
to detect the kinds of apparent motion on TV, in movies, on our computer
screens, and in electronic displays such as those in Las Vegas or Times
Square. 2. In the present chapter we have described a number of
principles that also hold for object perception (Chapter 5). Find
examples from Chapter 5 of the following (page numbers are for this
chapter):

There are neurons that are specialized to respond to specific stimuli
(p. 183). ■■ More complex stimuli are processed in higher areas of the
cortex (p. 189). ■■ Experience can affect perception (p. 190). ■■ There
are parallels between physiology and perception (pp. 184, 188, 191). ■■

3.  We described how the representational momentum effect shows how
    knowledge can affect perception. Why could we also say that
    representational momentum illustrates an interaction between
    perception and memory?

KEY TERMS Akinetopsia (p. 175) Aperture problem (p. 186) Apparent motion
(p. 179) Biological motion (p. 189) Coherence (p. 184) Comparator
(p. 181) Corollary discharge signal (CDS) (p. 181) Corollary discharge
theory (p. 181) Delay unit (p. 182) Event (p. 177) Event boundary
(p. 177)

194

Global optic ﬂow (p. 181) Illusory motion (p. 179) Image displacement
signal (IDS) (p. 181) Implied motion (p. 190) Induced motion (p. 179)
Local disturbance in the optic array (p. 181) Microstimulation (p. 185)
Middle temporal (MT) area (p. 183) Motion aftereffect (p. 179) Motor
signal (MS) (p. 181)

Optic array (p. 181) Output unit (p. 182) Point-light walker (p. 178)
Real motion (p. 179) Reichardt detector (p. 182) Representational
momentum (p. 191) Shortest path constraint (p. 188) Transcranial
magnetic stimulation (TMS) (p. 185) Waterfall illusion (p. 179)

Chapter 8  Perceiving Motion

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

What's amazing about this picture is not only that the umbrellas appear
to be floating, but that your perception of their colors is created by
your nervous system when light that has no color is reflected from the
umbrellas into your eyes and activates three types of cone receptors in
the retina. How colorless light can cause color will be explained in
this chapter. Bruce Goldstein

Learning Objectives After studying this chapter, you will be able to ...
■■ Describe a number of important functions of color perception. ■■
Understand the relationship between the wavelength of light

and color and be able to apply this to explaining what happens when
wavelengths are mixed. ■■ Understand how we can perceive millions of
colors even though there are only six or seven colors in the visible
spectrum. ■■ Describe the trichromatic theory of color vision and how
the theory explains color deficiency. ■■ Describe the opponent-process
theory of color vision, and why some researchers have questioned a
proposed link between opponent neural responding and color perception.

■■ Understand limitations on our understanding of how color is

represented in the cortex. ■■ Describe experiments that show that we
need to go beyond

wavelength in order to fully understand color perception. ■■ Understand
what it means to say that we perceive color from

colorless wavelengths. ■■ Describe how behavioral experiments have been
used to study

infant color vision.

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Ch HAPTER a p t e r 49

Perceiving Color

Chapter Contents 9.1 Functions of Color Perception 9.2 Color and Light
Reflectance and Transmission Color Mixing

9.3 Perceptual Dimensions of Color TEST YOURSELF 9.1

9.4 The Trichromacy of Color Vision A Little History Color-Matching
Evidence for Trichromacy METHOD: Color Matching

Measuring the Characteristics of the Cone Receptors The Cones and
Trichromatic Color Matching

Color Vision With Only One Pigment: Monochromacy Color Vision With Two
Pigments: Dichromacy

9.7 Color in the World: Beyond Wavelength

TEST YOURSELF 9.2

Lightness Constancy

DEMONSTRATION: Adapting to Red

9.5 The Opponency of Color Vision

DEMONSTRATION: The Penumbra and

Behavioral Evidence for OpponentProcess Theory

DEMONSTRATION: Perceiving

METHOD: Hue Cancellation

Physiological Evidence for OpponentProcess Theory Questioning the Idea
of Unique Hues

9.6 Color Areas in the Cortex TEST YOURSELF 9.3

Some Questions We Will Consider: ■■ Why does mixing yellow and blue
paints create green?

(p. 201) ■■ Why do colors look the same indoors and outdoors? (p. 215)
■■ Does everyone perceive color the same way?

(pp. 207, 218, 224)

C

Color Constancy

olor is one of the most obvious and pervasive qualities in our
environment. We interact with it every time we note the color of a
traffic light, choose clothes that are color coordinated, or appreciate
the colors of a painting. We pick favorite colors (blue is the most
favored; Terwogt & Hoeksma, 1994), we associate colors with emotions (we
turn purple with rage, red with embarrassment, green with envy, and feel
blue; Terwogt & Hoeksma, 1994; Valdez & Mehribian, 1994), and we imbue
colors with special meanings (for example, in many cultures red
signifies danger; purple, royalty; green, ecology). But for all of our
involvement with color, we sometimes take it for granted, and---just as
with our other

Lightness Perception

Lightness at a Corner

SOMETHING TO CONSIDER:

We Perceive Color from Colorless Wavelengths DEVELOPMENTAL DIMENSION:
Infant Color Vision TEST YOURSELF 9.4 THINK ABOUT IT

perceptual abilities---we may not fully appreciate color unless we lose
our ability to experience it. The depth of this loss is illustrated by
the case of Mr. I., a painter who became color blind at the age of 65
after suffering a concussion in an automobile accident. In March 1986,
the neurologist Oliver Sacks received a letter from Mr. I., who,
identifying himself as a "rather successful artist," described how, ever
since he had been involved in an automobile accident, he had lost his
ability to experience colors. He exclaimed with some anguish, "My dog is
gray. Tomato juice is black. Color TV is a hodge-podge. ..." In the days
following his accident, Mr. I. became more and more depressed. His
studio, normally awash with the brilliant colors of his abstract
paintings, appeared drab to him, and his paintings, meaningless. Food,
now gray, became difficult for him to look at while eating, and sunsets,
once seen as rays of red, had become streaks of black against the sky
(Sacks, 1995). Mr. I.'s color blindness, a condition called cerebral
achromatopsia, was caused by cortical injury after a lifetime of
experiencing color, whereas most cases of total color 197

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

9.1 Functions of Color Perception Color serves important signaling
functions, both natural and contrived by humans. The natural and
human-made world provides many color signals that help us identify and
classify things: we know a banana is ripe when it has turned yellow, and
we know to stop when the traffic light turns red. In addition to its
signaling function, color helps facilitate perceptual organization
(Smithson, 2015), the processes discussed in Chapter 5 by which similar
elements become grouped together and objects are segregated from their
backgrounds (see Figures 5.18 and 5.19 on pages 97--98). Color's role in
perceptual organization is crucial to the survival of many species.
Consider, for example, a monkey foraging for fruit in the forest or
jungle. A monkey with good color vision easily detects red fruit against
a green background (Figure 9.1a), but a color-blind monkey would find it
more difficult to find the fruit (Figure 9.1b). Color vision thus
enhances the contrast of objects that, if they didn't appear colored,
would be more difficult to perceive. This link between good color vision
and the ability to detect colored food has led to the proposal that
monkey and human color vision may have evolved for the express purpose
of detecting fruit (Mollon, 1989, 1997; Sumner & Mollon, 2000; Walls,
1942). This suggestion sounds reasonable when we consider the difficulty
that color-blind humans have when confronted with the seemingly simple
task of picking berries. Knut Nordby (1990), a totally color-blind
vision scientist who sees the world in shades of gray, has described his
own experience: "Picking berries has always been a big problem. I often
have to grope around among the leaves with my fingers, feeling for the
berries by their shape" (p. 308). 198

Bruce Goldstein

blindness or of color deficiency (partial color blindness, which we'll
discuss in more detail later in this chapter) occur at birth because of
the genetic absence of one or more types of cone receptors. Most people
who are born partially color blind are not disturbed by their decreased
color perception compared to "normal," because they have never
experienced color as a person with normal color vision does. However,
some of their reports, such as the darkening of reds, are similar to
Mr. I.'s. People with total color blindness often echo Mr. I.'s
complaint that it is sometimes difficult to distinguish one object from
another, as when his brown dog, which he could easily see silhouetted
against a light-colored road, became very difficult to perceive when
seen against irregular foliage. Eventually, Mr. I. overcame his strong
psychological reaction and began creating striking black-and-white
pictures. But his account of his color-blind experiences provides an
impressive testament to the central place of color in our everyday
lives. (See Heywood et al., 1991; Nordby, 1990; Young et al., 1980; and
Zeki, 1990, for additional descriptions of cases of complete color
blindness.) Besides adding beauty to our lives, color has other
functions as well.

(a) 
(b) 

Figure 9.1 (a) Red berries in green foliage. (b) These berries become
more difficult to detect without color vision.

(a) 
(b) 

Figure 9.2 Participants in Tanaka and Presnell's (1999) experiment were
able to recognize appropriately colored objects like the fruits in (a)
more rapidly than inappropriately colored objects like the fruits in
(b).

Our ability to perceive color not only helps us detect objects that
might otherwise be obscured by their surroundings, it also helps us
recognize and identify things we can see easily. James Tanaka and Lynn
Presnell (1999) demonstrated this by asking observers to identify
objects like the ones in Figure 9.2, which appeared either in their
normal colors, like the yellow banana, or in inappropriate colors, like
the purple banana. The result was that observers recognized the
appropriately colored objects more rapidly and accurately. Thus, knowing
the colors of familiar objects helps us to recognize these objects
(Oliva & Schyns, 2000; Tanaka et al., 2001). Expanding our view beyond
single objects, color also helps us recognize natural scenes
(Gegenfurtner & Rieger, 2000) and rapidly perceive the gist of scenes
(Castelhano & Henderson, 2008) (see Figure 5.33, page 105). It has also
been suggested that color can be a cue to emotions signaled by facial
expressions. This was demonstrated by Christopher Thorstenson and
coworkers (2019) who found that when asked to rate the emotions of
ambiguous-emotion faces like the one in Figure 9.3, participants were
more likely to rate the face as expressing disgust when colored green
and as expressing anger when red. In the discussion that follows, we
will consider how our nervous system creates our perception of color. We
begin by considering the relationship between color and light, and will
then consider two theories of color vision.

Chapter 9  Perceiving Color

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Christopher Baker

Figure 9.3 How do you perceive the emotion of each of these versions of
the same face? Color has been shown to influence emotion judgments, with
red associated with "anger" and green associated with "disgust."

9.2 Color and Light For much of his career, Isaac Newton (1642--1727)
studied the properties of light and color. One of his most famous
experiments is diagrammed in Figure 9.4a (Newton, 1704). First,

Newton made a hole in a window shade, which let a beam of sunlight enter
the room. When he placed Prism 1 in its path, the beam of
white-appearing light was split into the components of the visual
spectrum shown in Figure 9.4b. Why did this happen? At the time, many
people thought that prisms (which were common novelties) added color to
light. Newton, Prism 2

Prism 1

hite

t

ligh

Prism 3

W

Prism 4

(a) 

400

500

600

700

Wavelength (nm) (b)

Figure 9.4 (a) Diagram of Newton's prism experiment. Light entered
through a hole in the window shade and then passed through the prism.
The colors of the spectrum were then separated by passing them through
holes in a board. Each color of the spectrum then passed through a
second prism. Different colors were bent by different amounts. (b) The
visible spectrum. 9.2 Color and Light

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

199

however, thought that white light was a mixture of differently colored
lights and that the prism separated the white light into its individual
components. To support this hypothesis, Newton next placed a board in
the path of the differently colored beams. Holes in the board allowed
only particular beams to pass through while the rest were blocked. Each
beam that passed through the board then went through a second prism,
shown as Prisms 2, 3, and 4, for the red, yellow, and blue rays of
light. Newton noticed two important things about the light that passed
through the second prism. First, the second prism did not change the
color appearance of any light that passed through it. For example, a red
beam continued to look red after it passed through the second prism. To
Newton, this meant that unlike white light, the individual colors of the
spectrum are not mixtures of other colors. Second, the degree to which
beams from each part of the spectrum were "bent" by the second prism was
different. Red beams were bent only a little, yellow beams were bent a
bit more, and violet beams were bent the most. From this observation,
Newton concluded that light in each part of the spectrum is defined by
different physical properties and that these physical differences give
rise to our perception of different colors. Throughout his career,
Newton was embroiled in debate with other scientists regarding what the
physical differences were between differently colored lights. Newton
thought that prisms separated differently colored light particles while
others thought the prism separated light into differently colored waves.
Clarity on these matters would come in the 19th century when scientists
conclusively showed that the colors of the spectrum are associated with
different wavelengths of light (Figure 9.4b). Wavelengths from about 400
to 450 nm appear violet; 450 to 490 nm, blue; 500 to 575 nm, green; 575
to 590 nm, yellow; 590 to 620 nm, orange; and 620 to 700 nm, red. Thus,
our perception of color depends critically on the wavelengths of light
that enter our eyes.

Reflectance and Transmission The colors of light in the spectrum are
related to their wavelengths, but what about the colors of objects? The
colors of objects are largely determined by the wavelengths of light
that are reflected from the objects into our eyes. Chromatic colors,
such as blue, green, and red, occur when some wavelengths are reflected
more than others, a process called selective reflection. The sheet of
paper illustrated in Figure 9.5a reflects long wavelengths of light and
absorbs short and medium wavelengths. As a result, only the long
wavelengths reach our eyes, and the paper appears red. Achromatic
colors, such as white, gray, and black, occur when light is reflected
equally across the spectrum. Because the sheet of paper in Figure 9.5b
reflects all wavelengths of light, it appears white. Individual objects
don't usually reflect a single wavelength of light, however. Figure 9.6a
shows reflectance curves that plot the percentage of light reflected
from lettuce and tomatoes at each wavelength in the visible spectrum.
Notice that both vegetables reflect a range of wavelengths, but 200

(a) Selective reflection of long wavelengths

(b) Equal reflection of all wavelengths

(c) Selective transmission of long wavelengths

Figure 9.5 (a) White light contains all of the wavelengths of the
spectrum. A beam of white light is symbolized here by showing beams with
wavelengths associated with blue, green, yellow, and red. When white
light hits the surface of the paper, the long-wavelength light is
selectively reflected and the rest of the wavelengths are absorbed. We
therefore perceive the paper as looking red. (b) When all of the
wavelengths are reflected equally, we see white. (c) In this example of
selective transmission, the long-wavelength light is transmitted and the
other wavelengths are absorbed by the liquid.

each selectively reflects more light in one part of the spectrum.
Tomatoes predominantly reflect long wavelengths of light into our eyes,
whereas lettuce principally reflects medium wavelengths. As a result,
tomatoes appear red, whereas lettuce appears green. You can also
contrast the reflectance curves for the lettuce and tomato with the
curves for the achromatic (black, gray, and white) pieces of paper in
Figure 9.6b, which are relatively flat, indicating equal reflectance
across the spectrum. The difference between black, gray, and white is
related to the overall amount of light reflected from an object. The
black paper in Figure 9.6b reflects less than 10 percent of the light
that hits it, whereas the white paper reflects more than 80 percent of
the light. Although most colors in the environment are created by the
way objects selectively reflect some wavelengths, the color of things
that are transparent, such as liquids, plastics, and glass, is created
by selective transmission. Selective transmission means that only some
wavelengths pass through the object or substance (Figure 9.5c). For
example, cranberry juice selectively transmits long-wavelength light and
appears red, whereas limeade selectively transmits medium-wavelength
light and appears green. Transmission curves---plots of the percentage
of light transmitted at each wavelength---look similar to the
reflectance curves in Figure 9.6, but with percent transmission

Chapter 9  Perceiving Color

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

90

80

80

Reflectance (percentage)

Reflectance (percentage)

90 70 60 50 40 30 20 10 0 400

Lettuce Tomato 450

(a) 

500 550 600 Wavelength (nm)

650

White paper

70 60 50 40 30

Gray card

20 Black paper

10 0 400

700

450

(b) 

500 550 600 Wavelength (nm)

650

700

Figure 9.6 Reflectance curves for (a) lettuce and tomatoes (adapted from
Williamson & Cummins, 1983) and (b) white, gray, and black paper
(adapted from Clulow, 1972).

Table 9.1  Predominant Wavelengths Reflected

or Transmitted and Perceived Color

WAVELENGTHS REFLECTED OR TRANSMITTED

PERCEIVED COLOR

Short

Blue

Medium

Green

Long and medium

Yellow

Long

Red

Long, medium, and short

White

S M L S

L

S M L

S M L

m

m

m

Blue paint

Yellow paint

Blue paint + Yellow paint

Green paint

Yellow paint

(a) 90

Color Mixing The idea that the color we perceive depends largely on the
wavelengths of light that reach our eyes provides a way to explain what
happens when we mix different colors together. We will describe two ways
of mixing colors: mixing paints and mixing lights.

Mixing Paints In kindergarten you learned that mixing yellow and blue
paints results in green. Why is this so? Consider the blobs of paint in
Figure 9.7a. The blue blob absorbs long-wavelength light and reflects
some short-wavelength light and some medium-wavelength light (see the
reflectance curve for "blue paint" in Figure 9.7b). The yellow blob
absorbs short-wavelength light and reflects some medium- and
longwavelength light (see the reflectance curve for "yellow paint" in
Figure 9.7b). The key to understanding what happens when colored paints
are mixed together is that when mixed, both paints still absorb the same
wavelengths they absorbed when alone, so the only wavelengths reflected
are those that are reflected by both paints in common.

Reflectance (percentage)

80

plotted on the vertical axis. Table 9.1 indicates the relationship
between the wavelengths reflected or transmitted and the color
perceived.

70 60 50

Blue paint

40 30 20 10 0 400

(b) 

450

500

550

600

650

700

Wavelength (nm)

Figure 9.7 Color mixing with paint. Mixing blue paint and yellow paint
creates a paint that appears green. This is subtractive color mixing.

So, as indicated in Table 9.2, a blob of blue paint absorbs all of the
long-wavelength light, while a blob of yellow paint absorbs all of the
short-wavelength light. Mix them together and the only wavelengths that
survive all this absorption are some of the medium-wavelengths, which
are associated with green. Because the blue and yellow blobs subtract
all of 9.2 Color and Light

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

201

Table 9.2 Mixing Blue and Yellow Paints (Subtractive Color Mixture)
Parts of the spectrum that are absorbed and reflected by blue and yellow
paint. ­Wavelengths that are reflected from the mixture are highlighted.
Light that is usually seen as green is the only light that is reflected
in common by both paints. WAVELENGTHS SHORT

MEDIUM

LONG

Blob of blue paint

Reflects all

Reflects some

Absorbs all

Blob of yellow paint

Absorbs all

Reflects some

Reflects some

Mixture of blue and yellow blobs

Absorbs all

Reflects some

Absorbs all

the wavelengths except some that are associated with green, mixing
paints is called subtractive color mixture. The reason that mixing blue
and yellow paints results in green is that both paints reflect some
light in the green part of the spectrum (notice that the overlap between
the blue and yellow paint curves in Figure 9.7b coincides with the peak
of the reflectance curve for green paint). If our blue paint had
reflected only short wavelengths and our yellow paint had reflected only
medium and long wavelengths, these paints would reflect no color in
common, so mixing them would result in little or no reflection across
the spectrum, and the mixture would appear black. Like objects, however,
most paints reflect a band of wavelengths. If paints didn't reflect a
range of wavelengths, then many of the color-mixing effects of paints
that we take for granted would not occur.

yellow spot are both reflected into the observer's eye. The
added-together light therefore contains short, medium, and long
wavelengths, as shown in Figure 9.9, which results in the perception of
white. Because mixing lights involves adding up the wavelengths of each
light in the mixture, mixing lights is called an additive color mixture.
We can summarize the connection between wavelength and color as follows:

Colors of light are associated with wavelengths in the visible spectrum.
■■ The colors of objects are associated with which wavelengths are
reflected (for opaque objects) or transmitted (for transparent objects).
■■ The colors that occur when we mix colors are also associated with
which wavelengths are reflected into the eye. Mixing paints causes fewer
wavelengths to be reflected (each paint subtracts wavelengths from the
mixture); mixing lights causes more wavelengths to be reflected (each
light adds wavelengths to the mixture). ■■

Short Mixing Lights Let's now think about what would happen if
wavelengths we mix together blue and yellow lights. If a light that
appears blue is projected onto a white surface and a light that appears
yellow is projected on top of the light that appears blue, the area
where the lights are superimposed is perceived as white (Figure 9.8).
Given your lifelong knowledge that yellow and blue make green, and our
discussion of paints above, this result may surprise you. Medium + long
wavelengths But you can understand why this occurs by considering the
wavelengths that are reflected into the eye by a mixture of blue and
Short + medium + long wavelengths yellow lights. Because the two spots
of light are projected onto a white surface, which reflects all
wavelengths, all of the wavelengths that hit the surface are reflected
into an observer's eyes (see the reflectance curve for white paper in
Figure 9.5). The blue Figure 9.8 Color mixing with light. Superimposing
a blue light and spot consists of a band of short wavelengths, so when
it is proa yellow light creates the perception of white in the area of
overlap. jected alone, the short-wavelength light is reflected into the
This is additive color mixing. observer's eyes (Table 9.3). Similarly,
the yellow spot consists of medium and long wavelengths, so when pre-
Table 9.3 Mixing Blue and Yellow Lights (Additive Color Mixture) sented
alone, these wavelengths are reParts of the spectrum that are reflected
from a white surface for blue and yellow spots flected into the
observer's eyes. of light projected onto the surface. Wavelengths that
are reflected from the mixture are The key to understanding what
highlighted. happens when colored lights are WAVELENGTHS superimposed is
that all of the light that is reflected from the surface by each light
SHORT MEDIUM LONG when alone is also reflected when the lights Spot of
blue light Reflected No reflection No reflection are superimposed. Thus,
where the two Spot of yellow light No reflection Reflected Reflected
spots are superimposed, the light from Reflected Reflected the blue spot
and the light from the Overlapping blue and yellow spots Reflected

202

Chapter 9  Perceiving Color

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Saturation

White

50 40

Blue

Yellow

A

30

B

20 10 0 400

450

500

550

600

650

700

Wavelength (nm)

Value

Intensity (percentage)

60

Figure 9.9 Spectral distribution of blue and yellow light. The dashed
curve, which is the sum of the blue and yellow distributions, is the
wavelength distribution for white light. Thus when blue and yellow
lights are superimposed, the wavelengths add and the result is the
perception of white.

We will see later in the chapter that wavelength is not the whole story
when it comes to color perception. For example, our perception of an
object's color can be influenced by the background on which the object
is seen, by the colors observers are exposed to the environment, and by
how viewers interpret how a scene is illuminated. But for now our main
focus is on the connection between wavelength and color.

9.3 Perceptual Dimensions of Color Isaac Newton described the visible
spectrum (Figure 9.4b) in his experiments in terms of seven colors: red,
orange, yellow, green, blue, indigo, and violet. His use of seven color
terms probably had more to do with mysticism than science, however, as
he wanted to harmonize the visible spectrum (seven colors) with musical
scales (seven notes), the passage of time (seven days in a week),
astronomy (there were seven known planets at the time), and religion
(seven deadly sins). Modern vision scientists tend to exclude indigo
from the list of spectral colors because humans actually have a
difficult time distinguishing it from blue and violet. There are also
many nonspectral colors---colors that do not appear in the spectrum
because they are mixtures of other colors, such as magenta (a mixture of
blue and red). Ultimately, the number of colors we can differentiate is
enormous: If you've ever decided to paint your bedroom wall, you will
have discovered a dizzying number of color choices in the paint
department of your local home improvement store. In fact, major paint
manufacturers have thousands of colors in their catalogs, and your
computer monitor can display millions of different colors. Although
estimates of how many colors humans can discriminate vary widely, a
conservative estimate is that we can tell the difference between about
2.3 million different colors (Linhares et al., 2008). How can we
perceive millions of colors when we can describe the visible spectrum in
terms of only six or seven colors? The answer is that there are three
perceptual dimensions

Figure 9.10 These 12 color patches have the same hue (red). Saturation
decreases from left to right. Lightness decreases from top to bottom.

of color, which together can create the large number of colors we can
perceive. We previously called colors like blue, green, and red
chromatic colors. Another term for these colors is hues. Figure 9.10
shows a number of color patches, most of which we would describe as
having a red hue. What makes these colors appear different is their
variation in the other two dimensions of color, saturation and value
(also called lightness). Saturation refers to the intensity of color.
Moving from left to right in Figure 9.10, progressively more white has
been added to each color patch and, as a result, saturation decreases.
When hues become desaturated, they can take on a faded or washedout
appearance. For example, color patch A in Figure 9.10 appears to be a
deep and vivid red, but color patch B appears to be a desaturated muted
pink. Value or lightness refers to the light-to-dark dimension of color.
Moving down the columns in Figure 9.10, value decreases as the colors
become darker. Another useful way to illustrate the relationship among
hue, saturation, and value is to arrange colors systematically within a
three-dimensional color space called a color solid. Figure 9.11a depicts
the dimensions of hue, saturation, and value in the Munsell color
system, that was developed by Albert Munsell in the early 1900s and is
still in wide use today. Different hues are arranged around the
circumference of the cylinder with perceptually similar hues placed next
to each other. Notice that the order of the hues around the cylinder
matches the order of the colors in the visible spectrum shown in Figure
9.4b. Saturation is depicted by placing more saturated colors toward the
outer edge of the cylinder and more desaturated colors toward the
center. Value is represented by the cylinder's height, with lighter
colors at the top and darker colors at the bottom. The color solid
therefore creates a coordinate system in which our perception of any
color can be defined by hue, saturation, and value. Now that we have
introduced the basic properties of color and color mixing, we are going
to focus on the connection between color vision and the cone receptors
in the retina. 9.3 Perceptual Dimensions of Color

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

203

Munsell Color System Value

Hue White

Saturation

10 Red 8

Purple

Orange Yellow

6 5 4 Blue

2

Green

0 Black

Figure 9.11 The Munsell color space. Hue is arranged in a circle around
the vertical, which represents value. Saturation increases with distance
away from the vertical.

TEST YOuRSELF 9.1 1. Describe the case of Mr. I. What does it illustrate
about color perception? 2. What are the various functions of color
vision? 3. What physical characteristic of light is most closely
associated with color perception? How is this demonstrated by
differences in reflection and transmission of light of different
objects? 4. Describe subtractive and additive color mixing. How can the
results of these two types of color mixing be related to the wavelengths
that are reflected into an observer's eyes? 5. What are spectral colors?
Nonspectral colors? How many different colors can humans discriminate?
6. What are hue, saturation, and value? Describe how the Munsell color
system represents different properties of color.

9.4 The Trichromacy of Color Vision We now shift to physiology, and
begin with the retina and physiological principles that are based on
wavelength.

A Little History We begin by discussing the retinal basis of color
vision by returning to Isaac Newton's prism experiment (Figure 9.4).
When Newton separated white light into its components to 204

reveal the visible spectrum, he argued that each component of the
spectrum must stimulate the retina differently in order for us to
perceive color. He proposed that "rays of light in falling upon the
bottom of the eye excite vibrations in the retina. Which vibrations,
being propagated along the fibres of the optick nerves into the brain,
cause the sense of seeing" (Newton, 1704). We know now that electrical
signals, not "vibrations," are what is transmitted down the optic nerve
to the brain, but Newton was on the right track in proposing that
activity associated with different lights gives rise to the perceptions
of different colors. About 100 years later, the British physicist Thomas
Young (1773--1829), starting with Newton's proposed vibrations,
suggested that Newton's idea of a link between each size of vibration
and each color won't work, because a particular place on the retina
can't be capable of the large range of vibrations required. His exact
words were: "Now, as it is almost impossible to conceive of each
sensitive point on the retina to contain an infinite number of
particles, each capable of vibrating in perfect unison with every
possible undulation, it becomes necessary to suppose the number limited,
for instance, to the three principal colors, red, yellow, and blue"
(Young, 1802). The actual quote from Young is included here because it
is so important. It is this proposal---that color vision is based on
three principal colors---that marks the birth of what is today called
the trichromacy of color vision, which in modern terminology states that
color vision depends on the activity of three different receptor
mechanisms. At the time it was proposed, however, Young's theory was
little more than an insightful idea that, if correct, would provide an
elegant solution to the puzzle of color perception. Young had little
interest in conducting experiments to test his ideas, however, and never
published any research to support his theory (Gurney, 1831; Mollon,
2003; Peacock, 1855). Thus, it was left to James Clerk Maxwell
(1831--1879) and Hermann von Helmholtz (whose proposal of unconscious
inference we discussed in Chapter 5) to provide the needed experimental
evidence for trichromatic theory (Helmholtz, 1860; Maxwell, 1855).
Although Maxwell conducted his experiments before Helmholtz, Helmholtz's
name became attached to Young's idea of three receptors, and
trichromatic theory became known as the Young-Helmholtz theory. That
trichromatic theory became known as the YoungHelmholtz theory rather
than the Young-Maxwell theory has been attributed to Helmholtz's
prestige in the scientific community and to the popularity of his
Handbook of Physiology (1860), in which he described the idea of three
receptor mechanisms (Heesen, 2015; Sherman, 1981). Even though Maxwell
was denied "naming rights" for his discoveries in color vision, if it is
any consolation to him, a 1999 poll of leading physicists named him the
third greatest physicist of all time, behind only Newton and Einstein,
for his work in electromagnetism (Durrani & Rogers, 1999). It is also
Maxwell's critical color-matching experiments that we will describe in
detail in the next section as we begin to consider the evidence for
trichromatic theory.

Chapter 9  Perceiving Color

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

The trichromacy of color vision is supported by the results of a
psychophysical procedure called color matching.

Color Matching

METHOD

The procedure used in a color matching experiment is shown in Figure
9.12. The experimenter presents a reference color that is created by
shining a single wavelength of light on a "reference field". The
observer then matches the reference color by mixing different
wavelengths of light in a "comparison field". In this example, the
observer is shown a 500-nm light in the reference field on the left and
then asked to adjust the amounts of 420nm, 560-nm, and 640-nm lights in
the comparison field on the right, until the perceived color of the
comparison field matches the reference field. 640-nm

560-nm Comparison 420-nm

Comparison

Reference

Bipartite field

Reference light 500-nm (a)

(b) 

Figure 9.12 A color matching experiment. (a) The observer's view of the
bipartite field. The comparison field is empty here, but becomes colored
when (b) the observer adjusts the amount of three wavelengths to create
a color that matches the color in the reference field.

The key finding from Maxwell's color-matching experiments was that any
reference color could be matched provided that observers were able to
adjust the proportions of three wavelengths in the comparison field. Two
wavelengths allowed participants to match some, but not all, reference
colors, and they never needed four wavelengths to match any reference
color. Based on the finding that people with normal color vision need at
least three wavelengths to match any other wavelength, Maxwell reasoned
that color vision depends on three receptor mechanisms, each with
different spectral sensitivities. (Remember from Chapter 3 that spectral
sensitivity indicates the sensitivity to wavelengths in the visible
spectrum, as shown in Figure 3.15 on page 50.) According to trichromatic
theory, light of a particular wavelength stimulates each receptor
mechanism to different degrees, and the pattern of activity in the three
mechanisms results in the perception of a color. Each wavelength is
therefore represented in the nervous system by its own pattern of
activity in the three receptor mechanisms.

The story of the physiology of trichromacy is a story of delayed
gratification, because almost 100 years passed between the proposal of
three receptor mechanisms and actually demonstrating their physiological
existence.

Measuring the Characteristics of the Cone Receptors In 1963 and 1964 a
number of research teams made a discovery that provided physiological
support for the trichromacy that was based on the results of Maxwell's
color matching experiments. The discovery of three types of cones in the
human retina was made using the technique of microspectrophotometry,
which made it possible to direct a narrow beam of light into a single
cone receptor. By presenting light at wavelengths across the spectrum,
it was determined that there were three types of cones, with the
absorption spectra shown in Figure 9.13. The short-wavelength pigment
(S), absorbed maximally at 419-nm; the middle-wavelength pigment (M), at
531-nm; and the long-wavelength pigment (L), at 558-nm (Brown & Wald,
1964; Dartnall et al., 1983; Marks et al., 1964). The reaction of vision
researchers to these measurements was interesting. On one hand, the
measurements of the cone spectra were hailed as an impressive and
important achievement. On the other hand, because of the results of the
color matching experiments done almost 100 years earlier, some said "we
knew it all along." But the new measurements were important because they
were not only consistent with trichromacy as predicted by color
matching, but they also revealed the exact spectra of the three cone
mechanisms, and, unexpectedly, revealed the large overlap between the L
and M cones. Another advance in describing the cones was provided by a
technique called adaptive optical imaging, which made it possible to
look into a person's eye and take pictures that showed how the cones are
arranged on the surface of the retina. This was an impressive
achievement, because the eye's cornea and lens contain imperfections
called aberrations that distort the light on its way to the retina. This
means that when your optometrist or ophthalmologist uses an
ophthalmoscope to look into your eye, they can see blood vessels and the
surface of the retina, but the image is too blurry to make out
individual receptors.

Relative proportion of light absorbed

Color-Matching Evidence for Trichromacy

S

1.0

M

L

.75 .50 .25 0

400

450

500 550 Wavelength (nm)

600

650

Figure 9.13 Absorption spectra of the three cone pigments. (From
Dartnall et al., 1983)

9.4 The Trichromacy of Color Vision

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

205

The Journal of Neuroscience

Adaptive optical imaging creates a sharp image by first measuring how
the optical system of the eye distorts the image reaching the retina,
and then taking a picture through a deformable mirror that cancels the
distortion created by the eye. The result is a clear picture of the cone
mosaic like the one in Figure 9.14, which shows foveal cones. In this
picture the cones are colored to distinguish the short-, medium-, and
long-wavelength cones. Figure 9.15 shows the relationship between the
responses of the three kinds of receptors derived from the absorption
spectrum, and the perception of different colors. In this figure, the
responses in the S, M, and L receptors are indicated by the size of the
receptors. For example, short-wavelength light, which appears blue in
the spectrum, is signaled by a large response in the S receptor, a
smaller response in the M receptor, and an even smaller response in the
L receptor. Yellow is signaled by a very small response in the S
receptor and large responses in the M and L receptors. White is signaled
by equal activity in all the receptors.

Figure 9.14 Cone mosaic showing long- (red), medium- (green), and
short-wavelength (blue) cones in the fovea. The colors were added after
the images were created. (From Roorda & Williams, 1999) S

M

L

S

M

The patterns of activity shown in Figure 9.15 indicate how different
wavelengths of light activate the three types of cone receptors. Later
in this chapter, we will see that this link between wavelength and
receptor activity is only part of the story of color vision, because our
perception of color is also affected by factors such as our state of
adaptation, the nature of our surroundings, and our interpretation of
the illumination. However, for now we will continue the story of the
connection between color perception and the activity of cones by
returning to the color-matching results that led to the proposal of
trichromatic theory.

The Cones and Trichromatic Color Matching Remember that in a
color-matching experiment, a wavelength in one field is matched by
adjusting the proportions of three different wavelengths in another
field (Figure 9.12). This result is interesting because the lights in
the two fields are physically different (they contain different
wavelengths) but they are perceptually identical (they look the same).
This situation, in which two physically different stimuli are
perceptually identical, is called metamerism, and the two identical
fields in a color-matching experiment are called metamers. The reason
metamers look alike is that they both result in the same pattern of
response in the three cone receptors. For example, when the proportions
of a 620-nm red light that looks red and a 530-nm green light that looks
green are adjusted so the mixture matches the color of a 580-nm light,
which looks yellow, the two mixed wavelengths create the same pattern of
activity in the cone receptors as the single 580nm light (Figure 9.16).
The 530-nm green light causes a large response in the M receptor, and
the 620-nm red light causes a large response in the L receptor.
Together, they result in a large response in the M and L receptors and a
much smaller response in the S receptor. This is the pattern for yellow
and is the same as the pattern generated by the 580-nm light. Thus,

530 + 620

L

580 L

Blue

Yellow

S

L S

M 8.0

1.0

Green

5.0 Red

White

Figure 9.15 Patterns of firing of the three types of cones to
wavelengths associated with different colors. The size of the cones
symbolizes the amount of activity in the short-, medium-, and
longwavelength cones. 206

M 8.0

1.0 5.0

Figure 9.16 Principle behind metamerism. The proportions of 530-nm and
620-nm lights in the field on the left have been adjusted so that the
mixture appears identical to the 580-nm light in the field on the right.
The numbers indicate the responses of the short-, medium-, and
long-wavelength receptors. There is no difference in the responses of
the two sets of receptors, so the two fields are perceptually
indistinguishable.

Chapter 9  Perceiving Color

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Percent light absorbed

even though the lights in these two fields are physically different, the
two lights result in identical physiological responses so they are
identical as far as the brain is concerned and they are therefore
perceived as being the same. One way to appreciate the connection
between the results of color matching experiments and cone pigments is
to consider what happens to color perception when there are fewer than
three types of receptors. We begin by considering what happens when
there is only a single type of receptor.

15 10

480 = 10%

5

600 = 5%

0

480

600

Wavelength (nm)

Color Vision With Only One Pigment: Monochromacy Monochromatism is a
rare form of color blindness that is usually hereditary and occurs in
only about 10 people out of 1 million (LeGrand, 1957). Monochromats
usually have no functioning cones, so their vision is created only by
the rods. Their vision, therefore, has the characteristics of rod vision
in both dim and bright lights so they see only in shades of lightness
(white, gray, and black) and can therefore be called color blind. A
person with normal color vision can experience what it is like to be a
monochromat by sitting in the dark for several minutes. When dark
adaptation is complete (see Figure 3.13), vision is controlled by the
rods, which causes the world to appear in shades of gray. Because
monochromats perceive all wavelengths as shades of gray, they can match
any wavelength by picking another wavelength and adjusting its
intensity. Thus, a monochromat needs only one wavelength to match any
wavelength in the spectrum. We can understand why color vision is not
possible in a person with just one receptor type by considering how a
person with just one pigment would perceive two lights, one 480 nm and
one 600 nm, which a person with normal color vision sees as blue and
red, respectively. The absorption spectrum for the single pigment, shown
in Figure 9.17a, indicates that the pigment absorbs 10 percent of 480-nm
light and 5 percent of 600-nm light. To discuss what happens when our
one-pigment observer looks at the two lights, we have to return to our
description of visual pigments in Chapter 3 (see page 45). Remember that
when light is absorbed by the retinal part of the visual pigment
molecule, the retina changes shape, a process called isomerization.
(Although we will usually specify light in terms of its wavelength,
light can also be described as consisting of small packets of energy
called photons, with one photon being the smallest possible packet of
light energy.) The visual pigment molecule isomerizes when the molecule
absorbs one photon of light. This isomerization activates the molecule
and triggers the process that activates the visual receptor and leads to
seeing the light. If the intensity of each light is adjusted so 1,000
photons of each light enter our one-pigment observer's eyes, we can see
from Figure 9.17b that the 480-nm light isomerizes 1,000 × 0.10 = 100
visual pigment molecules and the 600-nm light isomerizes 1,000 × 0.05 =
50 molecules. Because the 480-nm light isomerizes twice as many visual
pigment molecules as the

(a) 480

600

100

50

480 nm appears brighter (b)

Intensities of both lights is 1,000 photons 480

600

100

100

480 and 600 nm appear identical (c)

600-nm light increased to 2,000 photons

Figure 9.17 (a) Absorption spectrum of a visual pigment that absorbs 10
percent of 480-nm light and 5 percent of 600-nm light. (b) Visual
pigment molecules isomerized when the intensity of both 480-nm and
600-nm lights is 1,000, determined by multiplying intensity times the
percent of light absorbed. Because more visual pigments are isomerized
by the 480-nm light, it will appear brighter. (c) Molecules isomerized
when the intensity of the 480-nm light is 1,000 and the intensity of the
600-nm light is 2,000. In this case, both lights will look identical.

600-nm light, it will cause a larger response in the receptor, resulting
in perception of a brighter light. But if we increase the intensity of
the 600-nm light to 2,000 photons, as shown in Figure 9.17c, then this
light will also isomerize 100 visual pigment molecules. When the 1,000
photon 480-nm light and the 2,000 photon 600-nm light both isomerize the
same number of molecules, the result will be that the two spots of light
will appear identical. The difference in the wavelengths of light
doesn't matter, because of the principle of univariance, which states
that once a photon of light is absorbed by a visual pigment molecule,
the identity of the light's wavelength is lost. An isomerization is an
isomerization no matter what wavelength caused it. Univariance means
that the receptor does not know the wavelength of light it has absorbed,
only the total amount it has absorbed. Thus, by adjusting the
intensities of the two lights, we can 9.4 The Trichromacy of Color
Vision

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

207

Color Vision With Two Pigments: Dichromacy Let's consider what happens
when the retina contains two pigments with absorption spectra shown by
the dashed curves in Figure 9.18. Considering the ratios of responses to
two wavelengths we can see that the 480-nm light causes a large response
from pigment 1 and a smaller response from pigment 2, and that the
600-nm light causes a larger response in pigment 2 and a smaller
response in pigment 1. These ratios remain the same no matter what the
light intensities. The ratio of the response of pigment 1 to pigment 2
is always 10 to 2 for the 480-nm light and 5 to 10 for the 600-nm light.
Thus, just as in the case when there are three pigments, the visual
system can use ratio information such as this to identify the wavelength
of any light. According to this reasoning, two pigments should provide
information about which wavelength is present. People with just two
types of cone pigment, called dichromats, see chromatic colors, just as
our calculations predict, but because they have only two types of cones,
they confuse some colors that trichromats can distinguish.

Ratio for 480 nm = 10/2

1

2

Ratio for 600 nm = 5/10

Fraction absorbed

cause the single pigment to result in identical responses, so the lights
will appear the same even though their wavelengths are different. What
this means is that a person with only one visual pigment can match any
wavelength in the spectrum by adjusting the intensity of any other
wavelength and sees all of the wavelengths as shades of gray. Thus,
adjusting the intensity appropriately can make the 480-nm and 600-nm
lights (or any other wavelengths) look identical. The message of our
one-pigment example is that a person needs more than one type of
receptor to perceive chromatic color. We now consider what happens when
there are two types of cones.

10

10

10

5

5 2

2

480

600 Wavelength (nm)

Figure 9.18 Adding a second pigment (dashed curve) to the one in Figure
9.17. Now the 480-nm and 600-nm lights can be identified by the ratio of
response in the two pigments. The ratio for the 480-nm light is 10/2.
The ratio for the 600-nm light is 5/10. These ratios occur no matter
what the intensity of the light.

A dichromat, like our two-pigment observer in Figure 9.16, needs only
two wavelengths to match any other wavelength in the spectrum. Thus, one
way to determine the presence of color deficiency is by using the
color-matching procedure to determine the minimum number of wavelengths
needed to match any other wavelength in the spectrum. Another way to
diagnose color deficiency is by a color vision test that uses stimuli
called Ishihara plates. An example plate is shown in Figure 9.19a. In
this example, people with normal color vision see the number "74," but
people with a form of red--green color deficiency might see something
like the depiction in Figure 9.19b, in which the "74" is not visible.

Figure 9.19 (a) An example of an Ishihara plate for testing color
deficiency. A person with normal color vision sees a "74" when the plate
is viewed under standardized illumination. (b) The same Ishihara plate
as perceived by a person with a form of red--green color deficiency.

(a) 

208

(b) 

Chapter 9  Perceiving Color

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Once we have determined that a person's vision is color deficient, we
are still left with the question: What colors does a dichromat see
compared to a trichromat? To determine what a dichromat perceives
compared to a trichromat, we need to locate a unilateral dichromat---a
person with trichromatic vision in one eye and dichromatic vision in the
other. Both of the unilateral dichromat's eyes are connected to the same
brain, so this person can look at a color with his dichromatic eye and
then determine which color it corresponds to in his trichromatic eye.
Although unilateral dichromats are extremely rare, the few who have been
tested have helped us determine the nature of a dichromat's color
experience (Alpern et al., 1983; Graham et al., 1961; Sloan & Wollach,
1948). Let's now look at the three kinds of dichromats and the nature of
their color experience. There are three major forms of dichromatism:
protanopia, deuteranopia, and tritanopia. The two most common kinds,
protanopia and deuteranopia, are inherited through a gene located on the
X chromosome (Nathans et al., 1986). Males (XY) have only one X
chromosome, so a defect in the visual pigment gene on this chromosome
causes color deficiency. Females (XX), on the other hand, with their two
X chromosomes, are less likely to become color deficient because only
one normal gene is required for normal color vision. These forms of
color vision are therefore called sex-linked because women can carry the
gene for color deficiency without being color deficient themselves.
Thus, many more males than females are dichromats.

As we describe what the three types of dichromats perceive, we use as
our reference points Figures 9.20a and 9.21a, which show how a
trichromat perceives a bunch of colored paper flowers and the visible
spectrum, respectively. Protanopia affects 1 percent of males and 0.02
percent of females and results in the perception of colors shown in
Figure 9.20b. A protanope is missing the longwavelength pigment. As a
result, a protanope perceives short-wavelength light as blue, and as the
wavelength is increased, the blue becomes less and less saturated until,
at 492 nm, the protanope perceives gray (Figure 9.21b). The wavelength
at which the protanope perceives gray is called the neutral point. At
wavelengths above the neutral point, the protanope perceives yellow,
which becomes less intense at the long wavelength end of the spectrum.
■■ Deuteranopia affects about 1 percent of males and 0.01 percent of
females and results in the perception of color in Figure 9.20c. A
deuteranope is missing the mediumwavelength pigment. A deuteranope
perceives blue at short wavelengths, sees yellow at long wavelengths,
and has a neutral point at about 498 nm (Figure 9.21c) (Boynton, 1979).
■■ Tritanopia is very rare, affecting only about 0.002 percent of males
and 0.001 percent of females.

■■

400

500

600

700

(a) 

Protanope 400 (b)

(a) 

700 492

Deuteranope

(b) 400
(c) 

700 498

Tritanope 700

400 (d) (c)

(d) 

Figure 9.20 How colored paper flowers appear to (a) trichromats, (b)
protanopes, (c) deuteranopes, and (d) tritanopes. (Photograph by Bruce
Goldstein; color processing courtesy of Jay Neitz and John Carroll)

570

Figure 9.21 How the visible spectrum appears to (a) trichromats, (b)
protanopes, (c) deuteranopes, and (d) tritanopes. The number indicates
the wavelength of the neutral point. (Spectra courtesy of Jay Neitz and
John Carroll)

9.4 The Trichromacy of Color Vision

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

209

A tritanope is missing the short-wavelength pigment. A tritanope sees
colors as in Figure 9.20d and sees the spectrum as in Figure
9.21d---blue at short wavelengths, red at long wavelengths, and a
neutral point at 570 nm (Alpern et al., 1983). In addition to
monochromatism and dichromatism, there is one other prominent type of
color deficiency called anomalous trichromatism. An anomalous trichromat
needs three wavelengths to match any wavelength, just as a normal
trichromat does. However, the anomalous trichromat mixes these
wavelengths in different proportions from a trichromat, and an anomalous
trichromat is not as good as a trichromat at discriminating between
wavelengths that are close together. The story we have been telling
about the connection between the cone receptors and color vision has
taken place exclusively in the receptors in the retina. But there's more
to color vision than what's happening in the receptors because signals
from the receptors travel through the retina and out the back of the eye
to the lateral geniculate nucleus, then to the visual cortex, and
finally to other areas of the cortex. One result of this further
processing was noted by Ewald Hering (1834--1918) long before
researchers began to understand the nature of this processing. Hering's
insight was his description of the opponency of color vision. TEST
YOuRSELF 9.2

9.5 The Opponency of Color Vision What does opponency mean? For color
vision, it means that there are pairs of colors that have opponent, or
opposite, responses. Hering's theory, called the opponent-process theory
of color vision, stated that there are two pairs of chromatic colors,
red--green and blue--yellow (Hering, 1878, 1964). He picked these pairs
of colors based on phenomenological observations---observations in which
observers described the colors they were experiencing.

Behavioral Evidence for OpponentProcess Theory There are two types of
behavioral evidence for opponentprocess theory: phenomenological and
psychophysical.

Phenomenological Evidence Phenomenological evi­ dence, which is based on
color experience, was central to Hering's proposal of opponent-process
theory. His ideas about opponent colors were based on people's color
experiences when looking at a color circle like the one in Figure 9.22.
A color circle arranges perceptually similar colors next to each other
around its perimeter just like the color solid depicted in Figure 9.11.
Another property of the color circle is that

1.  What did Thomas Young say was wrong with Newton's idea that color is
    created by vibrations?
2.  How did Young explain color vision? Why is his explanation called
    the Young-Helmholtz theory?
3.  Describe Maxwell's color matching experiments. How did the results
    support the trichromacy of vision?
4.  What is the connection between trichromacy and the cone receptors
    and pigments?
5.  What is metamerism? How is it related to the results of color
    matching experiments?
6.  What is monochromacy? How does a monochromat match lights in a color
    matching experiment? Does a monochromat perceive chromatic color?
7.  What is the principle of univariance? How does the principle of
    univariance explain the fact that a monochromat can match any
    wavelength in the spectrum by adjusting the intensity of any other
    wavelength?
8.  Describe how pigment absorption spectra can explain how wavelength
    can be determined if there are only two receptor types.
9.  How would color matching results differ for a person with two types
    of cone receptors, compared to three?
10. What is dichromacy? What procedure was used to determine how a
    dichromat's color vision compared to a trichromat's?
11. What are the three types of dichromacy?

210

Figure 9.22 The color circle described by Hering. Colors on the left
appear blueish, colors on the right appear yellowish, colors on the top
appear reddish, and colors on the bottom appear greenish. Lines connect
opponent colors.

Chapter 9  Perceiving Color

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Psychophysical Evidence The idea of opponency was given a boost in the
1950s by Leo Hurvich and Dorthea Jameson's (1957) hue cancellation
experiments. The purpose of the hue cancelation experiments was to
provide quantitative measurements of the strengths of the B--Y and R--G
components of the opponent mechanisms. Let's consider how they used hue
cancellation to determine the strength of the blue mechanism.

METHOD

Hue Cancellation

We begin with a 430-nm light, which appears blue. Leo Hurvich and
Dorthea Jameson (1957) reasoned that since yellow is the opposite of
blue and therefore cancels it, they could determine the amount of
blueness in a 430-nm light by determining how much yellow needs to be
added to cancel all perception of "blueness." The blue dot in Figure
9.23 indicates the amount of yellow that was added to 430-nm light to
cancel all "blueness." Once this is determined for the 430-nm light, the
measurement is repeated for 440 nm and so on, across the spectrum, until
reaching the wavelength where there is no blueness, indicated by the
circle.

Strength of red or yellow mechanisms

Hue cancelation was then used to determine the strength of the yellow
mechanism by determining how much blue needs to be added to cancel
yellowness at each wavelength. For red and green, the strength of the
red mechanism is determined by measuring how much green needs to be
added to cancel the perception of redness, and the strength of the green
mechanism, by measuring how much red needs to be added to cancel the
perception of greenness.

R Y

R

Strength of blue or green mechanisms

colors across from each other are complementary colors--- colors which
when combined cancel each other to create white or gray. The difference
between a color circle and a color solid is simply that the color circle
focuses only on hue, without considering variations in saturation or
value. Hering identified four primary colors---red, yellow, green, and
blue---and proposed that each of the other colors are made up of
combinations of these primary colors. This was demonstrated using a
procedure called hue scaling, in which participants were given colors
from around the hue circle and told to indicate the proportions of red,
yellow, blue, and green that they perceived in each color. One result
was that each of the primaries was "pure." For example, there is no
yellow, blue, or green in the red. The other result was that each of the
intermediate colors like purple or orange were judged to contain
mixtures of two or more of the primaries. Results such as these led
Hering to call the primary colors unique hues. Hering proposed that our
color experience is built from the four primary chromatic colors
arranged into two opponent pairs: yellow--blue and red--green. To these
chromatic colors, Hering also considered black and white to be an
opponent achromatic pair. Ingenious as Hering's opponent-mechanism
proposal was, the theory wasn't widely accepted, for three reasons: (1)
its main competition, trichromatic theory, was championed by Helmholtz,
who had great prestige in the scientific community; (2) Hering's
phenomenological evidence, which was based on describing the appearance
of colors, could not compete with Maxwell's quantitative color mixing
data; and (3) there was no neural mechanism known at that time that
could respond in opposite ways.

B 400

G 500 600 Wavelength (nm)

700

Figure 9.23 Results of Hurvich and Jameson's (1957) hue cancellation
experiments. For the blue--yellow determinations, the blue curve is
inverted to symbolize that blue is opponent to yellow, and for the
red--green determinations, the green curve is inverted because green is
opponent to red.

In Figure 9.23 the blue and green curves have been inverted to emphasize
the fact that blue (plotted as negative in the figure) opposes yellow
(plotted as positive) and that green (negative) opposes red (positive).
These curves could just as well be reversed, with blue and green
positive and red and yellow negative. Hurvich and Jameson's hue
cancellation experiments were an important step toward acceptance of
opponent-process theory because they went beyond Hering's
phenomenological observations by providing quantitative measurements of
the strengths of the opponent mechanisms.

Physiological Evidence for OpponentProcess Theory Even more crucial for
the acceptance of opponent-process theory was the discovery of opponent
neurons that responded with an excitatory response to light from one
part of the spectrum and with an inhibitory response to light from
another part (DeValois, 1960; Svaetichin, 1956). 9.5 The Opponency of
Color Vision

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

211

−L +M

+L −M

(a) 

+M −L

+M −L

+L −M

(b) 
(c) 

Figure 9.24 Receptive fields of (a) a circular single-opponent cortical
neuron. This +M --L neuron has a center-surround receptive field. Its
firing increases when a medium-wavelength light is presented to the
center area of the receptive field and decreases when a long-wavelength
light is presented to the surrounding area. (b) A circular
double-opponent neuron. Firing increases to medium-wavelength light
presented to the center, and to long-wavelength light presented to the
surround. Firing decreases to long-wavelength light presented to the
center and medium-wavelength light presented to the surround. (c) A
side-by-side doubleopponent cortical neuron. This neuron increases
firing when a vertical medium-wavelength bar is presented to the left
side and when a vertical long-wavelength bar is presented to the right
side and decreases firing when a vertical long-wavelength bar is
presented to the left side and when a vertical medium-wavelength bar is
presented to the right side. (From Conway et al., 2010)

In an early paper that reported opponent neurons in the lateral
geniculate nucleus of the monkey, Russell DeValois (1960) recorded from
neurons that responded with an excitatory response to light from one
part of the spectrum and with an inhibitory response to light from
another part (also see Svaetichin, 1956). Later work identified opponent
cells with different receptive field layouts. Figure 9.24 shows three
receptive field layouts: (a) circular single opponent, (b) circular
double opponent, and (c) side-by-side single opponent (Conway et al.,
2010). The discovery of opponent neurons provided physiological evidence
for the opponency of color vision. The circuits in Figure 9.25 show how
the opponent neurons can be created by inputs from the three cones. In
Figure 9.25a, the L-cone sends excitatory input to a bipolar cell (see
Chapter 3, page 51), whereas the M-cone sends inhibitory input to the
cell. This creates an +L --M cell that responds with excitation to the
long wavelengths that cause the L-cone to fire and with inhibition to
the medium wavelengths that cause the M-cone to fire. Figure 9.25b shows
how excitatory input from the M-cone and inhibitory input from the
L-cone create an +M --L cell. Figure 9.25c shows that the +S --ML cell
also receives inputs from the cones. It receives an excitatory input
from the S cone and an inhibitory input from cell A, which sums the
inputs from the M and L cones. This arrangement makes sense if we
remember that we perceive yellow when both the M and the L receptors are
stimulated. Thus, cell A, which receives inputs from both of these
receptors, causes the "yellow" response of the +S --ML mechanism. Figure
9.25d shows the connections among neurons forming the +ML --S (or +Y
--B) cell. Opponent responding has also been observed in a number of
cortical areas, including the visual receiving area (V1) (Gegenfurtner &
Kiper, 2003; Nunez et al., 2018). 212

M

M

L

(--)

L

(+)

(+)

(--)

+M --L Response

+L --M Response (a)

(b) 

S

M

(+) (--)

+S --ML Response (c)

S

L

A (+)

M

(--) (+)

L

A (+)

+ML --S Response (d)

Figure 9.25 Neural circuits showing how (a) +L --M, (b) +M --L, (c) +S
--ML, and (d) +ML --S mechanisms can be created by excitatory and
inhibitory inputs from the three types of cone receptors.

Recent research has concluded that single-opponent cells like the ones
in Figures 9.24a and 9.24c respond to large areas of color and double
opponent cells like the one in Figure 9.24b respond to color patterns
and borders (Nunez et al., 2018).

Chapter 9  Perceiving Color

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Questioning the Idea of Unique Hues The results of the hue-cancellation
experiments and the discovery of opponent neurons were taken by
researchers in the 1950s and 1960s as supporting Hering's
opponent-process theory of vision. The fact that there are neurons that
respond in opposite ways to different parts of the spectrum does support
Hering's idea of opponency. However, remember that Hering also proposed
that blue, yellow, red, and green are unique hues. The proposed
specialness of unique hues led researchers who first recorded from
opponent neurons to give them names like +B --Y and +R --G that
corresponded to the unique hues. The implication of these labels is that
these neurons are responsible for our perception of these hues. This
idea has, however, been questioned by recent research. One argument
against the idea that there is a direct connection between the firing of
opponent neurons and perceiving primary or unique hues is that the
wavelengths that cause maximum excitation and inhibition don't match the
wavelengths associated with the unique hues (Skelton et al., 2017). And
returning to the hue scaling experiments we described earlier, recent
research has repeated these experiments, using different
primaries---orange, lime, purple, and teal---and obtained results
similar to what occurred with red, green, blue, and yellow. That is,
orange, lime, purple, and teal were rated as if they were "pure," so,
for example, orange was rated as not containing any lime, purple, or
teal (Bosten & Boehm, 2014). What does this all mean? Opponent neurons
are certainly important for color perception, because opponent
responding is how color is represented in the cortex. But perhaps, as
some researchers believe, the idea of unique hues may not be helping us
figure out how neural responding results in specific colors. Apparently,
it is not as simple as +M --L equals +G --R, which is directly related
to perceiving green and red (Ocelak, 2015; Witzel et al., 2019). If
responses of +M --L neurons can't be linked to the perception of green
and red, what is the function of these neurons? One idea is that
opponent neurons indicate the difference in responding of pairs of
cones. We can understand how this works at a neural level by looking at
Figure 9.26, which shows how a +L --M neuron receiving excitation from
the L-cone and inhibition from the M-cone responds to 500-nm and 600-nm
lights. Figure 9.26a shows that the 500-nm light results in an
inhibitory signal of --80 and an excitatory signal of +50, so the
response of the +L --M neuron would be --30. Figure 9.26b shows that the
600-nm light results in an inhibitory signal of --25 and an excitatory
signal of +75, so the response of the +L --M neuron would be +50. This
"difference information" could be important in dealing with the large
overlap in the spectra of the M and L cones. Neurons with side-by-side
receptive fields have also been used to provide evidence for a
connection between color and form. These neurons can fire to oriented
bars even when the intensity of the side-by-side bars is adjusted so
they appear equally bright. In other words, these cells fire when the
bar's form is determined only by differences in color. Evidence such as
this has been used to support the idea of a close bridge between the
processing of color and the processing of form in the cortex (Friedman
et al., 2003; Johnson et al., 2008). Thus, when

M−

L+

80

75

50 25

500

M

600

L

--80

M

+50

--25

--30 (a)

L

+75

+50 (b)

Figure 9.26 How opponent neurons determine the difference between the
receptor responses to different wavelengths. (a) The response of the +L
--M neuron to a 500-nm light is negative because the M receptor results
in an inhibitory response that is larger than receptor L's excitatory
response. This means the action of the 500-nm light on this neuron will
cause a decrease in any ongoing activity. (b) The response to a 600-nm
light is positive, so this wavelength causes an increase in the response
of this neuron.

you look out at a colorful scene, the colors you see are not only
"filling in" the objects and areas in the scene but may also be helping
define the edges and shapes of these objects and areas.

9.6 Color Areas in the Cortex What are the cortical mechanisms of color
perception? Is there one area in the cortex specialized for processing
information about color? If there is such an area, that would make color
similar to faces, bodies, and places, which can claim the fusiform face
area (FFA), extrastriate body area (EBA), and parahippocampal place area
(PPA) as specialized processing areas (see Chapter 5, page 110). The
idea of an area specialized for color was popularized by Semir Zeki
(1983a, 1983b, 1990) based on his finding that many neurons in a visual
area called V4 respond to color. However, additional evidence has led
many researchers to reject the idea of a "color center" in favor of the
idea that color processing is distributed across a number of cortical
areas. The finding that there are a number of color-processing areas
becomes even more interesting when the location of these areas is
compared to areas associated with processing faces and places. 9.6 Color
Areas in the Cortex

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

213

Figure 9.27 Images from 3-second video clips that were presented to
participants in Lafer-Sousa and coworkers' (2016) experiment. The
participants' brains were scanned as they watched the films.
(Lafer-Sousa et al., 2016)

Rosa Lafer-Sousa and coworkers (2016) scanned participants' brains while
they watched 3-second video clips that contained images like the ones
shown in Figure 9.27. Figure 9.28a shows data from one hemisphere of a
single individual. Notice that the color areas are sandwiched between
areas that responded to faces and places. Figure 9.28b shows this
"sandwiching" effect in a different view of the brain, which combines
the results from a number of participants. Faces, color, and places are
associated with different areas that are located next to each other. The
independence of shape and color is indicated by some cases of brain
damage. Remember patient D.F. from Chapter 4, who could mail a card but
couldn't orient the card or identify objects (Figure 4.26). She was
described to illustrate a dissociation between action and object
perception. But despite her difficulty in identifying objects, her color
perception was unimpaired. Another patient, however, had the opposite
problem with impaired color perception but normal form perception
(Bouvier & Engle, 2006). This double dissociation means that color and
form are processed independently (see Method: Double Dissociations in
Neuropsychology, page 81). But even though mechanisms for color, faces,
and places are independent, the areas for faces and places are
neighbors. That adjacency is likely what is behind the fact that 72
percent of patients with achromatopsia (color blindness), like Mr. I
Figure 9.28 (a) Cortical areas that responded best to color (red and
blue areas), faces (blue outline) and places (green outline) in one
hemisphere of an individual (b) Areas for color (red), faces (blue),
places (light green), and faces and places (dark green) determined from
group data. (Lafer-Sousa et al., 2016)

Color

Faces Individual

who we described at the beginning of the chapter, also have
prosopagnosia---problems recognizing faces. So color processing in the
cortex is both separate from other functions and closely related to them
at the same time. This relationship may be behind the fact that color
can play a role in perceptual organization (see page 97), attention (see
page 130), and motion perception (Ramachandran, 1987). We are left with
lots of data showing how neurons respond to different wavelengths and
how color is associated with numerous areas in the cortex, but we still
don't know how signals from the three types of cones are transformed to
cause our perception of color (Conway, 2009). TEST YOuRSELF 9.3 1. What
did Hering's opponent-process theory propose? 2. What was Hering's
phenomenological evidence for opponent-process theory? 3. Why wasn't
Hering's theory widely accepted? 4. Describe Hurvich and Jameson's hue
cancellation experiments. How was the result used to support
opponentprocess theory? 5. What is the physiological evidence for
opponency? 6. What are unique hues? 7. Describe the modern hue scaling
experiments that used colors other than red, green, blue, and yellow as
the "primaries." What are the implications of these results? 8. Has it
been possible to establish a connection between the firing of opponent
neurons and our perception of specific colors? 9. What functions have
been suggested for opponent neurons, in addition to their role in color
perception? 10. Where is color represented in the cortex? How are the
color areas related to areas for face and place processing?

Places Group

faces color places face & place

214

Chapter 9  Perceiving Color

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

9.7 Color in the World: Beyond Wavelength Throughout a normal day, we
view objects under many different lighting conditions: morning sunlight,
afternoon sunlight, indoors under incandescent light, indoors under
fluorescent light, and so forth. So far in this chapter, we have only
linked our perception of color to the light that is reflected from
objects. What happens when the light shining on an object changes? In
this section, we will think about the relationship between color
perception and the light that is available in the environment.

Color Constancy It is midday, with the sun high in the sky, and as you
are walking to class you notice a classmate who is wearing a green
sweater. Then, as you are sitting in class a few minutes later, you
again notice the same green sweater. The fact that the sweater appears
green both outdoors under sunlight illumination and indoors under
artificial illumination may not seem particularly remarkable. After all,
the sweater is green, isn't it? However, when we consider the
interaction between illumination and the properties of the sweater, we
can appreciate that your perception of the sweater as green, both
outside and inside, represents a remarkable achievement of the visual
system. This achievement is called color constancy---we perceive the
colors of objects as being relatively constant even under changing
illumination. We can appreciate why color constancy is an impressive
achievement by considering the interaction between illumination, such as
sunlight or lightbulbs, and the reflection properties of an object, such
as the green sweater. First, let's consider the illumination. Figure
9.29a shows the wavelengths of sunlight and the wavelengths emitted from

400 (a)

Reflectance

3 Sunlight LED 500 600 Wavelength (nm)

700

Reflected light

Reflectance curve

75

5

50 25 0 400 (b)

500 600 Wavelength (nm)

700

Relative intensity of reflected light

100

Incandescent (tungsten filament)

\% Light reflected from sweater

Relative amount of light

Illumination

incandescent (the "old style" tungsten bulbs that are being phased out)
and newer light-emitting diode (LED) lightbulbs. Sunlight contains
approximately equal amounts of energy at all wavelengths, which is a
characteristic of white light. The incandescent bulb, however, contains
much more energy at long wavelengths (which is why they look slightly
yellow), whereas LED bulbs emit light at substantially shorter
wavelengths (which is why they look slightly blue). Now consider the
interaction between the wavelengths produced by the illumination and the
wavelengths reflected from the green sweater. The reflectance curve of
the sweater is indicated in Figure 9.29b. It reflects mostly
mediumwavelength light, as we would expect of something that is green.
The actual light that is reflected from the sweater depends both on its
reflectance curve and on the illumination that reaches the sweater and
is then reflected from it. To determine the wavelengths that are
actually reflected from the sweater, we multiply the sweater's
reflectance curve at each wavelength by the amount of illumination at
each wavelength. The result of this calculation is shown in Figure
9.29c, which shows that light reflected from the sweater includes
relatively more long-wavelength light when it is illuminated by
incandescent light (the orange line in Figure 9.29c) than when it is
illuminated by light from an LED bulb (the blue line in Figure 9.29c).
The fact that we still see the sweater as green even though the
wavelength composition of the reflected light differs under different
illuminations is color constancy. Without color constancy, the color we
see would depend on how the sweater was being illuminated (Delahunt &
Brainard, 2004; Olkkonen et al., 2010). Why does a green sweater look
green even when viewed under different illuminations? The answer to this
question involves a number of different mechanisms (Smithson, 2005). We
begin by considering how the eye's sensitivity is affected by the color
of the illumination of the overall scene, a process called chromatic
adaptation.

400

500 Wavelength (nm)

600

(c) 

Figure 9.29 Determining what wavelengths are reflected from the green
sweater under different illuminations. Light reflected from the sweater
is determined by multiplying (a) the illumination of sunlight,
incandescent, and LED lightbulbs times (b) the sweater's reflectance.
The result is (c) the light reflected from the sweater. The maximum of
each of the curves in (c) has been set at the same level to make the
wavelength distributions easier to compare. 9.7 Color in the World:
Beyond Wavelength

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

215

Chromatic Adaptation The following demonstration highlights one factor
that contributes to color constancy. DEMONSTRATION

Adapting to Red

Illuminate Figure 9.30 with a bright light from your desk lamp; then,
with your left eye near the page and your right eye closed, look at the
field with your left eye for about 30 to 45 seconds. Then look at
various colored objects in your environment, blinking back and forth
between your left eye and your right eye.

Figure 9.30 Red adapting field for "Adapting to Red" demonstration.

You may have noticed that adapting your left eye to the red decreased
the redness of objects in the environment. This is an example of how
color perception can be changed by chromatic adaptation---prolonged
exposure to chromatic color. Adaptation to the red light selectively
reduced the sensitivity of your long-wavelength cones, which decreased
your sensitivity to red light and caused you to see the reds and oranges
viewed with your left (adapted) eye as less saturated and bright than
those viewed with the right eye. The idea that chromatic adaptation is
responsible for color constancy has been tested in an experiment by
Keiji Uchikawa and coworkers (1989). Observers viewed isolated patches
of colored paper under three different conditions (Figure 9.31):

(a) baseline---paper and observer illuminated by white light;
(b) observer not adapted---paper illuminated by red light, observer by
    white (the observer is not chromatically adapted); and
(c) observer adapted to red---both paper and observer illuminated by red
    light (the observer is chromatically adapted). The results from
    these three conditions are shown above each condition. In the
    baseline condition, a green paper is perceived as green. In the
    observer not adapted condition, the observer perceives the paper's
    color as being shifted toward the red. Color constancy does not
    occur in this condition because the observer is not adapted to the
    red light that is illuminating the paper. But in the observer
    adapted to red condition, perception is shifted only slightly to the
    red, so it appears more yellowish. Thus, the chromatic adaptation
    has created partial color constancy---the perception of the object
    is shifted after adaptation, but not as much as when there was no
    adaptation. This means that the eye can adjust its sensitivity to
    different wavelengths to keep color perception approximately
    constant as illumination changes. This principle operates when you
    walk into a room illuminated with yellowish tungsten light. The eye
    adapts to the long-wavelength--rich light, which decreases your
    eye's sensitivity to long wavelengths. This decreased sensitivity
    causes the long-wavelength light reflected from objects to have less
    effect than before adaptation, and this compensates for the greater
    amount of long-wavelength tungsten light that is reflected from
    everything in the room. Because of this adaptation, the yellowish
    tungsten illumination has only a small effect on your perception of
    color. A similar effect also occurs in environmental scenes, which
    can have different dominant colors in different seasons. For
    example, the same scene can be "lush" in summer, with a lot of green
    (Figure 9.32a) and "arid" in winter, with more yellows (Figure
    9.32b). Based on calculations taking into account how this
    "greenness" and "yellowness" would affect the cone receptors,
    Michael Webster (2011) determined that adaptation to the green in
    the lush scene would decrease the perception of green in that scene
    (Figure 9.32c), and adaptation to the yellow of the arid scene would
    decrease the perception of yellow in the arid scene (Figure 9.32d).
    Thus, adaptation "tones down"

Perception: Paper is green

Perception: Paper shifted toward red

Perception: Paper shifted only slightly toward red so it appears more
yellowish

(a) Baseline

(b) Observer not adapted

(c) Observer adapted to red

Figure 9.31 The three conditions in Uchikawa and coworkers' (1989)
experiment. See text for details. 216

Chapter 9  Perceiving Color

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Lush environment

a banana that was physically the same as the gray background appeared
slightly yellowish, and an orange looked slightly orange. This led
Hansen to conclude that the observer's knowledge of the fruit's
characteristic colors actually changed the colors they were
experiencing. The effect of memory on our experience of color is a small
one, but nonetheless may make a contribution to our ability to
accurately perceive the colors of familiar objects under different
illuminations.

Arid environment

the dominant colors in a scene, so if we compare the perceived color of
the lush and arid scenes in (c) and (d), we see that the colors are more
similar than before the chromatic adaptation. This adaptation also
causes novel colors to stand out, so yellow becomes more obvious in the
lush scene and the green stands out in the arid scene.

Taking Illumination Into Account Figure 9.33 shows two pictures of the
same house taken under different illuminations at different times of
day. The camera's colorcorrection mechanism has been turned off, so
changes in illumination cause differences in color between the two
pictures. The person who took these pictures reports, however, that the
side of the house looked yellow both times (Brainard et al., 2006).
While the camera disregarded the change in illumination, the human
observer's visual system took it into account. One "taking into account"
mechanism used by the visual system is chromatic adaptation, which we
discussed earlier (Gupta et al., 2020). But there are other mechanisms
at work as well. A number of researchers have shown that color constancy
works best when an object is surrounded by objects of many different
colors, a situation that often occurs when viewing objects in the
environment (Foster, 2011; Land, 1983, 1986; Land & McCann, 1971). It
has also been shown that under some conditions, color constancy is
better when objects are viewed with two eyes (which results in better
depth perception) compared to one (Yang & Shevell, 2002), and that
constancy is better when an object is observed in a three-dimensional
scene, compared to when the observer looks at the scene through a
kaleidoscope that scrambles the surrounding scene (Mizokami & Yagochi,
2014). Apparently, the surroundings and viewing conditions help us
achieve color constancy because the visual system---in ways that are
still not completely understood---uses the information in a scene to
estimate the characteristics of

Familiar Color Another thing that helps achieve color constancy is our
knowledge about the usual colors of objects in the environment. This
effect on perception of prior knowledge of the typical colors of objects
is called memory color. Research has shown that because people know the
colors of familiar objects, like a red stop sign or a green tree, they
judge these familiar objects as having richer, more saturated colors
than unfamiliar objects that reflect the same wavelengths (Ratner &
McCarthy, 1990). Thorsten Hansen and coworkers (2006) demonstrated an
effect of memory color by presenting observers with pictures of fruits
with characteristic colors, such as lemons, oranges, and bananas,
against a gray background. Observers also viewed a spot of light against
the same gray background. When the intensity and wavelength of the spot
of light were adjusted so the spot was physically the same as the
background, observers reported that the spot appeared the same gray as
the background. But when the intensity and wavelength of the fruits were
set to be physically the same as the background, observers reported that
the fruits appeared slightly colored. For example,

Figure 9.33 Photographs of a house taken at different times of day under
different lighting conditions. Because the camera's color-correction
mechanism is turned off, the change in illumination caused a change in
the color of the siding from yellow to green. The photographer, however,
reports that the siding looked yellow at both times of day. (Brainard et
al., 2006)

(a) 
(b) 

Bruce Goldstein

(b) 
(c) After adapting to lush scenes

After adapting to arid scenes

Figure 9.32 How chromatic adaptation to the dominant colors of the
environment can influence perception of the colors of a scene. The
dominant color of the scene in (a) is green. Looking at this scene
causes adaptation to green and decreases the perception of green in the
scene, as shown in (c). The dominant color of the arid scene in (b) is
yellow. Adapting to this scene causes a decreased perception of yellow
in the scene, as shown in (d). (Webster, 2011)

9.7 Color in the World: Beyond Wavelength

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

217

the illumination and to make appropriate corrections (Brainard, 1998,
2006; Mizokami, 2019; Smithson, 2005). The bottom line regarding color
constancy is that even though it has been studied in hundreds of
experiments, we still don't totally understand how the visual system
takes the illumination into account. In the next section, we will
consider another phenomenon that is related to color constancy and has
proven difficult to explain.

Sarah Lee/eyevine/Redux

#TheDress On February 26, 2015, a photograph of a striped dress similar
to the one in Figure 9.34 was posted online as #TheDress. What colors do
you see in this photograph? The actual dress had alternating blue and
black stripes (Figure 9.35). Many people saw it this way. But many other
people saw the dress as alternating white and gold stripes, with a
smaller group seeing black and white or other perceptions. The posting
created a sensation. Why did people report seeing different colors when
looking at the same picture? Vision scientists quickly stepped into the
discussion. The first step was to survey large groups of people to
confirm the phenomenon. One survey reported that 57 percent of the
people saw blue and black and 30 percent saw white and gold, with the
other 13 percent perceiving other colors (Lafer-Sousa et al., 2015).
Another survey reported quite different numbers:

Figure 9.35 The Dress being held by Cecilia Bleasdale, who posted the
picture on the Internet. This picture shows the black and blue stripes
that are perceived when the dress is viewed "in person."

Figure 9.34 An illustration of a striped dress similar to the one that
has been perceived differently by different people. To see the original
picture look up "The Dress" on Wikipedia. 218

27 percent blue and black and 59 percent white and gold (Wallisch,
2017). But whatever the numbers, there was no question that blue--black
and white--gold were the two predominant perceptions. Given all the
research that had been done in color vision before 2015, it would seem
that vision researchers should be able to provide an explanation for
these differing perceptions. However, that has not been the case. In the
years following #TheDress, over a dozen papers appeared in scientific
journals discussing how different people's perceptions could be
influenced by differences between people such as the following: the
wavelengths transmitted through the eye's optical system to the retina,
the ratio of L to M cones, higher-order processing, people's language,
how the picture is displayed on different devices, and interpretation of
how the dress is illuminated. Although there is some evidence that
differences in light transmission by the cornea and lens could make a
small contribution to the effect (Rabin et al., 2016), the main
explanations have suggested that differences in how people interpreted
the illumination was responsible for the effect. And these explanations
were based on the phenomenon of color constancy. Here's how the color
constancy explanation works: We've seen that perception of an object's
color tends to remain

Chapter 9  Perceiving Color

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Original

Replication

70

65

Percent reporting white/gold

Percent reporting white/gold

70

60

55

50

45

65

60

55

50

45 Strong owl

Owl

Lark

Strong lark

Strong owl

Owl

Lark

Strong lark

Figure 9.36 Percent of observers reporting the white--gold perception of
The Dress photograph, as a function of being an "owl" (stays up late at
night) or a "lark" (gets up early, goes to bed early). The two sets of
data were obtained a year apart. Being a lark increases the chances of
experiencing the white--gold perception. (Wallisch, 2017)

relatively constant even when the object is seen under different
ºilluminations. This occurs because the visual system takes the
illumination into account and essentially "corrects" for changes in
illumination so that the object's perception is based on the reflectance
properties of the object's surface. First, the actual dress is black and
blue, as shown in Figure 9.35. So what would happen to perception of a
black and blue dress if the illumination were rich in long wavelengths,
like the "yellowish" light of old-style incandescent bulbs? The
constancy mechanism will cause the visual system to decrease the effect
of long wavelengths, so the blue will stay about the same, since blue
objects usually reflect little long-wavelength light, and the black will
get darker and perhaps a little bluer. But what if a black and blue
dress is assumed to be illuminated by "cooler" light, similar to
daylight, which contains more short-wavelength "bluish" light?
Discounting short wavelengths from the blue stripes would cause the blue
stripes to be perceived as white and discounting short wavelengths from
the black stripes would push perception of the black stripes toward
yellow. If the reason for "black becoming yellow" isn't obvious,
remember that a black object reflects a small amount of all wavelengths
equally. Subtracting short wavelengths leaves the middle and long
wavelengths associated with yellow light. Some evidence consistent with
this idea has been provided by Pascal Wallisch (2017), who reported the
results of an online survey of over 13,000 people. The participants
reported how they perceived the dress and also whether they classified
themselves as "larks" (they get up early and go to bed early) or "owls"
(they go to bed late and get up late). An important difference between
larks and owls is that larks get more natural light, which contains more
short wavelengths than owls, who stay up and are exposed to yellowish
artificial incandescent light, which has a high content of long
wavelengths. Figure 9.36 shows the results of two surveys, taken a year
apart, which shows that larks are more likely to see the dress as

white--gold than owls. This result suggests that people's prior
experience with illumination may affect the assumptions they are making
about how the dress is being illuminated, with this assumption, in turn,
affecting their perception of the colors of the dress. However, it is
important to note that knowing someone is a lark or an owl doesn't do a
very good job of predicting how they will see the dress. After all, in
the second survey about 35 percent of larks saw the dress as blue--black
and 47 percent of the owls saw it as white--gold. Results such as those
of Wallisch plus many other considerations have caused many vision
researchers to suggest that the "corrective" mechanism of color
constancy is the most likely explanation for The Dress, while
acknowledging that we still don't totally understand the phenomenon. One
thing is for sure: The Dress confirms something we knew already---our
perception of color isn't determined solely by the wavelengths of light
entering our eyes. Other things, including assumptions about the
illumination, are at work. The other thing the dress shows is that we
still have a lot to learn about how color vision works. As color vision
researchers David Brainard and Anya Hulbert (2015) stated in a paper
published four months after the dress appeared, "A full understanding of
the individual differences in how the dress is perceived will ultimately
require data that relate on a person-by-person basis, the perception of
the dress to a full set of individual difference measurements of colour
vision." And Michael Webster (2018), in a comment published three years
after the dress appeared, noted that "occasionally we are reminded of
how little we know. ... The dress image ... made it obvious that our
understanding of color is not at a point where explanations could come
easily. In fact, very many aspects of color vision remain a mystery and
the subject of intense activity, and new findings are constantly
emerging that are challenging some of the most basic assumptions about
color or are expanding the field in new directions." 9.7 Color in the
World: Beyond Wavelength

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

219

Interestingly, a similar phenomenon has recently been reported for
speech, in which a sound recording created to indicate the correct
pronunciation of "Laurel," has been perceived in two different ways when
played back through a low-quality recorder. Some people hear "Laurel,"
but others hear "Yanny." Reacting to this result, Daniel Pressnitzer and
coworkers (2018) exclaimed, "At long last, the world had the auditory
equivalent of the visual sensation known as #TheDress." As we will
discuss in Chapter 14, "Speech Perception," the explanation for Laurel/
Yanny is different than the explanation for The Dress. However, one
thing they have in common is that they both show that two people can
perceive the same stimulus differently.

Lightness Constancy Just as we perceive chromatic colors like red and
green as remaining relatively constant even when the illumination
changes, we also perceive achromatic colors---white, gray, and
black---as remaining about the same when the illumination changes.
Imagine, for example, a black Labrador retriever lying on a living room
rug illuminated by a lightbulb. A small percentage of the light that
hits the retriever's coat is reflected, and we see it as black. But when
the retriever runs outside into the much brighter sunlight, its coat
still appears black. Even though more light is reflected in the
sunlight, the perception of the shade of achromatic color (white, gray,
and black) remains the same. The fact that we see whites, grays, and
blacks as staying about the same shade under different illuminations is
called lightness constancy. The visual system's problem is that the
intensity of light reaching the eye from an object depends on two
things: (1) the illumination---the total amount of light that is
striking the object's surface---and (2) the object's reflectance---the
proportion of this light that the object reflects into our eyes. When
lightness constancy occurs, our perception of lightness is determined
not by the intensity of the illumination hitting an object, but by the
object's reflectance. Objects that look black reflect less than 10
percent of the light. Objects that look gray reflect about 10 to 70
percent of the light (depending on the shade of gray); and objects that
look white, like the pages of a book, reflect 80 to 95 percent of the
light. Thus, our

Figure 9.37 A black-and-white checkerboard illuminated by (a) tungsten
light and (b) sunlight.

100 units

perception of an object's lightness is related not to the amount of
light that is reflected from the object, which can change depending on
the illumination, but to the percentage of light reflected from the
object, which remains the same no matter what the illumination. You can
appreciate the existence of lightness constancy by imagining a
checkerboard, like the one in Figure 9.37, illuminated by room light.
Let's assume that the white squares have a reflectance of 90 percent,
and the black squares have a reflectance of 9 percent. If the
illumination inside the room is 100 units, the white squares reflect 90
units and the black squares reflect 9 units (Figure 9.37a). Now, if we
take the checkerboard outside into bright sunlight, where the
illumination is 10,000 units, the white squares reflect 9,000 units of
light and the black squares reflect 900 units (Figure 9.37b). But even
though the black squares when outside reflect much more light than the
white squares did when the checkerboard was inside, the black squares
still look black. Your perception is determined by the reflectance, not
the amount of light reflected. What is responsible for lightness
constancy? There are a number of possible explanations.

The Ratio Principle One observation about our perception of lightness is
that when an object is illuminated evenly--- that is, when the
illumination is the same over the whole object, as in our checkerboard
example---then lightness is determined by the ratio of reflectance of
the object to the reflectance of surrounding objects. According to the
ratio principle, as long as this ratio remains the same, the perceived
lightness will remain the same (Jacobson & Gilchrist, 1988; Wallach,
1963). For example, consider one of the black squares in the
checkerboard. The ratio of a black square to the surrounding white
squares is 9/90 = 0.10 under low illuminations and 900/9,000 = 0.10
under high illuminations. Because the ratio of the reflectances is the
same, our perception of the lightness remains the same. The ratio
principle works well for flat, evenly illuminated objects like our
checkerboard. However, things get more complicated in three-dimensional
scenes, which are usually illuminated unevenly.

10,000 units

90 units 9 units

(a) 

220

9,000 units 900 units

(b) 

Chapter 9  Perceiving Color

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Lightness Perception Under Uneven Illumination If you look around, you
will probably notice that the illumination is not even over the entire
scene, as was the case for our two-dimensional checkerboard. The
illumination in three-dimensional scenes is usually uneven because of
shadows cast by one object onto another or because one part of an object
faces the light and another part faces away from the light. For example,
in Figure 9.38, in which a shadow is cast across a wall, we need to
determine whether the changes in appearance we see across the wall are
due to differences in the properties of different parts of the wall or
to differences in the way the wall is illuminated. The problem for the
perceptual system is that it has to somehow take the uneven illumination
into account. One way to state this problem is that the perceptual
system needs to distinguish between reflectance edges and illumination
edges. A reflectance edge is an edge where the reflectance of two
surfaces changes. The border between areas a and c in Figure 9.38 is a
reflectance edge because the two surfaces are made of different
materials that reflect different amounts of light. An illumination edge
is an edge where the lighting changes. The border between a and b is an
illumination edge because area a is receiving more light than area b,
which is in shadow. Some explanations for how the visual system
distinguishes between these two types of edges have been proposed (see
Adelson, 1999; Gilchrist, 1994; and Gilchrist et al., 1999, for
details). The basic idea behind these explanations is that the
perceptual system uses a number of sources of information to take
illumination into account.

the uneven illumination created by shadows into account. It must
determine that this change in illumination caused by a shadow is due to
an illumination edge and not to a reflectance edge. Obviously, the
visual system usually succeeds in doing this because although the light
intensity is reduced by shadows, you don't usually see shadowed areas as
gray or black. For example, in the case of the wall in Figure 9.39, the
shadowed area looks different than the sunny area, but you know it's a
shadow so you infer that the color of the bricks in the shadowed area is
actually the same as the color in the sunny area. In other words, you
are taking into account the fact that less light is falling on the
shadowed area. How does the visual system know that the change in
intensity caused by the shadow is an illumination edge and not a
reflectance edge? One thing the visual system may take into account is
the shadow's meaningful shape. In this particular example, we know that
the shadow was cast by a tree, so we know it is the illumination that is
changing, not the color of the bricks on the wall. Another clue is
provided by the nature of the shadow's contour, as illustrated by the
following demonstration.

The Information in Shadows In order for lightness constancy to work, the
visual system needs to be able to take

(a) 
(b) 

Figure 9.38 This unevenly illuminated wall contains both reflectance
edges (between a and c) and illumination edges (between a and b). The
perceptual system must distinguish between these two types of edges to
accurately perceive the actual properties of the wall, and other parts
of the scene as well.

Bruce Goldstein

Bruce Goldstein

(c) 

Figure 9.39 In this photo, you assume that the shadowed and unshadowed
areas are bricks with the same lightness but that less light falls on
some areas than on others because of the shadow cast by the tree. 9.7
Color in the World: Beyond Wavelength

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

221

DEMONSTRATION

Perception

The Penumbra and Lightness

Now create a hole in another card and, with the hole a few inches from
the corner of the folded card, view the corner with one eye about a foot
from the hole (Figure 9.41b). If, when viewing the corner through the
hole, you perceive the corner as a flat surface, your perception of the
left and right surfaces will change.

Place an object, such as a cup, on a white piece of paper on your desk.
Then illuminate the cup at an angle with your desk lamp and adjust the
lamp's position to produce a shadow with a slightly fuzzy border, as in
Figure 9.40a. (Generally, moving the lamp closer to the cup makes the
border get fuzzier.) The fuzzy border at the edge of the shadow is
called the shadow's penumbra. Now take a marker and draw a thick line,
as shown in Figure 9.40b, so you can no longer see the penumbra. What
happens to your perception of the shadowed area inside the black line?

Bruce Goldstein

(a) 
(b) 

Figure 9.41 Viewing a shaded corner. (a) Illuminate the card so one side
is illuminated and the other is in shadow. (b) View the card through a
small hole so the two sides of the corner are visible, as shown.

(b) 

Figure 9.40 (a) A cup and its shadow. (b) The same cup and shadow with
the penumbra covered by a black border.

Covering the penumbra causes most people to perceive a change in the
appearance of the shadowed area. Apparently, the penumbra provides
information to the visual system that the dark area next to the cup is a
shadow, so the edge between the shadow and the paper is an illumination
edge. However, masking off the penumbra eliminates that information, so
the area covered by the shadow is seen as a change in reflectance. In
this demonstration, lightness constancy occurs when the penumbra is
present but does not occur when it is masked.

The Orientation of Surfaces The following demonstration provides an
example of how information about the orientation of a surface affects
our perception of lightness.

DEMONSTRATION

Perceiving Lightness at a Corner

Stand a folded index card on end so that it resembles the outside corner
of a room, and illuminate it so that one side is illuminated and the
other is in shadow. When you look at the corner, you can easily tell
that both sides of the corner are made of the same white material but
that the nonilluminated side is shadowed (Figure 9.41a). In other words,
you perceive the edge between the illuminated and shadowed "walls" as an
illumination edge.

222

(b) 

In this demonstration, the illumination edge you perceived at first
became transformed into an erroneous perception of a reflectance edge,
so you saw the shadowed white paper as being gray paper. The erroneous
perception occurs because viewing the shaded corner through a small hole
eliminated information about the conditions of illumination and the
orientation of the corner. In order for lightness constancy to occur, it
is important that the visual system have adequate information about the
conditions of illumination. Without this information, lightness
constancy can break down and a shadow can be seen as a darkly pigmented
area. Figure 9.42a provides another example of a possible confusion
between perceiving an area as being "in shadow" or perceiving it as
being made of "dark material." This photograph of the statue of St. Mary
was taken at night in the Grotto of Our Lady of Lourdes at the
University of Notre Dame. As I (BG) observed the statue at night, it was
unclear to me whether the dark area above Mary's arms was colored blue,
like the sash, or whether it was simply in shadow. I suspected the
shadow explanation, but the almost perfect color match between that area
and the sash made me wonder whether the area about Mary's arms were, in
fact, blue. The statue is perched on a high ledge, so it wasn't easy to
tell, so I returned the next morning to see Mary in daylight. Figure
9.42b reveals that the dark area was, in fact, a shadow. Mystery solved!
As with color perception, sometimes we are fooled by conditions of
illumination or by ambiguous information, but most of the time we
perceive lightness accurately.

Chapter 9  Perceiving Color

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Bruce Goldstein

Figure 9.42 (a) A statue of St. Mary illuminated at night from below.
(b) The same statue during the day.

Something To Consider:

We Perceive Color From Colorless Wavelengths Our discussion so far has
been dominated by the idea that there is a connection between wavelength
and color. This idea is most strongly demonstrated by the visual
spectrum in which each wavelength is associated with a specific color
(Figure 9.43a). But this connection between wavelength and color can be
misleading, because it might lead you to believe that wavelengths are
colored---450-nm light is blue, 520-nm light is green, and so on. As it
turns out, however, wavelengths are completely colorless. This is
demonstrated by considering

400

500

what happens to our perception of color under dim illumination, as
happens at dusk. As illumination decreases, we dark adapt and our vision
shifts to the rods (pp. 46, 54). This causes hues such as blue, green,
and red to become less distinct and eventually disappear altogether,
until the spectrum, once lushly colored, becomes a series of different
shades of gray (Figure 9.43b). This effect of dark adaptation
illustrates that the nervous system constructs color from wavelengths
through the action of the cones. The idea that color is not a property
of wavelengths was asserted by Isaac Newton in his Optiks (1704): The
Rays to speak properly are not coloured. In them there is nothing else
than a certain Power and Disposition to stir up a Sensation of this or
that

600

700

600

700

Wavelength (nm) (a)

400

500 Wavelength (nm)

(b) 

Figure 9.43 (a) Visible spectrum in color. (b) Spectrum as perceived at
low intensities, when only the rod receptors are controlling vision.
Something to Consider: We Perceive Color From Colorless Wavelengths

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

223

Colour. ... So Colours in the Object are nothing but a Disposition to
reflect this or that sort of Rays more copiously than the rest. Newton's
idea is that the colors we see in response to different wavelengths are
not contained in the rays of light themselves, but that the rays "stir
up a sensation of this or that color." Stating this idea in modern-day
physiological terms, we would say that light rays are simply energy, so
there is nothing intrinsically "blue" about short wavelengths or "red"
about long wavelengths, and that we perceive color because of the way
our nervous system responds to this energy. We can appreciate the role
of the nervous system in creating color experience by considering not
only what happens when vision shifts from cone to rod receptors but also
the fact that people like Mr. I., the artist who lost his ability to see
color in a car accident, see no colors, even though they are receiving
the same stimuli as people with normal color vision. Also, many animals
perceive either no color or a greatly reduced palette of colors compared
to humans, and others sense a wider range of colors than humans,
depending on the nature of their visual systems. For example, Figure
9.44 shows the absorption spectra of a honeybee's visual pigments. The
pigment that absorbs short-wavelength light enables the honeybee to see
short wavelengths that can't be detected by humans (Menzel & Backhaus,
1989; Menzel et al., 1986). What "color" do you think bees perceive at
350 nm, which you can't see? You might be tempted to say "blue" because
humans see blue at the short-wavelength end of the spectrum, but you
really have no way of knowing what the honeybee is seeing, because, as
Newton stated, "The Rays ... are not coloured." There is no color in the
wavelengths; it is the bee's nervous system that creates the bee's
experience of color. For all we know, the honeybee's experience of color
at short wavelengths is quite different from ours, and may also be
different for wavelengths in the middle of the spectrum that humans and
honeybees can both see.

The idea that the nervous system is responsible for the quality of our
experience also holds for other senses. For example, we will see in
Chapter 11 that our experience of hearing is caused by pressure changes
in the air. But why do we perceive slow pressure changes as low pitches
(like the sound of a tuba) and rapid pressure changes as high pitches
(like a piccolo)? Is there anything intrinsically "high-pitched" about
rapid pressure changes (Figure 9.45a)? Or consider the sense of taste.
We perceive some substances as "bitter" and others as "sweet," but where
is the "bitterness" or "sweetness" in the molecular structure of the
substances that enter the mouth? Again, the answer is that these
perceptions are not in the molecular structures. They are created by the
action of the molecular structures on the nervous system (Figure 9.45b).
One of the themes of this book has been that our experience is created
by our nervous system, so the properties of the nervous system can
affect what we experience. We know, for example, that our ability to
detect dim lights and fine details is affected by the way the rod and
cone receptors converge onto other neurons in the retina (see Chapter 3,
pages 53, 54). The idea we have introduced here is that our perceptual
experience is not only shaped by the nervous system, as in the example
of rod and cone vision, but---in cases such as color vision, hearing,
taste, and smell---the very essence of our experience is created by the
nervous system.

Slow pressure changes (low pitch) Faster pressure changes (high pitch)
(a) Where are the high and low pitches?

HO 1.0

H3C

H N

Quinine molecule (bitter taste)

O

N Light absorbed

CH2OH H 0.5 HO

O H OH H

CH2OH O

H O

OH

H

H

HO

Sucrose molecule (sweet taste)

CH2OH OH

H

(b) Where are the bitter and sweet tastes?

0 300

400

500

600

700

Wavelength (nm)

Figure 9.44 Absorption spectra of honeybee visual pigments. 224

Figure 9.45 (a) Low and high pitches are associated with slow and fast
pressure waves, but pressure waves don't have "pitch." The pitch is
created by how the auditory system responds to the pressure waves. (b)
Molecules don't have taste. The nervous system creates different tastes
in response to the action of the molecules on the taste system.

Chapter 9  Perceiving Color

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

DEVELOPMENTAL DIMENSION Infant Color Vision We know that our perception
of color is determined by the action of three different types of cone
receptors (Figure 9.13). Because the cones are poorly developed at
birth, we can guess that the newborn would not have good color vision.
However, research has shown that color vision develops early and that
appreciable color vision is present within the first 3 to 4 months of
life. In a classic early experiment, Marc Bornstein and coworkers (1976)
assessed the color vision of 4-month-old infants by determining whether
they perceived the same color categories in the spectrum as adults.
People with normal trichromatic vision see the spectrum as a sequence of
color categories, starting with blue at the short-wavelength end,
followed by green, yellow, orange, and red, with fairly abrupt
transitions between one color and the next (see the spectrum in Figure
9.4b). Bornstein used the habituation procedure to determine whether
infants perceived color categories by presenting a 510-nm light---a
wavelength that appears green to an adult with normal color vision (see
Figure 9.44)---a number of times and measuring how long the infant
looked at it (Figure 9.46). The decrease in looking time (green dots)
indicates that habituation occurs as the infant becomes more familiar
with the color. The moment of truth in the habituation procedure is
based on the fact that infants like looking at novel stimuli. So
presenting a different light will catch the infant's attention if

the infant perceives it as different. This is what happens when a 480-nm
light is presented on trial 16. This light appears blue to an adult
observer and is therefore in a different category than the 510-nm light,
and the infants' increase in looking time, called dishabituation,
indicates that the infant also perceived the 480-nm light as different
than 510-nm light, and is therefore in a different category. However,
when this procedure is repeated, first habituating to a 510-nm light and
then presenting a 540-nm light (which is also perceived as green by
adults and so is in the same category), dishabituation does not occur,
indicating that the 540-nm light is in the same category as the 510-nm
light for the infants. From this result and the results of other
experiments, Bornstein concluded that 4-month-old infants categorize
colors the same way adult trichromats do. A more recent experiment used
another procedure, called the novelty-preference procedure, to study
infant color vision. Anna Franklin and Ian Davies (2004) had 4- to
6-month-old infants look at a display like the one in Figure 9.47a, in
which two side-by-side squares had the same color. In this
familiarization part of the experiment, the infants habituated---their
looking time to the colored areas decreased---as the stimulus was
repeatedly presented. In the novelty preference part of the experiment,
a new color was presented in one of the squares, as in Figure 9.47b, and
the infants' looking patterns were again measured. To determine whether
infants saw different colors across category boundaries, the infants
were shown two types of pairs in the novelty test. For the "within
pairs" condition, the new

510 nm Dishabituation Looking time

480 nm

Habituation

(a) Familiarization---looking direction random

(b) Novelty preference---looking is directed to the novel color

(c) Within pairs condition

(d) Between pairs condition

No dishabituation 540 nm

1−3

4−6

7−9 10−12 13−15 Test Trials

Figure 9.46 Results of the Bornstein et al. (1976) experiment. Looking
time decreases over the first 15 trials as the infant habituates to
repeated presentations of a 510-nm stimulus. Looking times for
presentation of 480-nm and 540-nm stimuli presented on trial 16 are
indicated by the dots on the right.

Figure 9.47 In Franklin and Davies' (2004) experiment, the looking times
of 4- to 6-month-old infants are measured during (a) the
"familiarization" part of the experiment in which the two squares are
identical, (b) the "novelty preference" part of the experiment, in which
one of the squares is changed, (c) the "within pairs" condition, in
which the two squares are in the same color category, and (d) the
"between pairs" condition, in which the squares are in different
categories.

Continued Something to Consider: We Perceive Color From Colorless
Wavelengths

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

225

color was within an adult category, as in Figure 9.47c, both of which
are in the "green" category. For the "between pairs" condition, the new
color was in a different adult category, as in Figure 9.47d, where the
new color is blue. Franklin and Davies found that when the colors in the
two squares were both within the same adult category, the infants'
looking times were equally distributed between the two squares. However,
if the colors were in different adult categories, infants looked at the
newly presented color about 70 percent of the time. Franklin and Davies
got the same result for other pairs of colors (red--pink and
blue--purple) and concluded that "4-month-old infants seem to have adult
perceptual color categories, at least to some degree." In another
experiment, which compared more colors in 6-month-old infants, Alice
Skelton and coworkers (2017) concluded that the infants distinguished
blue, green, purple, yellow, and red categories. What's particularly
significant about these results is that infants achieve this
categorization of color before they have

acquired language. This has led researchers to conclude that sorting
colors into categories depends not on higher-level processes like
language, but is determined by early mechanisms based on the cone
receptors and how they are wired-up (Maule and Franklin, 2019). As with
all research in which we are drawing conclusions about how things appear
to people, it is important to realize that research that indicates that
infants categorize colors in the same way as adults doesn't tell us how
those colors appear to the infants (Dannemiller, 2009). Just as it is
not possible to know whether two adults who call a light "red" are
having exactly the same experience, it is also not possible to know
exactly what the infants are experiencing when their looking behavior
indicates that they can tell the difference between two colors. In
addition, there is evidence that color vision continues to develop into
the teenage years (Teller, 1997). It is safe to say, however, that the
foundations of trichromatic vision are present at about 4 months of age.

TEST YOuRSELF 9.4 1. What is color constancy? What would our perceptual
world be like without color constancy? 2. Describe chromatic adaptation.
How is it demonstrated by Uchikawa's experiment? 3. How does color
constancy work when walking into a room illuminated by a tungsten light?
When the seasons change? 4. What is the evidence that memory can have a
small effect on color perception? 5. Describe the mechanisms that help
achieve color constancy by taking illumination into account. 6. What
does it mean to say that the surroundings help achieve color constancy?
7. What is #TheDress? What explanations have been proposed to explain
this phenomenon? 8. What is lightness constancy? Describe the roles of
illumination and reflectance in determining perceived lightness.

9.  How is the ratio principle related to lightness constancy?
10. Why is uneven illumination a problem for the visual system? What are
    the two types of edges that are associated with uneven illumination?
11. How is lightness perception affected by shadows? What cue for
    shadows occurs at the shadow's border?
12. Describe the "folded card" demonstration. What does it show about
    how lightness is affected by our perception of the orientation of
    surfaces?
13. What does it mean to say that color is created by the nervous
    system?
14. Describe the habituation procedure and the noveltypreference
    procedure for determining how infants categorize color. What
    conclusion was reached from these experiments? What do the results
    of these experiments tell us about what the infants are
    experiencing?

Think About It 1. A person with normal color vision is called a
trichromat. This person needs to mix three wavelengths to match all
other wavelengths and has three cone pigments. A person who is color
deficient is called a dichromat. This person needs only two wavelengths
to match all other wavelengths and has only two operational cone
pigments. A tetrachromat needs four wavelengths to match all other
wavelengths and has four cone pigments. If a tetrachromat were to meet a
trichromat, would the tetrachromat think that the trichromat was color
deficient? How 226

would the tetrachromat's color vision be "better than" the trichromat's?
(p. 208) 2. When we discussed color deficiency, we noted the difficulty
in determining the nature of a color-deficient person's color
experience. Discuss how this is related to the idea that color
experience is a creation of our nervous system. (p. 223) 3. When you
walk from outdoors, which is illuminated by sunlight, to an indoor space
that is illuminated by

Chapter 9  Perceiving Color

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

tungsten or LED bulbs, your perception of colors remains fairly
constant. But under some illuminations, such as sodium-vapor lights that
sometimes illuminate highways or parking lots, colors do seem to change.
Why do you think color constancy would hold under some illuminations but
not others? (p. 215) 4. Figure 9.48 shows two displays (Knill & Kersten,
1991). The display in (b) was created by changing the top and bottom of
the display in (a), while keeping the intensity distributions across the
centers of the displays constant. (You can convince yourself that this
is true by masking off the top and bottom of the displays.) But even
though the intensities are the same, the display in (a) looks like a
dark surface on the left and a light surface on the right, whereas the
display in (b) looks like two curved cylinders with a slight shadow on
the

left one. How would you explain this, based on what we know about the
causes of lightness constancy? (p. 220)

(a) 
(b) 

Figure 9.48 The light distribution is identical for (a) and (b),
although it appears to be different. (Figure courtesy of David Knill and
Daniel Kersten)

KEY TERMS #TheDress (p. 218) Aberration (p. 205) Achromatic colors
(p. 200) Adaptive optical imaging (p. 205) Additive color mixture
(p. 202) Anomalous trichromatism (p. 210) Cerebral achromatopsia
(p. 197) Chromatic adaptation (p. 216) Chromatic colors (p. 200) Color
blind (p. 207) Color circle (p. 210) Color constancy (p. 215) Color
deficiency (p. 198) Color matching (p. 205) Color solid (p. 203) Cone
mosaic (p. 206) Desaturated (p. 203) Deuteranopia (p. 209) Dichromat
(p. 208) Dichromatism (p. 209) Dishabituation (p. 225) Habituation
procedure (p. 225)

Hue (p. 203) Hue cancellation (p. 211) Hue scaling (p. 211) Illumination
edge (p. 221) Ishihara plate (p. 208) Lightness constancy (p. 220)
Memory color (p. 217) Metamerism (p. 206) Metamers (p. 206)
Microspectrophotometry (p. 205) Monochromat (p. 207) Monochromatism
(p. 207) Munsell color system (p. 203) Neutral point (p. 209)
Nonspectral colors (p. 203) Novelty-preference procedure (p. 225)
Opponent neurons (p. 211) Opponent-process theory of color vision
(p. 210) Partial color constancy (p. 216) Penumbra (p. 222) Primary
colors (p. 211)

Principle of univariance (p. 207) Protanopia (p. 209) Ratio principle
(p. 220) Reflectance (p. 220) Reflectance curves (p. 200) Reflectance
edge (p. 221) Saturation (p. 203) Selective reflection (p. 200)
Selective transmission (p. 200) Spectral colors (p. 203) Subtractive
color mixture (p. 202) Transmission curves (p. 200) Trichromacy of color
vision (p. 204) Trichromat (p. 208) Tritanopia (p. 209) Unilateral
dichromat (p. 209) Unique hues (p. 211) Value (p. 203) Young-Helmholtz
theory (p. 204)

Key Terms

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

227

Looking out over this scene, we are able to perceive the distances of
objects ranging from very close to far into the distance. We are also
able to make judgments about the sizes of objects, and although the
church steeple is smaller in our field of view than the nearby structure
on the right, we know that the church is much larger. Our perception of
depth and size are closely related. Bruce Goldstein

Learning Objectives After studying this chapter, you will be able to ...
■■ Describe the basic problem involved in perceiving depth based

on the two-dimensional information on the retina. ■■ Describe the
different monocular (one-eyed) cues for depth. ■■ Understand how the two
eyes cooperate to create binocular

(two-eyed) cues for depth. ■■ Describe how neural signals coming from
the two eyes are com-

bined to create depth perception.

■■ Understand how animals ranging from monkeys, to cats, to pi-

geons, to insects perceive depth. ■■ Understand how perceiving an
object's size depends on being

able to perceive how far away it is. ■■ Describe how the connection
between the perception of size and

depth has been used to explain size illusions. ■■ Describe procedures
that have been used to determine the types

of information young infants use to perceive depth.

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

C hapter 1 0

Perceiving Depth and Size Chapter Contents 10.1 Perceiving Depth

The Correspondence Problem

10.2 Oculomotor Cues

10.5 The Physiology of Binocular Depth Perception

DEMONSTRATION: Feelings in Your Eyes

10.3 Monocular Cues Pictorial Cues Motion-Produced Cues DEMONSTRATION:
Deletion and

Accretion

10.4 Binocular Depth Information DEMONSTRATION: Two Eyes:

Two Viewpoints

Seeing Depth With Two Eyes Binocular Disparity Disparity (Geometrical)
Creates Stereopsis (Perceptual)

10.6 Depth Information Across Species TEST YOURSELF 10.1

Illusion With Books

The Ponzo Illusion The Ames Room SOMETHING TO CONSIDER:

The Changing Moon

DEVELOPMENTAL DIMENSION:

10.7 Perceiving Size The Holway and Boring Experiment Size Constancy
DEMONSTRATION: Perceiving Size at a

Distance

DEMONSTRATION: Size--Distance

Scaling and Emmert's Law

Infant Depth Perception Binocular Disparity Pictorial Cues

METHOD: Preferential Reaching TEST YOURSELF 10.2 THINK ABOUT IT

10.8 Illusions of Depth and Size The Müller-Lyer Illusion

Some Questions We Will Consider: ■■ How can we see far into the distance
based on the two-

dimensional image on the retina? (pp. 231, 236) ■■ Why do we see depth
better with two eyes than with one

eye? (p. 236) ■■ Why don't people appear to shrink in size when they
walk

away? (p. 250)

O

DEMONSTRATION: The Müller-Lyer

ur final chapter on vision focuses on the perception of depth and size.
At first, you might think that depth and size are separate issues in
perception, but they are in fact closely related. To see why, let's
consider Figure 10.1a. What do you see in this image? Most people see
what appears to be a very small man standing on a chair. This is,
however, an illusion created by a misperception of the man's distance
from the camera. Although the man appears to be standing on a chair that
is next to the woman, he is actually standing on a platform located next
to the black curtain (Figure 10.1b). The illusion that the man is
standing on a chair is created by lining

up the camera so a structure located across from the woman lines up with
the platform, to create the perception of a chair. Showing the woman
apparently pouring into the man's glass enhances the misperception of
the man's distance, and our incorrect perception of his depth leads to
an incorrect perception of his size. The illusion in Figure 10.1 was
specifically created to trick your brain into misperceiving the man's
depth and size, but why don't we confuse a small man who is close by and
a large man who is far away in our everyday perception of the world? We
will answer this question by describing the many ways we use different
sources of optical and environmental information to help us determine
the depth and size of objects in our everyday environments.

10.1 Perceiving Depth You can easily tell that the page or screen text
you are reading is about 12 to 18 inches away and, when you look up at
the scene around you, that other objects are located at

229

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Peter Thompson

Figure 10.1 (a) Misperception of the man's depth leads to an incorrect
perception of his size. (b) When the illusion of the "chair" is removed,
the man's actual depth can be determined, and he appears to be of normal
height.

(a) 

distances ranging from your nose (very close!) to across the room, down
the street, or even as far as the horizon, depending on where you are.
What's amazing about this ability to see the distances of objects in a
three-dimensional environment is that your perception of these objects,
and the scene as a whole, is based on the two-dimensional image on your
retina. We can appreciate the problem of perceiving threedimensional
depth based on the two-dimensional information on the retina by
considering two points on the scene in Figure 10.2a. Light is reflected
from point T on the tree and from point H on the house onto points T'
and H' on the retina at the back of the eye. Looking just at these
points on the flat surface of the retina (Figure 10.2b), we have no way
of knowing how far the light has traveled to reach each point. For all
we know, the light stimulating either point on the retina could have
come from 1 foot away or from a distant star. Clearly, we need to expand
our view beyond single points on the retina to determine where objects
are located in space. When we expand our view from two isolated points
to the entire retinal image, we increase the amount of information
available to us because now we can see the images of the house and the
tree. However, because this image is twodimensional, we still need to
explain how we get from the ﬂat image on the retina to the
three-dimensional perception of the scene. One way researchers have
approached this problem is by the cue approach to depth perception,
which focuses on identifying information in the retinal image that is
correlated with depth in the scene. For example, when one object
partially covers another object, as the tree in the foreground in Figure
10.2a covers part of the house, the object that is partially covered
must be farther than the object that is covering it. This situation,
called occlusion, is a cue that one object is in front of another.
According to cue theory, we 230

(b) 

learn the connection between this cue and depth through our previous
experience with the environment. After this learning has occurred, the
association between particular cues and depth becomes automatic, and
when these depth cues are present, we experience the world in three
dimensions. A number of different types of cues that signal depth in a
scene have been identified. We can divide these cues into three major
groups: 1. Oculomotor. Cues based on our ability to sense the position
of our eyes and the tension in our eye muscles.

T H

H T (a) Eye and scene

H T

(b) Image of scene on retina

Figure 10.2 (a) In the scene, the house is farther away than the tree,
but images of points H on the house and T on the tree fall on points H'
and T' on the two-dimensional surface of the retina on the back of the
eye. (b) These two points on the retinal image, considered by
themselves, do not tell us the distances of the house and the tree.

Chapter 10  Perceiving Depth and Size

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

2. Monocular. Cues based on the visual information available within one
eye. 3. Binocular. Cues that depend on visual information within both
eyes.

10.2 Oculomotor Cues The oculomotor cues are created by (1) convergence,
the inward movement of the eyes that occurs when we look at nearby
objects, and (2) accommodation, the change in the shape of the lens that
occurs when we focus on objects at various distances. The idea behind
these cues is that we can feel the inward movement of the eyes that
occurs when the eyes converge to look at nearby objects, and we feel the
tightening of eye muscles that change the shape of the lens to focus on
a nearby object. You can experience the feelings in your eyes associated
with convergence and accommodation by doing the following demonstration.
DEMONSTRATION

Feelings in Your Eyes

Look at your finger as you hold it at arm's length. Then, as you slowly
move your finger toward your nose, notice how you feel your eyes looking
inward and become aware of the increasing tension inside your eyes.

The feelings you experience as you move your finger closer are caused by
(1) the change in convergence angle as your eye muscles cause your eyes
to look inward, as in Figure 10.3a, and (2) the change in the shape of
the lens as the eye accommodates to focus on a near object (see Figure
3.9, page 44). If you move your finger farther away, the lens ﬂattens,
and your eyes move away from the nose until they are both looking
straight ahead, as in Figure 10.3b. Convergence and accommodation
indicate when an object is close; they are useful up to a distance of
about arm's length, with convergence being the more effective of the two
(Cutting & Vishton, 1995; Mon-Williams & Tresilian, 1999; Tresilian et
al., 1999).

(a) 
(b) 

Figure 10.3 (a) Convergence of the eyes occurs when a person looks at
something that is very close. (b) The eyes look straight ahead when the
person observes something that is far away.

10.3 Monocular Cues Monocular cues work with only one eye. They include
accommodation, which we have described under oculomotor cues; pictorial
cues, which are sources of depth information in a twodimensional
picture; and movement-based cues, which are sources of depth information
created by movement.

Pictorial Cues Pictorial cues are sources of depth information that can
be depicted in a picture, such as the illustrations in this book or an
image on the retina (Goldstein, 2001).

Occlusion We have already described the depth cue of occlusion.
Occlusion occurs when one object hides or partially hides another from
view. The partially hidden object is seen as being farther away, so the
mountains in Figure 10.4 are perceived as being farther away than the
cactus and the hill. Note that occlusion does not provide precise
information about an object's distance. It simply indicates that the
object that is partially covered is farther away than another object,
but from occlusion alone we can't tell how much farther. Relative Height
In the photograph of the scene in Figure 10.4a, some objects are near
the bottom of the frame and others nearer the top. The height in the
frame of the photo corresponds to the height in our field of view, and
objects that are higher in the field of view are usually farther away.
This is illustrated in Figure 10.4b, in which dashed lines 1, 2, and 3
have been added under the front motorcycle, the rear motorcycle, and one
of the telephone poles. Notice that dashed lines higher in the picture
are under objects that are farther away. You can demonstrate the "higher
is farther" principle by looking out at a scene and placing your finger
at the places where objects contact the ground. When you do this, you
will notice that if all the objects are on a flat surface (no hills!),
your finger is higher for farther objects. According to the cue of
relative height, objects with their bases closer to the horizon are
usually seen as being more distant. This means that being higher in the
field of view causes objects on the ground to appear farther away (see
lines 1, 2, and 3 in Figure 10.4b), whereas being lower in the field of
view causes objects in the sky to appear farther away (see lines 4 and
5). Familiar and Relative Size We use the cue of familiar size when we
judge distance based on our prior knowledge of the sizes of objects. We
can apply this idea to the coins in Figure 10.5a. If you are inﬂuenced
by your knowledge of the actual size of dimes, quarters, and
half-dollars (Figure 10.5b), you might say that the dime is closer than
the quarter. An experiment by William Epstein (1965) shows that under
certain conditions, our knowledge of an object's size inﬂuences our
perception of that object's distance (see also McIntosh & Lashley,
2008). The stimuli in Epstein's experiment were equal-sized photographs
of a dime, a quarter, and a half-dollar (Figure 10.5a), which were
positioned the same distance from an observer. By placing 10.3 Monocular
Cues

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

231

4

5

2

3

1

(a) 
(b) 

Figure 10.4 (a) A scene in Tucson, Arizona, containing a number of depth
cues: occlusion (the cactus on the right occludes the hill, which
occludes the mountain); relative height (the far motorcycle is higher in
the field of view than the closer one); relative size (the far
motorcycle and telephone pole are smaller than the near ones); and
perspective convergence (the sides of the road converge in the
distance). (b) 1, 2, and 3 indicate the increasing height in the field
of view of the bases of the motorcycles and the far telephone pole,
which reveals that being higher in the field of view causes objects on
the ground to appear farther away; 4 and 5 reveal that being lower in
the field of view causes objects in the sky to appear farther away, so
cloud 5 appears farther from the viewer than cloud 4.

these photographs in a darkened room, illuminating them with a spot of
light, and having subjects view them with one eye, Epstein created the
illusion that these pictures were real coins. When the observers judged
the distance of each of the coin photographs, they estimated that the
dime was closest, the quarter was farther than the dime, and the
half-dollar was the farthest of all. Thus, the observers' judgments were
inﬂuenced by their knowledge of the sizes of these coins. This result
does not occur, however, when observers view the scene with both eyes,
because, as we will see when we discuss binocular (two-eyed) vision, the
use of two eyes provides information indicating the coins are at the
same distance. The cue of familiar size is therefore most effective when
other information about depth is minimized (see also Coltheart, 1970;
Schiffman, 1967).

A depth cue related to familiar size is relative size. According to the
cue of relative size, when two objects are known to be of equal physical
size, the one that is farther away will take up less of your field of
view than the one that is closer. For example, knowing (or assuming)
that the two telephone poles, or the two motorcycles, in Figure 10.4 are
about the same size, we can determine which pole, or motorcycle, is
closer than the other.

Perspective Convergence When you look down parallel railroad tracks that
appear to converge in the distance, you are experiencing perspective
convergence. This cue was often used by Renaissance artists to add to
the impression of depth in their paintings, as in Pietro Perugino's
painting in Figure 10.6. Notice that in addition to the perspective

PRISMA ARCHIVO / Alamy Stock Photo

(a) 
(b) 

Figure 10.5 (a) Photographs similar to those used in Epstein's (1965)
familiar-size experiment. Each coin was photographed to be the same size
as a real quarter. (b) The actual size of a dime, quarter, and
half-dollar. 232

Figure 10.6 Pietro Perugino, Christ Handing the Keys to St. Peter
(Sistine Chapel). The convergence of lines on the plaza illustrates
perspective convergence. The sizes of the people in the foreground and
middle ground illustrate relative size.

Chapter 10  Perceiving Depth and Size

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Perspective Atmospheric perspective occurs because the farther away an
object is, the more air and particles (dust, water droplets, airborne
pollution) we have to look through, so that distant objects appear less
sharp than nearer objects and sometimes have a slight blue tint. Figure
10.7 illustrates atmospheric perspective. The details in the foreground
are sharp and well defined, but details become less and less visible as
we look farther into the distance. The reason that farther objects look
bluer is related to the reason the sky appears blue. Sunlight contains a
distribution of all of the wavelengths in the spectrum, but the
atmosphere preferentially scatters short-wavelength light, which appears
blue. This scattered light gives the sky its blue tint and also creates
a veil of scattered light between us and objects we are looking at. The
blueness becomes obvious, however, only when we are looking through a
large distance or when there are many particles in the atmosphere to
scatter the light. If, instead of viewing this cliff along the coast of
Maine, you were standing on the moon, where there is no atmosphere and
hence no atmospheric perspective, far craters would not look blue and
would look just as clear as near ones. But on Earth, there is
atmospheric perspective, with the exact amount depending on the nature
of the atmosphere. Texture Gradient When a number of similar objects are
equally spaced throughout a scene, as in Figure 10.8, they create a
texture gradient, which results in a perception of depth, with elements
seen as being spaced more closely being perceived as farther.

Bruce Goldstein

convergence provided by the lines on the plaza, Perugino has included
people in the middle ground, further enhancing the perception of depth
through the cue of relative size. Figure 10.4 illustrates both
perspective convergence (the road) and relative size (the motorcycles)
in our Tucson mountain scene.

Figure 10.8 A photograph taken in Death Valley, in which the decrease in
spacing of the elements with increasing distance illustrates a texture
gradient.

Shadows Shadows---decreases in light intensity caused by the blockage of
light---can provide information regarding the locations of these
objects. Consider, for example, Figure 10.9a, which shows seven spheres
and a checkerboard. In this picture, the location of the spheres
relative to the checkerboard is unclear. They could be resting on the
surface of the checkerboard or ﬂoating above it. But adding shadows, as
shown in Figure 10.9b, makes the spheres' locations clearer---the ones
on the left are resting on the checkerboard, and the ones on the right
are ﬂoating above it. This illustrates how shadows can help determine
the location of objects (Mamassian, 2004; Mamassian et al., 1998).
Shadows also enhance the three-dimensionality of objects. For example,
shadows make the circles in Figure 10.9 appear spherical and help define
some of the contours in the mountains in Figure 10.10, which appear
three-dimensional in the early morning when there are shadows (Figure
10.10a), but

Bruce Goldstein

(a) 

Figure 10.7 A scene on the coast of Maine showing the effect of
atmospheric perspective.

(b) 

Figure 10.9 (a) Where are the spheres located in relation to the
checkerboard? (b) Adding shadows makes their location clearer. (Courtesy
of Pascal Mamassian)

10.3 Monocular Cues

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

233

Bruce Goldstein

(a) 
(b) 

Figure 10.10 (a) Early morning shadows emphasize the mountain's
contours. (b) When the sun is overhead, the shadows vanish and it
becomes more difficult to see the mountain's contours.

flat in the middle of the day when the sun is directly overhead and
there are no shadows (Figure 10.10b).

Motion-Produced Cues All of the cues we have described so far work if
the observer is stationary. But once we start moving, new cues emerge
that further enhance our perception of depth. We will describe two
motion-produced cues: (1) motion parallax and (2) deletion and
accretion.

Motion Parallax Motion parallax occurs when, as we move, nearby objects
appear to glide rapidly past us, but more distant objects appear to move
more slowly. Thus, when you look out the side window of a moving car or
train, nearby objects appear to speed by in a blur, whereas objects that
are farther away may appear to be moving only slightly. Also, if, when
looking out the window, you keep your eyes fixed on one object, objects
farther and closer than the object you are looking at appear to move in
opposite directions. We can understand why motion parallax occurs by
noting how the image of a near object (the tree in Figure 10.11a) and a
far object (the house in Figure 10.11b) move across the retina as an eye
moves from position 1 to position 2 without rotating. First let's
consider the tree: Figure 10.11a shows one eye that moves from 1 to 2,
so the tree's image moves all the way across the retina from T1 to T2,
as indicated by the dashed arrow. Figure 10.11b shows that the house's
image moves a shorter distance, from H1 to H2. Because the image of the
tree travels a larger distance across the retina than the house, in the
same amount of time, it appears to move more rapidly. Motion parallax is
one of the most important sources of depth information for many animals.
For example, before 234

jumping out toward an object such as prey, locusts move their body from
side to side to create movement of its head to generate motion parallax
signals that indicate the distance of their target (Wallace, 1959). By
artificially manipulating environmental information in a way that alters
the motion parallax signals obtained by a locust, researchers can
"trick" the animal into either jumping short of, or beyond, their
intended target (Sobel, 1990). The information provided by motion
parallax has been used to enable humandesigned mechanical robots to
determine how far they are from obstacles as they navigate through the
environment (Srinivasan & Venkatesh, 1997). Motion parallax is also
widely used to create an impression of depth in cartoons and video
games.

Deletion and Accretion As an observer moves sideways, some things become
covered, and others become uncovered. Try the following demonstration.

DEMONSTRATION

Deletion and Accretion

Close one eye. Position your hands as shown in Figure 10.12, so your
right hand is at arm's length and your left hand at about half that
distance, just to the left of the right hand. Then as you look at your
right hand, move your head sideways to the left, being sure to keep your
hands still. As you move your head, your left hand appears to cover your
right hand. This covering of the farther right hand is deletion. If you
then move your head back to the right, the nearer hand moves back and
uncovers the right hand. This uncovering of the far hand is accretion.
Deletion and accretion occur all the time as we move through the
environment and create information that the object or surface being
covered and uncovered is farther away (Kaplan, 1969).

Chapter 10  Perceiving Depth and Size

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Move

T1 Position 1

T1

T2

Move

Position 2

(a) 

H1

H1 Position 1

H2 Position 2

(b) 

Bruce Goldstein

Figure 10.11 One eye moving past (a) a nearby tree; (b) a faraway house.
Because the tree is closer, its image moves farther across the retina
than the image of the house.

Figure 10.12 Position of the hands for the "Deletion and Accretion"
demonstration. See text for explanation.

Integrating Monocular Depth Cues Our discussion so far has described a
number of the monocular cues that contribute to our perception of depth.
But it is important to understand that each of these cues gives us "best
guess" information regarding object depth and that each cue can, by
itself, be uninformative in certain situations. For example, relative
height is most useful when objects are on a flat plane and we can see
where they touch the ground, shadow is most useful if the scene is
illuminated at an angle, familiar size is most useful if we have prior
knowledge of the objects' sizes, and so forth. Furthermore, as shown in
Table 10.1, monocular depth cues work over different distances: some
only at close range (accommodation, convergence); some at close and
medium ranges (motion parallax, deletion and accretion); some at long
range (atmospheric perspective, relative height, texture gradients); and
some at the whole range of depth perception (occlusion, relative size;
Cutting & Vishton, 1995). Thus, for a nearby object, we don't look for
atmospheric perspective but instead rely more on convergence, occlusion,
or relative size information. Additionally, some depth cues only provide
information on relative depth (Table 10.1a) while others can contribute
to a more precise determination of actual depth (Table 10.1b). No depth
cue is perfect. No depth cue is applicable to every 10.3 Monocular Cues

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

235

Table 10.1a Cues That Indicate Relative Depth DEPTH CUE

0--2 METERS

Occlusion

√

2--20 METERS ABOVE 20 METERS

√

√

Deletion & accretion

√

√

Relative height

√

√

Atmospheric perspective

√

Table 10.1b Cues That Contribute to Determination

of Actual Depth

DEPTH CUE

Relative size

0--2 METERS

√

Texture gradients Motion parallax

√

Accommodation

√

Convergence

√

2--20 METERS ABOVE 20 METERS

√

√

√

√

√

situation. But by combining different depth cues when they are
available, we can achieve a reasonable interpretation of depth.

10.4 Binocular Depth Information The power of monocular cues to signal
depth is plain to see when you close one eye. When you do so, you can
still tell what is near and what is far away. However, closing one eye
removes some of the information that your brain uses to compute the
depth of objects. Two-eyed depth perception involves mechanisms that
take into account differences in the images formed on the left and right
eyes. The following demonstration illustrates these differences.
DEMONSTRATION

Two Eyes: Two Viewpoints

Close your right eye. Hold a finger on your left hand at arm's length.
Position a right-hand finger about a foot away, so it covers the other
finger. Then open the right eye and close the left. When you switch
eyes, how does the position of your front finger change relative to the
rear finger?

When you switched from looking with your left eye to your right, you
probably noticed that the front finger appeared to move to the left
relative to the far finger. Figure 10.13 diagrams what happened on your
retinas. The solid line in Figure 10.13a shows that when the left eye
was open, the images of the near and far fingers were lined up with the
same place on the retina. This occurred because you were looking 236

directly at both objects, so both images would fall on the foveas of the
left eye. The solid lines in Figure 10.13b show that when the right eye
was open, the image of the far finger still fell on the fovea because
you were looking at it, but the image of the near finger was now off to
the side. Whereas the fingers were lined up relative to the left eye,
the right eye "looks around" the near finger, so the far finger becomes
visible. These different viewpoints for the two eyes are the basis of
stereoscopic vision, which creates stereoscopic depth perception---depth
perception created by input from both eyes. Before describing these
mechanisms, we will consider what it means to say that stereoscopic
depth perception is qualitatively different from monocular depth
perception.

Seeing Depth With Two Eyes One way to appreciate the qualitative
difference between monocular depth perception and stereoscopic depth
perception is to consider the story of Susan Barry, a neuroscientist at
Mt. Holyoke College. Her story---first described by neurologist Oliver
Sacks, who dubbed her "Stereo Sue" (Sacks, 2006, 2010), and then in her
own book, Fixing My Gaze (Barry, 2011)---begins with Susan's childhood
eye problems. She was cross-eyed, so when she looked at something with
one eye, the other eye would be looking somewhere else. For most people,
both eyes aim at the same place and work in coordination with each
other, but in Susan's case, the input was uncoordinated. Situations such
as this, along with a condition called "walleye" in which the eyes look
outward, are forms of strabismus, or misalignment of the eyes. When this
occurs, the visual system suppresses vision in one of the eyes to avoid
double vision, so the person sees the world with only one eye at a time.
Susan had a number of operations as a child that made her strabismus
less noticeable to others, but her vision was still dominated by one
eye. Although her perception of depth was only achieved through
monocular cues, she was able to get along quite well. She could drive,
play softball, and do most of the things people with stereoscopic vision
can do. For example, she describes her vision in a college classroom as
follows: I looked around. The classroom didn't seem entirely flat to me.
I knew that the student sitting in front of me was located between me
and the blackboard because the student blocked my view of the
blackboard. When I looked outside the classroom window, I knew which
trees were located further away because they looked smaller than the
closer ones. (Barry, 2011, Chapter 1) Although Susan could use these
monocular cues to perceive depth, her knowledge of the neuroscience
literature and various other experiences she describes in her book led
her to realize that she was still seeing with one eye despite her
childhood operations. She therefore consulted an optometrist, who
confirmed her one-eyed vision and assigned eye exercises designed to
improve the coordination between her two eyes. These exercises enabled
Susan to coordinate her eyes, and one day after leaving the
optometrist's office, she had her first experience with stereoscopic
depth perception, which she describes as follows:

Chapter 10  Perceiving Depth and Size

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Far finger

Far finger

Near finger

Near finger

Near finger Far finger

Far finger and near finger For left eye, near finger covers far finger
(a)

Right eye closed

Left eye closed

For right eye, both near and far fingers are visible

(b) 

Figure 10.13 Location of images on the retina for the "Two Eyes: Two
Viewpoints" demonstration. See text for explanation.

I got into my car, sat down in the driver's seat, placed the key in the
ignition, and glanced at the steering wheel. It was an ordinary steering
wheel against an ordinary dashboard, but it took on a whole new
dimension that day. The steering wheel was floating in its own space,
with a palpable volume of empty space between the wheel and the
dashboard. I closed one eye and the steering wheel looked "normal"
again; that is, it lay flat just in front of the dashboard. I reopened
the closed eye, and the steering wheel floated before me. (Barry, 2011,
Chapter 6)

Bruce Goldstein

From that point on, Susan had many more experiences that astounded her,
but it is important to note that Susan didn't suddenly gain stereovision
equivalent to that experienced by a person with stereoscopic vision from
birth. Her stereovision

occurred first for nearby objects and then, as her training progressed,
was extended to farther distances. But what she did experience
dramatically illustrates the richness that stereoscopic vision adds to
the experience of depth perception. The added experience of depth
created by stereoscopic depth perception is also illustrated by the
difference between standard movies and 3-D movies. Standard movies,
which project images on a flat screen, create a perception of depth
based on monocular depth cues like occlusion, relative height, shadows,
and motion parallax. Three-dimensional movies add stereoscopic depth
perception. This is achieved by using two cameras placed side by side.
Like each of your eyes, each camera receives a slightly different view
of the scene (Figures 10.14a and 10.14b). These two images are then
overlaid on the movie screen (Figure 10.14c).

(a) Left camera

(b) Right camera

(c) Overlay of camera images

Figure 10.14 (a) and (b) 3-D movies are filmed using two side-by-side
cameras so that each camera records a slightly different view of the
scene. (c) The images are then projected onto the same 2-D surface.
Without 3-D glasses, both images are visible to both eyes. 3-D glasses
separate the images so that one is only seen by the left eye and the
other is only seen by the right eye. When the left and right eyes
receive these different images, stereoscopic depth perception occurs.
10.4 Binocular Depth Information

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

237

Left

Right

A

A F

F Slide

Figure 10.15 Corresponding points on the two retinas. To determine
corresponding points, imagine that the left eye is slid on top of the
right eye. F indicates the fovea, where the image of an object occurs
when an observer looks directly at the object, and A is a point in the
peripheral retina. Images on the fovea always fall on corresponding
points. Notice that the As, which also fall on corresponding points, are
the same distance from the fovea in the left and right eyes.

When wearing 3-D glasses, the lenses separate the two overlapping images
so that each eye only receives one of the images. This image separation
can be achieved in several ways, but the most common method used in 3-D
movies uses polarized light---light waves that vibrate in only one
orientation. One image is polarized so its vibration is vertical and the
other is polarized so its vibration is horizontal. The

glasses you wear have polarized lenses that let only vertically
polarized light into one eye and horizontally polarized light into the
other eye. Thus, sending these two different views to two different eyes
duplicates what happens in the real 3-D world, and suddenly some objects
can appear to be recessed behind the screen while others appear to jut
far out in front of it.

Binocular Disparity Binocular disparity, the difference in the images on
the left and right retinas, is the basis of stereoscopic vision. We now
look more closely at the information on the left and right retinas that
the brain uses to create an impression of depth.

Corresponding Retinal Points We begin by introducing corresponding
retinal points---points on the retina that would overlap if the eyes
were superimposed on each other (Figure 10.15). We can illustrate
corresponding points by considering what Owen sees in Figure 10.16a,
when he is looking directly at Julie. Figure 10.16b shows where Julie's
images are located on Owen's retinas. Because Owen is looking directly
at Julie, her images fall on Owen's foveas in both eyes, indicated by
the red dots. The two foveas are corresponding points, so Julie's images
fall on corresponding points. Julie Horopter

Horopter

Julie

Owen

(a) 

F

F

Julie

Julie

(b) 

Figure 10.16 (a) Owen is looking at Julie's face, with a tree off to the
side. (b) Owen's eyes, showing where the images of Julie and the tree
fall on each eye. Julie's images fall on the fovea, so they are on
corresponding points. The arrows indicate that the tree's images are
located the same distances from the fovea in the two eyes, so they are
also on corresponding points. The dashed line in (a) and (b) is the
horopter. The images of objects that are on the horopter fall on
corresponding points. 238

Chapter 10  Perceiving Depth and Size

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

In addition, the images of other objects also fall on corresponding
points. Consider, for example, the tree in Figure 10.16b. The tree's
images are on the same place relative to the foveas---to the left and at
the same distance (indicated by the arrows). This means that the tree's
images are on corresponding points. (If you were to slide the eyes on
top of each other, Julie's images would overlap, and the tree's images
would overlap.) Thus, whatever a person is looking at directly (like
Julie) falls on corresponding points, and some other objects (like the
tree) fall on corresponding points as well. Julie, the tree, and any
other objects that fall on corresponding points are located on a surface
called the horopter. The blue dashed lines in Figures 10.16a and 10.16b
show part of the horopter.

Noncorresponding Points and Absolute Disparity The images of objects
that are not on the horopter fall on noncorresponding points. This is
illustrated in Figure 10.17a, which shows Julie again, with her images
on corresponding points, and a new character, Bill, who is located in
front of the horopter. Because Bill is not on the horopter, his image
falls on noncorresponding points in each retina. The degree to which
Bill's image deviates from falling on corresponding points is called
absolute disparity. The amount of absolute disparity, called the angle
of disparity, is indicated by the blue arrow in Figure 10.17a; it is the
angle between the corresponding point on the right eye for the left-eye
image of Bill (blue dot) and the actual location of the image on the
right eye (red dot). Figure 10.17b shows that binocular disparity also
occurs Bill

Julie

Julie

Looking at Julie

Bill

Corresponding point for Bill

Corresponding point for Bill

Angle of disparity for Bill

Angle of disparity for Bill

Left eye view (a) Bill is in front of the horopter

Right eye view

Left eye view

Right eye view

(b) Bill is behind the horopter

Figure 10.17 (a) When an observer looks at Julie, Julie's images fall on
corresponding points. Because Bill is in front of the horopter, his
images fall on noncorresponding points. The angle of disparity,
indicated by the blue arrow, is determined by measuring the angle
between where the corresponding point for Bill's image would be located
and where Bill's image is actually located. (b) Disparity is also
created when Bill is behind the horopter. The pictures at the bottom of
the figure illustrate how the positions of Julie and Bill are seen by
each eye. 10.4 Binocular Depth Information

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

239

when objects are behind the horopter (farther away than a fixated
object). Although objects appearing in front of the horopter (Figure
10.17a) and behind the horopter (Figure 10.17b) both result in retinal
disparity, the disparity is somewhat different in the two situations. To
understand this difference, let's think about what each eye sees
individually in Figures 10.17a and 10.17b. The pictures at the bottom of
Figure 10.17a show what the left and right eyes see when Bill is in
front of Julie. In this situation, the left eye sees Bill to Julie's
right, while the right eye sees Bill to Julie's left. This pattern of
disparity where the left eye sees an object (e.g., Bill) to the right of
the observer's fixation point (e.g., Julie) and the right eyes sees that
same object to the left of the fixation point is called crossed
disparity (you can remember this by thinking about the fact that you
would need to "cross" your eyes in order to fixate Bill). Crossed
disparity occurs whenever an object is closer to the observer than where
the observer is looking. Now let's consider what happens when an object
is behind the horopter. The pictures at the bottom of Figure 10.17b show
what each eye sees when Bill is behind Julie. This time, the left eye
sees Bill to Julie's left and the right eye sees Bill is to Julie's
right. This pattern of disparity where the left eye sees an object to
the left of the observer's fixation point and the right eyes sees that
same object to the right of the fixation point is called uncrossed
disparity (in order to fixate on Bill you would need to "uncross" your
eyes). Uncrossed disparity occurs whenever an object is behind the
horopter. Thus, by determining whether an object produces crossed or
uncrossed disparity, the visual system can determine whether that object
is in front of or behind a person's point of fixation.

Absolute Disparity Indicates Distance From the Horopter Determining
whether absolute disparity is crossed or uncrossed indicates whether an
object is in front of or behind the horopter. This is of course
important information to have, but it provides only part of the story.
To perceive depth accurately, we also need to know the distance between
an object and the horopter. This information is provided by the amount
of disparity associated with an object. Figure 10.18 shows that the
angle of disparity is greater for objects at greater distances from the
horopter. The observer is still looking at Julie, and Bill is in front
of the horopter where he was in Figure 10.17a, but now we have added
Dave, who is located even farther from the horopter than Bill. When we
compare Dave's angle of disparity in this figure (large green arrow) to
Bill's (small purple arrow), we see that Dave's disparity is greater.
The same thing happens for objects farther away than the horopter, with
greater distance also associated with greater absolute disparity. The
angle of disparity therefore provides information about an object's
distance from the horopter, with greater angles of disparity indicating
greater distances from the horopter. There is also another type of
disparity, called relative disparity, which is related to how we judge
the distance between two objects. In our discussion, we will continue to
focus on absolute disparity. 240

Julie

Looking at Julie

Bill

Dave

Corresponding points

Angle of disparity

Dave Bill

Dave Bill

Figure 10.18 Objects that are farther away from the horopter are
associated with greater angles of disparity. In this case, both Dave and
Bill are in front of the horopter, but Dave is farther from the horopter
than Bill, as indicated by comparing the large green arrow (Dave's
disparity) to the small purple arrow (Bill's disparity).

Disparity (Geometrical) Creates Stereopsis (Perceptual) We have seen
that disparity information contained in the images on the retinas
provides information indicating an object's relative distance from where
the observer is looking. Notice, however, that our description of
disparity has focused on geometry---looking at where objects' images
fall on the retina--- but has not mentioned perception, the observer's
experience of an object's depth or its relation to other objects in the
environment (Figure 10.19). We consider the relationship between
disparity and what observers perceive by introducing stereopsis---the
impression of depth that results from information provided by binocular
disparity. In order to demonstrate that disparity creates stereopsis, we
need to isolate disparity information from other depth cues, such as
occlusion and relative height, because these other cues can also
contribute to our perception of depth. In order to show that disparity
alone can result in depth perception, Bela Julesz (1971) created a
stimulus called the random-dot stereogram, which contains no pictorial
cues. By creating stereoscopic images of random-dot patterns, Julesz
showed that observers can perceive depth in displays that contain no
depth information other than disparity. Two such random-dot patterns,
which together constitute a random-dot stereogram, are shown in Figure
10.20. These patterns were constructed by

Chapter 10  Perceiving Depth and Size

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

patterns (Figure 10.20b). In these diagrams, the black dots are
indicated by 0s, As, and Xs and the white dots by 1s, Bs, and Ys. The As
and Bs indicate the square-shaped section where the shift is made in the
pattern. Notice that the As and Bs are shifted one unit to the right in
the right-hand pattern. The Xs and Ys indicate areas uncovered by the
shift that must be filled in with new black dots and white dots to
complete the pattern. As you look at Figure 10.20a on the page,
information from both the left and right images travel to both your left
and right eyes and it is difficult, if not impossible, to tell that the
dots have been shifted. The visual system can, however, detect a
difference between these images if we separate the visual information on
the page so that the left image is only seen by the left eye and the
right image is only seen by the right eye. With two side-by-side images
(rather than slightly overlapping as in Figure 10.14c) this separation
is accomplished by using a device called a stereoscope (Figure 10.21)
that uses two lenses to focus the left image on the left eye and the
right image on the right eye. When viewed in this way, the disparity
created by the shifted section results in perception of a small square
ﬂoating above the background. Because binocular disparity is the only
depth information present in these stereograms, disparity alone must be
causing the perception of depth. Psychophysical experiments,
particularly those using Julesz's random-dot stereograms, show that
retinal disparity

Perception of depth

(Stereopsis)

Geometry of images (Disparity)

Figure 10.19 Disparity is related to geometry---the locations of images
on the retina. Stereopsis is related to perception---the experience of
depth created by disparity.

first generating two identical random-dot patterns on a computer and
then shifting a square-shaped section of the dots one or more units to
the side. In the stereogram in Figure 10.20a, a section of dots from the
pattern on the left has been shifted one unit to the right to form the
pattern on the right. This shift is too subtle to be seen in the dot
patterns, but we can understand how it is accomplished by looking at the
diagrams below the dot

Figure 10.20 (a) A random-dot stereogram. (b) The principle for
constructing the stereogram. See text for explanation.

(a) 

1

0

1

0

1

0

0

1

0

1

1

0

1

0

1

0

0

1

0

1

1

0

0

1

0

1

0

1

0

0

1

0

0

1

0

1

0

1

0

0

0

0

1

1

0

1

1

0

1

0

0

0

1

1

0

1

1

0

1

0

0

1

0

A

A

B

B

1

0

1

0

1

0

Y

A

A

B

B

0

1

1

1

1

B

A

B

A

0

0

1

1

1

1

X

B

A

B

A

0

1

0

0

1

A

A

B

A

0

1

0

0

0

1

X

A

A

B

A

1

0

1

1

1

B

B

A

B

1

0

1

1

1

1

Y

B

B

A

B

0

1

1

0

0

1

1

0

1

1

0

1

1

0

0

1

1

0

1

1

0

1

1

1

0

0

1

1

0

1

1

1

1

1

0

0

1

1

0

1

1

1

0

1

0

0

0

1

1

1

1

0

0

1

0

0

0

1

1

1

1

0

(b) 10.4 Binocular Depth Information

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

241

BSIP/Getty Images

Science & Society Picture Library/Getty Images

(a) 
(b) 

Figure 10.21 Examples of (a) antique and (b) modern stereoscopes.

creates a perception of depth. But before we can fully understand the
mechanisms responsible for depth perception, we must answer one more
question: How does the visual system match the parts of the images in
the left and right eyes that correspond to one another? This is called
the correspondence problem, and as we will see, it has still not been
fully explained.

The Correspondence Problem Let's return to the stereoscopic images of
Figure 10.14c. When we view this image through 3-D glasses (p. 237), we
see different parts of the image at different depths because of the
disparity between images on the left and right retinas. Thus, the cactus
and the window appear to be at different distances when viewed through
the glasses because they create different amounts of disparity. But in
order for the visual system to calculate this disparity, it must compare
the images of the cactus on the left and right retinas and the images of
the window on the left and right retinas. One way the visual system
might do this is by matching the images on the left and right retinas on
the basis of the specific features of the objects. Explained in this
way, the solution seems simple: Most things in the world are quite
discriminable from one another, so it is easy to match an image on the
left retina with the image of the same thing on the right retina.
Returning to Figure 10.14, the upper-left windowpane that falls on the
left retina could be matched with 242

the upper-left pane on the right retina, and so on. But determining
corresponding points based on object features can't be the whole answer
to the correspondence problem, because that strategy won't work for
Julesz's random dot stereogram (Figure 10.20). You can appreciate the
problem involved in matching similar parts of a stereogram by trying to
match up the points in the left and right images of the stereogram in
Figure 10.20a. Most people find this to be an extremely difficult task,
involving switching their gaze back and forth between the two pictures
and comparing small areas of the pictures one after another. But even
though matching similar features on a random-dot stereogram is much more
difficult and timeconsuming than matching features in the real world,
the visual system somehow matches similar parts of the two stereogram
images, calculates their disparities, and creates a perception of depth.
From the random-dot stereogram example, it is clear that the visual
system accomplishes something rather amazing when it solves the
correspondence problem. Researchers in fields as diverse as psychology,
neuroscience, mathematics, and engineering have put forth a number of
specific proposals, all too complex to discuss here, that seek to
explain how the visual system fully solves the correspondence problem
(Goncalves & Welchman, 2017; Henricksen et al., 2016; Kaiser et al.,
2013; Marr & Poggio, 1979). Despite these efforts, however, a totally
satisfactory solution to the correspondence problem has yet to be
proposed.

Chapter 10  Perceiving Depth and Size

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

50

Firing rate

40 30 20 10 0

--1

0 +1 Disparity (degrees)

+2

Figure 10.22 Disparity tuning curve for a neuron sensitive to absolute
disparity. This curve indicates the neural response that occurs when
stimuli presented to the left and right eyes create different amounts of
disparity. (From Uka & DeAngelis, 2003)

10.5 The Physiology of Binocular Depth Perception The idea that
binocular disparity provides information about the positions of objects
in space implies that there should be neurons that signal different
amounts of disparity. These neurons, which are called binocular depth
cells or disparityselective cells, were discovered when research in the
1960s and 1970s revealed neurons that respond to disparity in the
primary visual cortex, area V1 (Barlow et al., 1967; Hubel & Wiesel,
1970). These cells respond best when stimuli presented to the left and
right eyes create a specific amount of disparity (Hubel et al., 2015;
Uka & DeAngelis, 2003). Figure 10.22 shows a disparity tuning curve for
one of these neurons. This particular neuron responds best when the left
and right

Elimination of disparityselective neurons by selective rearing
eliminates binocular depth perception

CEPTION PER

A

C

Microstimulation of disparity-selective neurons changes depth perception

SIO PHY

Binocular disparity causes perception of depth (stereopsis)

Figure 10.23 The three relationships in the perceptual process, as
applied to binocular disparity. We have described experiments relating
disparity to perception (A) and relating disparity to physiological
responding (B). The final step is to determine the relationship between
physiological responses to disparity and perception (C). This has been
studied by selective rearing, which eliminates disparityselective
neurons, and by microstimulation, which activates disparity-selective
neurons.

LO

G

TI

MU

LUS

eyes are stimulated to create an absolute disparity of about 1 degree.
The relationship between binocular disparity and the firing of binocular
depth cells is an example of the stimulus-- physiology relationship in
the diagram of the perceptual process in Figure 10.23 (relationship B).
This diagram, which we introduced in Chapter 1 (see Figure 1.13, page
12) and repeated in Chapter 8 (see Figure 8.12, page 184), also depicts
two other relationships. The stimulus--perception relationship (A) is
the relationship between binocular disparity and the perception of
depth. The final relationship, between physiology and perception (C),
involves demonstrating a connection between disparity-selective neurons
and depth perception. This has been achieved in a number of ways. An
early demonstration of a connection between binocular neurons and
perception involved the selective rearing procedure we described in our
discussion of the relationship between feature detectors and perception
in Chapter 4 (see page 72). Applying this procedure to depth perception,
Randolph Blake and Helmut Hirsch (1975) reared cats so that their vision
was alternated between the left and right eyes every other day during
the first 6 months of their lives. After this 6-month period of
presenting stimuli to just one eye at a time, Blake and Hirsch recorded
from neurons in the cat's visual cortex and found that (1) these cats
had few binocular neurons and (2) they performed poorly on the depth
perception task. Thus, eliminating binocular neurons eliminates
stereopsis and confirms what everyone suspected all along---that
disparityselective neurons are responsible for stereopsis (also see
Olson & Freeman, 1980). Early research on disparity-selective neurons
focused on neurons in the primary visual receiving area, V1. But later
research has shown that neurons sensitive to disparity are found in many
areas outside V1 (Minini et al., 2010; Parker et al., 2016) (Figure
10.24). Gregory DeAngelis and coworkers

S

Y

B Binocular disparity causes firing of disparity-selective cells 10.5
The Physiology of Binocular Depth Perception

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

243

Stimulate neuron signaling closer distance

Perceived depth during stimulation Perceived depth before stimulation

Minini et al., 2010

2

Figure 10.24 fMRI responding to disparity in random-dot stereograms, as
indicated by the colored areas, shows that responses to stereoscopic
depth is widespread in the cortex, including much of the visual cortex
in the occipital lobe and parts of the parietal cortex.

(1998) studied disparity-selective neurons in area MT, by using
       microstimulation (see Method: Microstimulation, Chapter 8,
       page 185) to pass an electrical charge through neurons in that
       area. Because neurons that are sensitive to the same disparities
       tend to be organized in clusters, stimulating one of these
       clusters activates a group of neurons that respond best to a
       specific disparity (Hubel et al., 2015). DeAngelis trained
       monkeys to indicate the depth created by presenting images with
       different disparities. Presumably, the monkey perceived depth
       because the disparate images on the monkey's retinas activated
       disparity-selective neurons in the cortex. But when DeAngelis
       stimulated neurons that were tuned to a disparity different from
       what was indicated by the images on the retina, the monkey
       shifted its depth judgment toward the disparity signaled by the
       stimulated neurons (Figure 10.25). The results of the selective
       rearing and the microstimulation experiments indicate that
       binocular depth cells are a physiological mechanism responsible
       for depth perception, thus providing the physiology--perception
       relationship of the perceptual process in Figure 10.23.

1

Figure 10.25 While the monkey was observing a random-dot stereogram,
DeAngelis and coworkers (1998) stimulated neurons in the monkey's cortex
that were sensitive to a particular amount of disparity. This
stimulation shifted the monkey's perception of the depth of the field of
dots from position 1 to position 2.

10.6 Depth Information Across Species Humans make use of a number of
different sources of depth information in the environment. But what
about other species? Many animals have excellent depth perception. Cats
leap on their prey; monkeys swing from one branch to the next; and a
male houseﬂy maintains a constant distance of about 10 cm as it follows
a ﬂying female. There is no doubt that many animals are able to judge
distances in their environment, but what depth information do they use?
Considering the information used by different animals, we find that
animals use the entire range of cues described in this chapter. Some
animals use many cues, and others rely on just one or two. To make use
of binocular disparity, an animal must have eyes that have overlapping
visual fields. Thus, animals such as cats, monkeys, and humans that have
frontal eyes (Figure 10.26), which result in overlapping fields of view,
can use disparity to perceive depth. Animals in addition to cats,
monkeys, and humans that use disparity to perceive depth include owls
(Willigen, 2011), horses (Timney & Keil, 1999), and insects (Rossel,
1983). To

Figure 10.26 Frontal eyes, such as those of the cat, have overlapping
fields of view that provide good stereoscopic depth perception.

Seen by both eyes

Seen by right eye

Bruce Goldstein

Seen by left eye

244

Chapter 10  Perceiving Depth and Size

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Mantis Prism

"Newcastle University, UK"

Fly

(a) 
(b) 

Figure 10.27 (a) The apparatus used by Rossel (1983) to determine if the
praying mantis is sensitive to binocular disparity. The mantis is upside
down, with prisms in front of its eyes. A fly is mounted in front of the
mantis. (b) A mantis fitted with red and purple glasses that enable it
to perceive stereo projections on a screen in three dimensions.

determine if insects use disparity to perceive depth Samuel Rossel
(1983) used the praying mantis, an insect with large overlapping eye
fields. He positioned the mantis upside down (something mantises do
often) and placed prisms in front of its eyes, as shown in Figure
10.27a. When Rossel moved a fly toward the mantis and determined when
the mantis reached for the fly with its legs---a response called
striking---he found that the mantis' striking was determined by the
fly's apparent distance as determined by the strength of the prisms. In
other words, the degree of disparity was controlling the mantis'
perception of the fly's distance.

A more recent mantis experiment created a "mantis cinema" in which the
mantis wore red--purple glasses, as in Figure 10.27b. This arrangement,
which has the advantage of being able to control disparity by varying
the separation of the red and purple images on the projection screen,
has confirmed and extended Rossel's findings (Nityanada et al., 2016,
2018). Animals with lateral eyes, such as the rabbit (Figure 10.28),
have much less overlap and therefore can use disparity only in the small
area of overlap to perceive depth. Note, however, that in sacrificing
binocular disparity, animals with lateral eyes gain a wider field of
view---something that is extremely important

Seen by both eyes

Seen by right eye

© Bruce Goldstein

Seen by left eye

Figure 10.28 Lateral eyes, such as those of the rabbit, provide a
panoramic view but stereoscopic depth perception occurs only in the
small area of overlap.

10.6 Depth Information Across Species

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

245

for animals that need to constantly be on the lookout for predators. The
pigeon is an example of an animal with lateral eyes that are placed so
that the visual fields of the left and right eyes overlap only in a
35-degree area surrounding the pigeon's beak. This overlapping area,
however, happens to be exactly where pieces of grain would be located
when the pigeon is pecking at them, and psychophysical experiments have
shown that the pigeon does have a small area of binocular depth
perception right in front of its beak (McFadden, 1987; McFadden & Wild,
1986). Motion parallax is probably insects' most important method of
judging distance, and they use it in a number of different ways
(Collett, 1978; Srinivasan & Venkatesh, 1997). For example, we've
mentioned that locusts use motion parallax information generated by
moving their heads from side to side, as they observe potential prey
(p. 234). T. S. Collett (1978) measured a locust's "peering
amplitude"---the distance of this sideto-side sway---as it observed prey
at different distances, and found that the locust swayed more when
targets were farther away. Since more distant objects move less across
the retina than nearer objects for a given amount of observer movement,
a larger sway would be needed to cause the image of a far object to move
the same distance across the retina as the image of a near object. The
locust may therefore be judging distance by noting how much sway is
needed to cause the image to move a certain distance across its retina
(also see Sobel, 1990). These examples show how depth can be determined
from different sources of information in light. But bats use a form of
energy we usually associate with sound to sense depth. Bats sense
objects by using a method similar to the sonar system used in World War
II to detect underwater objects such as submarines and mines. Sonar,
which stands for sound navigation and ranging, works by sending out
pulses of sound and using information contained in the echoes of this
sound to determine the location of objects. Donald Griffin (1944) coined
the term echolocation to describe the biological sonar system used by
bats to avoid objects in the dark. Bats emit pulsed sounds that are far
above the upper limit of human hearing, and they sense objects'
distances by noting the interval between when they send out the pulse
and when they receive the echo (Figure 10.29). Since they use sound
echoes to sense objects, they can avoid obstacles even when it is
totally dark (Suga, 1990). Although we don't have any way of knowing
what the bat experiences when these echoes return, we do know that the
timing of these echoes provides the information the bat needs to locate
objects in its environment. (See the discussion of human echolocation in
Chapter 12, page 307. Also see von der Emde et al., 1998, for a
description of how electric fish sense depth based on
"electrolocation.") From these examples, we can see that animals use a
number of different types of information to determine depth and
distance, with the type of information used depending on the animal's
specific needs and on its anatomy and physiological makeup. 246

(a) 
(b) 
(c) 

Figure 10.29 When a bat sends out its pulses, it receives echoes from a
number of objects in the environment. This figure shows the echoes
received by the bat from (a) a nearby moth; (b) a tree located about 2
meters away; and (c) a house, located about 4 meters away. The echoes
from more distant objects take longer to return. The bat locates the
positions of objects in the environment by sensing how long it takes the
echoes to return.

TEST YOuRSELF 10.1 1. What is the basic problem of depth perception, and
how does the cue approach deal with this problem? 2. What monocular cues
provide information about depth in the environment? 3. What do comparing
the experience of "Stereo Sue" and the experience of viewing 3-D and 2-D
movies tell us about what binocular vision adds to our perception of
depth? 4. What is binocular disparity? What is the difference between
crossed and uncrossed disparity? What is the difference between absolute
disparity and relative disparity? How are absolute and relative
disparity related to the depths of objects in a scene? 5. What is
stereopsis? What is the evidence that disparity creates stereopsis? 6.
What does perception of depth from a random-dot stereogram demonstrate?
7. What is the correspondence problem? Has this problem been solved? 8.
Describe each of the relationships in the perceptual process of Figure
10.23, and provide examples for each relationship that has been
determined by psychophysical and physiological research on depth
perception.

Chapter 10  Perceiving Depth and Size

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

9. Describe how frontal eyes determine binocular disparity. Describe the
praying mantis experiment. What did it demonstrate? 10. Describe how
lateral eyes affect depth perception. Also describe how some insects use
motion parallax to perceive depth and how bats use echoes to sense
objects. Ground

10.7 Perceiving Size Now that we have described our perception of depth,
we will turn our attention to the perception of size. As we noted at the
start of this chapter, size perception and depth perception are related.
Consider, for example, the following story based on an actual incident
at an Antarctic research facility where a helicopter pilot was flying
through whiteout weather conditions: As Frank pilots his helicopter
across the Antarctic wastes, blinding light, reﬂected down from thick
cloud cover above and up from the pure white blanket of snow below,
makes it difficult to see the horizon, details on the surface of the
snow, or even up from down. He is aware of the danger because he has
known pilots dealing with similar conditions who ﬂew at full power
directly into the ice. He thinks he can make out a vehicle on the snow
far below, and he drops a smoke grenade to check his altitude. To his
horror, the grenade falls only three feet before hitting the ground.
Realizing that what he thought was a truck was actually a small box,
Frank pulls back on the controls and soars up, his face drenched in
sweat, as he comprehends how close he just came to becoming another
whiteout fatality. This account illustrates that our ability to perceive
an object's size can sometimes be drastically affected by our ability to
perceive the object's distance. A small box seen close up can, in the
absence of accurate information about its distance, be misperceived as a
large truck seen from far away (Figure 10.30). The idea that we can
misperceive size when accurate depth information is not present was
demonstrated in a classic experiment by A. H. Holway and Edwin Boring
(1941).

Figure 10.30 When a helicopter pilot loses the ability to perceive
distance because of a "whiteout," a small box that is close can be
mistaken for a truck that is far away.

An important feature of the test stimuli in the right corridor was that
they all cast exactly the same-sized image on the retina. We can
understand how this was accomplished by introducing the concept of
visual angle.

What Is Visual Angle? Visual angle is the angle of an object relative to
the observer's eye. Figure 10.32a shows how we determine the visual
angle of a stimulus (a person, in this example) by extending lines from
the person to the lens of the observer's eye. The angle between the
lines is the visual angle. Notice that the visual angle depends both on
the size of the stimulus and on its distance from the observer; when the
person moves closer, as in Figure 10.32b, the visual angle becomes
larger.

Visual angle = 1° Far Near

The Holway and Boring Experiment Observers in Holway and Boring's
experiment sat at the intersection of two hallways and saw a luminous
test circle when looking down the right hallway and a luminous
comparison circle when looking down the left hallway (Figure 10.31). The
comparison circle was always 10 feet from the observer, but the test
circles were presented at distances ranging from 10 feet to 120 feet. An
important property of the fixed-in-place comparison circle was that its
size could be adjusted. The observer's task on each trial was to adjust
the diameter of the comparison circle in the left corridor to match his
or her perception of the sizes of the various test circles presented in
the right corridor.

Comparison

Test Test circles (presented one at a time at different distances)

Figure 10.31 Setup of Holway and Boring's (1941) experiment. The
observer changes the diameter of the comparison circle in the left
corridor to match his or her perception of the size of test circles
presented in the right corridor. Each test circle has a visual angle of
1 degree and is presented separately. This diagram is not drawn to
scale. The actual distance of the far test circle was 100 feet.

10.7 Perceiving Size

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

247

Visual angle (a) Size of retinal image

Visual angle Observer's eye (b)

Figure 10.32 (a) The visual angle depends on the size of the stimulus
(the woman in this example) and its distance from the observer. (b) When
the woman moves closer to the observer, both the visual angle and the
size of the image on the retina increase. This example shows that
halving the distance between the stimulus and the observer doubles the
size of the image on the retina.

Jennifer Bittel

The visual angle tells us how large the object will be on the back of
the eye. There are 360 degrees around the entire circumference of the
eyeball, so an object with a visual angle of 1 degree would take up
1/360 of this circumference---about 0.3 mm in an average-sized adult
eye. One way to get a feel for visual angle is to fully extend your arm
and look at your thumb, as the woman in Figure 10.33 is doing. The
approximate visual angle of the width of the thumb at arm's length is 2
degrees. Thus, an object that is exactly covered by the thumb held at
arm's length, such as the phone in Figure 10.33, has a visual angle of
approximately 2 degrees. This "thumb technique" provides a way to
determine the approximate visual angle of any object in the environment.
It also illustrates an important property of visual angle: A small
object that is near (like the thumb) and a larger object that is far
(like the phone) can have the same visual angle. A good example of this
is illustrated in Figure 10.34, which shows a photograph taken by one of
my students. To take this picture,

Figure 10.34 The visual angle between the two fingers is the same as the
visual angle of the Eiffel Tower.

the student adjusted the distance between her fingers so that the Eiffel
Tower just fit between them. When she did this, the space between her
fingers, which were about a foot away, had the same visual angle as the
Eiffel Tower, which was hundreds of yards away.

How Holway and Boring Tested Size Perception in a Hallway The idea that
objects with different sizes can have the same visual angle was used in
the creation of the test circles in Holway and Boring's experiment. As

28

Observer's eye

Thumb 28

248

Figure 10.33 The "thumb" method of determining the visual angle of an
object. When the thumb is at arm's length, whatever its width covers has
a visual angle of about 2 degrees. The woman's thumb covers the width of
her iPhone, so the visual angle of the iPhone, from the woman's point of
view, is 2 degrees. Note that the visual angle will change if the
distance between the woman and the iPhone changes.

Chapter 10  Perceiving Depth and Size

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Size of comparison circle

Inches 30

small (point N in Figure 10.35). Thus, when good depth cues were
present, the observer's judgments of the size of the circles matched the
physical sizes of the circles. Holway and Boring then determined how
eliminating depth information would affect the observer's judgments of
size. They did this by having the observer view the test circles with
one eye, which eliminated binocular disparity (line 2 in Figure 10.35);
then by having the observer view the test circles through a peephole,
which eliminated motion parallax (line 3); and finally by adding drapes
to the hallway to eliminate shadows and reﬂections (line 4). Each time
some depth information was eliminated, the observer's judgments of the
sizes of the test circles became less accurate. When all depth
information was eliminated, the observer's perception of size was
determined not by the actual size of the test circles but by the
relative sizes of the circle's images on the observer's retinas. Because
all of the test circles in Holway and Boring's experiment had the same
retinal size, eliminating depth information caused them to be perceived
as being about the same size. Thus, the results of this experiment
indicate that size estimation is based on the actual sizes of objects
when there is good depth information (blue lines), but that size
estimation is strongly inﬂuenced by the object's visual angle when depth
information is eliminated (red lines). An example of size perception
that is determined by visual angle is our perception of the sizes of the
sun and the moon, which, by cosmic coincidence, have the same visual
angle. The fact that they have identical visual angles becomes most
obvious during an eclipse of the sun. Although we can see the ﬂaming
corona of the sun surrounding the moon, as shown in Figure 10.36, the
moon's disk almost exactly covers the disk of the sun. If we calculate
the visual angles of the sun and the moon, the result is 0.5 degrees for
both. As you can see in Figure 10.36, the moon is small (diameter 2,200
miles) but close (245,000 miles from Earth), whereas the sun is large
(diameter 865,400 miles) but far away (93 million miles from Earth).
Even though these two celestial bodies are vastly different in size, we
perceive

1

Physical size F

2

20 N

3

10

4 Visual angle

0

0

50

100

Distance of test circle (ft)

Figure 10.35 Results of Holway and Boring's experiment. The dashed line
marked physical size is the result that would be expected if the
observers adjusted the diameter of the comparison circle to match the
actual diameter of each test circle. The line marked visual angle is the
result that would be expected if the observers adjusted the diameter of
the comparison circle to match the visual angle of each test circle.

shown in Figure 10.31, small circles that were positioned close to the
observer and larger circles that were positioned farther away all had
visual angles of 1 degree. Because objects with the same visual angle
create the same-sized image on the retina, all of the test circles had
the same-sized image on the observers' retinas, no matter where in the
hallway they were located. In the first part of Holway and Boring's
experiment, many depth cues were available, including binocular
disparity, motion parallax, and shading, so the observer could easily
judge the distance of the test circles. The results, plotted in Figure
10.35, show that when the observers viewed a large test circle that was
located far away (far circle in Figure 10.31), they made the comparison
circle large (point F in Figure 10.35); when they viewed a small test
circle that was located nearby (near circle in Figure 10.31), they made
the comparison circle

0.58

Moon

2,200 miles

Sun

865,400 miles

245,000 miles

0.58

Eclipse of the sun

93,000,000 miles

Figure 10.36 The moon's disk almost exactly covers the sun during an
eclipse because the sun and the moon have the same visual angles. 10.7
Perceiving Size

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

249

DEMONSTRATION

Perceiving Size at a Distance

Bruce Goldstein

Hold a coin between the fingertips of each hand so you can see the faces
of both coins. Hold one coin about a foot from you and the other at
arm's length. Observe the coins with both of your eyes open and note
their sizes. Under these conditions, most people perceive the near and
far coins as being approximately the same size. Now close one eye, and
holding the coins so they appear side-by-side, notice how your
perception of the size of the far coin changes so that it now appears
smaller than the near coin. This demonstrates how size constancy is
decreased under conditions of poor depth information.

Figure 10.37 All of the palm trees appear to be the same size when
viewed in the environment, even though the farther ones have a smaller
visual angle.

them to be the same size because, as we are unable to perceive their
distance, we base our judgment on their visual angles. In yet another
example, we perceive objects viewed from a high-ﬂying airplane as very
small. Because we have no way of accurately estimating the distance from
the airplane to the ground, we perceive size based on objects' visual
angles, which are very small because we are so high up.

Size Constancy One of the most obvious features of the scene in Figure
10.37, on the campus of the University of Arizona, is that looking down
the row of palm trees, each more distant tree becomes smaller in the
picture. If you were standing on campus observing this scene, the more
distant trees would appear to take up less of your field of view, as in
the picture, but at the same time you would not perceive the farther
tree as shorter than the near trees. Even though the far trees take up
less of your field of view (or to put it another way, have a smaller
visual angle), they appear constant in size. The fact that our
perception of an object's size is relatively constant even when we view
the object from different distances is called size constancy. To
introduce the idea of size constancy to my perception classes, I (BG)
ask someone in the front row to estimate my height when I'm standing
about 3 feet away. Their guess is usually accurate, around 5 feet 9
inches. I then take one large step back so I am now twice as far away
and ask the person to estimate my height again. It probably doesn't
surprise you that the second estimate of my heights is about the same as
the first. The point of this demonstration is that even though my image
on our students' retinas becomes half as large when I double my distance
(compare Figures 10.32a and 10.32b), I do not appear to shrink to about
3 feet tall, but still appear to be my normal size. The following
demonstration illustrates size constancy in another way. 250

Although students often propose that size constancy works because we are
familiar with the sizes of objects, research has shown that observers
can accurately estimate the sizes of unfamiliar objects viewed at
different distances (Haber & Levin, 2001).

Size Constancy as a Calculation The link between size constancy and
depth perception has led to the proposal that size constancy is based on
a mechanism called size-- distance scaling that takes an object's
distance into account (Gregory, 1966). Size--distance scaling operates
according to the equation S 5 K(R 3 D) where S is the object's perceived
size, K is a constant, R is the size of the retinal image, and D is the
perceived distance of the object. (Since we are mainly interested in R
and D, and K is a scaling factor that is always the same, we will omit K
in the rest of our discussion.) According to the size--distance
equation, as a person walks away from you, the size of the person's
image on your retina (R) gets smaller, but your perception of the
person's distance (D) gets larger. These two changes balance each other,
and the net result is that you perceive the person's size (S) as staying
the same. DEMONSTRATION

and Emmert's Law

Size--Distance Scaling

You can demonstrate size--distance scaling to yourself by illuminating
the red circle in Figure 10.38 with your desk lamp (or increase the
brightness on your computer screen) and look at the 1 sign for about 60
seconds. Then look at the white space to the side of the circle. If you
blink, you should see the circle's afterimage floating in front of the
page. Before the afterimage fades, also look at a wall far across the
room. You should see that the size of the afterimage depends on where
you look. If you look at a distant surface, such as the far wall of the
room, you see a large afterimage that appears to be far away. If you
look at a near surface, such as a piece of paper, you see a small
afterimage that appears to be close.

Chapter 10  Perceiving Depth and Size

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

+

Figure 10.39 illustrates the principle underlying the effect you just
experienced, which was first described in 1881 by Emil Emmert
(1844--1911). Staring at the square bleached a small circular area of
visual pigment on your retina. This bleached area of the retina
determined the retinal size of the afterimage and remained constant no
matter where you were looking. The perceived size of the afterimage, as
shown in Figure 10.39, is determined by the distance of the surface
against which the afterimage is viewed. This relationship between the
apparent distance of an afterimage and its perceived size is known as
Emmert's law: The farther away an afterimage appears, the larger it will
seem. This result follows from our size--distance scaling equation, S 5
R 3 D. The size of the bleached area of pigment on the retina (R) always
stays the same, so that increasing the afterimage's distance (D)
increases the magnitude of R 3 D. We therefore perceive the size of the
afterimage (S) as larger when it is viewed against the far wall. The
size--distance scaling effect demonstrated by the afterimage
demonstration is working constantly when we look at objects in the
environment, with the visual system taking both an object's size in the
field of view (which determines retinal size) and its distance into
account to determine our perception of its size. This process, which
happens without any effort on our

Afterimage on wall

part, helps us perceive a stable environment. Just think of how
confusing it would be if objects appeared to shrink or expand just
because we happened to be viewing them from different distances.
Luckily, because of size constancy, this doesn't happen.

Other Information for Size Perception Although we have been stressing
the link between size constancy and depth perception and how
size--distance scaling works, other sources of information in the
environment also help us achieve size constancy. One source of
information for size perception is relative size. We often use the sizes
of familiar objects as a yardstick to judge the size of other objects.
Figure 10.40 shows two views of Henry Bruce's sculpture The Giant's
Chair. In Figure 10.40a, it is difficult to determine how large the
chair is; if you assume that the camera is positioned on the ground, you
might think it looks to be a nice normal size for a chair. Figure
10.40b, however, leads us to a different conclusion. The presence of a
man next to the chair indicates that the chair is extraordinarily large.
This idea that our perception of the sizes of objects can be inﬂuenced
by the sizes of nearby objects explains why we often fail to appreciate
how tall basketball players are, when all we see for comparison are
other basketball players. But as soon as a person of average height
stands next to one of these players, the player's true height becomes
evident.

Adam Burton/Getty Images

Figure 10.38 Look at the small 1 for 60 seconds.

(a) 

Retinal image of circle (bleached pigment)

Figure 10.39 The principle behind the observation that the size of an
afterimage increases as the afterimage is viewed against more distant
surfaces.

David Clapp/Getty Images

Afterimage on book

(b) 

Figure 10.40 (a) The size of the chair is ambiguous until (b) a person
is standing next to it. 10.7 Perceiving Size

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

251

The Müller-Lyer Illusion

Bruce Goldstein

In the Müller-Lyer illusion, the right vertical line in Figure 10.42
appears to be longer than the left vertical line, even though they are
both exactly the same length (measure them). A number of different
explanations have been proposed to explain this illusion. An influential
early explanation involves size--distance scaling.

Figure 10.41 Two cylinders resting on a texture gradient. The fact that
the bases of both cylinders cover the same number of units on the
gradient indicates that the bases of the two cylinders are the same
size.

Another source of information for size perception is the relationship
between objects and texture information on the ground. We saw that a
texture gradient occurs when elements that are equally spaced in a scene
appear to be more closely packed as distance increases (Figure 10.8).
Figure 10.41 shows two cylinders sitting on a texture gradient formed by
a cobblestone road. Even if we have trouble perceiving the depth of the
near and far cylinders, we can tell that they are the same size because
their bases both cover the same portion of a paving stone.

10.8 Illusions of Depth and Size Visual illusions fascinate people
because they demonstrate how our visual system can be "tricked" into
seeing inaccurately (Bach & Poloschek, 2006). We have already described
a number of types of illusions. Illusions of lightness include the
Chevreul illusion (p. 58) and Mach bands (p. 59), in which small changes
in lightness are seen near a border even though no changes are present
in the physical pattern of light. Attentional effects include change
blindness (p. 138), in which two alternating scenes appear similar even
though there are differences between them. Illusions of motion are those
in which stationary stimuli are perceived as moving (p. 179). We will
now describe some illusions of size---situations that lead us to
misperceive the size of an object. We will see that the connection
between the perception of size and the perception of depth has been used
to explain some of these illusions. We begin with the Müller-Lyer
illusion. 252

Misapplied Size Constancy Scaling Why does the Müller-Lyer display cause
a misperception of size? Richard Gregory (1966) explains the illusion on
the basis of a mechanism he calls misapplied size constancy scaling. He
points out that size constancy normally helps us maintain a stable
perception of objects by taking distance into account (as expressed in
the size--distance scaling equation). Thus, size constancy scaling
causes a 6-foot-tall person to appear 6 feet tall no matter what his
distance. Gregory proposes, however, that the very mechanisms that help
us maintain stable perceptions in the three-dimensional world sometimes
create illusions when applied to objects drawn on a two-dimensional
surface. We can see how misapplied size constancy scaling works by
comparing the left and right lines in Figure 10.42 to the left and right
lines that have been superimposed on the corners in Figure 10.43. Both
lines are the same size, but according to Gregory the lines appear to be
at different distances because the fins on the right line in Figure
10.43 make this line look like part of an inside corner of a room, and
the fins on the left line make this line look like part of a corner
viewed from outside. Because inside corners appear to "recede" and
outside corners "jut out," our size--distance scaling mechanism treats
the inside corner as if it is farther away, so the term D in the
equation S 5 R 3 D is larger and this line therefore appears longer.
(Remember that the retinal sizes, R, of the two lines are the same, so
perceived size, S, is determined by the perceived distance, D.) At this
point, you could say that although the Müller-Lyer figures may remind
Gregory of inside and outside corners, they don't look that way to you
(or at least they didn't until Gregory told you to see them that way).
But according to Gregory, it is not necessary that you be consciously
aware that these lines can represent three-dimensional structures; your
perceptual system

Figure 10.42 The Müller-Lyer illusion. Both lines are actually the same
length.

Chapter 10  Perceiving Depth and Size

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Inside corner

Bruce Goldstein

Outside corner

Figure 10.43 According to Gregory (1966), the Müller-Lyer line on the
left corresponds to an outside corner, and the line on the right
corresponds to an inside corner. Note that the two vertical lines are
the same length (measure them!).

unconsciously takes the depth information contained in the Müller-Lyer
figures into account, and your size--distance scaling mechanism adjusts
the perceived sizes of the lines accordingly. Gregory's theory of visual
illusions has not, however, gone unchallenged. For example, figures like
the dumbbells in Figure 10.44, which contain no obvious perspective or
depth, still result in an illusion. And Patricia DeLucia and Julian
Hochberg (1985, 1986, 1991; Hochberg, 1987) have shown that the
Müller-Lyer illusion occurs for a three-dimensional display like the one
in Figure 10.45. When viewed as a threedimensional display, the distance
between corners B and C appears to be greater than the distance between
A and B, even though they are the same, and it is obvious that the
spaces between the two sets of fins are not at different depths. You can
experience this effect for yourself by doing the following
demonstration.

The Müller-Lyer Illusion With Books

DEMONSTRATION

Pick three books that are the same size and arrange two of them with
their corners making a 90-degree angle and standing in positions A and
B, as shown in Figure 10.45. Then, without using a ruler, position the
third book at position C, so that distance x appears to be equal to
distance y. Check your placement, looking down at the books from the top
and from other angles as well. When you are satisfied that distances x
and y appear about equal, measure the distances with a ruler. How do
they compare?

A

B

x

Figure 10.44 The "dumbbell" version of the Müller-Lyer illusion. As in
the original Müller-Lyer illusion, the two straight lines are actually
the same length.

C

y

Figure 10.45 A three-dimensional Müller-Lyer illusion. The 2-foothigh
wooden "fins" stand on the floor. Although the distances x and y are the
same, distance y appears larger, just as in the twodimensional
Müller-Lyer illusion. 10.8 Illusions of Depth and Size

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

253

Conﬂicting Cues Theory R. H. Day (1989, 1990) has proposed the
conﬂicting cues theory, which states that our perception of line length
depends on two cues: (1) the actual length of the vertical lines and (2)
the overall length of the figure. According to Day, these two conﬂicting
cues are integrated to form a compromise perception of length. Because
the overall length of the figure with outward-oriented fins is greater
(Figure 10.42), the vertical line appears longer. Another version of the
Müller-Lyer illusion, shown in Figure 10.46, results in the perception
that the space between the dots is greater in the lower figure than in
the upper figure, even though the distances are actually the same.
According to Day's conﬂicting cues theory, the space in the lower figure
appears greater because the overall extent of the figure is greater.
Notice that conﬂicting cues theory can also be applied to the dumbbell
display in Figure 10.44. Thus, although Gregory believes that depth
information is involved in determining illusions, Day rejects this idea
and proposes that cues for length are what is important. Let's now look
at some more examples of illusions and the mechanisms that have been
proposed to explain them.

The Ponzo Illusion In the Ponzo (or railroad track) illusion, shown in
Figure 10.47, both animals are the same size on the page, and so have
the same visual angle, but the one on top appears longer. According to
Gregory's misapplied scaling explanation, the top animal appears bigger
because of depth information provided by the converging railroad tracks
that

(a) 
(b) 

Figure 10.46 An alternate version of the Müller-Lyer illusion. We
perceive that the distance between the dots in (a) is less than the
distance in (b), even though the distances are the same. (From Day,
1989) 254

William Vann/www.edupic.net

If you set distance y so that it was smaller than distance x, this is
exactly the result you would expect from the twodimensional Müller-Lyer
illusion, in which the distance between the outward-facing fins appears
enlarged compared to the distance between the inward-facing fins. You
can also duplicate the illusion shown in Figure 10.45 with your books by
using your ruler to make distances x and y equal. Then, notice how the
distances actually appear. The fact that we can create the Müller-Lyer
illusion by using three-dimensional stimuli such as these, along with
demonstrations like the dumbbell in Figure 10.44, is difficult for
Gregory's theory to explain.

Figure 10.47 The Ponzo (or railroad track) illusion. The two animals are
the same length on the page (measure them), but the far one appears
larger.

make the top animal appear farther away. Thus, just as in the
Müller-Lyer illusion, the scaling mechanism corrects for this apparently
increased depth (even though there really isn't any, because the
illusion is on a ﬂat page), and we perceive the top animal to be larger.
(Also see Prinzmetal et al., 2001, and Shimamura & Prinzmetal, 1999, for
another explanation of the Ponzo illusion.)

The Ames Room The Ames room causes two people of equal size to appear
very different in size (Ittelson, 1952). In Figure 10.48a, you can see
that the woman on the left looks much taller than the man on the right,
but when they change sides, in Figure 10.48b, the man appears much
larger than the woman. This perception occurs even though both people
are actually about the same height. The reason for this erroneous
perception of size lies in the construction of the room. The shapes of
the wall and the windows at the rear of the room make it look like a
normal rectangular room when viewed from a particular observation point;
however, as shown in the diagram in Figure 10.49, the Ames room is
actually shaped so that the right corner of the room is almost twice as
far from the observer as the left corner. What's happening in the Ames
room? The construction of the room causes the person on the right to
have a much smaller visual angles than the person on the left. We think
that we are looking into a normal rectangular room at two people who
appear to be at the same distance, so we perceive the one with the
smaller visual angle as shorter. We can understand why

Chapter 10  Perceiving Depth and Size

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

this occurs by returning to our size--distance scaling equation, S 5 R 3
D. Because the perceived distance (D) is the same for the two people,
but the size of the retinal image (R) is smaller for the people on the
right, their perceived size (S) is smaller. Another explanation for the
Ames room is based not on size--distance scaling but on relative size.
The relative size explanation states that our perception of the size of
the two people is determined by how they fill the distance between the
bottom and top of the room. Because the person on the left fills the
entire space and the person on the right occupies only a little of it,
we perceive the person on the left as taller (Sedgwick, 2001).

SOMETHING TO CONSIDER:

(a) 

STEPHANIE PILICK/Getty Images

The Changing Moon

(b) 

Figure 10.48 The Ames room. Although the man and woman are about the
same height, (a) the woman appears taller or (b) the man appears taller
because of the distorted shape of the room.

Every so often the moon makes the news. "A supermoon is going to
happen," it is proclaimed, because the moon is going to be closer than
usual to the earth. This closeness, which is caused by the moon's
slightly elliptical orbit, is claimed to make the supermoon appear
larger than usual. In reality, however, the supermoon is only about 14
percent larger than normal, an effect that falls far short of "super,"
and which most people wouldn't even notice if they hadn't heard about it
in the news. The best way to perceive the moon as larger, it turns out,
is not to rely on a slight change in the moon's distance, but to rely on
your mind. You have probably noticed that when the moon is on the
horizon it appears much larger than when it is high in the sky, an
effect called the moon illusion (Figure 10.50). We say that this effect
is caused by your mind, because the visual angles of the horizon moon
and the elevated moon are

Peephole

Twice as far from observer as the woman on the left.

Figure 10.49 The Ames room, showing its true shape. The person on the
right is actually almost twice as far away from the observer as the
person on the left; however, when the room is viewed through the
peephole, this difference in distance is not seen. In order for the room
to look normal when viewed through the peephole, it is necessary to
enlarge the right side of the room.

Figure 10.50 An artist's conception of the how the moon is perceived
when it is on the horizon and when it is high in the sky. Note that the
visual angle of the horizon moon is depicted as larger than the visual
angle of the moon high in the sky. This is because the picture is
simulating the illusion. In the environment, the visual angles of the
two moons are the same. Something to Consider: The Changing Moon

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

255

J.Lopez Photography/Shutterstock

Rising moon over LA Figure 10.51 Time-lapse photograph showing that the
moon's visual angle remains constant as it rises above the horizon.

the same. This must be so because the moon's physical size (2,200 miles
in diameter) stays the same and it remains the same distance from Earth
(245,000 miles) throughout the night. This constancy of the moon's
visual angle is shown in Figure 10.51, which is a time-lapse photograph
of the moon rising. The camera, which is not susceptible to the moon
illusion, just records the size of the moon's image in the sky. You can
demonstrate this constancy to yourself by viewing the moon when it is on
the horizon and when it is high in the sky through a
quarter-inch-diameter hole held at about arm's length. For most people,
the moon just fits inside this hole, wherever it is in the sky. What
causes the mind to enlarge the horizon moon? According to apparent
distance theory, the answer has to do with the perceived distance of the
moon. The moon on the horizon appears more distant because it is viewed
across the filled space of the terrain, which contains depth
information. However, when the moon is higher in the sky, it appears
less distant because it is viewed through empty space, which contains
little depth information. The idea that the horizon is perceived as
farther away than the sky overhead is supported by the fact that when
people estimate the distance to the horizon and the distance to the sky
directly overhead, they report that the horizon appears to be farther
away. That is, the heavens appear "ﬂattened" (Figure 10.52).

The key to the moon illusion, according to apparent distance theory, is
the size--distance scaling equation, S 5 R 3 D. Retinal size, R, is the
same for both locations of the moon (since the visual angle is always
the same no matter where the moon appears in the sky), but D is greater
when the moon is on the horizon, so it appears larger (Kaufman &
Kaufman, 2000). This is the principle we invoked in the Emmert's law
demonstration to explain why the afterimage appears larger when it is
viewed against a faraway surface (King & Gruber, 1962). Lloyd Kaufman
and Irvin Rock (1962a, 1962b) did a number of experiments that support
the apparent distance theory. In one of their experiments, they showed
that when the horizon moon was viewed over the terrain, which made it
seem farther away, it appeared 1.3 times larger than the elevated moon;
however, when the terrain was masked off so that the horizon moon was
viewed through a hole in a sheet of cardboard, the illusion vanished.
Some researchers, however, question the idea that the horizon moon
appears farther, as shown in the ﬂattened heavens effect in Figure
10.52, because some observers see the horizon moon as ﬂoating in space
in front of the sky (Plug & Ross, 1994). Another theory of the moon
illusion, the angular size contrast theory, proposes that the high in
the sky moon appears smaller because the large expanse of sky
surrounding it makes it appear smaller by comparison. However, when the
moon is on the horizon, less sky surrounds it, so it appears larger
(Baird et al., 1990). Even though scientists have been proposing
theories to explain the moon illusion for hundreds of years, there is
still no agreement on an explanation (Hershenson, 1989). Apparently a
number of factors are involved, in addition to the ones we have
considered here, including atmospheric perspective (looking through haze
on the horizon can increase size perception), color (redness increases
perceived size), and oculomotor factors (convergence of the eyes, which
tends to occur when we look toward the horizon and can cause an increase
in perceived size; Plug & Ross, 1994). Just as many different sources of
depth information work together to create our impression of depth, many
different factors may work together to create the moon illusion, and
perhaps the other illusions as well.

Elevated moon "Flattened heavens"

Same visual angle Horizon moon H

256

Figure 10.52 When observers are asked to consider the sky as a surface
and to compare the distance to the horizon (H) and the distance to the
top of the sky on a clear moonless night, they usually say that the
horizon appears farther away. This results in the "flattened heavens"
shown here.

Chapter 10  Perceiving Depth and Size

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

DEVELOPMENTAL DIMENSION Infant Depth Perception At what age are infants
able to use different kinds of depth information? A 3-day-old infant,
sitting in a specially designed baby seat in a dark room, sees an optic
flow stimulus (see Chapter 7, page 150) made up of dots moving on
monitors placed on either side of the infant's head. When the flow is
moving from front to back, like what happens when moving forward in
depth (see Figure 7.3a), the infant's head pushes backward, with more
pressure for higher rates of flow. Thus, 3-day-old infants are sensitive
to optic flow (Jouen et al., 2000). And at the age of 3 weeks, infants
blink in response to a stimulus that appears to be moving toward their
face (Nanez, 1988). Both of these observations indicate that behaviors
related to depth are present in infants less than a month old. But what
about the kinds of depth information we have described in this chapter?
Different types of information become operative at different times. We
first consider binocular disparity, which becomes effective between 3
and 6 months of age, and then pictorial depth cues, which become
effective slightly later, between 4 and 7 months.

infant whose visual system cannot yet use disparity information, all he
or she sees is a random collection of dots. In Fox's experiment, an
infant wearing special viewing glasses was seated in his or her mother's
lap in front of a television screen (Figure 10.53). The child viewed a
random-dot stereogram that appeared, to an observer sensitive to
disparity information, as a rectangle-in-depth, moving either to the
left or to the right. Fox's premise was that an infant sensitive to
disparity will move his or her eyes to follow the moving rectangle. He
found that infants younger than about 3 months of age would not follow
the rectangle, but that infants between 3 and 6 months of age would
follow it. He therefore concluded that the ability to use disparity
information to perceive depth emerges sometime between 3½ and 6 months
of age. This time for the emergence of binocular depth perception has
been confirmed by other research using a variety of different methods
(Held et al., 1980; Shimojo et al., 1986; Teller, 1997).

Binocular Disparity

Another type of depth information is provided by pictorial cues. These
cues develop later than disparity, presumably because they depend on
experience with the environment and the development of cognitive
capabilities. In general, infants begin to use pictorial cues such as
overlap, familiar size, relative size, shading, linear perspective, and
texture gradients sometime between about 4 and 7 months of age (Kavšek
et al., 2009; Shuwairi & Johnson, 2013; Yonas et al., 1982). We will
describe research on two of these cues: familiar size and cast shadows.

One requirement for the operation of binocular disparity is that the
eyes must be able to binocularly fixate, so that the two eyes are both
looking directly at the object and the two foveas are directed to
exactly the same place. Newborns have only a rudimentary, imprecise
ability to fixate binocularly, especially on objects that are changing
in depth (Slater & Findlay, 1975). Richard Aslin (1977) determined when
binocular fixation develops by making some simple observations. He
filmed infants' eyes while he moved a target back and forth between 12
cm and 57 cm from the infant. When the infant is directing both eyes at
a target, the eyes should diverge (rotate outward) as the target moves
away and should converge (rotate inward) as the target moves closer.
Aslin's films indicate that although some divergence and convergence do
occur in 1- and 2-monthold infants, these eye movements do not reliably
direct both eyes toward the target until about 3 months of age. Although
binocular fixation may be present by 3 months of age, this does not
guarantee that the infant can use the resulting disparity information to
perceive depth. To determine when infants can use this information to
perceive depth, Robert Fox and coworkers (1980) presented random-dot
stereograms to infants ranging in age from 2 to 6 months (see page 241
to review random-dot stereograms). The beauty of random-dot stereograms
is that the binocular disparity information in the stereograms results
in stereopsis. This occurs only (1) if the stereogram is observed with a
device that presents one picture to the left eye and the other picture
to the right eye, and (2) if the observer's visual system can convert
this disparity information into the perception of depth. Thus, if we
present a random-dot stereogram to an

Pictorial Cues

Depth From Familiar Size Granrud and coworkers (1985) conducted a
two-part experiment to see whether infants

Figure 10.53 The setup used by Fox et al. (1980) to test infants'
ability to use binocular disparity information. If the infant can use
disparity information to see depth, he or she sees a rectangle moving
back and forth in front of the screen. (From Shea et al., 1980)

Continued Something to Consider: The Changing Moon

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

257

Familiarization

Test

Appears closer if infant remembers it was small (a)

(b) 
(c) 
(d) 

Figure 10.54 Stimuli for Granrud et al.'s (1985) familiar size
experiment. See text for details. (From Granrud et al., 1985)

can use their knowledge of the sizes of objects to help them perceive
depth. In the familiarization period, 5- and 7-monthold infants played
with a pair of wooden objects for 10 minutes. One of these objects was
large (Figure 10.54a), and one was small (Figure 10.54b). In the test
period, about a minute after the familiarization period, objects (c) and
(d) were the same size and were presented at the same distance from the
infant. The prediction was that infants sensitive to familiar size would
perceive the object at (c) to be closer if they remembered, from the
familiarization period, that this shape was smaller than the other one.
If the infant remembered the green object as being small, then seeing it
as big in their field of view could lead the infant to think it was the
same small object, but located much closer. How can we determine whether
an infant perceives one object as closer than another? The most widely
used method is observing an infant's reaching behavior. METHOD

Preferential Reaching

The preferential reaching procedure is based on observations that
infants as young as 2 months old will reach for nearby objects and that
5-month-old infants are extremely likely to reach for an object that is
placed within their reach and unlikely to reach for an object that is
beyond their reach (Yonas & Hartman, 1993). Infants' sensitivity to
depth has therefore been measured by presenting two objects side by
side. As with the preferential looking procedure (Chapter 3, page 60),
the left--right position of the objects is changed across trials. The
ability to perceive depth is inferred when the infant consistently
reaches more for the object that contains information indicating it is
closer. When a real depth difference is presented, infants use binocular
information and reach for the closer object almost 100 percent of the
time. To test infants' use of pictorial depth information only, an eye
patch is placed on one eye (this eliminates the availability of
binocular information, which overrides pictorial depth cues). If infants
are sensitive to the pictorial depth information, they reach for the
apparently closer object approximately 60 percent of the time.

258

When Granrud and coworkers presented the objects to infants,
7-month-olds reached for object (c), as would be predicted if they
perceived it as being closer than object (d). The 5-month-olds, however,
did not reach for object (c), which indicated that these infants did not
use familiar size as information for depth. Thus, the ability to use
familiar size to perceive depth appears to develop sometime between 5
and 7 months. This experiment is interesting not only because it
indicates when the ability to use familiar size develops, but also
because the infant's response in the test phase depends on a cognitive
ability---the ability to remember the sizes of the objects that he or
she played with in the familiarization phase. The 7-monthold infant's
depth response in this situation is therefore based on both what is
perceived and what is remembered.

Depth From Cast Shadows We know that shadows provide information
indicating an object's position relative to a surface, as occurred in
Figure 10.9 (p. 233). To determine when this ability is present in
infants, Albert Yonas and Carl Granrud (2006) presented 5- and
7-month-old infants with a display like the one in Figure 10.55. Adults
and older children consistently report that the object on the right
appears nearer than the object on the left. When the infants viewed this
display monocularly (to eliminate binocular depth information that would
indicate that the objects were actually flat), the 5-month-old infants
reached for both the right and left objects on 50 percent of the trials,
indicating no preference for the right object. However, the 7-monthold
infants reached for the right object on 59 percent of the trials. Yonas
and Granrud concluded from this result that 7-month-old infants perceive
depth information provided by cast shadows. This finding fits with other
research that indicates that sensitivity to pictorial depth cues
develops between 5 and 7 months (Kausek et al., 2009). But what makes
these results especially interesting is that they imply that the infants
were able to tell that the dark areas under the toy were shadows

Figure 10.55 Stimuli presented to 7-month-old children in Granrud et
al.'s (1985) familiar size experiment: Left: familiarization stimulus;
Right: test stimulus.

Chapter 10  Perceiving Depth and Size

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

and not dark markings on the wall. It is likely that this ability, like
the other pictorial depth cues, is based largely on learning from
interacting with objects in the environment. In this case, infants need
to know something about shadows, including an understanding that most
light comes from above (see page 105). The research we have described
shows that infant depth perception develops during the first year of
life, beginning

TEST YOuRSELF 10.2 1. Describe the Holway and Boring experiment. What do
the results of this experiment tell us about how size perception is
inﬂuenced by depth perception? 2. What are some examples of situations
in which our perception of an object's size is determined by the
object's visual angle? Under what conditions does this occur? 3. What is
size constancy, and under what conditions does it occur? 4. What is
size--distance scaling? How does it explain size constancy? 5. Describe
two types of information (other than depth) that can inﬂuence our
perception of size.

with involuntary responses to motion and potentially threatening stimuli
in the first weeks, to binocular disparity at 3 to 6 months and
pictorial depth cues at 4 to 7 months. As impressive as this early
development is, it is also important to realize that it takes many
years, stretching into late childhood, before the different sources of
depth information become coordinated and integrated with each other to
achieve the adult experience of depth perception (Nardini et al., 2010).

6.  Describe how illusions of size, such as the Müller-Lyer illusion,
    the Ponzo illusion, the Ames room, and the moon illusion, can be
    explained in terms of size--distance scaling.
7.  What are some problems with the size--distance scaling explanation
    of (a) the Müller-Lyer illusion and (b) the moon illusion? What
    alternative explanations have been proposed?
8.  What is the evidence that infants have some responses to
    depth-related stimuli in the first month of life?
9.  Describe experiments that showed when infants can perceive depth
    using binocular disparity and using pictorial (monocular) cues.
    Which develops first? What methods were used?

Think About It 1. One of the triumphs of art is creating the impression
of depth on a two-dimensional canvas. Go to a museum or look at pictures
in an art book, and identify the depth information that helps increase
the perception of depth in these pictures. You may also notice that you
perceive less depth in some pictures, especially abstract ones. In fact,
some artists purposely create pictures that are perceived as "ﬂat." What
steps do these artists have to take to accomplish this? (p. 231) 2.
Texture gradients are said to provide information for depth perception
because elements in a scene become more densely packed as distance
increases. The examples of texture gradients in Figures 10.6 and 10.8
contain regularly spaced elements that extend over large distances. But

regularly spaced elements are more the exception than the rule in the
environment. Make an informal survey of your environment, both inside
and outside, and decide (a) whether texture gradients are present in
your environment and (b) if you think the principle behind texture
gradients could contribute to the perception of depth even if the
texture information in the environment is not as obvious as in the
examples in this chapter. (p. 233) 3. How could you determine the
contribution of binocular vision to depth perception? One way would be
to close one eye and notice how this affects your perception. Try this,
and describe any changes you notice. Then devise a way to quantitatively
measure the accuracy of depth perception that is possible with two-eyed
and one-eyed vision. (p. 249)

Key Terms Absolute disparity (p. 239) Accommodation (p. 231) Accretion
(p. 234) Ames room (p. 254) Angle of disparity (p. 239)

Angular size contrast theory (p. 256) Apparent distance theory (p. 256)
Atmospheric perspective (p. 233) Binocular depth cell (p. 243) Binocular
disparity (p. 238)

Binocularly fixate (p. 257) Conﬂicting cues theory (p. 254) Convergence
(p. 231) Correspondence problem (p. 242) Corresponding retinal points
(p. 238) Key Terms

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

259

Crossed disparity (p. 240) Cue approach to depth perception (p. 230)
Deletion (p. 234) Disparity-selective cell (p. 243) Disparity tuning
curve (p. 243) Echolocation (p. 246) Emmert's law (p. 251) Familiar size
(p. 231) Frontal eyes (p. 244) Horopter (p. 239) Lateral eyes (p. 245)
Misapplied size constancy scaling (p. 252)

260

Monocular cue (p. 231) Moon illusion (p. 255) Motion parallax (p. 234)
Müller-Lyer illusion (p. 252) Noncorresponding points (p. 239) Occlusion
(p. 230) Oculomotor cue (p. 231) Perspective convergence (p. 232)
Pictorial cue (p. 231) Ponzo illusion (p. 254) Random-dot stereogram
(p. 240) Relative disparity (p. 240) Relative height (p. 231)

Relative size (p. 232) Size constancy (p. 250) Size--distance scaling
(p. 196) Stereopsis (p. 240) Stereoscope (p. 241) Stereoscopic depth
perception (p. 236) Stereoscopic vision (p. 236) Strabismus (p. 236)
Texture gradient (p. 233) Uncrossed disparity (p. 240) Visual angle
(p. 247)

Chapter 10  Perceiving Depth and Size

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

This picture of hair cell receptors in the inner ear, created by
scanning electron micrography and colored to stand out from the
surrounding structure, provides an up-close look at the place where
hearing begins, when pressure waves set these receptors into motion,
creating electrical signals which are sent to the brain. SPL/Science
Source

Learning Objectives After studying this chapter, you will be able to ...
■■ Describe the physical aspects of sound, including sound waves,

tones, sound pressure, and sound frequencies. ■■ Describe the perceptual
aspects of sound, including thresholds,

loudness, pitch, and timbre. ■■ Identify the basic structures of the ear
and describe how sound

acts on these structures to cause electrical signals.

■■ Understand evidence supporting the idea that perceiving a

tone's pitch depends on both where vibrations occur in the inner ear and
on the timing of these vibrations. ■■ Describe what happens as nerve
impulses travel along the path-

way that leads from the ear to the cortex, and how pitch is represented
in the cortex.

■■ Describe how different frequencies of sound vibrations

■■ Describe some of the mechanisms responsible for hearing loss.

are translated into neural activity in the auditory nerve.

■■ Describe procedures that have been used to measure infants' thresh-

olds for hearing and their ability to recognize their mother's voice.

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

C hapte r 1 1

Hearing

Chapter Contents 11.1 Physical Aspects of Sound Sound as Pressure
Changes Pure Tones METHOD: Using Decibels to Shrink

Large Ranges of Pressures

Complex Tones and Frequency Spectra

11.2 Perceptual Aspects of Sound Thresholds and Loudness Pitch Timbre
Test Yourself 11.1

11.3 From Pressure Changes to Electrical Signals The Outer Ear The
Middle Ear The Inner Ear

11.4 How Frequency Is Represented in the Auditory Nerve

11.6 The Physiology of Pitch Perception: The Brain

Békésy Discovers How the Basilar Membrane Vibrates The Cochlea Functions
as a Filter

The Pathway to the Brain Pitch and the Brain

METHOD: Neural Frequency

Tuning Curves

The Outer Hair Cells Function as Cochlear Amplifiers Test Yourself 11.2

there a sound? (p. 264) ■■ How do sound vibrations inside the ear lead
to the percep-

tion of different pitches? (p. 280) ■■ How can sound damage the auditory
receptors? (p. 284)

J

ill Robbins, a student in my class, wrote the following about the
importance of hearing in her life:

Hearing has an extremely important function in my life. I was born
legally blind, so although I can see, my vision is highly impaired and
is not correctable. Even though I am not usually shy or embarrassed,
sometimes I do not want to call attention to myself and my
disability.... There are many methods that I can use to improve my sight
in class, like sitting close to the board or copying from a friend, but
sometimes these things are impossible. Then I use my hearing

SOMETHING TO CONSIDER: Explaining

Sound to an 11-Year Old Hearing

Place and Pitch Temporal Information and Pitch Problems Remaining to Be
Solved

■■ If a tree falls in the forest and no one is there to hear it, is

Presbycusis Noise-Induced Hearing Loss Hidden Hearing Loss

DEVELOPMENTAL DIMENSION: Infant

11.5 The Physiology of Pitch Perception: The Cochlea

Some Questions We Will Consider:

11.7 Hearing Loss

Thresholds and the Audibility Curve Recognizing Their Mother's Voice
Test Yourself 11.3 THINK ABOUT IT

to take notes.... My hearing is very strong. While I do not need my
hearing to identify people who are very close to me, it is definitely
necessary when someone is calling my name from a distance. I can
recognize their voice, even if I cannot see them. Hearing is extremely
important to Jill because of her reduced vision. But even people with
clear vision depend on hearing more than they may realize. Unlike
vision, which depends on light traveling from objects to the eye, sound
travels around corners to make us aware of events that otherwise would
be invisible. For example, in my office in the psychology department, I
hear things that I would be unaware of if I had to rely only on my sense
of vision: people talking in the hall; a car passing by on the street
below; an ambulance, siren blaring, heading up the hill toward the
hospital. If it weren't for hearing, my world at this particular moment
would be limited to what I can see in my office and the scene directly
outside my window. Although the silence might make it easier to
concentrate on writing this book, without hearing I would be unaware of
many of the events in my environment.

263

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Our ability to hear events that we can't see serves an important
signaling function for both animals and humans. For an animal living in
the forest, the rustle of leaves or the snap of a twig may signal the
approach of a predator. For humans, hearing provides signals such as the
warning sound of a smoke alarm or an ambulance siren, the distinctive
high-pitched cry of a baby who is distressed, or telltale noises that
indicate problems in a car engine. Hearing not only informs us about
things that are happening that we can't see, but perhaps most important
of all, it adds richness to our lives through music and facilitates
communication by means of speech. This chapter is the first of four
chapters on hearing. We begin, as we did for vision, by asking some
basic questions about the stimulus: How can we describe the pressure
changes in the air that is the stimulus for hearing? How is the stimulus
measured? What perceptions does it cause? We then describe the anatomy
of the ear and how the pressure changes make their way through the
structures of the ear in order to stimulate the receptors for hearing.
Once we have established these basic facts about auditory system
stimulus and structure, we consider one of the central questions of
auditory research: What is the physiological mechanism for our
perception of pitch, which is the quality that orders notes on a musical
scale, as when we go from low to high pitches by moving from left to
right on a piano keyboard? We will see that the search for the
physiological mechanism of pitch has led to a number of different
theories and that, although we understand a great deal about how the
auditory system creates pitch, there are still problems that remain to
be solved. Near the end of this chapter, we complete our description of
the structure of the auditory system by describing the pathway from the
ear to the auditory cortex. This sets the stage for the next three
chapters, in which we will expand our horizons beyond pitch to consider
how hearing occurs in the natural environment, which contains many sound
sources (Chapter 12), and also what mechanisms are responsible for our
ability to perceive complex stimuli like music (Chapter 13) and speech
(Chapter 14). The starting point for all of this is the perceptual
process that we introduced in Chapter 1, which begins with the distal
stimulus---the stimulus in the environment. The distal stimulus for
vision, in our example in Figure 1.4 (page 7), was a tree, which our
observer was able to see because light was reflected from the tree into
the eye. Information about the tree, transmitted by the light, then
created a representation of the tree on the visual receptors. But what
happens when a bird, perched on the tree, sings? The back and forth
action of the bird's vocal organ is transformed into a sound
stimulus---pressure changes in the air. These pressure changes trigger a
sequence of events that results in a representation of the bird's song
within the ears, the sending of neural signals to the brain, and our
eventual perception of the bird's song. We will see that sound stimuli
can be simple repeating pressure changes, like those often used in
laboratory research, or more complex pressure changes such as those
produced by our singing bird, musical instruments, or a 264

person talking. The properties of these air pressure changes determine
our ability to hear and are translated into sound qualities such as soft
or loud, low-pitched or high-pitched, mellow or harsh. We begin by
describing sound stimuli and their effects.

11.1 Physical Aspects of Sound The first step in understanding hearing
is to define what we mean by sound and to describe the characteristics
of sound. One way to answer the question "What is sound?" is to consider
the following question: If a tree falls in the forest and no one is
there to hear it, is there a sound? This question is useful because it
shows that we can use the word sound both as a physical stimulus and as
a perceptual response. The answer to the question about the tree depends
on which of the following definitions of sound we use. Physical
definition: Sound is pressure changes in the air or other medium. ■■
Perceptual definition: Sound is the experience we have when we hear. ■■

The answer to the question "Is there a sound?" is "yes" if we are using
the physical definition, because the falling tree causes pressure
changes whether or not someone is there to hear them. The answer to the
question is "no" if we are using the perceptual definition, because if
no one is in the forest, there will be no experience. This difference
between physical and perceptual is important to be aware of as we
discuss hearing in this chapter and the next three (also see page 268).
Luckily, it is usually easy to tell from the context in which the terms
are used whether "sound" refers to the physical stimulus or to the
experience of hearing. For example, "the piercing sound of the trumpet
filled the room" refers to the experience of sound, but "the sound had a
frequency of 1,000 Hz" refers to sound as a physical stimulus. In
general, we will use the term "sound" or "sound stimulus" to refer to
the physical stimulus and "sound perception" to refer to the experience
of sound. We begin by describing sound as a physical stimulus.

Sound as Pressure Changes A sound stimulus occurs when the movements or
vibrations of an object cause pressure changes in air, water, or any
other elastic medium that can transmit vibrations. Let's begin by
considering a loudspeaker, which is really a device for producing
vibrations to be transmitted to the surrounding air. In extreme cases,
such as standing near a speaker at a rock concert, these vibrations can
be felt, but even at lower levels, the vibrations are there. The
speaker's vibrations affect the surrounding air, as shown in Figure
11.1a. When the diaphragm of the speaker

Chapter 11  Hearing

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

(a)

Figure 11.1 (a) The effect of a vibrating speaker diaphragm on the
surrounding air. Dark areas represent regions of high air pressure, and
light areas represent areas of low air pressure. (b) When a pebble is
dropped into still water, the resulting ripples appear to move outward.
However, the water is actually moving up and down, as indicated by
movement of the boat. A similar situation exists for the sound waves
produced by the speaker in (a).

Decrease in pressure (rarefaction)

(b) 

moves out, it pushes the surrounding air molecules together, a process
called compression, which causes a slight increase in the density of
molecules near the diaphragm. This increased density results in a local
increase in the air pressure above atmospheric pressure. When the
speaker diaphragm moves back in, air molecules spread out to fill in the
increased space, a process called rarefaction. The decreased density of
air molecules caused by rarefaction causes a slight decrease in air
pressure. By repeating this process hundreds or thousands of times a
second, the speaker creates a pattern of alternating high- and
low-pressure regions in the air, as neighboring air molecules affect
each other. This pattern of air pressure changes, which travels through
air at 340 meters per second (and through water at 1,500 meters per
second), is called a sound wave. You might get the impression from
Figure 11.1a that this traveling sound wave causes air to move outward
from the speaker into the environment. However, although air pressure
changes move outward from the speaker, the air molecules at each
location move back and forth but stay in about the same place. What is
transmitted is the pattern of increases and decreases in pressure that
eventually reach the listener's ear. What is actually happening is
analogous to the ripples created by a pebble dropped into a still pool
of water (Figure 11.1b). As the ripples move outward from the pebble,
the water at any particular place moves up and down. The fact that the
water does not move forward becomes obvious when you realize that the
ripples would cause a toy boat to bob up and down---not to move outward.

Pure Tones To describe the pressure changes associated with sound, we
will first focus on a simple kind of sound wave called a pure tone. A
pure tone occurs when changes in air pressure occur in a pattern
described by a mathematical function called a sine wave, as shown in
Figure 11.2. Tones with this pattern of pressure changes are
occasionally found in the environment. A

Increased pressure

Air pressure

Increase in pressure (compression)

Amplitude Atmospheric pressure

Time

Decreased pressure (a)

One cycle

(b) 

Figure 11.2 (a) Plot of sine-wave pressure changes for a pure tone. (b)
Pressure changes are indicated, as in Figure 11.1, by darkening
(pressure increased relative to atmospheric pressure) and lightening
(pressure decreased relative to atmospheric pressure).

person whistling or the high-pitched notes produced by a flute are close
to pure tones. Tuning forks, which are designed to vibrate with a
sine-wave motion, also produce pure tones. For laboratory studies of
hearing, computers generate pure tones that cause a speaker diaphragm to
vibrate in and out with a sine-wave motion. This vibration can be
described by noting its frequency---the number of cycles per second that
the pressure changes repeat---and its amplitude---the size of the
pressure change. 11.1 Physical Aspects of Sound

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

265

High

Perception: High Pitch

Table 11.1 Relative Amplitudes and Decibels

for Environmental Sounds

Frequency (physical)

Sound

Perception: Low Pitch

Low 1/100 second

Figure 11.3 Three different frequencies of a pure tone. Higher
frequencies are associated with the perception of higher pitches.

Sound Frequency Frequency, the number of cycles per second that the
change in pressure repeats, is measured in units called Hertz (Hz), in
which 1 Hz is 1 cycle per second. Figure 11.3 shows pressure changes for
three frequencies, ranging from high (top) to low (bottom). The middle
stimulus in Figure 11.3, which repeats five times in 1/100 second, is a
500-Hz tone. As we will see, humans can perceive frequencies ranging
from about 20 Hz to about 20,000 Hz, with higher frequencies usually
being associated with higher pitches. Sound Amplitude and the Decibel
Scale One way to specify a sound's amplitude would be to indicate the
difference in pressure between the high and low peaks of the sound wave.
Figure 11.4 shows three pure tones with different amplitudes.

High

Perception: Louder

Relative Amplitude

Decibels (DB)

Barely audible (­threshold)

1

0

Leaves rustling

10

20

Quiet residential ­community

100

40

Average speaking voice

1,000

60

Express subway train

100,000

100

Propeller plane at takeoff

1,000,000

120

Jet engine at takeoff (pain threshold)

10,000,000

140

The range of amplitudes we can encounter in the environment is extremely
large, as shown in Table 11.1, which indicates the relative amplitudes
of environmental sounds, ranging from a whisper to a jet taking off.
When we discuss how amplitude is perceived, later in the chapter, we
will see that the amplitude of a sound wave is associated with the
loudness of a sound. We can dramatize how large the range of amplitudes
is as follows: If the pressure change plotted in the middle record of
Figure 11.4, in which the sine wave representing a nearthreshold sound
like a whisper is about ½-inch high on the page, then in order to plot
the graph for a very loud sound, such as music at a rock concert, you
would need to represent the sine wave by a curve several miles high!
Because this is somewhat impractical, auditory researchers have devised
a unit of sound called the decibel (dB), which converts this large range
of sound pressures into a more manageable scale.

Using Decibels to Shrink Large Ranges of Pressures

METHOD

The following equation is used for transforming sound pressure level
into decibels: Air pressure (physical)

dB = 20 × logarithm10 (p/po)

Perception: Softer

Low Time

Figure 11.4 Three different amplitudes of a pure tone. Larger amplitude
is associated with the perception of greater loudness. 266

The key term in this equation is "logarithm." Logarithms are often used
in situations in which there are extremely large ranges. One example of
a large range can be seen in a classic animated film by Charles Eames
(1977) called Powers of Ten. The first scene shows a person lying on a
picnic blanket on a beach. The camera then zooms out, as if the person
were being filmed from a spaceship taking off. The rate of zoom
increases by a factor of 10 every 10 seconds, so the "spaceship's" speed
and view increase extremely rapidly. From the 10 × 10 meter scene
showing the man on the blanket the scene becomes 100 meters on a side,
so Lake Michigan becomes visible, and as the camera speeds away at
faster and faster rates, it reaches 10,000,000 meters, so the Earth is

Chapter 11  Hearing

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

that when the sound pressure increases from 1 to 10,000,000, the
decibels increase only from 0 to 140. This means that we don't have to
deal with graphs that are several miles high! When specifying the sound
pressure in decibels, the notation SPL, for sound pressure level, is
added to indicate that decibels were determined using the standard
pressure po of 20 micropascals. In referring to the sound pressure of a
sound stimulus in decibels, the term level or sound level is usually
used.

Complex Tones and Frequency Spectra We have been using pure tones to
illustrate frequency and amplitude. Pure tones are important because
they are the fundamental building blocks of sounds, and pure tones have
been used extensively in auditory research. Pure tones are, however,
rare in the environment. As noted earlier, sounds in the environment,
such as those produced by most musical instruments or people speaking,
have waveforms that are more complex than the pure tone's sine-wave
pattern of pressure changes. Figure 11.5a shows the pressure changes
associated with a complex tone that would be created by a musical
instrument. Waveforms

Frequency spectra

1+2+3+4 Level

visible, and eventually 1,000 million million meters near the edge of
the Milky Way. The film actually continues to zoom out, until reaching
the outer limits of the universe. But we will stop here! When numbers
become this huge, they become difficult to deal with, especially if they
need to be plotted on a graph. Logarithms come to the rescue by
converting numbers into exponents or powers. The logarithm of a number
is the exponent to which the base, which is 10 for common logarithms,
has to be raised to produce that number. Other bases are used for
different applications. For example, logarithms to the base 2, called
binary logarithms, are used in computer science. Common logarithms are
illustrated in Table 11.2. The logarithm of 10 is 1 because the base,
10, has to be raised to the first power to equal 10. The logarithm of
100 is 2 because 10 has to be raised to the second power to equal 100.
The main thing to take away from this table is that multiplying a number
by 10 corresponds to an increase of just 1 log unit. A log scale,
therefore, converts a huge and unmanageable range of numbers to a
smaller range that is easier to deal with. Thus the increase in size
from 1 to 1,000 million million that occurs as Charles Eames's spaceship
zooms out to the edge of the Milky Way is converted into a more
manageable scale of 14 log units. The range of sound pressures
encountered in the environment, while not as astronomical as the range
in Eames's film, ranges from 1 to 10,000,000, which in powers of 10 is a
range of 7 log units. Let's now return to our equation, dB = 20 ×
logarithm (p/po). According to this equation, decibels are 20 times the
logarithm of a ratio of two pressures: p, the pressure of the sound we
are considering; and po, the reference pressure, usually set at 20
micropascals, which is the pressure near hearing threshold for a 1,000Hz
tone. Let's consider this calculation for two sound pressures. If the
sound pressure, p, is 2,000 micropascals, then

(a) 

0 200 400 600 800 1,000 1 (b)

dB 5 20 3 log(2,000/20) 5 20 3 log 100 0 200 400 600 800 1,000

and since the log of 100 is 2,

2

dB 5 20 3 2 5 40

(c) 

If we multiply the sound pressure by 10 so p is 20,000 micropascals,
then

0 200 400 600 800 1,000 3

dB 5 20 3 log(20,000/20) 5 20 3 log 1,000

(d) 

The log of 1,000 is 3, so

0 200 400 600 800 1,000

dB 5 20 3 3 5 60 Notice that multiplying sound pressure by 10 causes an
increase of 20 decibels. Thus, looking back at Table 11.1, we can see

4 (e) 0 200 400 600 800 1,000

Table 11.2 Common Logarithms 0 Number

Power of 10

Logarithm

101

1

2

10

2

1,000

3

10

3

10,000

104

4

10 100

5

10

15

20

Frequency (Hz)

Time (ms)

Figure 11.5 Left: Waveforms of tones. Vertical excursions indicate
changes in pressure. Horizontal time scale is shown below. (a) A complex
periodic sound with a fundamental frequency of 200 Hz. The vertical axis
is "pressure." (b) Fundamental (first harmonic) = 200 Hz; (c) second
harmonic = 400 Hz; (d) third harmonic = 600 Hz; (e) fourth harmonic =
800 Hz. Right: Frequency spectra for each of the tones on the left.
(Adapted from Plack, 2005) 11.1 Physical Aspects of Sound

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

267

Notice that the waveform repeats (for example, the waveform in Figure
11.5a repeats four times). This property of repetition means that this
complex tone, like a pure tone, is a periodic waveform. From the time
scale at the bottom of the figure, we see that the tone repeats four
times in 20 msec. Because 20 msec is 20/1,000 sec = 1/50 sec, this means
that the pattern for this tone repeats 200 times per second. That
repetition rate is called the fundamental frequency of the tone. Complex
tones like the one in Figure 11.5a are made up of a number of pure tone
(sine-wave) components added together. Each of these components is
called a harmonic of the tone. The first harmonic, a pure tone with
frequency equal to the fundamental frequency, is usually called the
fundamental of the tone. The fundamental of this tone, shown in Figure
11.5b, has a frequency of 200 Hz, which matches the repetition rate of
the complex tone. Higher harmonics are pure tones with frequencies that
are whole-number (2, 3, 4, etc.) multiples of the fundamental frequency.
This means that the second harmonic of our complex tone has a frequency
of 200 × 2 = 400 Hz (Figure 11.5c), the third harmonic has a frequency
of 200 × 3 = 600 Hz (Figure 11.5d), and so on. These additional tones
are the higher harmonics of the tone. Adding the fundamental and the
higher harmonics in Figures 11.5b, c, d, and e results in the waveform
of the complex tone (that is, Figure 11.5a). Another way to represent
the harmonic components of a complex tone is by frequency spectra, shown
on the right of Figure 11.5. Notice that the horizontal axis is
frequency, not time, as is the case for the waveform plot on the left.
The position of each line on the horizontal axis indicates the frequency
of one of the tone's harmonics, and the height of the line indicates the
harmonic's amplitude. Frequency spectra provide a way of indicating a
complex tone's fundamental frequency and harmonics that add up to the
tone's complex waveform. Although a repeating sound wave is composed of
harmonics with frequencies that are whole-number multiples of the
fundamental frequency, not all the harmonics need to be present for the
repetition rate to stay the same. Figure 11.6 shows what happens if we
remove the first harmonic of a complex tone. The tone in Figure 11.6a is
the one from Figure 11.5a, which has a fundamental frequency of 200 Hz.
The tone in Figure 11.6b is the same tone with the first harmonic (200
Hz) removed, as indicated by the frequency spectrum on the right. Note
that removing a harmonic changes the tone's waveform, but that the rate
of repetition remains the same. Even though the fundamental is no longer
present, the 200-Hz repetition rate corresponds to the frequency of the
fundamental. The same effect also occurs when removing higher harmonics.
Thus, if the 400-Hz second harmonic is removed, the tone's waveform
changes, but the repetition rate is still 200. You may wonder why the
repetition rate remains the same even though the fundamental or higher
harmonics have been removed. Looking at the frequency spectra on the
right, we can see that the spacing between harmonics equals the
repetition rate. When the fundamental is removed, this spacing remains,
so there is still information in the waveform indicating the frequency
of the fundamental. 268

1+2+3+4

0 200 400 600 800 1,000 0

5

(a) 

10

15

20

15

20

Frequency (Hz)

Time (ms)

2+3+4

0 200 400 600 800 1,000 0 (b)

5

10

Frequency (Hz)

Time (ms)

Figure 11.6 (a) The complex tone from Figure 11.5a and its frequency
spectrum; (b) the same tone with its first harmonic removed. (Adapted
from Plack, 2005)

11.2 Perceptual Aspects of Sound Our discussion so far has been focused
on physical aspects of the sound stimulus. Everything we have described
so far can be measured by a sound meter that registers pressure changes
in the air. A person need not be present, as occurs in our example of a
tree falling in the forest when no one is there to hear it. But now
let's add a person (or an animal) and consider what people actually
hear. We will consider two perceptual dimensions: (1) loudness, which
involves differences in the perceived magnitude of a sound, illustrated
by the difference between a whisper and a shout; and (2) pitch, which
involves differences in the low to high quality of sounds, illustrated
by what we hear playing notes from left to right on a piano keyboard.

Thresholds and Loudness We consider loudness by asking the following two
questions about sound: "Can you hear it?" and "How loud does it sound?"
These two questions come under the heading of thresholds (the smallest
amount of sound energy that can just barely be detected) and loudness
(the perceived intensity of a sound that ranges from "just audible" to
"very loud").

Loudness and Level Loudness is the perceptual quality most closely
related to the level or amplitude of an auditory stimulus, which is
expressed in decibels. Thus, decibels are often associated with
loudness, as shown in Table 11.1, which indicates that a sound of 0 dB
SPL is just barely detectible and 120 dB SPL is extremely loud (and can
cause permanent damage to the receptors inside the ear).

Chapter 11  Hearing

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Threshold of feeling

100

20

100

10

80

dB (SPL)

Loudness

120

2.0 1.0

80

60 40

A

D B

Conversational speech

40 C

Audibility curve (threshold of hearing)

20 0.2

0

0.1

20

Equal loudness curves

100

500 1,000

5,000 10,000

Frequency (Hz) 0

20

40

60

80

100

120

Intensity (dB)

Figure 11.7 Loudness of a 1,000 Hz tone as a function of intensity,
determined using magnitude estimation. (Adapted from Gulick et al.,
1989)

The relationship between level in decibels (physical) and loudness
(perceptual) was determined by S. S. Stevens, using the magnitude
estimation procedure (see Chapter 1, page 16; Appendix B, page 418).
Figure 11.7 shows the relationship between decibels and loudness for a
1,000-Hz pure tone. In this experiment, loudness was judged relative to
a 40-dB SPL tone, which was assigned a value of 1. Thus, a pure tone
that sounds 10 times louder than the 40-dB SPL tone would be judged to
have a loudness of 10. The dashed lines indicate that increasing the
sound level by 10 dB (from 40 to 50) almost doubles the sound's
loudness. It would be tempting to conclude from Table 11.1 and the curve
in Figure 11.7 that "higher decibels" equals greater loudness. But it
isn't quite that simple, because thresholds and loudness depend not only
on decibels but also on frequency. One way to appreciate the importance
of frequency in the perception of loudness is to consider the audibility
curve.

Thresholds Across the Frequency Range: The Audibility Curve A basic fact
about hearing is that we only hear within a specific range of
frequencies. This means that there are some frequencies we can't hear,
and that even within the range of frequencies we can hear, some are
easier to hear than others. Some frequencies have low thresholds--- it
takes very little sound pressure change to hear them; other frequencies
have high thresholds---large changes in sound pressure are needed to
make them heard. This is illustrated by the curve in Figure 11.8, called
the audibility curve. This audibility curve, which indicates the
threshold for hearing versus frequency, indicates that we can hear
sounds between about 20 Hz and 20,000 Hz and that we are most sensitive
(the threshold for hearing is lowest) at frequencies between 2,000 and
4,000 Hz, which happens to be the range of frequencies that is most
important for understanding speech. The light green area above the
audibility curve is called the auditory response area because we can
hear tones that fall

Figure 11.8 The audibility curve and the auditory response area. Hearing
occurs in the light green area between the audibility curve (the
threshold for hearing) and the upper curve (the threshold for feeling).
Tones with combinations of dB and frequency that place them in the light
red area below the audibility curve cannot be heard. Tones above the
threshold of feeling result in pain. The frequencies between the places
where the dashed line at 10 dB crosses the audibility function indicate
which frequencies can be heard at 10 dB SPL. (From Fletcher & Munson,
1933)

within this area. At intensities below the audibility curve, we can't
hear a tone. For example, we wouldn't be able to hear a 30-Hz tone at 40
dB SPL (point A). The upper boundary of the auditory response area is
the curve marked "threshold of feeling." Tones with these high
amplitudes are the ones we can "feel"; they can become painful and can
cause damage to the auditory system. Although humans hear frequencies
between about 20 Hz and 20,000 Hz, other animals can hear frequencies
outside the range of human hearing. Elephants can hear stimuli below 20
Hz. Above the high end of the human range, dogs can hear frequencies
above 40,000 Hz, cats can hear above 50,000 Hz, and the upper range for
dolphins extends as high as 150,000 Hz. But what happens between the
audibility curve and the threshold of feeling? To answer this question,
we can pick any frequency and select a point, such as point B, that is
just slightly above the audibility curve. Because that point is just
above threshold, it will sound very soft. However, as we increase the
level by moving up the vertical line, the loudness increases (also see
Figure 11.7). Thus, each frequency has a threshold or "baseline"---the
decibels at which it can just barely be heard, as indicated by the
audibility curve---and loudness increases as we increase the level above
this baseline. Another way to understand the relationship between
loudness and frequency is by looking at the red equal loudness curves in
Figure 11.8. These curves indicate the sound levels that create the same
perception of loudness at different frequencies. An equal loudness curve
is determined by presenting a standard pure tone of one frequency and
level and having a listener adjust the level of pure tones with
frequencies across the range of hearing to match the loudness of the
standard. For example, the curve marked 40 in Figure 11.8 was determined
by matching the loudness of frequencies across the range of hearing to
the loudness of a 1,000-Hz 40-dB SPL tone (point C). 11.2 Perceptual
Aspects of Sound

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

269

This means that a 100-Hz tone needs to be played at 60 dB (point D) to
have the same loudness as the 1,000-Hz tone at 40 dB. Notice that the
audibility curve and the equal loudness curve marked 40 bend up at high
and low frequencies, but the equal loudness curve marked 80 is almost
flat between 30 and 5,000 Hz, meaning that tones at a level of 80 dB SPL
are roughly equally loud between these frequencies. Thus, at threshold,
the level can be very different for different frequencies, but at some
level above threshold, different frequencies can have a similar loudness
at the same decibel level.

Pitch Pitch, the perceptual quality we describe as "high" or "low," can
be defined as the property of auditory sensation in terms of which
sounds may be ordered on a musical scale extending from low to high
(Bendor & Wang, 2005). The idea that pitch is associated with the
musical scale is reflected in another definition of pitch, which states
that pitch is that aspect of auditory sensation whose variation is
associated with musical melodies (Plack, 2014). While often associated
with music, pitch is also a property of speech (low-pitched or
high-pitched voice) and other natural sounds. Pitch is most closely
related to the physical property of fundamental frequency (the
repetition rate of the sound waveform). Low fundamental frequencies are
associated with low pitches (like the sound of a tuba), and high
fundamental frequencies are associated with high pitches (like the sound
of a piccolo). However, remember that pitch is a perceptual, not a
physical, property of sound. So pitch can't be measured in a physical
way. For example, it isn't correct to say that a sound has a "pitch of
200 Hz." Instead we say that a particular sound has a low pitch or a
high pitch, based on how we perceive it. One way to think about pitch is
in terms of a piano keyboard. Hitting a key on the left of the keyboard
creates a lowpitched rumbling "bass" tone; moving up the keyboard
creates higher and higher pitches, until tones on the far right are
high-pitched and might be described as "tinkly." The physical property
that is related to this low to high perceptual experience is fundamental
frequency, with the lowest note on the piano having a fundamental
frequency of 27.5 Hz and the highest note 4,186 Hz (Figure 11.9). The
perceptual experience of

increasing pitch that accompanies increases in a tone's fundamental
frequency is called tone height. In addition to the increase in tone
height that occurs as we move from the low to the high end of the piano
keyboard, something else happens: the letters of the notes A, B, C, D,
E, F, and G repeat, and we notice that notes with the same letter sound
similar. Because of this similarity, we say that notes with the same
letter have the same tone chroma. Every time we pass the same letter on
the keyboard, we have gone up an interval called an octave. Tones
separated by octaves have the same tone chroma. For example, each of the
As in Figure 11.9, indicated by the arrows, has the same tone chroma.
Notes with the same chroma have fundamental frequencies that are
separated by a multiple of two. Thus, A1 has a fundamental frequency of
27.5 Hz, A2's is 55 Hz, A3's is 110 Hz, and so on. This doubling of
frequency for each octave results in similar perceptual experiences.
Thus, a male with a low-pitched voice and a female with a high-pitched
voice can be regarded as singing "in unison," even when their voices are
separated by one or more octaves. While the connection between pitch and
fundamental frequency is nicely illustrated by the piano keyboard, there
is more to the story than fundamental frequency. If the fundamental or
other harmonics of a complex tone are removed, the tone's pitch remains
the same, so the two waveforms in Figure 11.6 result in the same pitch.
The fact that pitch remains the same, even when the fundamental or other
harmonics are removed, is called the effect of the missing fundamental.
The effect of the missing fundamental has practical consequences.
Consider, for example, what happens when you listen to someone talking
to you on a land-line phone. Even though the telephone does not
reproduce frequencies below about 300 Hz, you can hear the low pitch of
a male voice that corresponds to a 100-Hz fundamental frequency because
of the pitch created by the higher harmonics (Truax, 1984). Another way
to illustrate the effect of the missing fundamental is to imagine
hearing a long tone created by bowing a violin in a quiet room. We then
turn on a noisy air conditioner that creates a loud low-frequency hum.
Even though the air conditioner noise may make it difficult to hear the
lower harmonics of the violin's tone, the tone's pitch remains the same
(Oxenham, 2013).

Frequency (Hz)

27.5 30.9 32.7 36.7 41.2 43.7 49.0 55.0 61.7 65.4 73.4 82.4 87.3 98.0
110.0 123.5 130.8 146.8 164.8 174.6 196.0 220.0 246.9 261.6 293.7 329.6
349.2 392.0 440.0 493.9 523.2 587.3 659.2 698.5 784.0 880.0 987.8 1046.5
1174.7 1318.5 1396.9 1568.0 1760.0 1975.5 2093.0 2349.3 2637.0 2793.0
3136.0 3520.0 3951.1 4186.0

Tone height increases

Piano keyboard A0 B0 C1 D1 E1 F1 G1 A1 B1 C2 D2 E2 F2 G2 A2 B2 C3 D3 E3
F3 G3 A3 B3 C4 D4 E4 F4 G4 A4 B4 C5 D5 E5 F5 G5 A5 B5 C6 D6 E6 F6 G6 A6
B6 C7 D7 E7 F7 G7 A7 B7 C8

Same tone chroma

Figure 11.9 A piano keyboard, indicating the frequency associated with
each key. Moving up the keyboard to the right increases frequency and
tone height. Notes with the same letter, like the As (arrows), have the
same tone chroma. 270

Chapter 11  Hearing

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Timbre

Response (dB)

Although removing harmonics does not affect a tone's pitch, another
perceptual quality, the tone's timbre (pronounced TIM-ber or TAM-ber),
does change. Timbre is the quality that distinguishes between two tones
that have the same loudness, pitch, and duration, but still sound
different. For example, when a flute and an oboe play the same note with
the same loudness, we can still tell the difference between these two
instruments. We might describe the sound of the flute as clear and the
sound of the oboe as reedy. When two tones have the same loudness,
pitch, and duration but sound different, this difference is a difference
in timbre. Timbre is closely related to the harmonic structure of a
tone. In Figure 11.10, frequency spectra indicate the harmonics of a
guitar, a bassoon, and an alto saxophone playing the note G3 with a
fundamental frequency of 196 Hz. Both the relative strengths of the
harmonics and the number of harmonics are different in these
instruments. For example, the guitar has more high-frequency harmonics
than either the bassoon or the alto saxophone. Although the frequencies
of the harmonics are always multiples of the fundamental frequency,
harmonics may be absent, as is true of some of the high-frequency
harmonics of the bassoon and the alto saxophone. It is also easy to
notice differences in the timbre of Guitar 20 10 0

2

4

8 1,000

2

4

8 10,000

Response (dB)

Frequency (Hz) Bassoon

30 20 10 0

2

4

8 1,000

2

4

8 10,000

Response (dB)

Frequency (Hz) Alto saxophone

30

people's voices. When we describe one person's voice as sounding "nasal"
and another's as being "mellow," we are referring to the timbres of
their voices. The difference in the harmonics of different instruments
is not the only factor that creates the distinctive timbres of musical
instruments. Timbre also depends on the time course of a tone's attack
(the buildup of sound at the beginning of the tone) and of the tone's
decay (the decrease in sound at the end of the tone). Thus, it is easy
to tell the difference between a high note played on a clarinet and the
same note played on a flute. It is difficult, however, to distinguish
between the same instruments when their tones are recorded and the
tone's attack and decay are eliminated by erasing the first and last 1/2
second of each tone's recording (Berger, 1964; also see Risset &
Mathews, 1969). Another way to make it difficult to distinguish one
instrument from another is to play an instrument's tone backward. Even
though this does not affect the tone's harmonic structure, a piano tone
played backward sounds more like an organ than a piano because the
tone's original decay has become the attack and the attack has become
the decay (Berger, 1964; Erickson, 1975). Thus, timbre depends both on
the tone's steady-state harmonic structure and on the time course of the
attack and decay of the tone's harmonics. The sounds we have been
considering so far---pure tones and the tones produced by musical
instruments---are all periodic sounds. That is, the pattern of pressure
changes in the waveform repeats, as in the tone in Figure 11.5a. There
are also aperiodic sounds, which have waveforms that do not repeat.
Examples of aperiodic sounds would be a door slamming shut, a large
group of people talking simultaneously, and noises such as the static on
a radio not tuned to a station. Only periodic sounds can generate a
perception of pitch. We will focus in this chapter on pure tones and
musical tones because these sounds are the ones that have been used in
most of the basic research on the operation of the auditory system. In
the next section, we will begin considering how the sound stimuli we
have been describing are processed by the auditory system so that we can
experience sound. TEST YOuRSELF 11.1 1. What are some of the functions
of sound? Especially note what information sound provides that is not
provided by vision.

20

2.  What are two possible definitions of sound? (Remember the tree
    falling in the forest.)

10

3.  How is the sound stimulus described in terms of pressure changes in
    the air? What is a pure tone? Sound frequency?

0

2

4

8 1,000

2

4

8 10,000

Frequency (Hz)

Figure 11.10 Frequency spectra for a guitar, a bassoon, and an alto
saxophone playing a tone with a fundamental frequency of 196 Hz. The
position of the lines on the horizontal axis indicates the frequencies
of the harmonics and their height indicates their intensities. (From
Olson, 1967)

4.  What is the amplitude of a sound? Why was the decibel scale
    developed to measure amplitude? Is decibel "perceptual" or
    "physical"?
5.  What is a complex tone? What are harmonics? Frequency spectra?

11.2 Perceptual Aspects of Sound

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

271

Outer

6.  How does removing one or more harmonics from a complex tone affect
    the repetition rate of the sound stimulus?

Middle

7.  What is the relationship between sound level and loudness? Which one
    is physical, and which one is perceptual?
8.  What is the audibility curve, and what does it tell us about the
    relationship between a tone's physical characteristics (level and
    frequency) and perceptual characteristics (threshold and loudness)?
9.  What is pitch? What physical property is it most closely related to?
    What are tone height and tone chroma?
10. What is the effect of the missing fundamental?
11. What is timbre? Describe the characteristics of complex tones and
    how these characteristics determine timbre.

11.3 From Pressure Changes to Electrical Signals Now that we have
described the stimuli and their perceptual effects, we are ready to
begin describing what happens inside the ear. What we will be describing
in this next part of our story is a journey that begins as sound enters
the ear and culminates deep inside the ear at the receptors for hearing.
The auditory system accomplishes three basic tasks during this journey.
First, it delivers the sound stimulus to the receptors; second, it
transduces this stimulus from pressure changes into electrical signals;
and third, it processes these electrical signals so they can indicate
qualities of the sound source, such as pitch, loudness, timbre, and
location. As we describe this journey, we will follow the sound stimulus
through a complex labyrinth on its way to the receptors. But this is not
simply a matter of sound moving through one dark tunnel after another.
It is a journey in which sound sets structures along the pathway into
vibration, with these vibrations being transmitted from one structure to
another, starting with the eardrum at the beginning and ending with the
vibration of small hairlike parts of the hearing receptors called
stereocilia deep within the ear. The ear is divided into three
divisions: outer, middle, and inner. We begin with the outer ear.

The Outer Ear Sound waves first pass through the outer ear, which
consists of the pinnae, the structures that stick out from the sides of
the head, and the auditory canal, a tubelike recess about 3 cm long in
adults (Figure 11.11). Although the pinnae are the most obvious part of
the ear and help us determine the location of sounds, as we will see in
Chapter 12, it is the part of the ear we could most easily do without.
Van Gogh did not make himself deaf in his left ear when he attacked his
pinna with a razor in 1888. The auditory canal protects the delicate
structures of the middle ear from the hazards of the outside world. The
auditory canal's 3-cm recess, along with its wax, protects the delicate
272

Incus

Inner Semicircular canals

Malleus

Auditory nerve Cochlea

Pinna Auditory canal

Stapes Tympanic Round membrane window (eardrum) Oval window (under
footplate of stapes)

Figure 11.11 The ear, showing its three subdivisions---outer, middle,
and inner. (From Lindsay & Norman, 1977)

tympanic membrane, or eardrum, at the end of the canal and helps keep
this membrane and the structures in the middle ear at a relatively
constant temperature. In addition to its protective function, the
auditory canal has another effect: to enhance the intensities of some
sounds by means of the physical principle of resonance. Resonance occurs
in the auditory canal when sound waves that are reflected back from the
closed end of the auditory canal interact with sound waves that are
entering the canal. This interaction reinforces some of the sound's
frequencies, with the frequency that is reinforced the most being
determined by the length of the canal. The frequency reinforced the most
is called the resonant frequency of the canal. Measurements of the sound
pressures inside the ear indicate that the resonance that occurs in the
auditory canal has a slight amplifying effect that increases the sound
pressure level of frequencies between about 1,000 and 5,000 Hz, which,
as we can see from the audibility curve in Figure 11.8, covers the most
sensitive range of human hearing.

The Middle Ear When airborne sound waves reach the tympanic membrane at
the end of the auditory canal, they set it into vibration, and this
vibration is transmitted to structures in the middle ear, on the other
side of the tympanic membrane. The middle ear is a small cavity, about 2
cubic centimeters in volume, that separates the outer and inner ears
(Figure 11.12). This cavity contains the ossicles, the three smallest
bones in the body. The first of these bones, the malleus (also known as
the hammer), is set into vibration by the tympanic membrane, to which it
is attached, and transmits its vibrations to the incus (or anvil),
which, in turn, transmits its vibrations to the stapes (or stirrup). The
stapes then transmits its vibrations to the inner ear by pushing on the
membrane covering the oval window. Why are the ossicles necessary? We
can answer this question by noting that both the outer ear and middle
ear are filled with air, but the inner ear contains a watery liquid that
is much

Chapter 11  Hearing

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Malleus

Incus Area of stapes footplate

Stapes Tympanic membrane (eardrum)

Area of tympanic membrane

Oval window

Auditory canal

(a) Round window

Figure 11.12 The middle ear. The three bones of the middle ear transmit
the vibrations of the tympanic membrane to the inner ear. (b) Air

Air

Cochlear fluid

Outer

Middle

Inner

Figure 11.13 Environments inside the outer, middle, and inner ears. The
fact that liquid fills the inner ear poses a problem for the
transmission of sound vibrations from the air of the middle ear.

denser than the air (Figure 11.13). The mismatch between the low density
of the air and the high density of this liquid creates a problem:
pressure changes in the air are transmitted poorly to the much denser
liquid. This mismatch is illustrated by the difficulty you would have
hearing people talking to you if you were underwater and they were above
the surface. If vibrations had to pass directly from the air in the
middle ear to the liquid in the inner ear, less than 1 percent of the
vibrations would be transmitted (Durrant & Lovrinic, 1977). The ossicles
help solve this problem in two ways: (1) by concentrating the vibration
of the large tympanic membrane onto the much smaller stapes, which
increases the pressure by a factor of about 20 (Figure 11.14a); and (2)
by being hinged to create a lever action---an effect similar to what
happens when a fulcrum is placed under a board, so that pushing down on
the long end of the board makes it possible to lift a heavy weight on
the short end (Figure 11.14b). We can appreciate the effect of the
ossicles by noting that in patients whose ossicles have been damaged
beyond surgical repair, it is necessary to increase the sound pressure
by a factor of 10 to 50 to achieve the same hearing as when the ossicles
were functioning (Bess & Humes, 2008). Not all animals require the
concentration of pressure and lever effect provided by the ossicles in
the human ear. For example, there is only a small mismatch between the
density of water, which transmits sound in a fish's environment, and the
liquid inside the fish's ear. Thus, fish have no outer or middle ear.
The middle ear also contains the middle-ear muscles, the smallest
skeletal muscles in the body. These muscles are

Figure 11.14 (a) A diagrammatic representation of the tympanic membrane
and the stapes, showing the difference in size between the two. (b) How
lever action can amplify a small force, presented on the right, to lift
the large weight on the left. The lever action of the ossicles amplifies
the sound vibrations reaching the tympanic inner ear. (From Schubert,
1980)

attached to the ossicles, and at very high sound levels they contract to
dampen the ossicles' vibration. This reduces the transmission of
low-frequency sounds and helps to prevent intense low-frequency
components from interfering with our perception of high frequencies. In
particular, contraction of the muscles may prevent our own
vocalizations, and sounds from chewing, from interfering with our
perception of speech from other people---an important function in a
noisy restaurant!

The Inner Ear We will describe first the structure of the inner ear, and
then what happens when structures of the inner ear are set into
vibration.

Inner Ear Structure The main structure of the inner ear is the
liquid-filled cochlea, the snaillike structure shown in green in Figure
11.11, and shown partially uncoiled in Figure 11.15a. Figure 11.15b
shows the cochlea completely uncoiled to form a long straight tube. The
most obvious feature of the uncoiled cochlea is that the upper half,
called the scala vestibuli, and the lower half, called the scala
tympani, are separated by a structure called the cochlear partition.
This partition extends almost the entire length of the cochlea, from its
base near the stapes to its apex at the far end. Note that this diagram
is not drawn to scale and so does not show the cochlea's true
proportions. In reality, the uncoiled cochlea would be a cylinder 2 mm
in diameter and 35 mm long. Although the cochlear partition is indicated
by a thin line in Figure 11.15b, it is actually relatively large and
contains the 11.3 From Pressure Changes to Electrical Signals

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

273

Figure 11.15 (a) A partially uncoiled cochlea. (b) A fully uncoiled
cochlea. The cochlear partition, which is indicated here by a line,
actually contains the basilar membrane and the organ of Corti, as shown
in Figure 11.16.

Oval window Stapes

Round window

Scala tympani

(a) 

Stapes

Oval window

Cochlear partition

Scala vestibuli

Cochlear partition

Scala vestibuli Scala tympani

Base Round window

Apex

Cross-section cut (see Figure 11.16)

(b) 

structures that transform the vibrations inside the cochlea into
electricity. We can see the structures within the cochlear partition by
taking a cross-section cut of the cochlea, as shown in Figure 11.15b,
and looking at the cochlea end-on and in cross section, as in Figure
11.16a. When we look at the cochlea in this way, we see the organ of
Corti, which contains the hair cells, the receptors for hearing. It is
important to remember that Figure 11.16 shows just one place along the
organ of Corti, but as shown in Figure 11.15, the cochlear partition,
which contains the organ of Corti, extends the entire length of the
cochlea. There are, therefore, hair cells from one end of the Figure
11.16 (a) Cross section of the cochlea. (b) Close-up of the organ of
Corti, showing how it rests on the basilar membrane. Arrows indicate the
motions of the basilar membrane and tectorial membrane that are caused
by vibration of the cochlear partition. Although not obvious in this
figure, the cilia of the outer hair cells are embedded in the tectorial
membrane, but the cilia of the inner hair cells are not.

Scala vestibuli

Inner hair cells

Tectorial membrane

Organ of Corti

Stereocilia Outer hair cells Auditory nerve

(Adapted from Denes & Pinson, 1993)

274

cochlea to the other. In addition, there are two membranes, the basilar
membrane and the tectorial membrane, which also extend the length of the
cochlea, and which play crucial roles in activating the hair cells. The
hair cells are shown in red in Figure 11.16b and in yellow in Figure
11.17. At the tips of the hair cells are small processes called
stereocilia, which bend in response to pressure changes. The human ear
contains one row of inner hair cells and about three rows of outer hair
cells, with about 3,500 inner hair cells and 12,000 outer hair cells.
The stereocilia of the tallest row of outer hair cells are embedded in
the tectorial

Basilar membrane Scala tympani

(a) 

Organ of Corti

Auditory nerve fibers

Basilar membrane

(b) 

Chapter 11  Hearing

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Steve Gschmeissner/Science Photo Library/Corbis

cortex in auditory nerve fibers. We will return to the outer hair cells
later in the chapter. Transduction for hearing also involves a sequence
of events that creates ion flow. First, the stereocilia of the hair
cells bend in one direction (Figure 11.18a). This bending causes
structures called tip links to stretch, and this opens tiny ion channels
in the membrane of the stereocilia, which function like trapdoors. When
the ion channels are open, positively charged potassium ions flow into
the cell and an electrical signal results. When the stereocilia bend in
the other direction (Figure 11.18b), the tip links slacken, the ion
channels close, and ion flow stops. Thus, the back-and-forth bending of
the hair cells causes alternating bursts of electrical signals (when the
stereocilia bend in one direction) and no electrical signals (when the
stereocilia bend in the opposite direction). The electrical signals in
the hair cells result in the release of neurotransmitters at the synapse
separating the inner hair cells from the auditory nerve fibers, which
causes these auditory nerve fibers to fire.

Figure 11.17 Scanning electron micrograph showing inner hair cells (top)
and the three rows of outer hair cells (bottom). The hair cells have
been colored to stand out.

Tip link

membrane, and the stereocilia of the rest of the outer hair cells and
all of the inner hair cells are not (Moller, 2006).

Vibration Bends the Stereocilia The scene we have described---the organ
of Corti sitting on the basilar membrane, with the tectorial membrane
arching over the hair cells---is the staging ground for events that
occur when vibration of the stapes in the middle ear sets the oval
window into motion. The back and forth motion of the oval window
transmits vibrations to the liquid inside the cochlea, which sets the
basilar membrane into motion (blue arrow in Figure 11.16b). The
up-and-down motion of the basilar membrane has two results: (1) it sets
the organ of Corti into an up-and-down vibration, and (2) it causes the
tectorial membrane to move back and forth, as shown by the red arrow.
These two motions mean that the tectorial membrane slides back and
forward just above the hair cells. The movement of the tectorial
membrane causes the stereocilia of the outer hair cells that are
embedded in the membrane to bend. The stereocilia of the other outer
hair cells and the inner hair cells also bend, but in response to
pressure waves in the liquid surrounding the stereocilia (Dallos, 1996).
Bending Causes Electrical Signals We have now reached the point in our
story where the vibrations that have reached the inner ear become
transformed into electrical signals. This is the process of transduction
we described for vision in Chapter 2, which occurs when the
light-sensitive part of a visual pigment molecule absorbs light, changes
shape, and triggers a sequence of chemical reactions that ends up
affecting the flow of ions (charged molecules) across the visual
receptor membrane. As we describe this process for hearing, we will
focus on the inner hair cells, because these are the main receptors
responsible for generating signals that are sent to the

Tip link

Ion flow

Ion flow

Inner hair cell

Ion flow

Transmitter released

Auditory nerve fiber (a)

(b) 

Figure 11.18 How movement of stereocilia causes an electrical change in
the hair cell. (a) When the stereocilia are bent to the right, the tip
links are stretched and ion channels are opened. Positively charged
potassium ions (K+) enter the cell, causing the interior of the cell to
become more positive. (b) When the stereocilia move to the left, the tip
links slacken, and the channels close. (Based on Plack, 2005) 11.3 From
Pressure Changes to Electrical Signals

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

275

Pressure

Sound stimulus Inner hair cell

(a) Auditory nerve firing

Auditory nerve fiber firing Neural response

Single fibers

Sound stimulus

(b) Many fibers

Figure 11.19 How hair cell activation and auditory nerve fiber firing
are synchronized with pressure changes of the stimulus. The auditory
nerve fiber fires when the cilia are bent to the right. This occurs at
the peak of the sine-wave change in pressure.

The Electrical Signals Are Synchronized With the Pressure Changes of a
Pure Tone Figure 11.19 shows how the bending of the stereocilia follows
the increases and decreases of the pressure of a pure tone sound
stimulus. When the pressure increases, the stereocilia bend to the
right, the hair cell is activated, and attached auditory nerve fibers
will tend to fire. When the pressure decreases, the stereocilia bend to
the left, and no firing occurs. This means that auditory nerve fibers
fire in synchrony with the rising and falling pressure of the pure tone.
This property of firing at the same place in the sound stimulus is
called phase locking. For high-frequency tones, a nerve fiber may not
fire every time the pressure changes because it needs to rest after it
fires (see refractory period, Chapter 2, page 24). But when the fiber
does fire, it fires at the same time in the sound stimulus, as shown in
Figures 11.20a and 11.20b. Since many fibers respond to the tone, it is
likely that if some "miss" a particular pressure change, other fibers
will be firing at that time. Therefore, when we combine the response of
many fibers, each of which fires at the peak of the sound wave, the
overall firing matches the frequency of the sound stimulus, as shown in
Figure 11.20c. What this means is that a sound's repetition rate
produces a pattern of nerve firing in which the timing of nerve spikes
matches the timing of the repeating sound stimulus.

11.4 How Frequency Is Represented in the Auditory Nerve Now that we know
how electrical signals are created, the next question is, how do these
signals provide information about a tone's frequency? The search for the
answer to the question of how frequency is signaled by activity in the
auditory nerve has 276

(c) 

Time

Figure 11.20 (a) Pressure changes for a 250-Hz tone. (b) Pattern of
nerve spikes produced by two separate nerve fibers. Notice that the
spikes always occur at the peak of the pressure wave. (c) The combined
spikes produced by 500 nerve fibers. Although there is some variability
in the single neuron response, the response of the large group of
neurons represents the periodicity of the 250-Hz tone. (Based on Plack,
2005)

focused on determining how the basilar membrane vibrates to different
frequencies. Pioneering research on this problem was carried out by
Georg von Békésy (1899--1972), who won the Nobel Prize in physiology and
medicine in 1961 for his research on the physiology of hearing.

Békésy Discovers How the Basilar Membrane Vibrates Békésy determined how
the basilar membrane vibrates to different frequencies by observing the
vibration of the basilar membrane. He accomplished this by boring a hole
in cochleas taken from animal and human cadavers. He presented different
frequencies of sound and observed the membrane's vibration by using a
technique similar to that used to create stop-action photographs of
high-speed events (Békésy, 1960). When he observed the membrane's
position at different points in time, he saw the basilar membrane's
vibration as a traveling wave, like the motion that occurs when a person
holds the end of a rope and "snaps" it, sending a wave traveling down
the rope. Figure 11.21a shows a perspective view of this traveling wave.
Figure 11.21b shows side views of the traveling wave caused by a pure
tone at three successive moments in time. The solid horizontal line
represents the basilar membrane at rest. Curve 1 shows the position of
the basilar membrane at one moment during its vibration, and curves 2
and 3 show the positions of the membrane at two later moments. Békésy's

Chapter 11  Hearing

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Apex Base

(a) 

1 2

Base

Apex

3

(b) 

Figure 11.21 (a) A traveling wave like the one observed by Békésy. This
picture shows what the membrane looks like when the vibration is
"frozen" with the wave about two-thirds of the way down the membrane.
(b) Side views of the traveling wave caused by a pure tone, showing the
position of the membrane at three instants in time as the wave moves
from the base to the apex of the cochlear partition. \[(a) Adapted from
Tonndorf, 1960; (b) Adapted from Békésy, 1960\]

measurements showed that most of the membrane vibrates, but that some
parts vibrate more than others. Although the motion takes the form of a
traveling wave, the important thing is what happens at particular points
along the basilar membrane. If you were at one point on the basilar

membrane you would see the membrane vibrating up and down at the
frequency of the tone. If you observed the entire membrane, you would
see that vibration occurs over a large portion of the membrane, and that
there is one place that vibrates the most. Békésy's most important
finding was that the place that vibrates the most depends on the
frequency of the tone, as shown in Figure 11.22. The arrows indicate the
extent of the up-anddown displacement of the basilar membrane at
different places on the membrane. The red arrows indicate the place
where the membrane vibrates the most for each frequency. Notice that as
the frequency increases, the place on the membrane that vibrates the
most moves from the apex at the end of the cochlea toward the base at
the oval window. Thus, the place of maximum vibration, which is near the
apex of the basilar membrane for a 25-Hz tone, has moved to nearer the
base for a 1,600-Hz tone. Because the place of maximum vibration depends
on frequency, this means that basilar membrane vibration effectively
functions as a filter that sorts tones by frequency.

The Cochlea Functions as a Filter We can appreciate how the cochlea acts
like a filter that sorts sound stimuli by frequency by leaving hearing
for a moment and considering Figure 11.23a, which shows how coffee beans
are filtered to sort them by size. Beans with a variety of sizes are
Beans

Coffee bean sorter

25 Hz

100 Hz

(a) 

Oval window vibration 400 Hz Base Apex

1,600 Hz Base

Apex

Figure 11.22 The amount of vibration at different locations along the
basilar membrane is indicated by the size of the arrows at each
location, with the place of maximum vibration indicated in red. When the
frequency is 25 Hz, maximum vibration occurs at the apex of the cochlear
partition. As the frequency is increased, the location of the maximum
vibration moves toward the base of the cochlear partition. (Based on
data in Békésy, 1960)

(b) 

2,000 Hz 500 Hz 100 Hz 25 Hz

Sound frequency filter (Basilar membrane)

Figure 11.23 Two ways of sorting. (a) Coffee beans of different sizes
are deposited at the left end of the sieve. By shaking and gravity, the
beans travel down the sieve. Smaller coffee beans drop through the small
holes at the beginning of the sieve; larger ones drop through the larger
holes near the end. (b) Sound vibrations of different frequencies, which
occur at the oval window, on the left, set the basilar vibration into
motion. Higher frequencies cause vibration at the base of the basilar
membrane, near the oval window. Low frequencies cause vibrations nearer
the apex of the basilar membrane. 11.4 How Frequency Is Represented in
the Auditory Nerve

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

277

5,000

100

7,000

Threshold (dB SPL)

6,000 2,000 700 600 4,000

1,500 500 125

150

400

800 200 60 250 75 100

2,500

1,000

Base of cochlea

3,500

Figure 11.24 Tonotopic map of the guinea pig cochlea. Numbers indicate
the location of the maximum electrical response for each frequency.
(From Culler et al., 1943)

deposited at one end of a sieve that contains small holes at the
beginning and larger holes toward the far end. The beans travel down the
sieve, with smaller beans dropping through the first holes and larger
and larger beans dropping through holes farther down the sieve. The
sieve, therefore, filters coffee beans by size. Just as the different
sized holes along the length of the sieve separate coffee beans by size,
the different places of maximum vibration along the length of the
basilar membrane separate sound stimuli by frequency (Figure 11.23b).
High frequencies cause more vibration near the base end of the cochlea,
and low frequencies cause more vibration at the apex of the cochlea.
Thus, vibration of the basilar membrane "sorts" or "filters" by
frequency so hair cells are activated at different places along the
cochlea for different frequencies. Figure 11.24 shows the results of
measurements made by placing electrodes at different positions on the
outer surface of a guinea pig's cochlea and stimulating with different
frequencies (Culler, 1935; Culler et al., 1943). This "map" of the
cochlear illustrates the sorting of frequencies, with high frequencies
activating the base of the cochlea and low frequencies activating the
apex. This map of frequencies is called a tonotopic map. Another way of
demonstrating the connection between frequency and place is to record
from single auditory nerve fibers located at different places along the
cochlea. Measurement of the response of auditory nerve fibers to
frequency is depicted by a fiber's neural frequency tuning curve.

METHOD

Neural Frequency Tuning Curves

A neuron's frequency tuning curve is determined by presenting pure tones
of different frequencies and measuring the sound level necessary to
cause the neuron to increase its firing above the baseline or
"spontaneous" rate in the absence of sounds.

278

0

0.2

3,000

300

50

0.5

1

2

3

4 5 6 7 8 9 10

Frequency (kHz)

Figure 11.25 Frequency tuning curves of cat auditory nerve fibers. The
characteristic frequencies of some of the fibers are indicated by the
arrows pointing to the frequency axis. The frequency scale is in
kilohertz (kHz), where 1 kHz = 1,000 Hz. Only a small number of curves
are shown here. Each of the 3,500 inner hair cells has its own tuning
curve, and because each inner hair cell sends signals to about 20
auditory nerve fibers, each frequency is represented by a number of
neurons located at that frequency's place along the basilar membrane.
(Adapted from Miller et al., 1997) This level is the threshold for that
frequency. Plotting the threshold for each frequency results in
frequency tuning curves like the ones in Figure 11.25. The arrows under
some of the curves indicates the frequency to which the neuron is most
sensitive (has the lowest sound level threshold). This frequency is
called the characteristic frequency of the particular auditory nerve
fiber.

The cochlea's filtering action is reflected by the fact that (1) the
neurons respond best to one frequency and (2) each frequency is
associated with nerve fibers located at a specific place along the
basilar membrane, with fibers originating near the base of the cochlea
having high characteristic frequencies and those originating near the
apex having low characteristic frequencies.

The Outer Hair Cells Function as Cochlear Amplifiers While Békésy's
measurements located the places where specific frequencies caused
maximum vibration along the basilar membrane, he also observed that this
vibration was spread out over a large portion of the membrane. Later
researchers realized that one reason for Békésy's broad vibration
patterns was that his measurements were carried out on "dead" cochleas
that were isolated from animal and human cadavers. When modern
researchers used more advanced technology that enabled them to measure
vibration in live cochleas, they showed that the pattern of vibration
for specific frequencies was much narrower than what Békésy had observed
(Khanna & Leonard, 1982; Rhode, 1971, 1974). But what was responsible
for this narrower vibration? In 1983 Hallowell Davis published a paper
titled "An Active Process in Cochlear Mechanics," which began with the
attention-getting statement: "We are in the midst of a major
breakthrough in auditory

Chapter 11  Hearing

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

100 Outer hair cells destroyed 80 Cell contracts

Basilar membrane (a)

(b) 

Figure 11.26 The outer hair cell cochlear amplifier mechanism occurs
when the cells (a) elongate when stereocilia bend in one direction and
(b) contract when the stereocilia bend in the other direction. This
results in an amplifying effect on the motion of the basilar membrane.

physiology." He went on to propose a mechanism that he named the
cochlear amplifier, which explained why neural turning curves were
narrower than what would be expected based on Békésy's measurements of
basilar membrane vibration. Davis proposed that the cochlear amplifier
was an active mechanical process that took place in the outer hair
cells. We can appreciate what this active mechanical process is by
describing how the outer hair cells respond to and influence the
vibration of the basilar membrane.1 The major purpose of outer hair
cells is to influence the way the basilar membrane vibrates, and they
accomplish this by changing length (Ashmore, 2008; Ashmore et al.,
2010). While ion flow in inner hair cells causes an electrical response
in auditory nerve fibers, ion flow in outer hair cells causes mechanical
changes inside the cell that causes the cell to expand and contract, as
shown in Figure 11.26. The outer hair cells become elongated when the
stereocilia bend in one direction and contract when they bend in the
other direction. This mechanical response of elongation and contraction
pushes and pulls on the basilar membrane, which increases the motion of
the basilar membrane and sharpens its response to specific frequencies.
The importance of the cochlear amplifier is illustrated by the frequency
tuning curves in Figure 11.27. The solid blue curve shows the frequency
tuning of a cat's auditory nerve fiber with a characteristic frequency
of about 8,000 Hz. The dashed red curve shows what happened when the
cochlear amplifier was eliminated by destroying the outer hair cells
with a chemical that attacked the outer hair cells but left the inner
hair cells intact. Whereas originally the fiber had a low threshold at
8,000 Hz, indicated by the arrow, it now takes much higher intensities
to get the auditory nerve fiber to respond to 8,000 Hz and nearby
frequencies (Fettiplace & Hackney, 2006; Liberman & Dodds, 1984). The
conclusion from Figure 11.27 and the results of other experiments is
that the cochlear amplifier greatly sharpens the tuning of each place
along the cochlea.

1

Theodore Gold (1948), who was to become a well-known researcher in
cosmology and astronomy, made the original proposal that there is an
active process in the cochlea. But it wasn't until many years later that
further developments in auditory research led to the proposal of the
cochlear amplifier mechanism (see Gold, 1989).

Threshold (dB)

Cell elongates

60

40

20

0

0.5

1.0

10

20

Frequency (kHz)

Figure 11.27 Effect of outer hair cell damage on the frequency tuning
curve. The solid curve is the frequency tuning curve of a neuron with a
characteristic frequency of about 8,000 Hz (arrow). The dashed curve is
the frequency tuning curve for the same neuron after the outer hair
cells were destroyed by injection of a chemical. (Adapted from
Fettiplace & Hackney, 2006)

All of our descriptions so far have been focused on physical events that
occur within the inner ear. Our story has featured physical processes
such as trapdoors opening and ions flowing, nerve firing that is
synchronized with the sound stimulus, and basilar membrane vibrations
that separate different frequencies along the length of the cochlea. All
of this information is crucial for understanding how the ear functions.
The next section will look at the connection between these physical
processes and perception. TEST YOuRSELF 11.2 1. Describe the structure
of the ear, focusing on the role that each component plays in
transmitting the vibrations that enter the outer ear to the auditory
receptors in the inner ear. 2. Focusing on the inner ear, describe (a)
what causes the bending of the stereocilia of the hair cells; (b) what
happens when the stereocilia bend; (c) how phase locking causes the
electrical signal to follow the timing of the sound stimulus. 3.
Describe Békésy's discovery of how the basilar membrane vibrates.
Specifically, what is the relationship between sound frequency and
basilar membrane vibration? 4. What does it mean to say that the cochlea
acts as a filter? How is this supported by the tonotopic map and by
neural frequency tuning curves? What is a neuron's characteristic
frequency? 5. How do the outer hair cells function as cochlear
amplifiers?

11.4 How Frequency Is Represented in the Auditory Nerve

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

279

11.5 The Physiology of Pitch Perception: The Cochlea

280

400 600 Hz

800

(a) Frequency spectrum

(b) Basilar membrane

Figure 11.28 (a) Frequency spectrum for a complex tone with fundamental
frequency 200 Hz, showing the fundamental and three harmonics. (b)
Basilar membrane. The shaded areas indicate approximate locations of
peak vibration associated with each harmonic in the complex tone.

frequency of 440 Hz shown in Figure 11.29a. Figure 11.29b shows the
cochlear filter banks which correspond to frequency tuning curves like
the ones in Figure 11.25. When the 440 Hz tone is presented, it most
strongly activates the filter highlighted in red and the 880 Hz second
harmonic is highlighted in green. Now let's move up to higher harmonics.
The 5,720-Hz 13th harmonic and the 6,160-Hz 14th harmonic both activate
the two overlapping filters highlighted in purple. This means Spectrum
440 880 0 Level (dB)

Our starting point is the connection between a tone's frequency and the
perception of pitch. Given that low frequencies are associated with low
pitch and higher frequencies with higher pitch, it has been proposed
that pitch perception is determined by the firing of neurons that
respond best to specific frequencies. This idea follows from Békésy's
discovery that specific frequencies cause maximum vibration at specific
places along the basilar membrane, which creates a tonotopic map like
the one in Figure 11.24. The association of frequency with place led to
the following explanation of the physiology of pitch perception: A pure
tone causes a peak of activity at a specific place on the basilar
membrane. The neurons connected to that place respond strongly to that
frequency, as indicated by the auditory nerve fiber frequency tuning
curves in Figure 11.25, and this information is carried up the auditory
nerve to the brain. The brain identifies which neurons are responding
the most and uses this information to determine the pitch. This
explanation of the physiology of pitch perception has been called the
place theory, because it is based on the relation between a sound's
frequency and the place along the basilar membrane that is activated.
This explanation is elegant in its simplicity, and it became the
standard explanation of the physiology of pitch. Meanwhile, however,
some auditory researchers were questioning the validity of place theory.
One argument against place was based on the effect of the missing
fundamental, in which removing the fundamental frequency of a complex
tone does not change the tone's pitch (p. 270). Thus, the tone in Figure
11.6a, which has a fundamental frequency of 200 Hz, has the same pitch
after the 200 Hz fundamental is removed, as in Figure 11.6b. What this
means is that there is no longer peak vibration at the place associated
with 200 Hz. A modified version of place theory explains this result by
considering how the basilar membrane vibrates to complex tones. Figure
11.28 shows that a complex tone causes peaks in vibration for the
fundamental (200 Hz) and for each harmonic. Thus, removing the
fundamental eliminates the peak at 200 Hz, but peaks would remain at
400, 600, and 800, and this pattern of places, spaced 200 Hz apart,
matches the fundamental frequency so can be used to determine the pitch.
As it turns out, however, the idea that pitch can be determined by
harmonics, as in Figure 11.28, works only for low harmonics---harmonics
that are close to the fundamental. We can see why this is so by
considering the tone with fundamental

Apex 200

0

(a) 

1000 2000 3000 4000 5000 6000 7000 8000 Frequency (Hz)

Auditory filterbank 0 210 220 230 240

0

(b) 
(c) 

5,720 6,160

220 240

Response (dB)

Place and Pitch

Base

Excitation (dB)

We are now ready to describe what we know about the relation between
physiological events in the auditory system and the perception of pitch.
We begin by describing physiological processes in the ear and will then
move on to the brain.

800 600 400 200

1000 2000 3000 4000 5000 6000 7000 8000 Frequency (Hz)

Excitation pattern 0 220 240

0

1000 2000 3000 4000 5000 6000 7000 8000 Center frequency (Hz)

Figure 11.29 (a) Frequency spectrum for the first 18 harmonics for a
tone with 440-Hz fundamental frequency. (b) Cochlear filter bank. Note
that the filters are narrower at lower frequencies. The red filter is
activated by the 440-Hz harmonic; the green one by the 880-Hz harmonic;
the purple ones by the 5,720- and 6,160-Hz harmonics. The filters
correspond to the frequency tuning curves of cochlear nerve fibers like
the ones shown in Figure 11.25. (c) Excitation pattern on the basilar
membrane, which shows individual peaks of vibration for the early
(resolved) harmonics and no peaks for the later (unresolved) harmonics.
(Adapted from Oxenham, 2013)

Chapter 11  Hearing

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

that lower harmonics activate separated filters while high harmonics can
activate the same filters. Taking the properties of the filter bank into
account results in the excitation curve in Figure 11.29c, which is
essentially a picture of the amplitude of basilar membrane vibration
caused by each of the tone's harmonics (Oxenham, 2013). What stands out
about the excitation curve is that the tone's lower harmonics each cause
a distinct bump in the excitation curve. Because each of these lower
harmonics can be distinguished by a peak, they are called resolved
harmonics, and frequency information is available for perceiving pitch.
In contrast, the excitations caused by the higher harmonics create a
smooth function that doesn't indicate the individual harmonics. These
higher harmonics are called unresolved harmonics. What's important about
resolved and unresolved harmonics is that a series of resolved harmonics
results in a strong perception of pitch, but unresolved harmonics result
in a weak perception of pitch. Thus, a tone with the spectral
composition 400, 600, 800, and 1,000 Hz results in a strong perception
of pitch corresponding to the 200-Hz fundamental. However, the smeared
out pattern that would be caused by higher harmonics of the 200 Hz
fundamental, such as 2,000, 2,200, 2,400, and 2,600 Hz results in a weak
perception of pitch corresponding to 200 Hz. What this all means is that
place information provides an incomplete explanation of pitch
perception. In addition to the fact that unresolved harmonics result in
poor pitch perception, other research revealed other phenomena that were
difficult for even this modified version of place theory to explain.
Edward Burns and Neal Viemeister (1976) created a sound stimulus that
wasn't associated with vibration of a particular place on the basilar
membrane, but which created a perception of pitch. This stimulus was
called amplitude-modulated noise. Noise is a stimulus that contains many
random frequencies so it doesn't create a vibration pattern on the
basilar membrane that corresponds to a specific frequency. Amplitude
modulation means that the level (or intensity) of the noise was changed
so the loudness of the noise fluctuated rapidly up and down. Burns and
Viemeister found that this noise stimulus resulted in a perception of
pitch, which they could change by varying the rate of the up-and-down
changes in level. The conclusion from this finding, that pitch can be
perceived even in the absence of place information, has been
demonstrated in a large number of experiments using different types of
stimuli (Oxenham, 2013; Yost, 2009).

Temporal Information and Pitch If place isn't the answer, what is? One
way to answer this question is to look back at Figure 11.6 and note what
happens when the 200-Hz fundamental frequency is removed. Notice that
although the waveform of the tone changes, the timing, or repetition
rate, remains the same. Thus, there is information in the timing of a
tone stimulus that is associated with the tone's pitch. We also saw that
this timing occurs in the neural response to a tone because of phase
locking.

When we discussed phase locking on page 276, we saw that because nerve
fibers fire at the same time in the sound stimulus, the sound produces a
pattern of nerve firing in groups of neurons that matches the frequency
of the sound stimulus (Figure 11.20). Thus, the timing of firing of
groups of neurons provides information about the fundamental frequency
of a complex tone, and this information exists even if the fundamental
frequency or other harmonics are absent. The reason phase locking has
been linked to pitch perception is that pitch perception occurs only for
frequencies up to about 5,000 Hz, and phase locking also occurs only up
to 5,000 Hz. The idea that tones have pitch only for frequencies up to
5,000 Hz may be surprising, especially given that the audibility curve
(Figure 11.8) indicates that the range of hearing extends up to 20,000
Hz. However, remember from page 270 that pitch is defined as that aspect
of auditory sensation whose variation is associated with musical
melodies (Plack, Barker, & Hall, 2014). This definition is based on the
finding that when tones are strung together to create a melody, we only
perceive a melody if the tones are below 5,000 Hz (Attneave & Olson,
1971). It is probably no coincidence that the highest note on an
orchestral instrument (the piccolo) is about 4,500 Hz. Melodies played
using frequencies above 5,000 Hz sound rather strange. You can tell that
something is changing but it doesn't sound musical. So it seems that our
sense of musical pitch may be limited to those frequencies that create
phase locking. The existence of phase locking below 5,000 Hz, along with
other evidence, has led most researchers to conclude that temporal
coding is the major mechanism of pitch perception.

Problems Remaining to Be Solved You may, at this point, be getting the
idea that there is nothing simple about the physiology of pitch
perception. The complexity of the problem of pitch perception is
highlighted further by research by Andrew Oxenham and coworkers (2011)
in which they asked the question: "Can pitch be perceived for
frequencies above 5,000 Hz?" (which, remember, is supposed to be the
upper frequency limit for perceiving pitch). They answered this question
by showing that if a large number of high-frequency harmonics are
presented, participants do, in fact, perceive pitch. For example, when
presented with 7,200, 8,400, 9,600, 10,800, and 12,000 Hz, which are
harmonics of a tone with 1,200 Hz fundamental frequency, participants
perceived a pitch corresponding to 1,200 Hz, which is the spacing
between the harmonics (although the perception of pitch was weaker than
the perception to lower harmonics). A particularly interesting aspect of
this result is that although each harmonic presented alone did not
result in perception of pitch (because they are all above 5,000 Hz),
pitch was perceived when a number of harmonics were presented together.
This result raises a number of questions. Is it possible that phase
locking occurs above 5,000 Hz? Is it possible that some kind of place
mechanism is responsible for the pitch Oxenham's participants heard? We
don't know the answer to these questions because we don't know what the
limits of phase locking 11.5 The Physiology of Pitch Perception: The
Cochlea

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

281

are in humans. And just to make things even more interesting, it is
important to remember that while pitch perception may depend on the
information created by vibration of the basilar membrane and by the
firing of auditory nerve fibers that are carrying information from the
cochlea, pitch perception is not created by the cochlea. It is created
by the brain.

11.6 The Physiology of Pitch Perception: The Brain Remember that vision
depends on information in the retinal image but our experience of seeing
occurs when this information is transmitted to the cortex. Similarly,
hearing depends on information created by the cochlea, but our
experience of hearing depends on processing that occurs after signals
leave the cochlea. We begin by describing the trip that nerve impulses
take as they travel from the auditory nerve to the auditory cortex.

The Pathway to the Brain Signals generated in the hair cells of the
cochlea are transmitted out of the cochlea in nerve fibers of the
auditory nerve (refer back to Figure 11.16). The auditory nerve carries
the signals generated by the inner hair cells away from the cochlea
along the auditory pathway, eventually reaching the auditory cortex, as
shown in Figure 11.30. Auditory nerve fibers from the cochlea synapse in
a sequence of subcortical structures--- structures below the cerebral
cortex. This sequence begins with

Primary auditory cortex (A1)

Medial geniculate nucleus

Left ear

Superior olivary nucleus

Auditory nerve

Cochlear nucleus

Inferior colliculus

Figure 11.30 Diagram of the auditory pathways. This diagram is greatly
simplified, as numerous connections between the structures are not
shown. Note that auditory structures are bilateral---they exist on both
the left and right sides of the body---and that messages can cross over
between the two sides. (Adapted from Wever, 1949) 282

the cochlear nucleus and continues to the superior olivary nucleus in
the brain stem, the inferior colliculus in the midbrain, and the medial
geniculate nucleus in the thalamus. From the medial geniculate nucleus,
fibers continue to the primary auditory cortex in the temporal lobe of
the cortex. If you have trouble remembering this sequence of structures,
remember the acronym SONIC MG (a very fast sports car), which represents
the three structures between the cochlear nucleus and the auditory
cortex, as follows: SON = superior olivary nucleus; IC = inferior
colliculus; MG = medial geniculate nucleus. A great deal of processing
occurs as signals travel through the subcortical structures along the
pathway from the cochlea to the cortex. Processing in the superior
olivary nucleus is important for locating sounds because it is here that
signals from the left and right ears first meet (indicated by the
presence of both red and blue arrows in Figure 11.30). We will discuss
how signals from the two ears help us locate sounds in Chapter 12.

Pitch and the Brain Something interesting happens as nerve impulses are
traveling up the SONIC MG pathway to the auditory cortex. The temporal
information that dominated pitch coding in the cochlea and auditory
nerve fibers becomes less important. The main indication of this is that
phase locking, which occurred up to about 5,000 Hz in auditory nerve
fibers, occurs only up to 100--200 Hz in the auditory cortex (Oxenham,
2013; Wallace et al., 2000). But while temporal information decreases as
nerve impulses travel toward the cortex, experiments in the marmoset
have demonstrated the existence of individual neurons that seem to be
responding to pitch, and experiments in humans have located areas in the
auditory cortex that also appear to be responding to pitch.

Pitch Neurons in the Marmoset An experiment by Daniel Bendor and Xiaoqin
Wang (2005) determined how neurons in regions partially overlapping the
primary auditory cortex of a marmoset (a species of New World monkey)
responded to complex tones that differed in their harmonic structure but
would be perceived by humans as having the same pitch. When they did
this, they found neurons that responded similarly to complex tones with
the same fundamental frequency but with different harmonic structures.
For example, Figure 11.31a shows the frequency spectra for a tone with a
fundamental frequency of 182 Hz. In the top record, the tone contains
the fundamental frequency and the second and third harmonics; in the
second record, harmonics 4--6 are present; and so on, until at the
bottom, only harmonics 12--14 are present. Even though these stimuli
contain different frequencies (for example, 182, 364, and 546 Hz in the
top record; 2,184, 2,366, and 2,548 Hz in the bottom record), they are
all perceived by humans as having a pitch corresponding to the 182-Hz
fundamental frequency. The corresponding cortical response records
(Figure 11.31b) show that these stimuli all caused an increase in
firing. To demonstrate that this firing occurred only when information
about the 182-Hz fundamental frequency was

Chapter 11  Hearing

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Frequency spectra

Figure 11.31 Records from a pitch neuron recorded from the auditory
cortex of marmoset monkeys. (a) Frequency spectra for tones with a
fundamental frequency of 182 Hz. Each tone contains three harmonic
components of the 182-Hz fundamental frequency. (b) Response of the
neuron to each stimulus. (Adapted from Bendor & Wang, 2005)

Cortical response Harmonic composition

Harmonic composition

f0 1--3 4--6 6--8 8--10 10--12 12--14

500 1,000 1,500 2,000 2,500

1--3 4--6 6--8 8--10 10--12 12--14 0

300

Frequency (Hz)

600

900 1,200

0

Time (ms)

(a) 
(b) 

present, Bendor and Wang showed that the neuron responded well to a
182-Hz tone presented alone, but not to any of the higher harmonics when
they were presented individually. These cortical neurons, therefore,
responded only to stimuli associated with the 182-Hz tone, which is
associated with a specific pitch. For this reason, Bendor and Wang
called these neurons pitch neurons.

Pitch Representation in the Human Cortex Research on where pitch is
processed in the human cortex has used brain scanning (fMRI) to measure
the response to stimuli associated with different pitches. This is not
as simple as it may seem, because when a neuron responds to sound, this
doesn't necessarily mean it is involved in perceiving pitch. To
determine whether areas of the brain are responding to pitch,
researchers have looked for brain regions that are more active in
response to a pitch-evoking sound, such as a complex tone, than to
another sound, such as a band of noise that has similar physical
features but does not produce a pitch. By doing this, researchers hope
to locate brain regions that respond to pitch, irrespective of other
properties of the sound. 70 65

A pitch-evoking stimulus and a noise stimulus used in an experiment by
Sam Norman-Haignere and coworkers (2013) are shown in Figure 11.32. The
pitch stimulus, shown in blue, is the 3rd, 4th, 5th, and 6th harmonics
of a complex tone with a fundamental frequency of 100 Hz (300, 400, 500,
and 600 Hz); the noise, shown in red, consists of a band of frequencies
from 300 to 600 Hz. Because the noise stimulus covers the same range as
the pitch stimulus, it is called frequency-matched noise. By comparing
fMRI responses generated by the pitchevoking stimulus to the response
from the frequency-matched noise, Norman-Haignere located areas in the
primary auditory cortex and some nearby areas that responded more to the
pitch-evoking stimulus. The colored areas in Figure 11.33a show areas in
the human cortex that were tested for their response to pitch. Figure
11.33b shows the proportions of fMRI voxels in each area in which the
response to the pitch stimulus was greater than the response to the
noise stimulus. The areas most responsive to pitch are located in the
anterior auditory cortex---the area close to the front of the brain. In
other experiments, Norman-Haignere determined that the regions that were
most responsive to pitch responded to resolved harmonics, but didn't
respond as well to unresolved harmonics. Because resolved harmonics are
associated with pitch perception, this result strengthens the conclusion
that these cortical areas are involved in pitch perception. Areas tested
for response to pitch

Proportion of voxels responsive to pitch

55 50

Proportion

Power (dB)

60

45 40 35 30 100

Anterior (front)

200

300

400

500

600

700

800

Frequency (Hz)

Figure 11.32 Blue: frequency spectra for the 300-, 400-, 500-, and
600-Hz harmonics of a pitch stimulus with fundamental frequency of 100
HZ. Orange: frequency-matched noise, which covers the same range, but
without the peaks that produce pitch.

(a) 

Posterior (rear)

0.5 0.4 0.3 0.2 0.1 Anterior

Posterior

(b) 

Figure 11.33 (a) Human cortex, showing areas, in color, tested by
Norman-Haignere et al. (2013). (b) Graph showing the proportion of
voxels in each area that responded to pitch. The more anterior areas
(located toward the front of the brain) contained more pitchresponsive
voxels. 11.6 The Physiology of Pitch Perception: The Brain

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

283

As we indicated at the beginning of this discussion, determining areas
of the brain that respond to pitch involves more than just presenting
tones and measuring responses. Researchers in many laboratories have
identified auditory areas in the human that respond to pitch, although
the results from different laboratories have varied slightly because of
differences in stimuli and procedures. The exact location of the human
pitchresponding areas is, therefore, still being discussed (Griffiths,
2012; Griffiths & Hall, 2012; Saenz & Langers, 2014). Whereas most of
the early research on the auditory system focused on the cochlea and
auditory nerve, the auditory cortex has become a major focus of recent
research. We will consider more research on the brain when we describe
the mechanisms responsible for locating sounds in space and for the
perceptual organization of sound (Chapter 12) and for how we perceive
music (Chapter 13) and speech (Chapter 14).

11.7 Hearing Loss Roughly 17 percent of the U.S. adult population
suffers from some form of impaired hearing (Svirsky, 2017). These losses
occur for a number of reasons. One cause of hearing loss is noise in the
environment, as the ears are often bombarded with noises such as crowds
of people talking (or yelling, if at a sporting event), construction
sounds, and traffic noise. Noises such as these are the most common
cause of hearing loss. Hearing loss is usually associated with damage to
the outer hair cells, and recent evidence indicates that damage to
auditory nerve fibers may be involved as well. When the outer hair cells
are damaged, the response of the basilar membrane becomes similar to the
broad response seen for the dead cochleas examined by Békésy; this
results in a loss of sensitivity (inability to hear quiet sounds) and a
loss of the sharp frequency tuning seen in healthy ears, as shown in
Figure 11.27 (Moore, 1995; Plack et al., 2004). The broad tuning makes
it harder for hearing-impaired people to separate out sounds---for
example, to hear speech sounds in noisy environments. Inner hair cell
damage can also cause a loss of sensitivity. For both inner and outer
hair cells, hearing loss occurs for the frequencies corresponding to the
frequencies detected by the damaged hair cells. Sometimes inner hair
cells are lost over an entire region of the cochlea (a "dead region"),
and sensitivity to

the frequencies that normally excite that region of the cochlea becomes
much reduced. Of course, you wouldn't want to purposely damage your hair
cells, but sometimes we expose ourselves to sounds that over the long
term do result in hair cell damage. One of the things that contributes
to hair cell damage is living in an industrialized environment, which
contains sounds that contribute to a type of hearing loss called
presbycusis.

Presbycusis Presbycusis is caused by hair cell damage resulting from the
cumulative effects over time of noise exposure, the ingestion of drugs
that damage the hair cells, and age-related degeneration. The loss of
sensitivity associated with presbycusis, which is greatest at high
frequencies, affects males more severely than females. Figure 11.34
shows the progression of loss as a function of age. Unlike the visual
problem of presbyopia (see Chapter 3, page 45), which is an inevitable
consequence of aging, presbycusis is more likely to be caused by factors
in addition to aging; people in preindustrial cultures, who have not
been exposed to the noises that accompany industrialization or to drugs
that could damage the ear, often do not experience large decreases in
high-frequency hearing in old age. This may be why males, who
historically have been exposed to more workplace noise than females, as
well as to noises associated with hunting and wartime, experience a
greater presbycusis effect. Although presbycusis may be unavoidable,
since most people are exposed over a long period of time to the everyday
sounds of our modern environment, there are situations in which people
expose their ears to loud sounds that could be avoided. This exposure to
particularly loud sounds results in noise-induced hearing loss.

Noise-Induced Hearing Loss Noise-induced hearing loss occurs when loud
noises cause degeneration of the hair cells. This degeneration has been
observed in examinations of the cochleas of people who have worked in
noisy environments and have willed their ear structures to medical
research. Damage to the organ of Corti is

Women

Men 0

20 50--59

40 60

70--74

80

> 85

100

20 40 50--59

60

70--74

80

> 85

100 0.25 0.50 1.0 2.0 4.0 Frequency (kHz)

284

Hearing loss (dB)

0 Hearing loss (dB)

Figure 11.34 Hearing loss in presbycusis as a function of age. All of
the curves are plotted relative to the 20-year-old curve, which is taken
as the standard. (Adapted from Bunch, 1929)

8.0

0.25 0.50 1.0 2.0 4.0 Frequency (kHz)

8.0

Chapter 11  Hearing

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Hidden Hearing Loss Is it possible to have normal hearing as measured by
a standard hearing test, but to have trouble understanding speech in
noisy environments? The answer for a large number of people is "yes."
People with "normal" hearing who have trouble hearing in noisy
environments may be suffering from a recently discovered type of hearing
loss called hidden hearing loss (Plack, Barker, & Prendergast, 2014). We
can understand why this type of hearing loss occurs by considering what
the standard hearing test measures. The standard hearing test involves
measuring thresholds for hearing tones across the frequency spectrum.
The person

sits in a quiet room and is instructed to indicate when he or she hears
very faint tones being presented by the tester. The results of this test
can be plotted as thresholds covering a range of frequencies---like the
audibility curve in Figure 11.8, or as an audiogram---a plot of hearing
loss versus frequency, like the curves in Figure 11.34. "Normal" hearing
is indicated by a horizontal function at 0 dB on the audiogram,
indicating no deviation from the normal standard. This hearing test,
along with the audiograms it produces, has been called the gold standard
of hearing test function (Kujawa & Liberman, 2009). One reason for the
popularity of this test is that it is thought to indicate hair cell
functioning. But for hearing complex sounds like speech, especially
under noisy conditions such as at a party or in the noise of city
traffic, the auditory nerve fibers that transmit signals from the
cochlea are also important. Sharon Kujawa and Charles Liberman (2009)
determined the importance of having intact auditory nerve fibers through
experiments on the effect of noise on hair cells and auditory nerve
fibers in the mouse. Kujawa and Liberman exposed the mice to a 100-dB
SPL noise for 2 hours and then measured their hair cell and auditory
nerve functioning using physiological techniques we won't describe here.
Figure 11.35a shows the results for the hair cells, when tested with a
75-dB tone. One day after the noise exposure, hair cell function was
decreased below normal (with normal indicated by the dashed line).
However, by 8 weeks after the noise exposure, hair cell function had
returned almost to normal. Figure 11.35b shows the response of the
auditory nerve fibers to the 75-dB tone. Their function was also
decreased right after the noise, but unlike the hair cells, auditory
nerve function never returned to normal. The response of nerve fibers to
low-level sounds did recover completely, but the response to high-level
sounds, like the 75-dB tone, remained below normal. This lack of
recovery reflects the fact that the noise exposure had permanently
damaged some of the auditory nerve fibers, particularly those that
represent information about high sound levels. It is thought that
similar effects occur in humans, so that

100

Relative response

often observed in these cases. For example, examination of the cochlea
of a man who worked in a steel mill indicated that his organ of Corti
had collapsed and no receptor cells remained (Miller, 1974). More
controlled studies of animals exposed to loud sounds provide further
evidence that high-intensity sounds can damage or completely destroy
inner hair cells (Liberman & Dodds, 1984). Because of the danger to hair
cells posed by workplace noise, the United States Occupational Safety
and Health Agency (OSHA) has mandated that workers not be exposed to
sound levels greater than 85 dB for an 8-hour work shift. In addition to
workplace noise, however, other sources of intense sound can cause hair
cell damage leading to hearing loss. If you turn up the volume on your
smartphone, you are exposing yourself to what hearing professionals call
leisure noise. Other sources of leisure noise are activities such as
recreational gun use, riding motorcycles, playing musical instruments,
and working with power tools. A number of studies have demonstrated
hearing loss in people who listen to music with earphones (Okamoto et
al., 2011; Peng et al., 2007), play in rock/ pop bands (Schmuziger et
al., 2006), use power tools (Dalton et al., 2001), and attend sports
events (Hodgetts & Liu, 2006). The amount of hearing loss depends on the
level of sound intensity and the duration of exposure. Given the high
levels of sound that occur in these activities, such as the levels above
90 dB SPL that can occur for the 3 hours of a hockey game (Hodgetts &
Liu, 2006), about 100 dB SPL for music venues such as clubs or concerts
(Howgate & Plack, 2011), and levels as high as 90 dB SPL while using
power tools in woodworking, it isn't surprising that both temporary and
permanent hearing losses are associated with these leisure activities.
These findings suggest that it might make sense to use ear protection
when in particularly noisy environments and to turn down the volume on
your phone. The potential for hearing loss from listening to music at
high volume for extended periods of time cannot be overemphasized,
because at their highest settings, smartphones reach levels of 100 dB
SPL or higher---far above OSHA's recommended maximum of 85 dB. This has
led Apple Computer to add a setting to their devices that limits the
maximum volume, although an informal survey of my students indicates,
not surprisingly, that few of them use this feature.

Normal

100

80

80

60

60

40

40

20

20

0

1 Day

8 Weeks

(a) Hair cell response

0

Normal

1 Day

8 Weeks

(b) Auditory nerve response

Figure 11.35 (a) Mouse hair cell response, as a percentage of normal, to
a 75-dB SPL tone following a 2-hour exposure to a 100-dB SPL tone. The
response is greatly decreased compared to normal (indicated by the
dashed line) 1 day after the exposure but has increased back to normal
by 8 weeks after the exposure. (b) The response of auditory nerve fibers
is also decreased 1 day after the exposure but fails to recover at 8
weeks, indicating permanent damage. (Based on data from Kujawa &
Liberman, 2009) 11.7 Hearing Loss

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

285

even when people have normal sensitivity to low-level sounds and
therefore have "clinically normal" hearing, the damaged auditory nerve
fibers are responsible for problems hearing speech in noisy environments
(Plack, Barker, & Prendergast, 2014). What is important about this
result is that even though some auditory nerve fibers were permanently
damaged, the behavioral thresholds to quiet sounds had returned to
normal. Thus, a normal audiogram does not necessarily indicate normal
auditory functioning. This is why hearing loss due to nerve fiber damage
has been described as "hidden" hearing loss (Schaette & McAlpine, 2011).
Hidden hearing loss can be lurking in the background, causing serious
problems in day-to-day functioning that involves hearing in noisy
environments. Further research on hidden hearing loss is focusing on
determining what causes it and on developing a test to detect it so this
type of hearing loss will no longer be hidden (Plack, Barker, &
Prendergast, 2014).

How does banging on the drum turn into the sound BOOM?

SOMETHING TO CONSIDER:

What makes some vibrations create a drum's lowpitched BOOM and others
create a bird's highpitched tweet? Slow vibrations create low pitches
and faster vibrations create high pitches, so the hairs vibrate more
slowly for BOOM and faster for tweet.

Explaining Sound to an 11-Year Old How would you answer the question
"What is Sound" in 300 words or less, in a way that would be meaningful
to an 11-yearold? That was the assignment for the 2016 edition of the
Flame Challenge, which was run by the Alan Alda Center for Communicating
Science at Stony Brook University. Given the word limitation and the
11-year-old audience (who voted on the finalists to determine the
winner), it's best to minimize technical details and focus on general
principles. The entry below, by your author (BG), won first place: A
drummer bangs on a bass drum. Sam, standing nearby, hears BOOM!

Sounds are vibrations, and the drum-head's back-andforth vibrations
create pressure waves in the air that set Sam's eardrums, just inside
his ears, into vibration. The magic of sound happens deeper inside Sam's
ears in a hollow tube-like structure called the inner ear or cochlea.
Imagine that you've shrunk yourself so small that you can look into this
tube. When you peek inside, you see thousands of tiny hairs lined up in
rows. Suddenly, the drummer bangs the drum! You feel the vibrations, and
then you see something spectacular---the hairs are moving back and forth
in time with the vibrations, and every movement is creating electrical
signals! These signals are sent down the auditory nerve towards the
brain and a fraction of a second later, when they reach the hearing
areas in the brain, Sam hears BOOM!

But sound is more than BOOM and tweet. You create sounds when talking
with friends or playing music. Music is really amazing, because when the
tiny hairs vibrate back and forth to music, electricity reaches the
brain's hearing areas, plus other brain areas that make you move and
that make you feel emotions like happy or sad. So sounds are vibrations
that make you hear, and might also make you feel like tapping your feet,
dancing, crying, or even jumping for joy. Pretty amazing, what tiny
hairs vibrating inside the ear can do! The Flame Challenge poses
different questions each year. How would you answer the question for
2014, "What is Color?"

DEVELOPMENTAL DIMENSION Infant Hearing What do newborn infants hear, and
how does hearing develop as infants get older? Although some early
psychologists believed that newborns were functionally deaf, recent
research has shown that newborns do have some auditory capacity and that
this capacity improves as the child gets older (Werner & Bargones,
1992).

Thresholds and the Audibility Curve What do infant audibility curves
look like, and how do their thresholds compare to adults'? Lynne Werner
Olsho and coworkers (1988) used the following procedure to determine
infants' audibility curves: An infant is fitted with earphones and sits
on the parent's lap. An observer, sitting out of view

286

of the infant, watches the infant through a window. A light blinks on,
indicating that a trial has begun, and a tone is either presented or
not. The observer's task is to decide whether the infant heard the tone
(Olsho et al., 1987). How can observers tell whether the infant has
heard a tone? They decide by looking for responses such as eye
movements, changes in facial expression, a wide-eyed look, a turn of the
head, or changes in activity level. These judgments resulted in the
curve in Figure 11.36a for a 2,000-Hz tone (Olsho et al., 1988).
Observers only occasionally indicated that the 3-month-old infants had
heard a tone that was presented at low intensity or not at all;
observers were more likely to say that the infant had heard the tone
when the tone was presented at high intensity. The infant's threshold
was determined from

Chapter 11  Hearing

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

50

80 60 40 20 0

(a) 

Threshold (dB SPL)

Percentage "yes" responses

100

NS

10

20 30 dB SPL

40

50

40

20 Adults

6 months

10 0 --10 100

60

3 months

30

(b) 

this curve, and the results from a number of other frequencies were
combined to create audibility functions such as those in Figure 11.36b.
The curves for 3- and 6-month-olds and adults indicate that infant and
adult audibility functions look similar and that by 6 months of age the
infant's threshold is within about 10 to 15 dB of the adult threshold.

Recognizing Their Mother's Voice Another approach to studying hearing in
infants has been to show that newborns can identify sounds they have
heard before. Anthony DeCasper and William Fifer (1980) demonstrated
this capacity in newborns by showing that 2-day-old infants will modify
their sucking on a nipple in order to hear the sound of their mother's
voice. They first observed that infants usually suck on a nipple in
bursts separated by pauses. They fitted infants with earphones and let
the length of the pause in the infant's sucking determine whether the
infant heard a recording of the mother's voice or a recording of a
stranger's voice (Figure 11.37). For half of the infants, long pauses
activated the tape of the mother's voice, and short pauses activated

Figure 11.37 This baby, from DeCasper and Fifer's (1980) study, could
control whether she heard a recording of her mother's voice or a
stranger's voice by the way she sucked on the nipple. (From DeCasper &
Fifer, 1980)

1,000 Frequency (Hz)

10,000

Figure 11.36 (a) Data obtained by Olsho et al. (1987), showing the
percentage of trials on which the observer indicated that a 3-month-old
infant had heard 2,000-Hz tones presented at different intensities. NS
indicates no sound. (b) Audibility curves for 3- and 6-month-old infants
determined from functions like the one in (a). The curve for
12-month-olds, not shown here, is similar to the curve for 6-month-olds.
The adult curve is shown for comparison. (Adapted from Olsho et al.,
1988)

the tape of the stranger's voice. For the other half, these conditions
were reversed. DeCasper and Fifer found that the babies regulated the
pauses in their sucking so that they heard their mother's voice more
than the stranger's voice. This is a remarkable accomplishment for a
2-day-old, especially because most had been with their mothers for only
a few hours between birth and the time they were tested. Why did the
newborns prefer their mother's voice? DeCasper and Fifer suggested that
newborns recognized their mother's voice because they had heard the
mother talking during development in the womb. This suggestion is
supported by the results of another experiment, in which DeCasper and M.
J. Spence (1986) had one group of pregnant women read from Dr. Seuss's
book The Cat in the Hat and another group read the same story with the
words cat and hat replaced with dog and fog. When the children were
born, they regulated the pauses in their sucking in a way that caused
them to hear the version of the story their mother had read when they
were in the womb. Moon and coworkers (1993) obtained a similar result by
showing that 2-day-old infants regulated their sucking to hear a
recording of their native language rather than a foreign language (see
also DeCasper et al., 1994). The idea that fetuses become familiar with
the sounds they hear in the womb was supported by Barbara Kisilevsky and
coworkers (2003), who presented loud (95-dB) recordings of the mother
reading a 2-minute passage and a stranger reading a 2-minute passage
through a loudspeaker held 10 cm above the abdomen of full-term pregnant
women. When they measured the fetus's movement and heart rate as these
recordings were being presented, they found that the fetus moved more in
response to the mother's voice, and that heart rate increased in
response to the mother's voice but decreased in response to the
stranger's voice. Kisilevsky concluded from these results that fetal
voice processing is influenced by experience, just as the results of
earlier experiments had suggested (see also Kisilevsky et al., 2009).

Something to Consider: Explaining Sound to an 11-Year Old

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

287

TEST YOuRSELF 11.3 1. Describe place theory. 2. How is place theory
challenged by the effect of the missing fundamental? How can a
modification of place theory explain the effect of the missing
fundamental? 3. Describe the Burns and Viemeister experiment, which used
amplitude-modulated noise, and its implications for place theory. 4.
What is the evidence supporting the idea that pitch perception depends
on the timing of auditory nerve firing? 5. What are resolved and
unresolved harmonics? What is the connection between resolved harmonics
and place theory? 6. What problems do Oxenham et al's. (2011) experiment
pose for understanding the physiology of pitch perception? 7. Describe
the pathway that leads from the ear to the brain.

8.  Describe the experiments that suggest a relationship between the
    firing of neurons in the auditory cortex and the pitch of complex
    tones in (a) the marmoset and (b) humans.
9.  What is the connection between hair cell damage and hearing loss?
    Exposure to occupational or leisure noise and hearing loss?
10. What is hidden hearing loss?
11. How would you summarize the main point of the essay on "What is
    Sound," which was written for 11-year olds, in one or two sentences?
12. Describe the procedures for measuring auditory thresholds in
    infants. How does the infant's audibility curve compare to the adult
    curve?
13. Describe experiments that show that newborn infants can recognize
    their mother's voice, and that this capacity can be traced to the
    infants' having heard the mother talking during development in the
    womb.

THINK ABOUT IT 1. We saw that decibels are used to compress the large
range of sound pressures in the environment into more manageable
numbers. Describe how this same principle is used in the Richter scale
to compress the range of earth vibrations from barely perceptible
tremors to major earthquakes into a smaller range of numbers.

2.  Presbycusis usually begins with loss of high-frequency hearing and
    gradually involves lower frequencies. From what you know about
    cochlear function, can you explain why the high frequencies are more
    vulnerable to damage? (p. 284)

KEY TERMS Amplitude (p. 265) Amplitude modulation (p. 281)
Amplitude-modulated noise (p. 281) Aperiodic sound (p. 271) Apex (of the
cochlea or basilar membrane) (p. 277) Attack (p. 271) Audibility curve
(p. 269) Audiogram (p. 285) Auditory canal (p. 272) Auditory response
area (p. 269) Base (of the cochlea or basilar membrane) (p. 277) Basilar
membrane (p. 274) Characteristic frequency (p. 278) Cochlea (p. 273)
Cochlear amplifier (p. 279) Cochlear nucleus (p. 282) Cochlear partition
(p. 273) Decay (p. 271) Decibel (dB) (p. 266) 288

Eardrum (p. 272) Effect of the missing fundamental (p. 270) Equal
loudness curve (p. 269) First harmonic (p. 268) Frequency (p. 265)
Frequency spectra (p. 268) Frequency tuning curve (p. 278) Fundamental
(p. 268) Fundamental frequency (p. 268) Hair cells (p. 274) Harmonic
(p. 268) Hertz (Hz) (p. 266) Hidden hearing loss (p. 285) Higher
harmonics (p. 268) Incus (p. 272) Inferior colliculus (p. 282) Inner ear
(p. 273) Inner hair cells (p. 274) Leisure noise (p. 285) Level (p. 267)

Loudness (p. 268) Malleus (p. 272) Medial geniculate nucleus (p. 282)
Middle ear (p. 272) Middle-ear muscles (p. 273) Noise (p. 281)
Noise-induced hearing loss (p. 284) Octave (p. 270) Organ of Corti
(p. 274) Ossicles (p. 272) Outer ear (p. 272) Outer hair cells (p. 274)
Oval window (p. 272) Periodic sound (p. 271) Periodic waveform (p. 268)
Phase locking (p. 276) Pinnae (p. 272) Pitch (p. 270) Pitch neuron
(p. 283) Place theory of hearing (p. 283) Presbycusis (p. 284)

Chapter 11  Hearing

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Primary auditory cortex (p. 282) Pure tone (p. 265) Resolved harmonics
(p. 281) Resonance (p. 272) Resonant frequency (p. 272) Sound (p. 264)
Sound level (p. 267) Sound pressure level (SPL) (p. 267)

Sound wave (p. 265) Stapes (p. 272) Stereocilia (p. 274) Subcortical
structures (p. 282) Superior olivary nucleus (p. 282) Tectorial membrane
(p. 274) Temporal coding (p. 281) Timbre (p. 271)

Tip links (p. 275) Tone chroma (p. 270) Tone height (p. 270) Tonotopic
map (p. 278) Traveling wave (p. 276) Tympanic membrane (p. 272)
Unresolved harmonics (p. 281)

Key Terms

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

289

Someone sitting at a riverside table along the San Antonio Riverwalk
could be hearing sounds created by conversations with others, passing
boats, and music from the restaurants' loudspeakers. Despite this
complexity of sounds in the environment, our auditory system is able to
determine where sounds are coming from and to separate sounds that are
created by different sources. iStock.com/dszc

Learning Objectives After studying this chapter, you will be able to ...
■■ Describe experiments that show how people use different cues

to determine the location of a sound source. ■■ Describe the
physiological processes that are involved in deter-

mining the location of a sound source. ■■ Understand how our perception
of sound location is determined

when listening to sounds inside a room.

■■ Understand how auditory scene analysis describes how we sep-

arate different sound sources that are occurring simultaneously in the
environment. ■■ Describe a number of ways hearing and vision interact in
the

environment. ■■ Describe interconnections between vision and hearing in
the

brain.

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

C ha p ter 1 2

Hearing in the Environment Chapter Contents 12.1 Sound Source
Localization

Architectural Acoustics

Binaural Cues for Sound Localization Spectral Cues for Localization

TEST YOURSELF 12.1

12.2 The Physiology of Auditory Localization

Simultaneous Grouping Sequential Grouping

TEST YOURSELF 12.2

SOMETHING TO CONSIDER:

THINK ABOUT IT

The Jeffress Neural Coincidence Model Broad ITD Tuning Curves in Mammals
Cortical Mechanisms of Localization

12.3 Hearing Inside Rooms Perceiving Two Sounds That Reach the Ears at
Different Times

12.4 Auditory Scene Analysis

Understanding Speech Interactions in the Brain Echolocation in Blind
People Listening to or Reading a Story

Interactions Between Hearing and Vision The Ventriloquism Effect The
Two-Flash Illusion

Some Questions We Will Consider: ■■ What makes it possible to tell where
a sound is coming

But how did you know to turn to the right, and where to look? Somehow
you could tell where the sound was coming from. This is sound
localization (p. 292).

from in space? (p. 292)

T

Scenario 2: Some Sounds Inside You're inside a deli, which is actually
just a small room with a meat counter at one end. You take a number and
are waiting your turn as the butcher calls numbers, one by one. Why do
you hear each number only once, despite the fact that the sound waves
the butcher is producing when he speaks travel multiple paths to reach
your ears: (1) a direct path, from his mouth to your ears; and (2)
multiple paths involving reflections off of the countertop, the walls,
the ceiling, etc. As you will see, what you hear depends mainly on sound
reaching your ears along the first path, a phenomenon called the
precedence effect (p. 300).

Scenario 1: Something Suddenly Happens Outside You're walking down the
street, lost in thought, although paying enough attention to avoid
bumping into oncoming pedestrians. Suddenly, you hear a screech of
brakes and a woman screaming. You quickly turn to the right and see that
no one was hurt.

Scenario 3: A Conversation With a Friend You're sitting in a coffee
shop, talking with a friend. But there are many other sounds as
well---other people talking nearby, the occasional screech of the
espresso machine, music from a speaker overhead, a car jams on its
brakes outside. How you can separate the sounds your friend is speaking
from all the other sounds in the room? The ability to separate each of
the sound sources and separate them in space is achieved by a process

■■ Why does music sound better in some concert halls than

in others? (p. 301) ■■ When we are listening to a number of musical
instru-

ments playing at the same time, how can we perceptually separate the
sounds coming from the different instruments? (p. 302)

he last chapter was focused mainly on laboratory studies of pitch,
staying mostly within the inner ear, with a trip to the cortex. This
chapter broadens our perception beyond pitch to consider other auditory
qualities, most of which depend on higher-order processes. Here are
three "scenarios," each of which is relevant to one of the auditory
qualities we will discuss.

291

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

called auditory stream analysis (p. 302). While all this is going on,
you are able to hear what your friend is saying word by word, and to
group her words together to create sentences. This is perceptual
grouping (p. 303).

12.1 Sound Source Localization

This chapter considers each of these situations. We begin by describing
mechanisms that enable us to determine where sound is coming from
(Scenario 1). We then consider the mechanisms that help us not be
confused by sound waves that are bouncing off the walls of a room, with
a side-trip to consider architectural acoustics (Scenario 2). We then
move on to auditory scene analysis, which involves perceptually
separating and arranging sounds in auditory space and grouping sounds
coming from a single source (Scenario 3). Every sound comes from
someplace. This may sound like an obvious statement because, of course,
something, with a specific location, must be producing each sound. But
while we often pay attention to where visible objects are, because they
may be destinations to reach, things to avoid, or scenes to observe, we
often pay less attention to where sounds are coming from. But locating
the sources of sounds, especially ones that might signal danger, can be
important for our survival. And even though most sounds don't signal
danger, sounds and their locations are constantly structuring our
auditory environment. In this section, we describe how you are able to
extract information that indicates the location of a sound's source, and
how the brain uses this information to create a neural representation of
sounds in space.

Let's begin by making an observation. Close your eyes for a moment,
listen, and notice what sounds you hear and where they are coming from.
This works best if you aren't in a totally quiet environment! The
results of my observation, sitting in a coffee shop, revealed multiple
sounds, coming from different locations. I hear the beat and vocals of a
song coming from a speaker above my head and slightly behind me, a woman
talking somewhere in front of me, and the "fizzy" sound of an espresso
maker off to the left. I hear each of the sounds---the music, the
talking, and the mechanical fizzing sound, as coming from different
locations in space. These sounds at different locations create an
auditory space, which exists all around, wherever there is sound. The
locating of sound sources in auditory space is called auditory
localization. We can appreciate the problem the auditory system faces in
determining these locations by comparing the information for location
for vision and hearing. Consider the tweeting bird and the meowing cat
in Figure 12.1. Visual information for the relative locations of the
bird and the cat is contained in the images of the bird and the cat on
the surface of the retina. The ear, however, is different. The bird's
"tweet, tweet" and the cat's "meow" stimulate the cochlea based on their
sound frequencies, and as we saw

"Tweet, tweet"

Cat

Tweet Meow Tweet Bird

"Meow"

Figure 12.1 Comparing location information for vision and hearing.
Vision: The bird and the cat, which are located at different places, are
imaged on different places on the retina. Hearing: The frequencies in
the sounds from the bird and cat are spread out over the cochlea, with
no regard to the animals' locations. 292

Chapter 12  Hearing in the Environment

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Azimuth (left--right)

Distance

Elevation (up--down)

Figure 12.2 The three directions used for studying sound localization:
azimuth (left--right), elevation (up--down), and distance.

in Chapter 11, these frequencies cause patterns of nerve firing that
result in our perception of a tone's pitch and timbre. But activation of
nerve fibers in the cochlea is based on the tones' frequency components
and not on where the tones are coming from. This means that two tones
with the same frequency that originate in different locations will
activate the same hair cells and nerve fibers in the cochlea. The
auditory system must therefore use information other than the place on
the cochlea to determine location. This information takes the form of
location cues that are created by the way sound interacts with the
listener's head and ears. There are two kinds of location cues: binaural
cues, which depend on both ears, and spectral cues, which depend on just
one ear. Researchers studying these cues have determined how well people
can utilize these cues to locate the position of a sound in three
dimensions: the azimuth, which extends from left to right (Figure 12.2);
elevation, which extends up and down; and the distance of the sound
source from the listener. Localization in distance is much less accurate
than azimuth or elevation localization, working best when the sound
source is familiar, or when cues are available from room reflections. In
this chapter, we will focus on the azimuth and elevation.

Binaural Cues for Sound Localization Binaural cues use information
reaching both ears to determine the azimuth (left--right position) of
sounds. The two binaural cues are interaural level difference and
interaural time difference. Both are based on a comparison of the sound
signals reaching the left and right ears. Sounds that are off to the
side are more intense at one ear than the other and reach one ear before
the other.

Interaural Level Difference Interaural level difference (ILD) is based
on the difference in the sound pressure level (or just "level") of the
sound reaching the two ears. A difference in level between the two ears
occurs because the head is a barrier that creates an acoustic shadow,
reducing the intensity of

sounds that reach the far ear. This reduction of intensity at the far
ear occurs for high-frequency sounds (greater than about 3,000 Hz for
humans), as shown in Figure 12.3a, but not for low-frequency sounds, as
shown in Figure 12.3b. We can understand why an ILD occurs for high
frequencies but not for low frequencies by drawing an analogy between
sound waves and water waves. Consider, for example, a situation in which
small ripples in the water are approaching the boat in Figure 12.3c.
Because the ripples are small compared to the boat, they bounce off the
side of the boat and go no further. Now imagine the same ripples
approaching the cattails in Figure 12.3d. Because the distance between
the ripples is large compared to the stems of the cattails, the ripples
are hardly disturbed and continue on their way. These two examples
illustrate that an object has a large effect on the wave if it is larger
than the distance between the waves (as occurs when short high-frequency
sound waves hit the head), but has a small effect if it is smaller than
the distance between the waves (as occurs for longer low-frequency sound
waves). For this reason, the ILD is an effective cue for location only
for high-frequency sounds.

Interaural Time Difference The other binaural cue, interaural time
difference (ITD), is the time difference between when a sound reaches
the left ear and when it reaches the right ear (Figure 12.4). If the
source is located directly in front of the listener, at A, the distance
to each ear is the same; the sound reaches the left and right ears
simultaneously, so the ITD is zero. However, if a source is located off
to the side, at B, the sound reaches the right ear before it reaches the
left ear. Because the ITD becomes larger as sound sources are located
more to the side, the magnitude of the ITD can be used as a cue to
determine a sound's location. Behavioral experiments show that ITD is
most effective for determining the locations of lowfrequency sounds
(Yost & Zhong, 2014) and ILD is most effective for high-frequency
sounds, so between them they cover the frequency range for hearing.
However, because most sounds in the environment contain low-frequency
components, ITD is the dominant binaural cue for hearing (Wightman &
Kistler, 1992). The Cone of Confusion While the time and level
differences provide information that enables people to judge location
along the azimuth coordinate, they provide ambiguous information about
the elevation of a sound source. You can understand why this is so by
imagining you are extending your hand directly in front of you at arm's
length and are holding a sound source. Because the source would be
equidistant from your left and right ears, the time and level
differences would be zero. If you now imagine moving your hand straight
up, increasing the sound source's elevation, the source will still be
equidistant from the two ears, so both time and level differences are
still zero. Because the time and level differences can be the same at a
number of different elevations, they cannot reliably indicate the
elevation of the sound source. Similar ambiguous information is provided
when the sound source is off to the side. 12.1 Sound Source Localization

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

293

Acoustic shadow

6,000 Hz

200 Hz

Spacing small compared to object

Spacing large compared to object

(a) 
(b) 
(c) 
(d) 

Figure 12.3 Why interaural level difference (ILD) occurs for high
frequencies but not for low frequencies. (a) Person listening to a
high-frequency sound; (b) person listening to a low-frequency sound. (c)
When the spacing between waves is smaller than the size of the object,
illustrated here by water ripples that are smaller than the boat, the
waves are stopped by the object. This occurs for the high-frequency
sound waves in (a) and causes the sound intensity to be lower on the far
side of the listener's head. (d) When the spacing between waves is
larger than the size of the object, as occurs for the water ripples and
the narrow stalks of the cattails, the object does not interfere with
the waves. This occurs for the low-frequency sound waves in (b), so the
sound intensity on the far side of the head is not affected.

A

B

These places of ambiguity are illustrated by the cone of confusion shown
in Figure 12.5. All points on the surface of this cone have the same ILD
and ITD. For example, points A and B would result in the same ILD and
ITD because the distance from A to the left and right ears is the same
as the distance from B to the right and left ears. Similar situations
occur for other points on the cone, and there are other smaller and
larger cones as well. In other words, there are many locations in space
where two sounds could result in the same ILD and ITD.

Spectral Cues for Localization Figure 12.4 The principle behind
interaural time difference (ITD). The tone directly in front of the
listener, at A, reaches the left and right ears at the same time.
However, when the tone is moved to the side, at B, it reaches the
listener's right ear before it reaches the left ear. 294

The ambiguous nature of the information provided by the ILD and ITD at
different elevations means that another source of information is needed
to locate sounds along the elevation coordinate. This information is
provided by spectral cues---cues in which information for localization
is contained in differences in the distribution (or spectrum) of
frequencies that

Chapter 12  Hearing in the Environment

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

sound (one containing many frequencies) is presented at elevations of 15
degrees above the head and 15 degrees below the head. Sounds coming from
these two locations would result in the same ILD and ITD because they
are the same distance from the left and right ears, but differences in
the way the sounds bounce around within the pinna create different
patterns of frequencies for the two locations (King et al., 2001). The
importance of the pinnae for determining elevation has been demonstrated
by showing that smoothing out the nooks and crannies of the pinnae with
molding compound makes it difficult to locate sounds along the elevation
coordinate (Gardner & Gardner, 1973). The idea that localization can be
affected by using a mold to change the inside contours of the pinnae was
also demonstrated by Paul Hofman and coworkers (1998). They determined
how localization changes when the mold is worn for several weeks, and
then what happens when the mold is removed. The results for one
listener's localization performance measured before the mold was
inserted are shown in Figure 12.7a. Sounds were presented at positions
indicated by the intersections of the blue grid. Average localization
performance is indicated by the red grid. The overlap between the two
grids indicates that localization was fairly accurate. After measuring
initial performance, Hofman fitted his listeners with molds that altered
the shape of the pinnae and therefore changed the spectral cue. Figure
12.7b shows that localization performance is poor for the elevation
coordinate immediately after the mold is inserted, but locations can
still be judged at locations along the azimuth coordinate. This is
exactly what we would expect if binaural cues are used for judging
azimuth location and spectral cues are responsible for judging elevation
locations.

A

Bruce Goldstein

B

Figure 12.5 The "cone of confusion." There are many pairs of points on
this cone that have the same left-ear distance and rightear distance and
so result in the same ITD and ILD. There are also other cones in
addition to this one.

reach each ear from different locations. These differences are caused by
the fact that before the sound stimulus enters the auditory canal, it is
reflected from the head and within the various folds of the pinnae
(Figure 12.6a). The effect of this interaction with the head and pinnae
has been measured by placing small microphones inside a listener's ears
and comparing frequencies from sounds that are coming from different
directions. This effect is illustrated in Figure 12.6b, which shows the
frequencies picked up by the microphone when a broadband

15° Elevation --15° Elevation

10 dB

15°

--15° lobe

Bruce Goldstein.

pinna

3

4

5

6

7

8

9

10

Frequency (kHz) (a)

(b) 

Figure 12.6 (a) Pinna showing sound bouncing around in nooks and
crannies. (b) Frequency spectra recorded by a small microphone inside
the listener's right ear for the same broadband sound coming from two
different locations. The difference in the pattern when the sound is 15
degrees above the head (blue curve) and 15 degrees below the head (red
curve) is caused by the way different frequencies bounce around within
the pinna when entering it from different angles. (Adapted from Plack,
2005; photo by Bruce Goldstein) 12.1 Sound Source Localization

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

295

Pre-control 30

0

--30 (a) Day 0 30

0

--30 (b)

Response elevation (deg)

Day 5 30

0

--30 (c) Day 19 30

0

--30 (d) Post-control 30

0

--30 0 --30 30 Response azimuth (deg) (e)

Figure 12.7 How localization changes when a mold is placed in the ear.
See text for explanation. (Reprinted from King et al., 2001)

Hofman continued his experiment by retesting localization as his
listeners continued to wear the molds. You can see from Figures 12.7c
and 12.7d that localization performance improved, until by 19 days
localization had become reasonably accurate. Apparently, the person had
learned, over a period of weeks, to associate new spectral cues to
different directions in space. What do you think happened when the molds
were removed? It would be logical to expect that once adapted to the new
set of spectral cues created by the molds, localization performance
would suffer when the molds were removed. However, as shown in Figure
12.7e, localization remained excellent immediately after removal of the
ear molds. Apparently, training with the molds created a new set of
correlations between spectral cues and location, but the old correlation
was still there as well. One way this could occur is if different sets
of neurons were involved in responding to each set of spectral cues,
just as separate brain areas are involved in processing different
languages in people who have learned a second language as adults (King
et al., 2001; Wightman & Kistler, 1998; also see Van Wanrooij & Van
Opstal, 2005). We have seen that each type of cue works best for
different frequencies and different coordinates. ILDs and ITDs work for
judging azimuth location, with ILD best for high frequencies and ITD for
low frequencies. Spectral cues work best for judging elevation,
especially for spectra extending to higher frequencies. These cues work
together to help us locate sounds. In real-world listening, we also move
our heads, which provides additional ILD, ITD, and spectral information
that helps minimize the effect of the cone of confusion and helps locate
continuous sounds. Vision also plays a role in sound localization, as
when you hear talking and see a person making gestures and lip movements
that match what you are hearing. Thus, the richness of the environment
and our ability to actively search for information help us zero in on a
sound's location.

12.2 The Physiology of Auditory Localization Having identified the cues
that are associated with where a sound is coming from, we now ask how
the information in these cues is represented in the nervous system. Are
there neurons in the auditory system that signal ILD or ITD? Because ITD
is the most important binaural cue for most listening situations, we
will focus on this cue. We begin by describing a neural circuit that was
proposed in 1948 by Lloyd Jeffress to show how signals from the left and
right ears can be combined to determine the ITD (Jeffress, 1948;
Vonderschen & Wagner, 2014).

The Jeffress Neural Coincidence Model The Jeffress model of auditory
localization proposes that neurons are wired so they each receive
signals from the two ears, as shown in Figure 12.8. Signals from the
left ear arrive along

296

Chapter 12  Hearing in the Environment

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Sound from straight ahead:

2

3

4

5

6

7

8

9 Right ear

Neural coincidence detector

Niall Benvie/Documentary Value/Corbis

1

Firing rate

Left ear

Axon

(a) Left ear

Left ear first

5 Right ear

(b) Sound from the right: Left ear

Right ear

(c) Left ear

(d) 

3 Right ear

Figure 12.8 How the circuit proposed by Jeffress operates. Axons
transmit signals from the left ear (blue) and the right ear (red) to
neurons, indicated by circles. (a) Sound in front. Signals start in left
and right channels simultaneously. (b) Signals meet at neuron 5, causing
it to fire. (c) Sound to the right. Signal starts in the right channel
first. (d) Signals meet at neuron 3, causing it to fire. (Adapted from
Plack, 2005)

the blue axon, and signals from the right ear arrive along the red axon.
If the sound source is directly in front of the listener, the sound
reaches the left and right ears simultaneously, and signals from the
left and right ears start out together, as shown in Figure 12.8a. As
each signal travels along its axon, it stimulates each neuron in turn.
At the beginning of the journey, neurons receive signals from only the
left ear (neurons 1, 2, 3) or the right ear (neurons 9, 8, 7), but not
both, and they do not fire. But when the signals both reach neuron 5
together, that neuron fires (Figure 12.8b). This neuron and the others
in this circuit are called coincidence detectors, because they only fire
when both signals coincide by arriving at the neuron simultaneously. The
firing of neuron 5 indicates that ITD = 0. If the sound comes from the
right, the sound reaches the right ear first, that gives the signal from
the right ear a head start, as shown in Figure 12.8c, so that it travels
all the way to neuron 3 before it meets up with the signal from the left
ear. Neuron 3, in this diagram, detects ITDs that occur when the sound
is coming from a specific location on the right. The other neurons in
the circuit fire to locations corresponding to other ITDs. We can
therefore call these coincidence detectors ITD detectors, since each one
fires best to a particular ITD.

0 Interaural time difference

Right ear first

Figure 12.9 ITD tuning curves for six neurons that each respond to a
narrow range of ITDs. The neurons on the left respond when sound reaches
the left ear first. The ones on the right respond when sound reaches the
right ear first. Neurons such as these have been recorded from the barn
owl and other animals. However, when we consider mammals, another story
emerges, as illustrated in Figure 12.10. (Adapted from McAlpine &
Grothe, 2003)

The Jeffress model therefore proposes a circuit that contains a series
of ITD detectors, each tuned to respond best to a specific ITD.
According to this idea, the ITD will be indicated by which ITD neuron is
firing. This has been called a "place code" because ITD is indicated by
the place (which neuron) where the activity occurs. One way to describe
the properties of ITD neurons is to measure ITD tuning curves, which
plot the neuron's firing rate against the ITD. Recording from neurons in
the brainstem of the barn owl, which has excellent auditory localization
abilities, has revealed narrow tuning curves that respond best to
specific ITDs, like the ones in Figure 12.9 (Carr & Konishi, 1990;
McAlpine, 2005). The neurons associated with the curves on the left
(blue) fire when the sound reaches the left ear first, and the ones on
the right (red) fire when sound reaches the right ear first. These are
the tuning curves that are predicted by the Jeffress model, because each
neuron responds best to a specific ITD and the response drops off
rapidly for other ITDs. The place code proposed by the Jeffress model,
with its narrow tuning curves, works for owls and other birds, but the
situation is different for mammals.

Broad ITD Tuning Curves in Mammals The results of research in which ITD
tuning curves are recorded from mammals may appear, at first glance, to
support the Jeffress model. For example, Figure 12.10 shows an ITD
tuning curve of a neuron in the gerbil's superior olivary nucleus (solid
line) (see Figure 11.30, page 282) (Pecka et al., 2008). This curve has
a peak at an ITD of about 200 microseconds and drops off on either side.
However, when we plot the owl curve on the same graph (dashed line) we
can see that the gerbil curve is much broader than the owl curve. In
fact, the gerbil curve is so broad that it peaks at ITDs far outside the
range of ITDs that a gerbil would actually hear in nature, indicated by
the light bar (also see Siveke et al., 2006). 12.2 The Physiology of
Auditory Localization

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

297

Right-hemisphere neurons 1

2

3

Left-hemisphere neurons

Firing rate

Owl

Gerbil

Left ear first

0 Interaural time difference

Right ear first

(a) 2400

0 ITD (ms)

1400 1

Figure 12.10 Solid curve: ITD tuning curve for a neuron in the gerbil
superior olivary nucleus. Dashed curve: ITD tuning curve for a neuron in
the barn owl's inferior colliculus. The owl curve appears extremely
narrow because of the expanded time scale compared to Figure 12.9. The
gerbil curve is broader than the range of ITDs that typically occur in
the environment. This range is indicated by the light bar (between the
dashed lines).

Because of the broadness of the ITD curves in mammals, it has been
proposed that coding for localization is based on broadly tuned neurons
like the ones shown in Figure 12.11a (Grothe et al., 2010; McAlpine,
2005). According to this idea, there are broadly tuned neurons in the
right hemisphere that respond when sound is coming from the left and
broadly tuned neurons in the left hemisphere that respond when sound is
coming from the right. The location of a sound is indicated by relative
responses of these two types of broadly tuned neurons. For example, a
sound from the left would cause the pattern of response shown in the
left pair of bars in Figure 12.11b; a sound located straight ahead, by
the middle pair of bars; and a sound to the right, by the far right
bars. This type of coding resembles the population coding we described
in Chapter 2, in which information in the nervous system is based on the
pattern of neural responding. This is, in fact, how the visual system
signals different wavelengths of light, as we saw when we discussed
color vision in Chapter 9, in which wavelengths are signaled by the
pattern of response of three different cone pigments (Figure 9.13, page
205). To summarize research on the neural mechanism of binaural
localization, we can conclude that it is based on sharply tuned neurons
for birds and broadly tuned neurons for mammals. The code for birds is a
place code because the ITD is indicated by firing of neurons at a
specific place in the nervous system. The code for mammals is a
population code because the ITD is determined by the firing of many
broadly tuned neurons working together. Next, we consider one more piece
of the story for mammals, which goes beyond considering how the ITD is
coded by neurons to consider how information about localization is
organized in the cortex. 298

L

2

R

L

3

R

L

R

(b) 

Figure 12.11 (a) ITD tuning curves for broadly tuned neurons like the
one shown in Figure 12.10. The left curve represents the tuning of
neurons in the right hemisphere; the right curve is the tuning of
neurons in the left hemisphere. (b) Patterns of response of the broadly
tuned curves for stimuli coming from the left, in front, and from the
right. (Adapted from McAlpine, 2005)

Cortical Mechanisms of Localization The neural basis of binaural
localization begins along the pathway from the cochlea to the brain, in
the superior olivary nucleus (remember the acronym SONIC MG, that stands
for superior olivary nucleus, inferior colliculus, and medial
geniculate; see Figure 11.30 page 282), which is the first place that
receives signals from the left and right ears. Although a great deal of
processing for location occurs as signals are traveling from the ear to
the cortex, we will focus on the cortex, beginning with area A1 (Figure
12.12).

Area A1 and Locating Sound In a pioneering study, Dewey Neff and
coworkers (1956) placed cats about 8 feet away from two food boxes---one
about 3 feet to the left, and one about 3 feet to the right. The cats
were rewarded with food if they approached the sound of a buzzer located
behind one of the boxes. Once the cats learned this localization task,
the auditory areas on both sides of the cortex were lesioned (see
Method: Brain Ablation, Chapter 4, page 80), and although the cats were
then trained for more than 5 months, they were never able to relearn how
to localize the sounds. Based on this finding, Neff concluded that an
intact auditory cortex is necessary for accurate localization of sounds
in space.

Chapter 12  Hearing in the Environment

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Parietal lobe: "where"

A

A1

P

Frontal lobe

Temporal lobe: "what"

Figure 12.12 Auditory pathways in the monkey cortex. Part of area A1 is
visible. The pathways shown here connect to the anterior and posterior
belt areas. A = anterior; P = posterior; green = auditory what pathway;
red = auditory where pathway. (Adapted from Rauschecker & Scott, 2009)

Later studies conducted more than 50 years after Neff's research focused
more precisely on area A1. Fernando Nodal and coworkers (2010) showed
that lesioning the primary auditory cortex in ferrets decreased, but did
not totally eliminate, the ferrets' ability to localize sounds. Another
demonstration that the auditory cortex is involved in localization was
provided by Shveta Malhotra and Stephen Lomber (2007), who showed that
deactivating auditory cortex in cats by cooling the cortex results in
poor localization (also see Malhotra et al., 2008). These studies of the
auditory cortex and localization are summarized in Table 12.1. It is one
thing to show that the auditory cortex is involved in localization, but
another thing to actually explain how it does it. We know that
information about ITD and ILD reaches the cortex. But how is that
information combined to create a map of auditory space? We don't know
the answer to this question, so research is continuing. One approach to
studying localization beyond A1 has focused on the idea that there are
two auditory pathways that lead away from A1, called the what and where
auditory pathways.

The What and Where Auditory Pathways Returning to Figure 12.12, which
shows the location of the primary auditory cortex, A1, we can also see
two areas labeled A and P on either side of the auditory cortex. A is
the anterior belt area, and P is the posterior belt area. Both of these
areas are auditory, but they have different functions. The anterior belt
is involved in perceiving complex sounds and patterns of sound and the
posterior belt is involved in localizing sounds. Additional research,

which we won't describe, has shown that these two parts of the belt are
the starting points for two auditory pathways, a what auditory pathway,
which extends from the anterior belt to the front of the temporal lobe
and then to the frontal cortex (green arrows in Figure 12.12), and a
where auditory pathway, which extends from the posterior belt to the
parietal lobe and then to the frontal cortex (red arrows). The what
pathway is associated with perceiving sounds and the where pathway with
locating sounds. If what and where pathways sound familiar, it is
because we described what and where pathways for vision in Chapter 4
(see Figure 4.23, page 80). Thus, the idea of pathways serving what and
where functions is a general principle that occurs for both hearing and
vision. It is also important to note that although the research we have
described is on ferrets, cats, and monkeys, evidence for what and where
auditory functions in humans has been provided by using brain scanning
to show that what and where tasks activate different brain areas in
humans (Alain et al., 2001, 2009; De Santis et al., 2007; Wissinger et
al., 2001). We have clearly come a long way from early experiments like
Neff's done in the 1950s, which focused on determining the function of
large areas of the auditory cortex (p. 298). In contrast, recent
experiments have focused on smaller auditory areas and have also shown
how auditory processing extends beyond the auditory areas in the
temporal lobe to other areas in the cortex. We will have more to say
about auditory pathways when we consider speech perception in Chapter
14.

12.3 Hearing Inside Rooms In this chapter and Chapter 11, we have seen
that our perception of sound depends on various properties of the sound,
including its frequency, sound level, and location in space. But we have
left out the fact that in our normal everyday experience, we hear sounds
in a specific setting, such as a small room, a large auditorium, or
outdoors. As we consider this aspect of hearing, we will see why we
perceive sounds differently when we are outside and inside, and how our
perception of sound quality is affected by specific properties of indoor
environments. Figure 12.13 shows how the nature of the sound reaching
your ears depends on the environment in which you hear the sound. If you
are listening to someone playing a guitar on an outdoor stage, your
perception is based mainly on direct sound, sound that reaches your ears
directly, as shown in Figure 12.13a. If, however, you are listening to
the same guitar in an auditorium, then your perception is based on
direct sound, which reaches your ears directly (path 1), plus indirect
sound

Table 12.1 Evidence That A1 Is Involved in Localization REFERENCE

WHAT DONE

RESULT

Neff et al. (1956)

Cat auditory areas destroyed

Localization ability lost

Nodal et al. (2010)

Ferret auditory cortex destroyed Localization ability decreased

Malhotra & Lomber (2007)

Cat auditory cortex cooled

Localization ability decreased 12.3 Hearing Inside Rooms

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

299

(paths 2, 3, and 4), which reaches your ears after bouncing off the
auditorium's walls, ceiling, and floor (Figure 12.13b). The fact that
sound can reach our ears directly from where the sound is originating
and indirectly from other locations creates a potential problem, because
even though the sound originates in one place, the sound reaches the
listener from many directions and at slightly different times. This is
the situation that we described in Scenario 2 at the beginning of the
chapter, in which some of the sound from the butcher's voice reaches the
person directly and some reaches the person after bouncing off the
walls. We can understand why we usually perceive just one sound, coming
from a single location, in situations such as the concert hall or the
deli, by considering the results of research in which listeners were
presented with sounds separated by time delays, as would occur when they
originate from two different locations.

Perceiving Two Sounds That Reach the Ears at Different Times Research on
sound reflections and the perception of location has usually simplified
the problem by simulating sound reaching the ears directly from a sound
source, followed by a delayed sound from a reflection. This simulation
is achieved having people listen to loudspeakers separated in space, as
shown in Figure 12.14. The speaker on the left is the lead speaker

(representing the actual sound source), and the one on the right is the
lag speaker (representing a single sound reflection). If a sound is
presented in the lead speaker followed by a long delay (tenths of a
second), and then a sound is presented in the lag speaker, listeners
typically hear two separate sounds---one from the left (lead) followed
by one from the right (lag) (Figure 12.14a). But when the delay between
the lead and lag sounds is much shorter, as often occurs in a room,
something different happens. Even though the sound is coming from both
speakers, listeners hear a single sound as coming only from the lead
speaker (Figure 12.14b). This situation, in which a single sound appears
to originate from near the lead speaker, is called the precedence effect
because we perceive the sound as coming from near the source that
reaches our ears first (Brown et al., 2015; Wallach et al., 1949). Thus,
even though the number called out by the butcher in our Scenario 2 first
reaches the listener's ears directly and is then followed a little later
by sound arriving along the indirect path, we just hear his voice once.
The point of the precedence effect is that a sound source and its
lagging reflections are perceived as a single fused sound, except if the
delay is too long, in which case the lagging sounds are perceived as
echoes. The precedence effect governs most of our indoor listening
experience. In small rooms, the indirect sounds reflected from the walls
have a lower level than the direct sound and reach our ears with delays
of about 5 to 10 ms. In larger rooms, like concert halls, the delays are
much longer. However, even though our perception of where the sound is
coming from is

Perceive first

Perceive second

Direct sound Lead

Lag

Delay: tenths of a second (a)

(a) Blue = Indirect sound 3 2 1

Delay: 5--20 ms Precedence effect

(b) 
(c) 

Figure 12.13 (a) When you hear a sound outdoors, sound is radiated in
all directions, indicated by the blue arrows, but you hear mainly direct
sound, indicated by the red arrow. (b) When you hear a sound inside a
room, you hear both direct sound (1) and indirect sound (2, 3, and 4)
that is reflected from the walls, floor, and ceiling of the room. 300

Perception on left

4

Figure 12.14 (a) When sound is presented first in one speaker and then
in the other, with enough time between them, they are heard separately,
one after the other. (b) If there is only a short delay between the two
sounds, then the sound is perceived to come from the lead speaker only.
This is the precedence effect.

Chapter 12  Hearing in the Environment

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

usually determined by the first sound that reaches our ears, the
indirect sound, which reaches our ears just slightly later, can affect
the quality of the sound we hear. The fact that sound quality is
determined by both direct and indirect sound is a major concern of the
field of architectural acoustics, which is particularly concerned with
how to design concert halls.

Architectural Acoustics Architectural acoustics, the study of how sounds
are reflected in rooms, is largely concerned with how indirect sound
changes the quality of the sounds we hear in rooms. The major factors
affecting indirect sound are the size of the room and the amount of
sound absorbed by the walls, ceiling, and floor. If most of the sound is
absorbed, then there are few sound reflections and little indirect
sound. If most of the sound is reflected, there are many sound
reflections and a large amount of indirect sound. Another factor
affecting indirect sound is the shape of the room. This determines how
sound hits surfaces and the directions in which it is reflected. The
amount and duration of indirect sound produced by a room is expressed as
reverberation time---the time it takes for the sound to decrease to
1/1000th of its original pressure (or a decrease in level by 60 dB). If
the reverberation time of a room is too long, sounds become muddled
because the reflected sounds persist for too long. In extreme cases,
such as cathedrals with stone walls, these delays are perceived as
echoes, and it may be difficult to accurately localize the sound source.
If the reverberation time is too short, music sounds "dead," and it
becomes more difficult to produce high-intensity sounds. Because of the
relationship between reverberation time and perception, acoustical
engineers have tried to design concert halls in which the reverberation
time matches the reverberation time of halls that are renowned for their
good acoustics, such as Symphony Hall in Boston and the Concertgebouw in
Amsterdam, which have reverberation times of about 2.0 seconds. However,
an "ideal" reverberation time does not always predict good acoustics.
This is illustrated by the problems associated with the design of New
York's Philharmonic Hall. When it opened in 1962, Philharmonic Hall had
a reverberation time close to the ideal of 2.0 seconds. Even so, the
hall was criticized for sounding as though it had a short reverberation
time, and musicians in the orchestra complained that they could not hear
each other. These criticisms resulted in a series of alterations to the
hall, made over many years, until eventually, when none of the
alterations proved satisfactory, the entire interior of the hall was
destroyed, and in 1992 the hall was completely rebuilt and renamed Avery
Fisher Hall. But that's not the end of the story, because even after
being rebuilt, the acoustics of Avery Fisher Hall were still not
considered adequate. So the hall has been renamed David Geffen Hall, and
plans are being discussed regarding the best way to improve its
acoustics. The experience with Philharmonic Hall, along with new
developments in the field of architectural acoustics, has led
architectural engineers to consider factors in addition to reverberation
time in designing concert halls. Some of these factors have been
identified by Leo Beranek (1996), who showed that

the following physical measures are associated with how music is
perceived in concert halls: Intimacy time: The time between when sound
arrives directly from the stage and when the first reflection arrives.
This is related to reverberation but involves just comparing the time
between the direct sound and the first reflection, rather than the time
it takes for many reflections to die down. ■■ Bass ratio: The ratio of
low frequencies to middle frequencies that are reflected from walls and
other surfaces. ■■ Spaciousness factor: The fraction of all of the sound
received by a listener that is indirect sound. ■■

To determine the optimal values for these physical measures, acoustical
engineers measured them in 20 opera houses and 25 symphony halls in 14
countries. By comparing their measurements with ratings of the halls by
conductors and music critics, they confirmed that the best concert halls
had reverberation times of about 2 seconds, but they found that 1.5
seconds was better for opera houses, with the shorter time being
necessary to enable people to hear the singers' voices clearly. They
also found that intimacy times of about 20 msec and high bass ratios and
spaciousness factors were associated with good acoustics (Glanz, 2000).
When these factors have been taken into account in the design of new
concert halls, such as the Walt Disney Concert Hall in Los Angeles, the
result has been acoustics rivaling the best halls in the world. In
designing Walt Disney Hall, the architects paid attention not only to
how the shape, configuration, and materials of the walls and ceiling
would affect the acoustics, but also to the absorption properties of the
cushions on each of the 2,273 seats. One problem that often occurs in
concert halls is that the acoustics depend on the number of people
attending a performance, because people's bodies absorb sound. Thus, a
hall with good acoustics when full could echo when there are too many
empty seats. To deal with this problem, the seat cushions were designed
to have the same absorption properties as an "average" person. This
means that the hall has the same acoustics when empty or full. This is a
great advantage to musicians, who usually rehearse in an empty hall.
Another concert hall with exemplary acoustics is the Leighton Concert
Hall in the DeBartolo Performing Arts Center at the University of Notre
Dame, which opened in 2004 (Figure 12.15). The innovative design of this
concert hall features an adjustable acoustic system that makes it
possible to adjust the reverberation to between 1.4 and 2.6 seconds.
This is achieved by motors that control the position of the canopy over
the stage and various panels and banners throughout the hall. These
adjustments make it possible to "tune" the hall for different kinds of
music, so short reverberation times can be achieved for singing and
longer reverberation times for orchestral music. Having considered how
we tell where sounds are coming from, how we can make sense of sounds
even when they are bouncing around in rooms, and how characteristics of
a room can affect what we hear, we are now ready to take the next step
in understanding how we make sense of sounds in 12.3 Hearing Inside
Rooms

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

301

Matt Cashore/University of Notre Dame

12.4 Auditory Scene Analysis

Figure 12.15 Leighton Concert Hall in the DeBartolo Performing Arts
Center at the University of Notre Dame. Reverberation time can be
adjusted by changing the position of the panels and banners on the
ceiling and draperies on the sides.

the environment by considering how we perceptually organize sounds when
there are many sound sources. TEST YOuRSELF 12.1 1. How is auditory
space described in terms of three coordinates? 2. What is the basic
difference between determining the location of a sound source and
determining the location of a visual object? 3. Describe the binaural
cues for localization. Indicate the frequencies and directions relative
to the listener for which the cues are effective. 4. Describe the
spectral cue for localization.

Our discussion so far has focused on localization---where a sound is
coming from. We saw that the auditory system uses differences in level
and timing between the two ears plus spectral information from sound
reflections inside the pinnae to localize sounds. We now add an
important complication that occurs constantly in the environment:
multiple sources of sound. At the beginning of the chapter, in Scenario
3, we described two people talking in a noisy coffee shop, with the
sounds of music, other people talking, and the espresso machine in the
background. The array of sound sources at different locations in the
environment is called the auditory scene, and the process by which the
stimuli produced by each source are separated is called auditory scene
analysis (ASA) (Bregman, 1990, 1993; Darwin, 2010; Yost, 2001). Auditory
scene analysis poses a difficult problem because the sounds from
different sources are combined into a single acoustic signal, so it is
difficult to tell which part of the signal is created by which source
just by looking at the waveform of the sound stimulus. We can better
understand what we mean when we say that the sounds from different
sources are combined into a single acoustic signal by considering the
trio in Figure 12.16. The guitar, the vocalist, and the keyboard each
create their own sound signal, but all of these signals enter the
listener's ear together and so are combined into a single complex
waveform. Each of the frequencies in this signal causes the basilar
membrane to vibrate, but just as in the case of the bird and the cat in
Figure 12.1, in which there was no information on the cochlea for the
locations of the two sounds, it isn't obvious

5.  What happens to auditory localization when a mold is placed in a
    person's ear? How well can a person localize sound once he or she
    has adapted to the mold? What happens when the mold is removed after
    the person has adapted to it?
6.  Describe the Jeffress model, and how neural coding for localization
    differs for birds and for mammals.
7.  Describe how auditory localization is organized in the cortex. What
    is the evidence that A1 is important for localization?
8.  What are the what and where auditory pathways? How are they related
    to the anterior and posterior belt areas?
9.  What is the difference between listening to sound outdoors and
    indoors? Why does listening indoors create a problem for the
    auditory system?
10. What is the precedence effect, and what does it do for us
    perceptually?
11. What are some basic principles of architectural acoustics that have
    been developed to help design concert halls?
12. Describe some of the techniques used to manipulate the acoustics of
    some modern concert halls.

302

Listener

Figure 12.16 Each musician produces a sound stimulus, but these signals
are combined into one signal, which enters the ear.

Chapter 12  Hearing in the Environment

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

what information might be contained in the sound signal to indicate
which vibration is created by which sound source. Auditory scene
analysis, the process by which the auditory scene is separated into
separate sources, considers two situations. The first situation involves
simultaneous grouping. This occurs for our musical trio, because all of
the musicians are playing simultaneously. The question asked in the case
of simultaneous grouping is "How can we hear the vocalist and each of
the instruments as separate sound sources?" Our example in Scenario 3,
in which the many different sounds reach the listener's ears in the
coffee shop are perceived as coming from separate sources, is another
example of simultaneous grouping. The second situation in ASA is
sequential grouping--- grouping that occurs as sounds follow one another
in time. Hearing the melody being played by the keyboard as a sequence
of notes that are grouped together is an example of sequential grouping,
as is hearing the conversation of a person you are talking with in a
coffee shop as a stream of words coming from a single source. Research
on ASA has focused on determining cues or information in both of these
situations.

Simultaneous Grouping To begin our discussion of simultaneous grouping
let's return to the problem facing the auditory system, when the guitar,
keyboard, and vocalist create pressure changes in the air, and these
pressure changes are combined to create a complex pattern of basilar
membrane vibration. How does the auditory system separate the
frequencies in the "combined" sound signal into the different sounds
made by the guitar, the vocalist, and the keyboard, when all are playing
at the same time? In Chapter 5, we posed an analogous question for
vision when we asked how elements in a scene become grouped together to
create separate objects. One of the answers for vision is provided by
the principles of perceptual organization proposed by the Gestalt
psychologists and others, that is based on what usually occurs in the
environment (see page 94). A similar situation occurs for auditory
stimuli, because a number of principles help us perceptually organize
elements of an auditory scene, and these principles are based on how
sounds are usually organized in the environment. We will now consider a
number of different types of information that are used to analyze
auditory scenes.

Location One way to analyze an auditory scene into its separate
components would be to use information about where each source is
located. According to this idea, you can separate the sound of the
vocalist from the sound of the guitar based on localization cues such as
the ILD and ITD. Thus, when two sounds are separated in space, the cue
of location helps us separate them perceptually. In addition, when a
source moves, it typically follows a continuous path rather than jumping
erratically from one place to another. For example, this continuous
movement of sound helps us perceive the sound from a passing car as
originating from a single source.

But the fact that information other than location is also involved
becomes obvious when we consider that sounds can be separated even if
they are all coming from the same location. For example, we can perceive
many different instruments in a composition that is recorded by a single
microphone and played back over a single loudspeaker (Litovsky, 2012;
Yost, 1997).

Onset Synchrony Onset time is one of the strongest cues for segregation.
If two sounds start at slightly different times, it is likely that they
came from different sources. This occurs often in the environment,
because sounds from different sources rarely start at exactly the same
time (Shamma & Micheyl, 2010; Shamma et al., 2011). Timbre and Pitch
Sounds that have the same timbre or pitch range are often produced by
the same source. A flute, for example, doesn't suddenly sound like the
timbre of a trombone. In fact, the flute and trombone are distinguished
not only by their timbres, but also by their pitch ranges. The flute
tends to play in a high pitch range, and the trombone in a low one.
These distinctions help the listener decide which sounds originate from
which source. Harmonicity Remember from Chapter 11 that periodic sounds
consist of a fundamental frequency, plus harmonics that are multiples of
the fundamental (Figure 11.5). Because it is unlikely that several
independent sound sources would create a fundamental and the pattern of
harmonics associated with it, when we hear a harmonic series we infer
that it came from a single source.

Sequential Grouping The question of how we group sequences of sounds
that occur over time also involves Gestalt grouping principles that
influence how components of stimuli are grouped together (Chapter 5,
page 96).

Similarity of Pitch We've mentioned, in discussing simultaneous
grouping, how differences in the pitch of a flute and trombone help us
isolate them as separate sources. Pitch also helps us organize the sound
from a single source in time. Similarity comes into play because
consecutive sounds produced by the same source usually are similar in
pitch. That is, they don't usually jump wildly from one pitch to a very
different pitch. As we will see when we discuss music in Chapter 13,
musical sequences typically contain small intervals between notes. These
small intervals cause notes to be grouped together, following the
Gestalt law of proximity (see page 98). The perception of a string of
sounds as belonging together is called auditory stream segregation
(Bregman, 1990; Michaeyl & Oxenham, 2010). Albert Bregman and Jeffrey
Campbell (1971) demonstrated auditory stream segregation based on pitch
by alternating high 12.4 Auditory Scene Analysis

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

303

High Hi

High stream Hi

Pitch

Figure 12.17 (a) When high and low tones are alternated slowly, auditory
stream segregation does not occur, so the listener perceives alternating
high and low tones. (b) Faster alternation results in segregation into
high and low streams.

Hi Low stream

Lo Lo

Lo

Low (a) Tones alternated slowly Perception: Hi--Lo--Hi--Lo--Hi--Lo

and low tones, as shown in the sequence in Figure 12.17. When the
high-pitched tones were slowly alternated with the lowpitched tones, as
in Figure 12.17a, the tones were heard as part of one stream, one after
another: Hi--Lo--Hi--Lo--Hi--Lo, as indicated by the dashed line. But
when the tones were alternated very rapidly, the high and low tones
became perceptually grouped into two auditory streams; the listener
perceived two separate streams of sound, one high-pitched and one
low-pitched (Figure 12.17b) (see Heise & Miller, 1951, and Miller &
Heise, 1950, for an early demonstration of auditory stream segregation).
This demonstration shows that stream segregation depends not only on
pitch but also on the rate at which tones are presented. Figure 12.18
illustrates a demonstration of grouping by similarity of pitch in which
two streams of sound are perceived as separated until their pitches
become similar. One stream is a series of repeating notes (red), and the
other is a scale that goes up (blue) (Figure 12.18a). Figure 12.18b
shows how this stimulus is perceived if the tones are presented fairly
rapidly.

(b) Tones alternated rapidly Perception: Two separate streams

At first the two streams are separated, so listeners simultaneously
perceive the same note repeating and a scale going up. However, when the
frequencies of the two stimuli become similar, something interesting
happens. Grouping by similarity of pitch occurs, and perception changes
to a back-and-forth "galloping" between the tones of the two streams.
Then, as the scale continues upward so the frequencies become more
separated, the two sequences are again perceived as separated. Another
example of how similarity of pitch causes grouping is an effect called
the scale illusion, or melodic channeling. Diana Deutsch (1975, 1996)
demonstrated this effect by presenting two sequences of notes
simultaneously through earphones, one to the right ear and one to the
left (Figure 12.19a).

Right ear

Left ear

(a) How notes are presented
(b) Physical stimulus Right ear

Left ear "Galloping" (b) What the listener hears (b) Perception

Figure 12.18 (a) Two sequences of stimuli: a sequence of similar notes
(red), and a scale (blue). (b) Perception of these stimuli: Separate
streams are perceived when they are far apart in frequency, but the
tones appear to jump back and forth between stimuli when the frequencies
are in the same range. 304

Figure 12.19 (a) These stimuli were presented to a listener's left ear
(blue) and right ear (red) in Deutsch's (1975) scale illusion
experiment. Notice how the notes presented to each ear jump up and down.
(b) Although the notes in each ear jump up and down, the listener
perceives a smooth sequence of notes. This effect is called the scale
illusion, or melodic channeling. (From Deutsch, 1975)

Chapter 12  Hearing in the Environment

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

(a) Tone bursts separated by silent gaps Noise

Noise

(b) Silent gaps filled in by noise
(c) Perception of b: tone appears to continue under noise

Figure 12.20 A demonstration of auditory continuity, using tones.

Notice that the notes presented to each ear jump up and down and do not
create a scale. However, Deutsch's listeners perceived smooth sequences
of notes in each ear, with the higher notes in the right ear and the
lower ones in the left ear (Figure 12.19b). Even though each ear
received both high and low notes, grouping by similarity of pitch caused
listeners to group the higher notes in the right ear (which started with
a high note) and the lower notes in the left ear (which started with a
low note). In Deutsch's experiment, the perceptual system applies the
principle of grouping by similarity to the artificial stimuli presented
through earphones and creates the illusion that smooth sequences of
notes are being presented to each ear. However, most of the time,
principles of auditory grouping like similarity of pitch help us to
accurately interpret similar sounds as coming from the same source,
because that is what usually happens in the environment.

Auditory Continuity Sounds that stay constant or that change smoothly
are often produced by the same source. This property of sound leads to a
principle that resembles the Gestalt principle of good continuation for
vision (see Chapter 5, page 96). Sound stimuli with the same frequency
or smoothly changing frequencies are perceived as continuous even when
they are interrupted by another stimulus (Deutsch, 1999). Richard Warren
and coworkers (1972) demonstrated auditory continuity by presenting
bursts of tone interrupted by gaps of silence (Figure 12.20a). Listeners
perceived these tones as stopping during the silence. But when Warren
filled in the gaps with noise (Figure 12.20b), listeners perceived the
tone as continuing behind the noise (Figure 12.20c). This demonstration
is analogous to the demonstration of visual good

6 8

continuation illustrated by coiled rope in Figure 5.15 (see page 97).
Just as the rope is perceived as continuous even when it is covered by
another coil of the rope, a tone can be perceived as continuous even
though it is interrupted by bursts of noise.

Experience The effect of past experience on the perceptual grouping of
auditory stimuli can be demonstrated by presenting the melody of a
familiar song, as in Figure 12.21a. These are the notes for the song
"Three Blind Mice," but with the notes jumping from one octave to
another. When people first hear these notes, they find it difficult to
identify the song. But once they have heard the song as it was meant to
be played (Figure 12.21b), they can follow the melody in the
octavejumping version shown in Figure 12.21a. This is an example of the
operation of a melody schema---a representation of a familiar melody
that is stored in a person's memory. When people don't know that a
melody is present, they have no access to the schema and therefore have
nothing with which to compare the unknown melody. But when they know
which melody is present, they compare what they hear to their stored
schema and perceive the melody (Deutsch, 1999; Dowling & Harwood, 1986).
Each of the principles of auditory grouping that we have described
provides information that helps us determine how sounds are grouped
together across time. There are two important messages that we can take
away from these principles. First, because the principles are based on
our past experiences, and what usually happens in the environment, their
operation is an example of prediction at work. You may remember the
statement "The brain is a prediction machine" from the Something to
Consider section "Prediction Is Everywhere" at the end of Chapter 7. In
that discussion we noted how prediction is involved in perceiving
objects by making inferences about the image on the retina (Chapter 5);
keeping a scene stationary as we move our eyes to scan the scene;
anticipating where to direct our attention when making a peanut butter
and jelly sandwich or driving down the street (Chapter 6); putting
ketchup on that burger, and predicting other people's intentions
(Chapter 7); and keeping a scene stationary as we follow a moving object
with our eyes (Chapter 8). Because prediction is so central to vision,
it may be no surprise that it is also involved in hearing (and, yes, you
were warned this was coming back in Chapter 7). Although we haven't
specifically mentioned prediction in this chapter, we

Figure 12.21 "Three Blind Mice." (a) Jumping octave version. (b) Normal
version.

(a) 

6 8 (b) 12.4 Auditory Scene Analysis

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

305

can appreciate that just as principles of visual organization provide
information about what is probably happening in a visual scene, so the
principles of auditory organization provide information about what is
probably happening in an auditory scene. This leads to the second
message, which is that each perceptual principle alone is not foolproof.
What this means is that basing our perceptions on just one principle can
lead to error---as in the case of the scale illusion, which is purposely
arranged so that similarity of pitch creates an erroneous perception.
However, in most naturalistic situations, we base our perceptions on a
number of these cues working together and predictions about what is "out
there" become stronger when supported by multiple sources of evidence.

from another place (the dummy's mouth). Movement of the dummy's mouth
"captures" the sound (Soto-Faraco et al., 2002, 2004). Another example
of visual capture occurred in movie theaters before the introduction of
digital surround sound. An actor's dialogue was produced by a speaker
located on one side of the screen but the image of the actor who was
talking was located in the center of the screen, many feet away. Despite
this separation, moviegoers heard the sound coming from its seen
location (the image at the center of the screen) rather than from where
it was actually produced (the speaker to the side of the screen). Sound
originating from a location off to the side was captured by vision.

SOMETHING TO CONSIDER:

The Two-Flash Illusion

Interactions Between Hearing and Vision The different senses rarely
operate in isolation. We see people's lips move as we listen to them
speak; our fingers feel the keys of a piano as we hear the music the
fingers are creating; we hear a screeching sound and turn to see a car
coming to a sudden stop. All of these combinations of hearing and other
senses are examples of multisensory interactions. We will focus on
interactions between hearing and vision. One area of multisensory
research is concerned with one sense "dominating" the other. If we ask
whether vision or hearing is dominant, the answer is "it depends." As we
will see next, in some cases vision dominates hearing.

The Ventriloquism Effect The ventriloquism effect, or visual capture, is
an example of vision dominating hearing. It occurs when sounds coming
from one place (the ventriloquist's mouth) appear to come

But vision doesn't always win out over hearing. Consider, for example,
an amazing effect called the two-flash illusion. When a single dot is
flashed onto a screen (Figure 12.22a), the participant perceives one
flash. When a single beep is presented at the same time as the dot, the
participant still perceives one flash. However, if the single dot is
accompanied by two beeps, the participant sees two flashes, even though
the dot was flashed only once (Figure 12.22b). The mechanism responsible
for this effect is still being researched, but the important finding for
our purposes is that sound creates a visual effect (de Haas et al.,
2012). Visual capture and the two-flash illusion, although both
impressive examples of auditory--visual interaction, result in
perceptions that don't match reality. But sound and vision occur
together all the time in real-life situations, and when they do, they
often complement each other, as when we are having a conversation.

Understanding Speech When you are having a conversation with someone,
you are not only hearing what the person is saying, but you may also

Figure 12.22 The two-flash illusion. (a) A single dot is flashed on the
screen. (b) When the dot is flashed once but is accompanied by two
beeps, the observer perceives two flashes.

"Flash"

"Beep"

"Flash"

"Beep"

"Flash"

(a) 

306

(b) 

Chapter 12  Hearing in the Environment

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

be watching his or her lips. Watching people's lip movements makes it
easier to understand what they are saying, especially in a noisy
environment. This is why theater lighting designers often go to great
lengths to be sure that the actors' faces are illuminated. Lip
movements, whether in everyday conversations or in the theater, provide
information about what sounds are being produced. This is the principle
behind speechreading (sometimes called lipreading), which enables deaf
people to determine what people are saying by watching their lip and
facial movements. In the chapter on speech perception, we will consider
some additional examples of interactions between vision and speech.

coming from a specific location in space and also see what is producing
the sound---a musician playing or a person talking--- the multisensory
neurons that fire to both sound and vision help us form a single
representation of space that involves both auditory and visual stimuli.
Another example of cross-talk in the brain occurs when the primary
receiving area associated with one sense is activated by stimuli that
are usually associated with another sense. An example is provided by
some blind people who used a technique called echolocation to locate
objects and perceive shapes in the environment.

Echolocation in Blind People

Interactions in the Brain

Daniel Kish, who has been blind since he was 13-months-old, finds his
way around by clicking his tongue and listening to the echoes that
bounce off of nearby objects. This technique, which is called
echolocation, enables Kish to identify the location and size of objects
while walking. Figure 12.24 shows Kish hiking in Iceland. He uses tongue
clicks to locate nearby objects using echolocation, and the canes to
detect details of the terrain. (See Kish's TED talk "How I Use Sonar to
Navigate the World" at www.ted.com.) To study the effect of echolocation
on the brain, Lore Thaler and coworkers (2011) had two expert
echolocators create their clicking sounds as they stood near objects,
and recorded the sounds and resulting echoes with small microphones
placed in the ears. To determine how these sounds would activate the
brain, they recorded brain activity using fMRI as the expert
echolocators and sighted control participants listened to

The idea that there are connections between vision and hearing is also
reflected in the interconnection of the different sensory areas of the
brain (Murray & Spierer, 2011). These connections between sensory areas
contribute to coordinated receptive fields (RFs) like the ones shown in
Figure 12.23 for a neuron in the monkey's parietal lobe that responds to
both visual stimuli and sound (Bremmer, 2011; Schlack et al., 2005).
This neuron responds when an auditory stimulus is presented in an area
that is below eye level and to the left (Figure 12.23a) and when a
visual stimulus originates from about the same area (Figure 12.23b).
Figure 12.23c shows that there is a great deal of overlap between these
two receptive fields. It is easy to see that neurons such as this would
be useful in our multisensory environment. When we hear a sound

30

30

20

20

20

10

10

10

0

0

0

--10

--10

--10

--20

--20

--20

--30 --30

--20

2 (a)

--10 0 10 Azimuth (deg)

4

6 Spikes/s

20

8

30

--30

10

--20

1 (b)

--10 0 10 Azimuth (deg)

5

9 Spikes/s

20

--30 30

--30

--20

--10

0

10

20

30

Elevation (deg)

Elevation (deg)

Overlay

Visual receptive field

Auditory receptive field 30

--30

Azimuth (deg)

13

18 (c)

Figure 12.23 Receptive fields of neurons in the monkey's parietal lobe
that respond to (a) auditory stimuli that are located in the lower left
area of space and (b) visual stimuli presented in the lower left area of
the monkey's visual field. (c) Superimposing the two receptive fields
indicates that there is a high level of overlap between the auditory and
visual fields. \[(a) and (b) from Bremmer, 2011\] Something to Consider:
Interactions Between Hearing and Vision

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

307

Daniel Kish

between the location of an echo and location on the visual cortex did
not occur for control groups of blind non-echolocators and sighted
participants. What their result means, according to Norman and Thaler,
is that learning to echolocate causes reorganization of the brain, and
the visual area is involved because it normally contains a "retinotopic
map" in which each point on the retina is associated with a specific
location of activity in the visual cortex (see page 75). The maps for
echolocation in the echolocator's visual cortex are therefore similar to
the maps of visual locations in sighted people's visual cortex. Thus,
when sound is used to achieve spatial awareness, the visual cortex
becomes involved.

Figure 12.24 Daniel Kish hiking in Iceland. Kish, who is blind, uses the
canes to detect details of the terrain, and echolocation to locate
nearby objects.

the recorded sounds and their echoes. Not surprisingly, they found that
the sounds activated the auditory cortex in both the blind and sighted
participants. However, the visual cortex was also strongly activated in
the echolocators but was silent in the control participants. Apparently,
the visual area is activated because the echolocators are having what
they describe as "spatial" experiences. In fact, some echolocators lose
their awareness of the auditory clicks as they focus on the spatial
information the echoes are providing (Kish, 2012). This report that
echoes are transformed into spatial experiences inspired Liam Norman and
Lore Thaler (2019) to use fMRI to measure the location of activity in
expert echolocators' visual cortex as they listened to echoes coming
from different locations. What they found was that echoes coming from a
particular position in space tended to activate a particular area in the
visual cortex. This link

Listening to or Reading a Story The idea that the brain's response can
be based not on the type of energy entering the eyes or ears but on the
outcome of the energy is also illustrated in an experiment by Mor Regev
and coworkers (2013), who recorded the fMRI response of participants as
they either listened to a 7-minute spoken story or read the words of the
story presented at exactly the same rate that the words had been spoken.
Not surprisingly, they found that listening to the story activated the
auditory receiving area in the temporal lobe and that reading the
written version activated the visual receiving area in the occipital
lobe. But moving up to the superior temporal gyrus in the temporal lobe,
which is involved in language processing, they found that the responses
from listening and from reading were synchronized in time (Figure
12.25). This area of the brain is therefore responding not to "hearing"
or "vision," but to the meaning of the messages created by hearing or
vision. (The synchronized responding did not occur in a control group
that was exposed to unidentifiable scrambled letters or sounds.) In the
chapter on speech perception, we will venture further into the idea that
sound can create meaning, and will describe additional relationships
between sound and vision and sound and meaning.

3

Response

2 1 0 21 22 23

50

100

150 Time (sec)

200

250

Figure 12.25 fMRI responses of the superior temporal gyrus, which is an
area that processes language. Red: Response to listening to a spoken
story. Green: Response to reading the story at exactly the same rate as
it was spoken. The responses do not match exactly, but are highly
correlated (correlation = 0.47). (From Regev et al., 2013)

308

Chapter 12  Hearing in the Environment

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

TEST YOuRSELF 12.2 1. What is auditory scene analysis, and why is it a
"problem" for the auditory system? 2. What is simultaneous grouping? 3.
Describe the following types of information that help solve the
simultaneous grouping problem: location, onset synchrony, timbre, pitch,
and harmonicity. 4. What is sequential grouping? 5. Describe how the
following are related to the sequential grouping problem: auditory
stream segregation, the scale illusion, auditory continuity, and
experience. 6. Describe why we can say that the principles of auditory
scene analysis involve prediction.

7.  Describe the ways that (a) vision dominates hearing and

<!-- -->

(b) hearing dominates vision.

<!-- -->

8.  Describe how visual and auditory receptive fields can overlap. What
    is the function of this overlap?
9.  What is echolocation, as applied to blind people?
10. How does echolocation affect the brain?
11. Describe the experiment in which brain activity in the superior
    temporal gyrus in the temporal lobe was measured when listening to a
    story and when reading a story. What does the result of this
    experiment demonstrate about what this brain area is responding to?

Think About It 1. We can perceive space visually, as we saw in the
chapter on depth perception, and through the sense of hearing, as we
have described in this chapter. How are these two ways of perceiving
space similar and different? (p. 292) 2. How good are the acoustics in
your classrooms? Can you hear the professor clearly? Does it matter
where you sit? Are you ever distracted by noises from inside or outside
the room? (p. 299)

3.  How is object recognition in vision like stream segregation in
    hearing? (p. 303)
4.  What are some situations in which (a) you use one sense in isolation
    and (b) the combined use of two or more senses is necessary to
    accomplish a task? (p. 306)

Key Terms Acoustic shadow (p. 293) Anterior belt area (p. 299)
Architectural acoustics (p. 301) Auditory localization (p. 292) Auditory
scene (p. 302) Auditory scene analysis (p. 302) Auditory space (p. 292)
Auditory stream segregation (p. 303) Azimuth (p. 293) Binaural cue
(p. 293) Coincidence detectors (p. 297) Cone of confusion (p. 294)
Direct sound (p. 299) Distance (p. 293)

Echolocation (p. 307) Elevation (p. 293) Indirect sound (p. 299)
Interaural level difference (ILD) (p. 293) Interaural time difference
(ITD) (p. 293) ITD detector (p. 297) ITD tuning curves (p. 297) Jeffress
model (p. 296) Location cues (p. 293) Melodic channeling (p. 304) Melody
schema (p. 305) Multisensory interactions (p. 306)

Posterior belt area (p. 299) Precedence effect (p. 300) Reverberation
time (p. 301) Scale illusion (p. 304) Sequential grouping (p. 303)
Simultaneous grouping (p. 303) Spectral cue (p. 294) Speechreading
(p. 307) Two-flash illusion (p. 306) Ventriloquism effect (p. 306)
Visual capture (p. 306) What auditory pathway (p. 299) Where auditory
pathway (p. 299)

Key Terms

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

309

Music has been described as "a form of emotional communication" and as
"organized sound." What's special about music is not only that it causes
people to perceive rhythm and melody, but that it also causes them to
experience emotions and memories, and sometimes even makes people feel
like moving. Nick White/Getty Images

Learning Objectives After studying this chapter, you will be able to ...
■■ Answer the questions "What is music?" "Does music have an

adaptive function?" and "What are the benefits of music?" ■■ Understand
the different aspects of musical timing, including

the beat, meter, rhythm, and syncopation. ■■ Describe how the mind can
influence the perception of meter. ■■ Understand the different
properties of melodies.

■■ Describe behavioral and physiological evidence that explains

the connection between music and emotion. ■■ Understand the evidence for
and against the idea that music

and language share mechanisms in the brain. ■■ Describe experiments that
have studied how infants respond to

the beat. ■■ Understand what it means to say that music is "special."

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

C hapter 1 3

Perceiving Music

Chapter Contents 13.1 What Is Music?

DEVELOPMENTAL DIMENSION: How

13.2 Does Music Have an Adaptive Function?

Trajectories Tonality TEST YOURSELF 13.1

13.3 Outcomes of Music

13.6 Creating Emotions

Musical Training Improves Performance in Other Areas Music Elicits
Positive Feelings Music Evokes Memories

Structural Features Linking Music and Emotion Expectancy and Emotion in
Music

Newborns' Response to the Beat Older Infants' Movement to the Beat
Infants' Response to Bouncing to the Beat

13.4 Musical Timing

Language Using the Event-Related Potential

The Beat Meter Rhythm Syncopation The Power of the Mind

13.5 Hearing Melodies Organized Notes Intervals

Infants Respond to the Beat

METHOD: Studying Syntax in

Physiological Mechanisms of Musical Emotions

METHOD: Head-Turning Preference

Procedure

13.7 Coda: Music Is "Special" TEST YOURSELF 13.2 THINK ABOUT IT

SOMETHING TO CONSIDER: Comparing Music and Language Mechanisms in the
Brain

Evidence for Shared Mechanisms Evidence for Separate Mechanisms

Some Questions We Will Consider: ■■ What is music and what is its
purpose? (p. 311) ■■ What aspects of music lead us to experience
emotions?

create songs, melodies, or longer compositions. But, as we will see,
music not only has melody, it also has rhythm, a beat that causes people
to move, and music also elicits memories, feelings, and emotions.

(p. 321) ■■ How can we compare the brain mechanisms of music

and language? (p. 327) ■■ Can infants respond to the beat? (p. 329)

M

usic is a special type of sound. One thing that makes it special, which
we noted in Chapter 11, is that musical pitches have a regularly
repeating pattern of pressure changes, in contrast to the more random
pressure changes of many environmental noises such as the sound of waves
crashing. But what's really special about music is its popularity. While
it is unlikely that someone would spend much time listening to
individual tones, people spend vast amounts of time listening to
sequences of tones strung together to

13.1 What Is Music? Most people know what music is, but how would you
describe it to someone who has never heard it? One answer to this
question might be the following definition provided by Leonard Meyer
(1956), one of the early music researchers, who said that music is "a
form of emotional conversation." Edgar Varèse, the French composer,
defined music as "organized sound" (Levitin and Tirovolas, 2009).
Wikipedia, borrowing from Varèse, defines music as "an art form and
cultural activity whose medium is sound organized in time." These
definitions of music may make sense to someone who already knows what
music is, but would likely leave a

311

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

person unfamiliar with music in the dark. Perhaps more helpful is
considering the following basic properties of music: 1. Pitch---The
quality of tones that extends from "high" or "low," and that is often
organized on a musical scale; the aspect of perception associated with
musical melodies (Figure 13.1a) (See Chapter 11, page 270.) 2.
Melody---A sequence of pitches that are perceived as belonging together
(Figure 13.1b) 3. Temporal structure---The time dimension of music,
which consists of a regular beat, organization of the beat into measures
(meter), and the time pattern created by the notes (rhythm) 4.
Timbre---The various qualities of sound that distinguish different
musical instruments from one another 5. Harmony, consonance,
dissonance---The qualities of sound (positive or negative) created when
two or more pitches are played together These properties describe how
the various qualities of music sound. A song has a recognizable melody
created by sequences of tones with different pitches. It has a rhythm,
with some notes being longer than others, some more emphasized. Its
timbre is determined by which instruments play it, or if it is sung by a
male or a female. And its sound is affected by whether it is played as
single notes or as a series of chords. But there is more to music than
properties like pitch, rhythm, and timbre. There are also people's
responses to music, one of the most prominent being emotional. After
all, Meyer's definition of music as "emotional conversation" identifies
emotion as a central feature of music, and as we will see, a great deal
of research has been done on the connection between music and emotion.
Other responses to music are movement, which occurs when we tap our feet
or dance in response to music or play a musical instrument, and memory,
when music creates new memories or brings back memories from our past.
So music creates responses that affect many aspects of our lives.
Perhaps this is what the 19th century philosopher Fredrich Nietzsche
meant when he said, "without music life would be a mistake."

       

C

D

E

 

Hey Jude

F

G (a)

A

B

C

    

don't make it bad (b)

Figure 13.1 (a) A musical scale, in which notes are arranged in pitch
from low to high. (b) A melody in which notes are perceived as belonging
together. The melody's rhythm is determined by how the notes are
arranged in time. 312

13.2 Does Music Have an Adaptive Function? How would you answer the
question: "What is the purpose of music?" One possible answer is that
"its purpose is to help people have fun." Another is "to make people
feel good." After all, music is pervasive in our environment---people
listen to it all the time, both electronically and at concerts, and many
people create it by singing or playing musical instruments. A skeptic
might respond to this answer by saying that it's fine that music is fun
and people seek it out, but we need to ask larger questions of why music
has become part of human life across all societies and what biological
function it might have. Vision and hearing allow us to navigate in the
environment safely and effectively, but what is the value of music?
Asking this question involves asking whether music has had a purpose in
evolution. That is, does music have an adaptive function that has
enhanced humans' ability to survive? An evolutionary adaptation is a
function which evolved specifically to aid in survival and reproduction.
Does music qualify? Charles Darwin's (1871) answer to this question was
that humans sang before they spoke, and so music served the important
purpose of laying the foundation for language. Additionally, Darwin saw
music as being a way to attract sexual partners (Miller, 2000; Peretz,
2006). At the other extreme, Steven Pinker (1997) described music as
"auditory cheesecake" arguing that music is constructed from mechanisms
serving functions such as emotion and language. Perhaps the strongest
argument that music is an evolutionary adaptation is its role in social
bonding and group cohesion, which facilitates people working together in
groups (Koelsch, 2011). After all, only humans learn to play musical
instruments and make music cooperatively in groups, and the ability to
synchronize movements in a group to an external pulse is uniquely human
and increases feelings of social bonding (Koelsch, 2018; Stupacher et
al., 2017; Tarr et al., 2014; 2016). The question of whether music has
had an adaptive function is difficult to answer conclusively, because it
involves making an inference about something that happened long ago
(Fitch, 2015; Peretz, 2006). There is no question, however, of the
importance of music to humans. Music has played an important role in
human cultures throughout history; ancient musical instruments---flutes
made of vulture bones---have been found that are 30,000--40,000 years
old, and it is likely that music dates back to the human beginnings
100,000-- 200,000 years ago (Jackendoff, 2009; Koelsch, 2011). Music is
also found in every known culture worldwide (Trehub et al., 2015). A
recent analysis of music in 315 cultures concluded that although there
are many differences in music in different cultures, there are
similarities based on underlying psychological mechanisms (Mehr et al.,
2019). Thus, although many different musical styles exist across
cultures, ranging from Western classical music, to Indian Raga, to
traditional Chinese music, to American jazz, the following
characteristics of music are shared across cultures (Thompson et al.,
2019).

Chapter 13  Perceiving Music

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Tones separated by octaves are perceived as similar. Music elicits
emotions. ■■ Sequences of notes close in pitch are perceived as part of
a group. ■■ Caregivers sing to their infants. ■■ Listeners move in
synchrony with the music. ■■ Music is performed in social contexts. ■■
■■

13.3 Outcomes of Music In addition to being universally prevalent and
having important social functions, music has a number of positive
outcomes. Here are three of them:

Musical Training Improves Performance in Other Areas The effects of
musical training are related to the fact that the brain is plastic---its
neurons and connections can be shaped by experience (Reybrouck et al.,
2018) (see Chapter 4, page 74). Musical training has been linked to
better performance in mathematics, greater emotional sensitivity,
improved language skills, and greater sensitivity to timing (Chobert et
al., 2011; Krause and Chandrasekaran, 2010; Zatorre, 2013). In a study
in which physicians and medical students were tested on their ability to
detect heartbeat irregularities, it was found that doctors who played a
musical instrument performed better than those without musical training
(Mangione & Nieman, 1997).

experienced in the past, you have experienced a music-evoked
autobiographical memory (MEAM). MEAMs are often associated with strong
emotions like happiness and nostalgia (Belfi et al., 2016; Janata et
al., 2007), but can also be associated with sad emotions. The ability of
music to elicit memories has led to the use of music as a therapeutic
tool for people with Alzheimer's disease, who typically suffer large
impairments of memory. Mohamad El Haj and coworkers (2013) asked healthy
control participants and participants with Alzheimer's to respond to the
instruction "describe in detail an event in your life" after (1) two
minutes of silence or (2) two minutes of listening to music that they
had chosen. The healthy controls were able to describe autobiographical
memories (memories about past life events) equally well in both
conditions, but the memory of Alzheimer's patients was better after
listening to the music (Figure 13.2). The ability of music to elicit
autobiographical memories in Alzheimer's patients inspired the film
Alive Inside (RossatoBennett, 2014), which won the audience award at the
2014 Sundance Film Festival. This film documents the work of a nonprofit
organization called Music & Memory (musicandmemory.org), which
distributed iPods to hundreds of long-term care facilities for use by
Alzheimer's patients. In a memorable scene, Henry, who suffers from
severe dementia, is shown immobile and unresponsive to questions and
what is going on around him (Figure 13.3a). But when the therapist puts
earphones on Henry and turns on the music, he comes alive. He starts
moving to the beat. He sings along with the music. And, most important
of all, memories that had been locked away by Henry's dementia are
released, and he becomes able to talk about some things he remembers
from his past (Figure 13.3b). (Also see

Music Elicits Positive Feelings

Music Evokes Memories Music has the power to elicit memories. If you've
ever had a piece of music trigger a memory for something that you had

Autobiographical performance in silence Autobiographical performance
after music exposure Autobiographical memory performance

Perhaps the most obvious benefit of music is that it makes us feel
better. When people are asked why they listen to music, the two main
reasons they cite are emotional impact and regulation of their emotions
(Chanda & Levitin, 2013; Rentfro & Greenberg, 2019). It is not
surprising, therefore, that the average person spends a great deal of
time listening to music and considers it one of life's most enjoyable
activities (Dube & La Bel, 2003). Not only does music make us feel
better, but in some cases music results in feelings of transcendence and
wonder or creates a pleasurable sensation of "chills" (Blood & Zatorre,
2001; Koelsch, 2014). This link between music and feelings has caused
music to be introduced in medical settings. Recently, for example,
musicians have presented virtual concerts to both patients and
health-care workers in COVID-19 hospital wards. One healthcare worker's
reaction to these concerts---"What can disrupt this pattern of despair
is the music"---beautifully captures the healing power of music (Weiser,
2020).

5 4 3 2 1 0

Healthy controls

Alzheimer

Figure 13.2 The results of El Haj et al.'s (2013) experiment, in which
normal control participants (left pair of bars) had better
autobiographical memory than Alzheimer's patients (right pair of bars).
Alzheimer's patients' autobiographical memory was enhanced by listening
to music that was meaningful to them. (Source: El Haj et al., 2013) 13.3
Outcomes of Music

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

313

Alive Inside LLC

Figure 13.3 Stills from the film Alive Inside. (a) Henry in his usual
unresponsive state. (b) Henry listening and singing along with music
that was meaningful to him. Listening to music also enhanced Henry's
ability to talk with his caregivers.

Baird & Thompson, 2018, 2019; Heaton, 2009; and Kogutek et al., 2016 for
more on the therapeutic effects of music.) One reason for all these
benefits is that music activates many areas across the brain. Figure
13.4 illustrates the brain areas associated with musical activity
(Levitin & Tirovolas, 2009). Daniel Levitin (2013) notes that "musical
behaviors activate nearly every region of the brain that has so far been
mapped." The figure identifies the auditory cortex as involved, because
it is where sounds are initially processed. But many "non-auditory"
areas are also activated by music, among them the amygdala and nucleus
acumbus (creating emotions), the hippocampus (eliciting memories), the
cerebellum and motor cortex (eliciting movement), the visual cortex
(reading music, watching performances), sensory cortex (touch feedback
from playing music), and the Motor Cortex Movement, foot-tapping,
dancing, and playing an instrument

Prefrontal Cortex Creation of expectations; violation and satisfaction
of expectations

prefrontal cortex (creating expectations about what will happen next in
a musical composition). Given music's wide reach across the brain, it is
no wonder that its effects range from improving mood, to enhancing
memory and moving in synchrony with other people. We begin describing
how music works by describing two basic characteristics of music, timing
and melody.

13.4 Musical Timing We begin by describing the time dimension of music.
We can distinguish a number of different properties that connect music
and time.

Sensory Cortex Tactile feedback from playing an instrument and dancing

Cerebellum Movement such as foot tapping, dancing, and playing an
instrument. Also involved in emotional reactions to music

Auditory Cortex The first stages of listening to sounds, the perception
and analysis of tones

Visual Cortex Reading music, looking at a performer's movements
(including one's own)

Corpus Collosum Connects left and right hemispheres

Hippocampus Emotional reactions to music. Memory for music and musical
experiences

Nucleus Accumbens Emotional reactions to music Amygdala Emotional
reactions to music

Cerebellum Movement such as foot tapping, dancing, and playing an
instrument. Also involved in emotional reactions to music

Figure 13.4 Core brain regions associated with musical activity. 314

Chapter 13  Perceiving Music

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

The Beat Every culture has some form of music with a beat (Patel, 2008).
The beat can be up front and obvious, as in rock music, or more subtle,
as in a quiet lullaby, but it is always there, sometimes operating
alone, most of the time creating a framework for notes that create a
rhythmic pattern and a melody. The beat is often accompanied by
movement, as when musicians and listeners tap their toes while playing
or listening, or when people get up and dance. The link between the beat
and movement is expressed not only by behaviors such as tapping or
swinging in time to the beat, but also by responses of motor areas in
the brain. Jessica Grahn and James Rowe (2009) demonstrated a connection
between the beat and a group of subcortical structures at the base of
the brain called the basal ganglia, which had been associated with
movement in previous research. Their participants listened to "beat"
patterns in which short notes falling on the beat created high awareness
of the beat, and non-beat patterns in which longer notes created a weak
awareness of the beat. As participants listened, staying perfectly still
in an fMRI brain scanner, their brain activity showed that the basal
ganglia response was greater to the beat stimuli than to the non-beat
stimuli. In addition, they determined that neural connectivity between
subcortical structures and cortical motor areas, calculated by
determining how well the response of one structure can be predicted from
the response of a connected structure (see Figure 2.20) (Friston et al.,
1997), was greater for the beat condition (Figure 13.5). Another link
between beat and movement was demonstrated by Joyce Chen and coworkers
(2008) who measured activity in the premotor cortex in three conditions:

(1) Tapping: Participants tapped along with the sequence.
(2) Listening with anticipation: Participants listened to the sequence,
    but they knew they would be asked to tap to it later.
(3) Passive listening: Participants listened passively to a rhythmic
    sequence. It isn't surprising that tapping caused the greatest
    response, because the premotor cortex is involved in creating
    movements. But a response also occurred in the listening with
    anticipation condition (70 percent of the response to tapping) and
    in the passive listening condition (55 percent of the response to
    tapping), even though participants were just listening, without
    moving. Thus, just as Grahn and Rowe found that just listening to
    beat stimuli activated the basal ganglia, Chen and coworkers found
    that motor areas in the cortex are activated just by listening to a
    beat, which, Chen suggests, may partially explain the irresistible
    urge to tap to the beat when hearing music. Taking this
    auditory-motor connection a step farther, Takako Fujioka and
    coworkers (2012) measured people's brain waves as they listened to
    sequences of beats at different tempos. Their results, in Figure
    13.6, show that the brain waves oscillate in time with the beat. The
    peak of the wave occurs on the beat, the wave then decreases and
    rebounds to predict the next beat. Underlying this response are
    neurons that are tuned to specific time intervals (see also Schaefer
    et al., 2014).

Meter Meter is the organization of beats into bars or measures, with the
first beat in each bar often being accented (Lerdahl & Jackendorff,
1983; Plack, 2014; Tan et al., 2013). There are two 390 ms

585 ms

780 ms

4%

2600

Figure 13.5 Grahn and Rowe (2009) found that connectivity between
subcortical structures (red) and cortical motor areas (blue) was
increased for the beat condition compared to the non-beat condition.

2400

2200

0

200 Time (ms)

400

600

800

Figure 13.6 Time course of brain activity measured on the surface of the
skull in response to sequences of equally spaced beats, indicated by the
arrows. Numbers indicate spacing, in milliseconds, between each beat.
The fastest tempo (about 152 beats per minute) is at the top, and the
slowest (about 77 beats per minute) is on the bottom. The brain
oscillations match the beats, peaking just after the beat, decreasing,
and then rebounding to predict the next beat. (Fujioka et al., 2012)
13.4 Musical Timing

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

315

basic kinds of meter in Western music: duple meter in which accents are
in multiples of two, such as 12 12 12 or 1234 1234 1234, like a march;
and triple meter, in which accents are in groups of three, such as 123
123 123, as in a waltz. Metrical structure can be achieved if musicians
accentuate some notes by using a stronger attack or by playing them
louder or longer. In the most common metric structure, called 4:4 time,
the first of every four beats is accented, and is most easily understood
by counting along, as in 1-2-3-4-1-2-3-4. Focusing on these accented
notes creates a kind of skeleton of the temporal flow of the music,
where accented notes form the beats. By accenting certain beats,
musicians bring an expressiveness to music beyond what is heard by
simply playing a string of notes. Thus, although the musical score may
be the starting point for a performance, the musicians' interpretation
of the score is what listeners hear, and the interpretation of which
notes are accented can influence the perceived meter of the composition
(Ashley, 2002; Palmer, 1997; Sloboda, 2000).

We know that a particular composition is going to be propelled along by
the regularity of the beat, and---here's the important thing---this
isn't simply an intellectual concept: the beat is a property that is
transformed into behaviors like tapping the feat, swaying, and dancing.
The fact that the temporal components of music---the beat, rhythm, and
meter---often elicit movement is important. But we have left out one
important thing: we noted that the beat marks equally spaced pulses of
time, so it occurs even when there are no notes. We now add to that the
observation that sometimes notes occur off the beat, a phenomenon called
syncopation.

Syncopation Look back at the music for the Star Spangled Banner (Figure
13.7), and notice that each beat falls on the beginning of a note.
Another example of the beat happening at the beginnings of notes is
shown in Figures 13.8a (all quarter notes) and 13.8b (the quarter notes
are represented by joined eighthnotes). The match between meter and
notes is obvious, with

Rhythm Music structures time by creating rhythm---the time pattern of
durations created by notes (Tan et al., 2013; Thompson, 2015). Although
we defined rhythm as the time pattern of durations, what is important is
not the durations of notes, but the interonset interval---the time
between the onset of each note. This is illustrated in Figure 13.7,
which shows the first measures of "The Star Spangled Banner." Note
onsets are indicated by the blue dots above the music, and the spaces
between these dots define the song's rhythm. Because note onsets are
what defines the rhythm, it is possible that two versions of this song,
one in which the notes are played briefly with spaces in between (like
notes plucked on a guitar) and another in which the notes are held so
the spaces are filled (like a note bowed on a violin), can both have the
same rhythm. But whatever the rhythm, we always come back to the pulse
of music---the beat---indicated by the red arrows in Figure 13.7.
Although the beat is associated with specific notes in this example, the
beat marks equally spaced pulses in time, and so occurs even when there
are no notes (Grahn, 2009). So the picture we have described is music as
movement through time, propelled by beats that are organized by meter,
which forms a framework for rhythmic patterns created by notes. One
thing that this temporal flow creates is prediction. Figure 13.7 First
line of "The Star-Spangled Banner." The blue dots indicate note onsets,
which define the rhythm. The red arrows indicate the beat. The stars
(\*) indicate accented beats, which define the meter.

Figure 13.8 Syncopation explained. (a) The top record shows a simple
melody consisting of four quarter notes in the first measure. (b) The
same melody, with each quarter note changed to two joined eighth notes.
The count below this record indicates that each quarter note begins on
the beat. This passage is therefore not syncopated. (c) Syncopation is
created by adding an eighth note at the beginning. The count indicates
that the three quarter-notes start off the beat (on "and"). This is an
example of syncopation.

Rhythm

3 4

- 316

- 

Beat

- 
- 

Chapter 13  Perceiving Music

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

We've seen that meter is the organization of beats into measures, with
the first beat in a bar often being accented. Although this seems to
imply that meter is determined solely by the time signature of a musical
composition, it turns out that meter is a cognitive function that can be
created by the listener's mind (Honig & Bower, 2019).

Mind Over Meter How can metrical structure be created by the mind? Even
though the ticking of a metronome creates a series of identical beats
with regular spacing, it is possible to transform this series of beats
into perceptual groups. We can, for example, imagine the beats of a
metronome in duple meter

(a) 

When I





trou-ble

find my self (b)

in

times of

   











   

 

   

comes to

me

Moth- er

Mar -

y

Figure 13.9 The Beatle's Let It Be contains syncopation. The beat is
indicated by the red arrows. The dashed arrow at (a) shows that the
beginning of the word self precedes the beat. Similarly, at (b) Mary
begins before the beat.

10 0

200 msec

400

600

Figure 13.10 Brain response to non-syncopated melody (dashed line) and
syncopated melody (solid line).

(TICK-toc) or, with a small amount of effort, in triple meter
(TICK-toc-toc) (Nozaradan et al., 2011). John Iversen and coworkers
(2009) studied the mental creation of meter using magnetoencephalography
(MEG) to measure participants' brain responses as they listened to
rhythmic sequences. MEG measures brain responses by recording magnetic
fields caused by brain activity and it records brain responses very
rapidly, so responses to specific notes in a rhythmic pattern can be
determined. Participants listened to two-tone sequences and were told to
mentally imagine that the beat occurred either on the first note or on
the second note of each sequence. Figure 13.11 shows that the MEG
response depended on which beat was accented in the listener's mind with
imagining the beat on the first note creating the blue curve and
imagining the beat on the second note creating the red curve. Thus, our
ability to change meter with our mind is reflected directly by activity
in the brain.

Movement Influences Meter A song's meter, like the one-two-three of a
waltz, influences how a dancer moves. But the relationship between music
and movement can also occur in the opposite direction, because movement
can influence the perceptual grouping or metrical structure of the
beats. This was illustrated in an experiment in which the experimenter
held hands with a participant as they bounced up and down in a duple
pattern (every other beat) or a triple pattern (every third beat)
(Phillips-Silver and Trainor, 2007). After bouncing with the
experimenter, participants listened to duple and triple patterns and
indicated which pattern they had heard while 1 Power

The Power of the Mind

40 Response

the beat falling right at each beginning of each quarter note, as we
count 1-and, 2-and, 3-and, 4-and. But Figure 13.8c shows a situation in
which this synchronized relation between the beat and the notes is
violated. In this example, the eighth note added at the beginning
changes the relation between the beat and the notes, so that the beat
comes in the middle of each of the quarter-notes. These notes begin "off
the beat" on the "and" count, which causes a "jumpiness" to the passage
called syncopation. Syncopated rhythms are at the heart of jazz and pop
music. For example, consider Figure 13.9, the music to the Beatle's Let
It Be. As in Figure 13.8c, some of the notes begin before the beat. The
beginnings of the notes for the words self and Mary, indicated by the
dashed arrows at (a) and (b), precede the beat, indicated by the red
arrows. This slight mismatch between the beat and some notes has been
linked to people's urge to dance, to "be in the groove" (Janata et al.,
2011; Levitin et al., 2018). Figure 13.10 shows that the brain's
response to a less predictable syncopated series is larger than to a
more predictable non-syncopated series (Vuust et al., 2009). As we will
see when we discuss emotions, this larger brain response to syncopation
is related to a difference between what a listener expects will happen
in the music, and what actually happens.

0.5

0

20.1

0

0.1

0.2 0.3 Time (sec)

0.4

0.5

0.6

Figure 13.11 Results of the Iversen and coworkers (2009) experiment.
Blue: MEG response when imagining the accent on the first note. Red: MEG
response when imagining the accent on the second note. 13.4 Musical
Timing

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

317

(unaccented--accented), but in Japanese it is long--short (accented--
unaccented). Comparisons of how native English-speakers and
Japanesespeakers perceive metrical grouping supports the idea that the
stress patterns in a person's language can influence the person's
perception of grouping. John Iversen and Aniruddh Patel (2008) had
participants listen to a sequence of alternating long and short tones
(Figure 13.12a) and then indicate whether they perceived the tones'
grouping as long--short or short-- long. The results indicated that
English-speakers were more likely to perceive the grouping as
short--long (Figure 13.12b) and Japanese speakers were more likely to
perceive the grouping as long--short (Figure 13.12c). This result also
occurs when comparing 7- to 8-month-old English and Japanese infants,
but it does not occur for 5- to 6-month-old infants (Yoshida et al.,
2010). It has been hypothesized that the shift that occurs between about
6 and 8 months happens because that is when infants are beginning to
develop the capacity for language. To end this section, let's consider
two musical quotes. The first, "The beat goes on," is the title and
first line of a song written by Sonny Bono in 1966. This idea is
consistent with our description of how the beat, which pushes music
through time, is an essential component of music. The beat of music has
also been likened to our heartbeat, because just as our heartbeat keeps
us alive, the musical beat is essential for music. This brings us to our
second quote, this one by Carlos Santana, which connects music and the
heart in another way: "There's a melody in everything. And once you feel
the melody, then you connect immediately with the heart... sometimes...
language gets in the way. But nothing penetrates the heart faster than
the melody." So we now turn to the melody.

bouncing. On 86 percent of the trials, participants picked the pattern
that matched the way they were bounced. This result also occurred when
the participants were bounced while blindfolded, but not when they just
watched the experimenter bounce. Based on the results of these and other
experiments, Phillips-Silver and Trainor concluded that the crucial
factor that causes movement to influence the perception of metrical
structure is stimulation of the vestibular system---the system that is
responsible for balance and sensing the position of the body. To check
this idea, Trainor and coworkers (2009) had adults listen to the
ambiguous series of beats while electrically stimulating their
vestibular system in a duple or triple pattern with electrodes placed
behind the ear. This caused the listener to feel as if his or her head
were moving back and forth, even though it remained stationary. This
experiment duplicated the results of the other experiments, with
listeners reporting hearing the pattern that matched the metrical
grouping created by stimulating the vestibular system on 78 percent of
the trials.

Language Stress Patterns Influence How Listeners Perceive Meter
Perception of meter is influenced not only by movement but by
longer-term experience---the stress patterns of a person's language.
Different languages have different stress patterns, because of the way
the languages are constructed. For example, in English, function words
like "the," "a," and "to" typically precede content words, as in "the
dog" or "to eat," where dog and eat are stressed when spoken. In
contrast, Japanese speakers place function words after the content
words, so "the book" in English (with book stressed) becomes "hon ga" in
Japanese (with hon stressed). Therefore, the dominant stress pattern in
English is short--long

Perceive short--long 100

E (b)

318

Perceive long--short

100

Listeners (%)

& Patel, 2008)

Sound bursts

(a) 

Listeners (%)

Figure 13.12 Differences between Japanese and American perceptions of
meter. (a) Participants listened to sequences of short and long tones.
On half the trials, the first tone was short; on the other half, long.
The durations of the tones ranged from about 150 ms to 500 ms (durations
varied for different experimental conditions), and the entire sequence
repeated for 5 seconds. (b) English-speaking participants (E) were more
likely than Japanese-speaking participants (J) to perceive the stimulus
as short-- long. (c) Japanese-speaking subjects were more likely than
English-speaking subjects to perceive the stimulus as long--short.
(Based on data from Iversen

E

J

J

(c) 

Chapter 13  Perceiving Music

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Now that we've considered the timing mechanisms that not only drive
music through time, but influence how some parts of a composition are
accentuated, we focus our attention on the notes. We've seen that notes
create rhythm, which depends on the arrangement and duration of notes,
and also that notes can be arranged in a way that creates the phenomenon
of syncopation, which can lead to "the groove." But we now focus on how
notes create melodies, because as Mozart declared, "melody is the
essence of music." We will be considering the following characteristics
of melodies: (1) how notes are organized to create melodies; (2) how we
perceive individual notes; and (3) how we are constantly predicting what
notes are going to come next.

Organized Notes Melody is defined as the experience of a sequence of
pitches as belonging together (Tan et al., 2010), so when you think of
the way notes follow one after another in a song or musical composition,
you are thinking about its melody. Remember from Chapter 11 that one
definition of pitch was that aspect of auditory sensation whose
variation is associated with musical melodies (Plack, 2014), and it was
noted that when a melody is played using frequencies above 5,000 Hz
(where 4,166 Hz is the highest note on the piano), you can tell
something is changing, but it doesn't sound musical. So melodies are
more than just sequence of notes---they are sequences of notes that
belong together and sound musical. Also, remembering our discussion of
auditory stream segregation in Chapter 12, we described some of the
properties that create sequential grouping---similarity of pitch,
continuity, and experience. These principles operate not only for music
but also for other sounds including, notably, speech. As we continue
this discussion of grouping from Chapter 12, we are still interested in
what causes notes to be grouped together, but as we focus on music,
let's take a different approach. Assume you are a composer, and your
goal is to create a melody. One way to approach this problem is to
consider how other composers have arranged notes. We begin with
intervals.

Intervals One characteristic that favors grouping notes in Western music
is the interval between notes. Small intervals are common in musical
sequences, in accordance with the Gestalt principle of proximity we
described in Chapter 5 (p. 98), which states that elements near each
other tend to be perceived as grouped together (Bharacha & Krumhansl,
1983; Divenyi & Hirsh, 1978). Large intervals occur less frequently
because large jumps increase the chances that the melodic line will
break into separate melodies (Plack, 2014). The prevalence of small
intervals is confirmed by the results of a survey of a large number of
compositions from different cultures, which shows that the

predominant interval is 1--2 semitones, where a semitone is the smallest
interval used in Western music, roughly the distance between two notes
in a musical scale, such as between C and C#, with 12 semitones in an
octave (Figure 13.13) (Vos & Troost, 1989). You can check the
predominance of small intervals yourself by listening to music while
paying attention to the spacing between successive pitches. Generally,
you will find that most of the intervals are small. But there are
exceptions. Consider, for example, the first two notes of "Somewhere
Over the Rainbow" (Some -- Where), which are separated by an octave (12
semitones), so they are perceptually similar. Generally, after a large
jump, the melody turns around to fill in the gap, a phenomenon called
gap fill (Meyer, 1956; Von Hipple & Huron, 2000). Another way to
describe how pitches become perceived as belonging together is to
consider musical phrases---how notes are perceived as forming segments
like phrases in language (Deutsch, 2013a; Sloboda & Gregory, 1980). To
begin considering phrases, try imagining one of your favorite
compositions (or, better yet, actually listen to one). As you hear one
note following another, can you divide the melody into segments? A
common way of subdividing melodies is into short segments called musical
phrases, which are similar to phrases in language Consider, for example,
the first line of the song in Figure 13.14: Twinkle, twinkle little
star, how I wonder what you are. We can split this sentence into two
phrases separated by the comma between star and how. But if we didn't
know the words and just listened to the music, it is likely that we
would divide the melody into the same two phrases. When people are asked
to listen to melodies and indicate the end of one unit and the beginning
of the next, they are able to segment the melodies into phrases
(Deliege, 1987; Deutsch, 2013a). The most powerful cue for the
perception of phrase boundaries is pauses, with longer intervals
separating one phrase from another (Deutsch, 2013a; Frankland & Cohen,
2004). Another cue for phrase perception is the pitch intervals between
notes. The interval separating the end of one phrase and the start of
another is often larger than the interval separating two notes within a
phrase.

40 Frequency of occurrence (%)

13.5 Hearing Melodies

30 20 10 0

0

1

2

3 4 5 6 7 8 9 Interval size in semitones

10

11 12

Figure 13.13 Frequency at which intervals occur in a survey of many
musical compositions. Green bars: classical composers and the Beatles.
Red bars: ethnic music from a number of different cultures. The most
common interval is 1--2 semitones. (From Vos & Troost, 1989)

13.5 Hearing Melodies

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

319

2 4 Twin-kle,

When David Huron (2006) measured intervals, in semitones, between the
notes in 4,600 folk songs containing about 200,000 intervals, he found
that the average interval within phrases was 2.0 semitones, whereas the
average interval between the end of one phrase and the beginning of the
next was 2.9 semitones. There is also evidence that longer notes tend to
occur at the end of a phrase (Clark & Krumhansl, 1990; Deliege, 1987;
Frankland & Cohen, 2004).

Trajectories Certain trajectories of notes are commonly found in music.
The arch trajectory---rise and then fall---is common (see the beginning
of "Twinkle, Twinkle, Little Star" in Figure 13.14 for an example of
this trajectory). Although there are fewer large changes in pitch than
small changes, when large changes do occur, they tend to go up (as in
the first two notes of "Somewhere Over the Rainbow"), and small changes
are likely to descend (Huron, 2006).

Tonality Another thing that determines how notes follow one another is
tonality. Tonality refers to the way the various tones in music seem to
vary between highly stable and highly unstable, as well as the way they
help orient listeners as to what notes to expect next while a piece of
music is playing. The most stable note within any key is called the
tonic, which is the note that names the key, and we usually expect a
melody to begin and end on the tonic note. This effect, which is called
return to the tonic, occurs in "Twinkle, Twinkle, Little Star," which
begins and ends on a C. Each key is also associated with a scale. In
Western music, the most common scale is the major scale, and consists of
seven distinct notes, made famous in the song "Doh a Deer" (doh, re, me,
fa, sol, la, ti, doh). The various notes in the scale vary in how stable
they are perceived to be and how often we expect them to occur. We can
specify a tonal hierarchy to indicate how stable each note is and so how
well it fits within a scale. The tonic note has the greatest stability,
the fifth note has the second highest level of stability, and the third
note is next in stability. Thus for a scale in the key of C, which is
associated with the scale C D E F G A B C, the three most stable notes
would be C, G, and E, which together form the three-note C-major chord.
Notes that are not in the scale have very low stability, and hence
seldom occur in conventional Western music. A classic experiment on
tonality is one by Carol Krumhansl and Edward Kessler (1982), in which
they measured perceptions of tonality by presenting a scale that
established a major or minor key, and then following the scale with a
probe tone. 320

twin-kle

lit - tle

star,

How I

won-der

what you

are.

Listeners assigned a rating of 1--7 to the probe to indicate how well it
fit with the scale presented previously (where 7 is the best fit). The
results of this experiment, indicated by the red line in Figure 13.15,
show that the tonic, C, received the highest rating, followed by G and
E, which are notes 1, 5, and 3 of the C major scale. (Note that the
experiment included keys in addition to C; this graph combines the
results from all of the keys.) Other notes in the scale, D, F, A, B,
received the next highest ratings, and notes not in the scale, like C#
and F#, received the lowest ratings. Krumhansl's experiment therefore
measured the tonal hierarchy for many different scales. Krumhansl (1985)
then considered the possibility that there is a relationship between the
tonal hierarchy and the way notes are used in a melody by referring to
statistical analyses of the frequency or duration of notes in
compositions by composers such as Mozart, Schubert, and Mendelssohn
(Hughes, 1977; Knopoff & Hutchinson, 1983; Youngblood, 1958). When she
compared these analyses to her tonal hierarchy, she found an average
correlation of 0.89. What this match means, says Krumhansl, is that
listeners and composers have internalized the statistical properties of
music and base their "best fit" ratings on the frequency with which they
have heard these tonalities in compositions. Krumhansl and Kessler's
experiment required listeners to rate how well tones fit into a scale.
The idea that certain notes are more likely to follow one another in a
composition has also been demonstrated by the cloze probability task, in

7 C Major profile 6

Average rating

Figure 13.14 The first line of "Twinkle, Twinkle, Little Star."

5

4

3

2

C

C#

D

D#

E

F F# G G# Probe tone

A

A#

B

Figure 13.15 Ratings of Krumhansl and Kessler's (1982) probe tone
experiment. Ratings indicate how well probe tones fit a scale, with 7
being the best possible fit. See text for details.

Chapter 13  Perceiving Music

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Table 13.1 Commonly Occurring Properties of Phrases

and Melodies

GROUPING

Phrases

Melody

6.  How are the results of Merchant's "oscillatory brain wave
    experiment" relevant to prediction?

COMMON CHARACTERISTICS

7.  What is meter? Metrical structure?

• Large time intervals between end of one phrase and beginning of the
next • Large pitch intervals between phrases, compared to within phrases

8.  What is rhythm? How is it related to notes?

• Melodies contain mostly small pitch intervals • Large pitch changes
tend to go up • Smaller pitch changes are likely to go down • Downward
pitch change often follows a large upward change • Melodies contain
mostly tones that fit the melody's tonality • There is a tendency to
return to the tonic at the end of a section of melody

which a listener is presented with a melody, which suddenly stops. The
listener's task is to sing the note they think comes next. An experiment
using this technique showed that listeners completed a novel melody by
singing the tonic note on an average of 81 percent of the trials, with
this number being higher for listeners with formal musical training
(Fogel et al., 2015). Thus, as we listen to music, we are focusing on
the notes we are hearing, while simultaneously anticipating the upcoming
notes. This anticipation, which is a good example of prediction, affects
grouping, with notes we anticipate being more easily grouped with other
notes to create melody. Table 13.1 summarizes a number of
characteristics that are associated with musical grouping. These
properties are not absolute---that is, they don't always occur in every
melody. However, taken together they describe things that occur often in
music, and thus are similar to the idea of regularities in the
environment, which we introduced in our discussion of perceiving visual
scenes in Chapter 5 (see page 105) and auditory prediction, which we
discussed in Chapter 12 (see page 305). Just as knowledge from a
lifetime of experience in viewing the environment can influence our
perception of scenes and objects in visual scenes, a similar situation
may occur for music, as listeners use their knowledge of regularities
like the ones in Table 13.1 to predict what is going to happen next in a
musical composition. TEST YOuRSELF 13.1 1. How has music been defined?
2. Describe five basic properties of music. 3. What is the purpose of
music? Is music an evolutionary adaptation? What characteristics of
music are shared across cultures? 4. Describe three benefits of music
that are associated with performance, feelings, and memories. 5.
Describe the beat. What is its purpose in music and what responses does
it generate behaviorally and in the brain?

9.  What is syncopation and what are its effects?
10. How is meter influenced by (a) the mind, (b) movement, and (c)
    patterns of a person's language?
11. What is melody?
12. What is the evidence that small intervals are associated with
    grouping that creates melody?
13. How are trajectories related to melodies?
14. Describe tonality, the tonic, return to the tonic, and tonal
    hierarchy.
15. Describe the cloze probability task. How was it used to demonstrate
    anticipation in music?
16. What is the relation between regularities in the environment and
    music, and a listener's ability to anticipate what is going to
    happen next in a musical composition?

13.6 Creating Emotions We have been placing the elements of music under
an analytical microscope, which has revealed things about how the timing
and arrangement of pitches create the organized sounds of music. This
approach is consistent with Varese's description of music from the
beginning of the chapter as "organized sound." But to take in the full
scope of music, we need to consider effects that extend beyond
perception. We need to focus on Meyer's description of music as being
"emotional communication." Why are we interested in emotions in a book
about perception? Because emotion is what gives music its power, and is
a central reason that making and listening to music has occurred in
every culture throughout human history. Ignoring emotion would,
therefore, be ignoring an essential component of music, which is why
emotion has been studied side-by-side with music perception. Two
approaches have been used by researchers to describe the emotional
response to music: (1) the cognitivist approach, which proposes that
listeners can perceive the emotional meaning of a piece of music, but
that they don't actually feel the emotions, and (2) the emotivist
approach, which proposes that a listener's emotional response to music
involves actually feeling the emotions (Thompson, 2015). One way to get
in touch with these two approaches is to consider how music can affect
people's experience when viewing a film. Imagine a tracking shot
following two people walking down the street holding hands. It's a nice
day, and they seem happy. But as they continue walking, a sound-track
fades in. The music is slow and has a pulse to it that sounds rather
foreboding, and you think "Something bad is about to happen to these
people." In this case the music may be perceived as a cue of an upcoming
threat, but it may not actually cause emotions in the viewer. 13.6
Creating Emotions

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

321

Structural Features Linking Music and Emotion

0

0

l fu Pe ac e

322

ac ef ul Pe

Pe ac ef

ap H

Slow Tempo

y

0

ap p

0

H

1

y

1

Sc ar

2

ul

2

py

3

Sa d

3

Sc (b)

Minor Key 4

ar y

Emotion rating

Major Key 4

Sa d

H

(a) 

ef ul

1

y

1

Pe ac

2

ap p

2

H

3

Sc ar y

3

ap py

4

Sa d

When we discussed melody, we saw how various characteristics of music
help create melodies (see Table 13.1). Researchers have taken the same
approach to emotion by looking for connections between features of music
and emotions. Thomas Eerola and coworkers (2013) had participants listen
to musical compositions that varied along a number of dimensions, such
as key: major or minor; tempo: slow to fast; register: ranging from
lower pitched to higher pitched. The participants rated each selection
on four dimensions: scary, sad, happy, and peaceful. Figure 13.16 shows
the results for the dimensions that had the largest effect on emotion:
key and tempo. Major keys were associated with happy and peaceful; minor
with scary and sad (Figure 13.16a); slow tempo

Sa d

(Erola et al., 2013)

of life, such as reactions to life experiences or reading literature,
the answer might be that emotions are often elicited by events or
stories: A personal relationship begins or ends, a pet dies, you get a
good grade on an exam. These kinds of events often elicit emotions. But
there is a difference between event- or story-elicited emotions and
music-elicited emotions. As Keith Oatley and Phillip Johnson-Laird
(2014) point out with regard to literary fiction: "Stories can evoke
real emotions about unreal events. You can laugh or weep about what you
know are fictions. Music is more puzzling, because it can move you, even
if it refers to nothing." So what is it about the sounds of music that
can evoke strong emotions, even while referring to nothing?

4

Sc ar y

Figure 13.16 Emotional responses scary, sad, happy, and peaceful, that
are associated with structural features of music. (a) Major and minor
keys. Major keys are associated with happy and peaceful, whereas minor
keys are associated with scary and sad. (b) Slow and fast tempos. Slow
tempo is associated with sad and peaceful, whereas fast tempos are
associated with happy.

Emotion rating

Alternatively, there are situations in which intense music can cause
viewers to feel emotions. An example is the shower scene in Alfred
Hitchcock's Psycho, in which Norman Bates, played by Anthony Perkins,
stabs Janet Leigh multiple times in the shower, accompanied by shrieking
strings, and then a series of slow deep chords as the dying Janet Leigh
slowly slides down the shower wall. For some people this music may just
add some tension to the scene, but others may feel strong emotions that
would not have occurred if the scene had been shot without a sound
track. (Interestingly, Hitchcock had planned to show the shower scene
without a sound track. Luckily for the film, Bernard Herrmann, the
film's music director, convinced Hitchcock to add the music.) Evidence
for the emotivist approach has been provided by laboratory experiments
in which participants are asked to indicate what they are feeling in
response to different musical selections. Avram Goldstein (1980) asked
participants to indicate when they experienced "thrills" as they
listened to music through earphones, where thrills is defined by the
Oxford English Dictionary as "a nervous emotion or tremor caused by
intense emotional excitement... producing a slight shudder of tightening
through the body." The results showed that many participants reported
thrills that corresponded to the emotional peaks and valleys of the
music. In another study, John Sloboda (1991) found that when musicians
were asked to report their physical responses to music, the most common
responses were shivers, laughter, lump in the throat, and tears. Our
concern here is how music causes emotions, either perceived or felt. If
we were to ask this question about other aspects

Fast Tempo

Chapter 13  Perceiving Music

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

with sad and peaceful; fast with happy (Figure 13.16b). Other dimensions
also had effects: Increasing loudness caused an increase in scary and a
decrease in peaceful. Increasing the register, so compositions were
played with a higher pitch, caused a decrease in scary and an increase
in happy. One of the classic examples of a combination of a minor key,
slow tempo and low register eliciting sadness, is Samuel Barber's Adagio
for Strings, which won a 2004 British Broadcasting Company poll to
identify "the saddest music in the world" by a wide margin (Larson,
2010). Barber's Adagio accompanies a scene at the end of the 1986
Vietnam-war film Platoon, in which battle-weary soldiers are slowly
trudging across a field. The gravity of the scene combined with the
effect of the music creates an almost unbearable feeling of sadness.
Other musical dimensions related to emotion have also been identified.
For example, increasing dissonance (which occurs when combinations of
notes don't sound harmonious when played together) causes an increase in
tension (Koelsch, 2014). Loud music creates arousal, which is the
brain's way of putting us on alert (Thompson, 2015). All of these
results link specific properties of musical sounds to specific emotions.

Research on expectancy for tones is based on the idea of musical
syntax---"rules" that specify how notes and chords should be combined in
music. The term syntax is associated more with language than with music.
Syntax in language refers to grammatical rules that specify correct
sentence construction. For example, the sentence "The cats won't eat"
follows the rules of syntax, whereas the phrase "The cats won't eating"
doesn't follow the rules. We will consider the idea of musical syntax
shortly, but first we describe a way syntax has been studied in language
using an electrical response called the event-related potential.

Studying Syntax in Language Using the Event-Related Potential

METHOD

The event-related potential (ERP) is recorded with small disc electrodes
placed on a person's scalp, as shown in Figure 13.17a. Each electrode
picks up signals from groups of neurons that fire together. A
characteristic of the ERP that makes it useful for studying language (or
music) is that it is

Expectancy and Emotion in Music

(a) The cats won't EAT . . . The cats won't EATING . . . -- 0

- P600 0

(b) 

200

400 600 Time (ms)

800

Courtesy Natasha Tokowicz

Expectation is the feeling that we know what's coming up in music and is
therefore another example of prediction. The beat creates a temporal
expectation that says, "this is going to continue, with one beat
following the other, so you know when to tap" (Zatorre et al., 2007).
But what if the beat suddenly changes, as when a persistent drum-beat
stops? Or if the relation between the beat and the notes becomes less
predictable, as occurs in syncopation? As we saw in Figure 13.10, a
lesspredictable syncopated rhythm causes a larger response in the brain
than a more predictable rhythm. Some results of a change in music that
violates a person's expectations are surprise, tension, and emotion, and
capturing the listener's attention. Another example of expectation is
when a musical theme is repeated over and over. For example, the famous
beginning of Beethoven's Fifth Symphony (Da Da Da Daaah) is followed by
many repetitions of that theme, which listeners anticipate throughout
the first movement of the symphony. As Stefan Koelsch and coworkers
(2019) state, "When listening to music, we constantly generate plausible
hypotheses about what will happen next." Expectation can also occur at a
less conscious level, which is necessary because music, like language,
often speeds by, leaving little time for conscious reflection
(Ockelford, 2008). This happens in language, when we anticipate what
words are most likely to come next in a sentence, and in music, as
demonstrated by the cloze probability task (p. 320), when we anticipate
which notes will come next (Fogel et al., 2015; Koelsch, 2011). The idea
that we have an expectancy, so we are constantly predicting what's
coming next, raises a question similar to the one we asked about an
unpredictable beat. What happens when expectations are violated so we
expect one phrase or note but hear something else? The answer is that
violations of expectancy for tones cause both physiological and
behavioral effects (Huron, 2006; Meyers, 1956).

Figure 13.17 (a) A person wearing electrodes for recording the
event-related potential (ERP). (b) ERP responses to eat (blue curve),
which is grammatically correct, and eating (red curve), which is not
grammatically correct, and so creates a P600 response. Note that
positive is down in this record. (\[b\] From Osterhout et al. 1997)

13.6 Creating Emotions

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

323

a rapid response, occurring on a time scale of fractions of a second, as
shown in the responses of Figure 13.17b. The ERP consists of a number of
waves that occur at different delays after a stimulus is presented. The
one we are concerned with is called the P600 response, where P stands
for "positive" and 600 indicates that it occurs about 600 milliseconds
after the stimulus is presented. We are interested in the P600 response
because it responds to violations of syntax (Kim & Osterhout, 2005;
Osterhout et al., 1997). The two curves in Figure 13.17b illustrate
this. The blue curve is the response that occurs after the word eat in
the sentence "The cats won't eat." The response to this grammatically
correct word shows no P600 response. However, the red curve, to the word
eating, which is grammatically incorrect in the sentence "The cats won't
eating," has a large P600 response. This is the brain's way of signaling
a violation of syntax.

The reason for introducing the idea that the P600 response indicates
violations of syntax in language is that the ERP has been used in a
similar way to determine how the brain responds to violations of syntax
in music. Aniruddh Patel and coworkers (1998) used this violation of
musical syntax to see if the P600 response occurred in music. Their
listeners heard a musical phrase like the one in Figure 13.18a, which
contained a target chord, indicated by the arrow above the music. There
were three different targets: (1) an "In key" chord that fit the piece,
shown on the musical staff; (2) a "Nearby key" chord that

Nearby key

Response

(a) 

Distant key

2

1

(b) 

Figure 13.18 (a) The musical phrase heard by subjects in Patel and
coworkers' (1998) experiment. The location of the target chord is
indicated by the downward pointing arrow. The chord in the music staff
is the "In key" chord. The other two chords were inserted in that
position for the "Nearby key" and "Distant key" conditions. (b) ERP
responses to the target chord: black 5 in key; green 5 nearby key; red 5
far key. 324

didn't fit as well; and (3) a "Distant key" chord that fit even less
well. In the first part of the experiment, listeners judged the phrase
as acceptable 80 percent of the time when it contained the in-key chord;
49 percent when it contained the nearbykey chord; and 28 percent when it
contained the distant-key chord. Listeners were apparently judging how
"grammatically correct" each version was. Patel then used the
event-related potential (ERP) to determine how the brain responds to
these violations of syntax. Figure 13.18b shows that there is no P600
response when the phrase contained the in-key chord (black record), but
that there are P600 responses for the two other chords, with the bigger
response for the more out-of-key chord (red record). Patel concluded
from this result that music, like language, has a syntax that influences
how we react to it. Other studies following Patel's have confirmed that
electrical responses like P600 occur to violations of musical syntax
(Koelsch, 2005; Koelsch et al., 2000; Maess et al., 2001; Vuust et al.,
2009). Violating syntax generates a "surprise response" in the brain so
when things get interesting or different, the brain perks up. This is
interesting, but what do unfulfilled expectations that lead to surprise
have to do with emotion in music? We can answer this question by
considering what happens when the music fails to return the tonic (see
page 320), because compositions often return to the tonic and listeners
expect this to happen. But what if it doesn't? Try singing the first
line of "Twinkle, Twinkle, Little Star," but stop at "you," before the
song has returned to the tonic. The effect of pausing just before the
end of the phrase, which could be called a violation of musical syntax,
is unsettling and has us longing for the final note that will bring us
back to the tonic. The idea of a link between expectation and the
emotional response to music has been the basis of proposals that
composers can choose to purposely violate a listener's expectations in
order to create emotion, tension, or a dramatic effect. Leonard Meyer
suggested this in his book Emotion and Meaning in Music (1956), in which
he argued that the principal emotional component of music is created by
the composer's choreographing of expectation (also see Huron, 2006;
Huron & Margulis, 2010). And just as music that meets the listener's
expectations has its charms, music that violates expectation can create
add to the emotional impact of music (Margulis, 2014). For example,
Mozart used novelty to grab the listener's attention. Figure 13.19 shows
excerpts from Mozart's 31st Symphony. The top phrase, from the opening
of the symphony, is an example of a composition that matches the
listener's prediction, because the first notes are Ds, followed by a
rapidly rising scale ending in D. This ending is highly expected, so
listeners familiar with Western music would predict the D if the scale
were to stop just before it was to happen. But something different
happens in the bottom phrase, which occurs later in the symphony. In
this phrase, the first notes are As, which, like the other phrase, are
followed by a rapidly rising scale. But at the end of the scale, when
listeners expect an A, they instead hear a B-flat, which doesn't sound
like it fits the note predicted by the return to the tonic (Koelsch et
al., 2019). This lack of fit

Chapter 13  Perceiving Music

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

experiments following Patel's have recorded similar responses to
unexpected notes. For example, the unexpected note in Mozart's symphony
generates a response called the early right anterior negativity (ERAN),
which occurs in the right hemisphere, slightly earlier than the P600
response recorded by Patel (Koelsch et al., 2019). Both of these
electrical "surprise responses" are physiological signals linked to the
surprise experienced by listeners.

Figure 13.19 Passages from Mozart's Symphony No. 31. The final note in
the top passage is expected, because it continues the scale leading up
to it. The final note in the bottom passage is unexpected, because it
doesn't continue the scale leading up to it.

is not, however, a mistake. It is Mozart's way of saying to the
listener, "Listen up. Something interesting is happening!"

Physiological Mechanisms of Musical Emotions The link between musical
emotions and physiological responding has been studied in a number of
ways, including recording electrical responses, brain scanning to
identify the structures that are involved, brain scanning to identify
chemical processes, and neuropsychological studies on the effect of
brain damage on musical emotions.

Recording Electrical Responses We described Patel's (1998) experiment,
which showed that the brain generates a P600 response to violations of
musical syntax. Many

Brain Scanning Another way to understand the physiology of emotion in
music is to look at which structures are associated with musical
emotions. We've seen that music activates areas throughout the brain
(Figure 13.4), and the same story emerges when we consider the array of
structures associated with the emotional processing of music. Brain
imaging studies have identified a number of structures associated with
musicassociated emotions (Peretz, 2006). Figure 13.20 shows the
locations of three of these areas, the amygdala, which is also
associated with the processing of non-musical emotions, the nucleus
accumbens, which is associated with pleasurable experiences, including
musical "chills," which often involve shaking and goosebumps, and the
hippocampus, which is one of the central structures for the processing
and storage of memories (Koelsch, 2014; Mori & Iwanaga, 2017). In an
early brain imaging study, Anne Blood and Robert Zatorre (2001) asked
participants to pick a musical selection that consistently elicited
pleasant emotional responses, including chills. These selections caused
an increase in heart rate and brain waves compared to control music that
didn't cause chills, and listening to their selection in a brain scanner
(positron emission tomography) caused increased activity in the
amygdala, hippocampus, and other structures associated with other
euphoria-inducing stimuli such as food, sex, and drugs of abuse.

Figure 13.20 Three of the main structures deep in the brain that are
involved in the recognition of emotions. There are others as well (see
Figure 3.21).

Nucleus accumbens

Amygdala

Hippocampus

13.6 Creating Emotions

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

325

Chemistry Because the intense emotions that can be elicited by music are
highly rewarding, it isn't surprising that music-elicited emotions have
been linked to activity in brain structures that are associated with
behaviors like eating, sex, and the use of recreational drugs. One of
these structures, the nucleus accumbens (NAcc) (see Figure 3.20), is
closely associated with the neurotransmitter dopamine, which is released
into the NAcc in response to rewarding stimuli (Figure 3.22a). When
Valorie Salimpoor and coworkers (2011) had participants rate the
intensity of chills and pleasure as they listened to music, they found
that more chills and pleasure were accompanied by greater activity in
the NAcc (Figure 3.22b).

Dopamine HO

NH2

HO (a)

Nucleus accumbens activation

Figure 13.21, also based on imaging studies, shows how structures
involved in music-elicited emotion structures are connected in a
network. We can, therefore, think of music not only as activating many
structures, but also as activating a network of structures that are
communicating with each other (see Distributed Representation, Chapter
2, page 33).

6 8 Intensity of chills

10

6 8 10 Reported pleasure

(b) 

Figure 13.22 (a) The structure of dopamine, which plays an important
role in creating music-elicited emotions. (b) The results of Salimpoor
and coworkers' (2011) experiment, which shows that higher intensity of
chills and reported pleasure are associated with higher activity in the
nucleus accumbens, which is closely linked to the release of dopamine.

ACC

ant Ins NAc OFC

Am

Hipp

Am

PH

Temp P

ACC

ant Ins NAc OFC

Am

Hipp

Am

PH

Temp P

Figure 13.21 Connections between brain areas that are involved in the
emotional processing of music. The hippocampus (Hipp), two areas in the
amygdala (Am), and the nucleus accumbens (NAc), from Figure 13.20, are
highlighted. The way these areas, plus others are connected to form a
network is consistent with the idea that music, like other psychological
responses, is based not only on which areas are involved, but on how
areas communicate with each other.

326

They concluded, based on this result, that the intense pleasure
experienced when listening to music is associated with dopamine activity
in the brain's reward system. Another study on the chemistry of musical
emotions showed that emotional responses to music were reduced when
participants were given the drug naltrexone, which counteracts the
effect of pleasure-inducing opioids (Mallik et al., 2017). They
concluded that the opioid system is one of the chemical systems
responsible for positive and negative responses to music. This relates
to our discussion of dopamine, because it has been shown that blocking
the opioid system reduces dopamine activity. Should we conclude from the
above results that music is a drug? Perhaps, or to be more accurate, we
can say that music can cause the release of mind-altering drugs (Ferreri
et al., 2019).

Neuropsychology Finally, neuropsychological research has linked brain
damage to deficits in music-elicited emotions. People with damage to
their amygdala don't experience the pleasurable musical "chill" response
(Griffiths et al., 2004) and can't recognize the emotions usually
associated with scary music (Gosselin et al., 2005). Patients who had
damage to their parahippocampus (an area surrounding the hippocampus)
rated dissonant music, which normal controls found unpleasant, as being
slightly pleasant (Gosselin et al., 2006). One of the messages of
research on brain structures involved in musical emotion is that there
is overlap between the areas involved in music-evoked emotions and
everyday emotions. As we will see in the next section, this overlap
between "music" and "non-music" brain areas is not limited to emotions.

Chapter 13  Perceiving Music

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

SOMETHING TO CONSIDER:

Comparing Music and Language Mechanisms in the Brain One of the major
areas of research on music and the brain has focused on comparing music
and language. This research is motivated by the fact that both language
and music (Slevc, 2012) 1. are strings of sounds, linked together. 2.
are unique to humans. 3. are found in all cultures. 4. can be combined
in song. 5. involve creating expectations of what is to come. 6. have
rhythm. 7. are organized according to rules of syntax (rules for how
words or notes should be combined). 8. involve cooperation between
people---conversation for language, playing in groups for music. While
music and language share many similarities, there are also
differences: 1. Language can convey specific thoughts, based on the
meaning of words and how they are arranged. Music doesn't have this
capacity. 2. Music elicits emotions. This can occur for language, but
isn't as central. 3. Music often repeats. This is less likely in
language. 4. Music is often played in groups, whereas language is
typically spoken by a single speaker. Are these similarities and
differences reflected in brain mechanisms? Interestingly, there is
physiological evidence that supports both the idea that music and
language are created by shared mechanisms and the idea that they are
created by separate mechanisms.

the off-key chords in a sequence of chords. The results of these tests,
shown in Figure 13.23, indicate that the patients performed poorly on
the language task compared to the controls (left pair of bars), and that
the patients also performed more poorly on the music task (right pair of
bars). Two things that are noteworthy about these results are: (1) there
is a connection between poor performance on the language task and poor
performance on the music task, which suggests a connection between the
two; and (2) the deficits in the music task for aphasia patients were
small compared to the deficits in the language tasks. These results
support a connection between brain mechanisms involved in music and
language, but not necessarily a strong connection. Brain mechanisms have
also been studied using neuroimaging. Some of these studies have shown
that different areas are involved in music and language (Fedorenko et
al., 2012). Other studies have shown that music and language activate
overlapping areas of the brain. For example, Broca's area, which is
involved in language syntax, is also activated by music (Fitch &
Martins, 2014; Koelsch, 2005, 2011; Kunert et al., 2015; Peretz &
Zatorre, 2005). It is important, however, to realize the limitations of
neuroimaging results. Just because neuroimaging identifies an area that
is activated by both music and language, this doesn't necessarily mean
that music and language are activating the same neurons within that
area. There is evidence that even if music and language activate the
same area, this activation can involve different neural networks (Figure
13.24) (Peretz et al., 2015).

Evidence for Separate Mechanisms In the previous section we saw that the
study of patients with brain damage provides some evidence for shared
mechanisms. But the effects of brain damage also provide evidence for
separate mechanisms. One of the techniques for determining separate
mechanisms is determining a double dissociation (see Method: Double
Dissociations in Neuropsychology, Chapter 4, Aphasic participants

Evidence for Shared Mechanisms

Controls 0.9 0.8 Performance

We've seen that violations of syntax in both music and language result
in an electrical "surprise response" in the brain (Figure 13.18). This
reflects the fact that key components of both music and language are
predicting what is going to happen, and may make use of similar
mechanisms. But we can't say, based on this finding alone, that music
and language involve overlapping areas of the brain. To look more
directly at the brain, Patel and coworkers (2008) studied a group of
stroke patients who had damage to Broca's area in the frontal cortex,
which we will see in the next chapter that is important for perceiving
speech (see Figure 14.17). This damage caused Broca's
aphasia---difficulty in understanding sentences with complex syntax (see
page 349). These patients and a group of controls were given (1) a
language task that involved understanding syntactically complex
sentences; and (2) a music task that involved detecting

1.0

0.7 0.6 0.5 0.4 0.3 0.2 0.1 0

Language syntax

Musical syntax

Figure 13.23 Performance on language syntax and musical syntax tasks for
aphasic participants and control participants.

Something to Consider: Comparing Music and Language in the Brain

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

327

Figure 13.24 Illustration of the idea that two different capacities,
such as language and music, might activate the same structure in the
brain (indicated by the circle), but when looked at closely, each
capacity could activate different networks (red or black) within the
structure. The small circles represent neurons, and the lines represent
connections.

page 81). For the music versus language question, that would involve
finding one person who is deficient in perceiving music but has normal
language abilities and another person who is deficient in language but
has normal musical abilities. People with a condition called congenital
amusia don't recognize tones as tones, and therefore do not experience
sequences of tones as music (Peretz, 2006). Oliver Sacks, in his book
Musicophilia: Tales of Music and the Brain (2007) describes the case of
D.L., a 76-year-old woman who had trouble singing and identifying tunes.
She couldn't tell whether one note was higher than another, and when
asked what she heard when music was played, she answered, "If you were
in my kitchen and threw all of the plates and pans on the floor, that's
what I hear." But despite her amusia, she had no trouble hearing,
remembering, or enjoying other sounds, including speech. The opposite
effect has been observed in patients who suffered brain damage

Recognition accuracy

High

Recognition accuracy

High

Low

Melodies Words

Low High

Low (a)

as adults who have lost the ability to recognize words but can still
recognize music (Fitch & Martins, 2014; Peretz, 2006). A recent
laboratory experiment that studied both behavioral and physiological
differences between music and speech was carried out by Philippe Albouy
and coworkers (2020), who had participants listen to songs being sung a
capella (an unaccompanied solo voice). The listeners were presented with
pairs of songs, and their task was to determine whether their words were
the same or different, or, in another task, if their melody was the same
or different. This task was easy when the listeners were presented with
unaltered songs. However, it became more difficult when the songs were
degraded. The key feature of this experiment is that the songs were
degraded in two different ways. Removing temporal details involved
changing the timing (fast--slow) of the sounds. Removing spectral
details involved changing the sound frequency (high--low) of the sounds.
Which type of degradation do you think affected the ability to recognize
the words? The answer is that recognizing words often depends on
split-second timing of the speech signal, which, for example, helps us
tell the difference between similar sounding words like bear and pear.
However, changing the frequency has little effect on word recognition,
which makes sense when we consider that we can recognize words when
spoken by either highpitched or low-pitched voices. In contrast, the
opposite happens for melodies. Changing spectral details affects
melodies, but changing the timing has little effect. This also makes
sense when we consider how melodies depend on the vertical (high--low)
position of notes on the musical score, but when we hear different
interpretations of songs, which often involve changing the pacing or
timing of the notes, we can still recognize the melody. The results of
this experiment are shown in Figure 13.25. Notice that changes in
temporal information (Figure 13.25a)

Temporal degradation

High

Low (b)

Spectral degradation

Figure 13.25 The results of Albouy and coworkers' (2020) experiment in
which the effect of temporal degradation or spectral degradation of
songs was measured for words and for melodies. (a) Temporal degradation
decreases word recognition but has little effect on melody recognition.
(b) Spectral degradation decreases melody recognition but has little
effect on word recognition. 328

Chapter 13  Perceiving Music

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

had little effect on the ability to recognize melodies, but had a large
effect on recognizing words, whereas the opposite happened when spectral
information was changed (Figure 13.25b). This difference between speech
and music becomes even more interesting when we consider the second part
of Albouy's experiment, in which she used fMRI to determine how brain
activity was affected by temporal or spectral degradation and found that
changes in the temporal pattern, which had affected sentence
recognition, had a large effect on the response of the left hemisphere
of the brain, whereas changes in the spectral pattern had a large effect
on the response of the right hemisphere. Based on these differences,
Albouy concluded that humans have developed two forms of auditory
communication, speech and music, and that each of these forms are served

by specialized neural systems that operate in different hemispheres. The
advantage of this separation is that separate areas are available for
encoding the different types of sounds involved in music and speech. In
the next chapter, we will consider the speech signal and how we
understand it. The conclusion from all of these studies---both
behavioral and physiological---is that while there is evidence that
music and language share mechanisms, there is also evidence for
separated mechanisms. Thus, it seems that the brain processes involved
in music and language are related, but the overlap isn't complete, as
might be expected when you consider the difference between reading a
book or listening to a conversation and listening to music. Clearly, our
knowledge of the relation between music and language is still a work in
progress (Eggermont, 2014).

DEVELOPMENTAL DIMENSION How Infants Respond to the Beat Infants love
music! They are soothed by mother's lullabies (Cirelli et al., 2019) and
hearing music keeps them from becoming distressed (Corbeil et al., 2016;
Trehub et al., 2015). But how do infants and young children respond to
the two components of music, timing and melody?

pattern contained a constant beat of 1 2 3 4 1 2 3 4 by the hihat, with
supporting beats by the drum and bass. But occasionally omitting the
downbeat (the first beat in a measure) caused electrical activity
associated with violating expectations. It appears, therefore, that
newborns can perceive the beat.

Newborns' Response to the Beat

Older Infants' Movement to the Beat

How can we tell if a newborn can detect the beat in music? István
Winkler and coworkers (2009) answered this question by measuring
electrical brain responses of 2- to 3-day-old infants as they listened
to short sequences of sounds created by a drum, bass, and hi-hat (Figure
13.26). The "standard"

Newborns don't move to the beat, but there is evidence that older
infants do. When 5- to 24-month-old infants' movements were videotaped
as they listened to rhythmic classical music or speech, it was found
that they moved their arms, hands, legs, torso, and head in response to
the music more than when they were listening to speech. The infants were
sitting on their mothers' laps during this experiment, so the ones who
could stand weren't "dancing." But there was some synchronization
between the movements and the music, and the researchers point out that
more synchrony might have occurred to music with a more pronounced beat
than what the infants heard in this experiment. (Check out "baby
dancing" on YouTube for some entertaining examples.)

Dr. Gábor Stefanics

Infants' Response to Bouncing to the Beat

Figure 13.26 Two-day-old infant having brain activity recorded while
listening to "the beat." Brain responses are recorded by electrodes
pasted onto the scalp. The electrode on the nose is the reference
electrode. (Photo courtesy of István Winkler and Gabor Stefanics)

Responses to the beat have also been demonstrated by bouncing infants in
synchrony with the beat, similar to the adult bouncing experiment
described on page 317, which showed that movement can affect the
perception of meter. As 7-monthold infants listened to a regular
repeating rhythm that had no accents, they were bounced up and down
(Figure 13.27) (Phillips-Silver & Trainor, 2005). These bounces occurred
either in a duple pattern (a bounce on every second beat) or in a triple
pattern (a bounce on every third beat). After being bounced for 2
minutes, the infants were tested to determine Continued

Something to Consider: Comparing Music and Language in the Brain

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

329

Zackery Pierce

continues to hear one of these patterns while he or she is looking at
the light. When the infant looks away, the sound goes off. This is done
for a number of trials, and the infant quickly learns that looking at
the light keeps the sound on. Thus, the question of whether the infant
prefers the duple or triple pattern can be answered by determining which
sound the infant chooses to listen to longer.

Figure 13.27 A mother (and co-author of this book) bouncing her son up
and down, as was done in Phillips-Silver and Trainor's (2005)
experiment, in which infants were bounced on either every second beat or
on every third beat.

whether this movement caused them to hear the regular pattern in groups
of two or in groups of three. The researchers used a head-turning
preference procedure to determine whether the infants preferred
listening to a pattern that had accents that corresponded to how they
had been bounced. METHOD

Head-Turning Preference Procedure

In the preference technique, an infant sitting on the mother's lap has
his or her attention directed to a flashing light illuminating a visual
display. When the infant looks at the light, it stays on and the infant
hears a repeating sound, which is accented to create either a duple or a
triple pattern. The infant

13.7 Coda: Music Is "Special" This chapter, like all the others in this
book, is about perception. For music, perception is concerned with how
tones arranged in a certain pattern and with certain timing are
perceived as "music." One way to think about the transformation of
sounds into the perception of music is to draw parallels between
perceiving music and perceiving visual stimuli. Perceiving music depends
on how sounds are arranged plus our expectations drawn from prior
experience with musical sounds. Similarly, visual perception depends on
the arrangement of visual stimuli plus our expectations based on past
experiences with visual stimuli. But despite these similarities, there
is something special about music, because while perception is the first
step, other things happen that extend beyond perception, with movement
and emotion at the top of the list. Think about what this means when we
compare music to vision. We know that movement is an important aspect of
vision. We not only perceive movement (Chapter 8), but vision depends on
movements of our body and eyes to direct our attention (Chapter 6), 330

Phillips-Silver and Trainor found that infants listened to the pattern
they had been bounced to for an average of 8 seconds but only listened
to the other pattern for an average of 6 seconds. The infants therefore
preferred the pattern they had been bounced to. To determine whether
this effect was due to vision, infants were bounced while blindfolded.
(Although the infants loved being bounced, they weren't so thrilled
about being blindfolded!) The result, when they were tested later using
the head-turning procedure, was the same as when they could see,
indicating that vision was not a factor. Also, when the infants just
watched the experimenter bounce, the effect didn't occur. Apparently
moving is the key to influencing metrical grouping. The experiments
we've described have focused on the beginnings of the infant's ability
to respond to the beat. This development continues into childhood, until
eventually, some people become experts at relating to the beat, either
through dancing to music or making music, and just about everybody
becomes expert at listening to and reacting to the beat.

and vision helps us direct movements that are necessary for us to take
action and interact with the environment (Chapter 7). In fact, in
Chapter 7 we introduced the idea that the primary purpose of the brain
is to allow organisms to interact with the environment (p. 147). So how
does the movement that accompanies music compare to vision's impressive
list of movement-related functions? One answer to this question is that
whereas vision enables us to direct our movements, music compels us to
move, especially when music has a strong beat or is syncopated. Another
aspect of music---emotion---also occurs in response to vision, when
looking at a beautiful sunset or art, or seeing joyful or disturbing
events, elicit emotions, but this occurs only occasionally. We are
usually just seeing what's out there, without necessarily experiencing
emotions, whereas emotion is a central characteristic of music. So when
compared to vision, music seems special because of its stronger link to
movement and emotions. Other perceptual qualities associated with
emotions are pain (Chapter 15) and taste and smell (Chapter 16).
However, in contrast to music, the emotions associated with pain, taste,
and smell are considered to have adaptive value by helping us avoid

Chapter 13  Perceiving Music

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

dangerous stimuli, and, in the case of taste and smell, also by helping
us seek out rewarding stimuli. This brings us back to the question "what
is music for?" from the beginning of the chapter. In connection with
this, let's return to Charles Darwin, who, although suggesting that
music may have laid the foundation for language, and is helpful in
attracting sexual partners (p. 312), seemed uncertain as to the "why" of
music, as indicated by the following statement in his famous book The
Descent of Man (1871): As neither the enjoyment nor the capacity of
producing musical notes are faculties of the least direct use to man...,
they must be ranked among the most mysterious with which he is endowed.
But Robert Zatorre (2018) noted, in a presentation titled "From
Perception to Pleasure: Musical Processing in the Brain," that in
Darwin's autobiography, which was written ten years after The Descent of
Man, he makes the following statement about music: If I were to live my
life again, I would have made a rule to read some poetry and listen to
some music at least once every week, so perhaps the parts of my brain
now atrophied, would thus have been kept active through use.... The loss
of these tastes is the loss of happiness and may possibly be injurious
to the intellect and to the moral character by enfeebling the emotional
part of our nature. What does this mean? Zattore suggests that, even
though Darwin may have been uncertain as to the purpose of music, he is
saying that without music, life is unpleasant and empty. Thus, music
raises many questions beyond how it is perceived, and that is what makes
it something special.

TEST YOuRSELF 13.2 1. Contrast the cognitivist approach and the
emotivist approach to music-elicited emotions. 2. Describe how
structural features of music are linked to emotions. 3. What causes
expectations in music? 4. Describe the event-related potential. How does
it respond both in language and music to violations of syntax? 5. Why
would composers want to violate a listener's expectations? 6. Describe
evidence for physiological mechanisms of music-elicited emotions
determined by (a) measuring electrical responses; (b) determining which
structures are activated; (c) looking at the chemistry of music-elicited
emotions; (d) neuropsychological research. 7. Compare music and
language, indicating similarities and differences. 8. What is the
evidence supporting the idea that music and language involve separate
brain mechanisms? 9. What is the evidence for shared mechanisms for
music and language? Describe Patel's experiment, Broca's aphasia
experiment, and neuroimaging evidence. 10. What is the overall
conclusion, taking the evidence for and against shared mechanisms
between music and language into account? 11. What is the evidence that
newborns and older infants can respond to the beat? 12. How is music
"special" when compared to vision? 13. What did Darwin say about music
that indicates it is special?

THINK ABOUT IT 1. It is well known that young people prefer pop music
over classical music, and that the popularity of classical music
increases as listeners get older. Why do you think this is? 2. If you
were stranded on a desert island and could only listen to a dozen
musical compositions, which would you

pick? What special properties do these compositions have that caused you
to put them on your list? 3. Do you or have you ever made music by
playing an instrument or singing? What do you get out of making music?
How does that relate to what you get out of listening to music?

KEY TERMS Arch trajectory (p. 320) Beat (p. 315) Broca's aphasia
(p. 327) Cloze probability task (p. 320) Cognitivist approach (to
musical emotion) (p. 321) Congenital amusia (p. 328) Consonance (p. 312)

Dissonance (p. 312) Dopamine (p. 326) Duple meter (p. 316) Early right
anterior negativity (ERAN) (p. 325) Emotivist approach (to musical
emotion) (p. 321) Event-related potential (ERP) (p. 323)

Evolutionary adaptation (p. 312) Gap fill (p. 319) Harmony (p. 312)
Inter-onset interval (p. 316) Interval (p. 319) Melody (p. 312) Meter
(p. 315) Metrical structure (p. 316) Key Terms

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

331

Music (p. 311) Music-evoked autobiographical memory (MEAM) (p. 313)
Musical phrases (p. 319) Musical syntax (p. 323) Nucleus accumbens
(p. 326) Pitch (p. 312)

332

Return to the tonic (p. 320) Rhythm (p. 316) Semitone (p. 319)
Syncopation (p. 317) Syntax (p. 323) Temporal structure (p. 312) Timbre
(p. 312)

Tonal hierarchy (p. 320) Tonality (p. 320) Tonic (p. 320) Triple meter
(p. 316) Vestibular system (p. 318)

Chapter 13  Perceiving Music

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

This communication between father and son is based on understanding
spoken language, which depends on how our perceptual system transforms
spoken sounds into the perception of words and sequences of words, and
on how cognitive processes help us interpret what we are hearing.
Porta/DigitalVision/Getty Images

Learning Objectives After studying this chapter, you will be able to ...
■■ Describe how the acoustic signal is created by the action of

articulation and is represented by phonemes. ■■ Understand the processes
responsible for variability in the

acoustic signal. ■■ Describe the motor theory of speech perception and
evidence

for and against the theory. ■■ Describe the multiple sources of
information for speech perception.

■■ Understand how people perceive degraded speech. ■■ Describe how
research involving brain damage and neural re-

cording has contributed to our understanding of how speech is processed
by the brain. ■■ Understand how cochlear implants work and how they have

been used in children. ■■ Describe infant-directed speech and how it
affects infants.

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

C h a pter 1 4

Perceiving Speech

Chapter Contents 14.1 The Speech Stimulus

TEST YOURSELF 14.1

TEST YOURSELF 14.2

The Acoustic Signal Basic Units of Speech

14.4 Information for Speech Perception

14.5 Speech Perception in Difficult Circumstances

14.2 Variability of the Acoustic Signal

Motor Processes The Face and Lip Movements Knowledge of Language The
Meaning of Words in Sentences

14.6 Speech Perception and the Brain

Variability From Context Variability in Pronunciation

14.3 Some History: The Motor Theory of Speech Perception

Demonstration: Perceiving

The Proposed Connection Between Production and Perception The Proposal
That "Speech Is Special"

Demonstration: Organizing Strings

of Sounds

Learning About Words in a Language

■■ Can computers perceive speech as well as humans?

(p. 335) ■■ Is each sound we hear associated with a specific pattern of

air pressure changes? (p. 338) ■■ Why does an unfamiliar foreign
language often sound like

a continuous stream of sound, with no breaks between words? (p. 345) ■■
What does a person with a cochlear implant hear com-

pared to a person with normal hearing? (p. 352)

A

DEVELOPMENTAL DIMENSION: Infant-

Degraded Sentences

Some Questions We Will Consider:

lthough we perceive speech easily under most conditions, beneath this
ease lurks processes as complex as those involved in perceiving the most
complicated visual scenes. One way to appreciate this complexity is to
consider attempts to use computers to recognize speech, a process called
automatic speech recognition (ASR). Attempts to create ASR systems began
in the 1950s, when Bell Laboratories designed the "Audrey" system, which
could recognize single spoken digits. Many decades of work, combined
with vastly improved computer technology, culminated in the introduction
of ASR systems like Apple's Siri, Amazon's

SOMETHING TO CONSIDER: Cochlear

Implants

Directed Speech

TEST YOURSELF 14.3 THINK ABOUT IT

Alexa, and Google's Voice, which do a good job of recognizing spoken
commands. However, Siri, Alexa, and Voice notwithstanding, the
performance of modern ASR systems ranges from very good, under ideal
conditions, when ASR can create printed transcriptions of spoken
language with as high as 95 percent accuracy (Spille et al., 2018), to
not so good, under less than ideal conditions. For example, Adam Miner
and coworkers (2020) had a person listen to a recording of a two-person
conversation in which the microphone was not optimally placed and there
was noise in the room. Despite the microphone placement and the noise,
the person was able to create an accurate written transcription of the
conversation. However, when an ASR device created a transcript from the
same recording, it made mistakes such as identifying words incorrectly,
missing words, and inserting words that weren't said, so that only 75
percent of the words were correct. Other experiments have shown that ASR
systems make errors when confronted with accents and non-standard speech
patterns (Koenecke et al., 2020). Thus, while ASR has come a long way
since the 1950s, 70 years of development has resulted in computer speech
recognition systems that still fall short of human listeners, who can
perceive speech even when confronted with phrases they have never heard,
and under a wide variety of conditions,

335

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

including the presence of various background noises, variations in
pronunciation, speakers with different dialects and accents, and the
often chaotic give-and-take that routinely occurs when people talk with
one another (Huang et al., 2014; Sinha, 2002). This chapter will help
you appreciate the complex perceptual problems posed by speech and will
describe research that has helped us begin to understand how the human
speech perception system has solved some of these problems.

Alveolar ridge Nasal cavity

Esophagus

336

Lips Teeth Vocal cords Larynx

Figure 14.1 The vocal tract includes the nasal and oral cavities and the
pharynx, as well as components that move, such as the tongue, lips, and
vocal cords.

indicates the pattern of frequencies and intensities over time that make
up the acoustic signal. Frequency is indicated on the vertical axis and
time on the horizontal axis; intensity is

Phoneme symbol

/I/

Outline of vocal tract traced from x-ray picture of mouth

Pressure changes

Amplitude

Speech sounds are produced by the position or the movement of structures
within the vocal apparatus, which creates patterns of pressure changes
in the air called the acoustic stimulus, or the acoustic signal. The
acoustic signal for most speech sounds is created by air that is pushed
up from the lungs past the vocal cords and into the vocal tract. The
sound that is produced depends on the shape of the vocal tract as air
escaping from the lungs is pushed through it. The shape of the vocal
tract is altered by moving the articulators, which include structures
such as the tongue, lips, teeth, jaw, and soft palate (Figure 14.1).
Let's first consider the production of vowels. Vowels are produced by
vibration of the vocal cords, and the specific sounds of each vowel are
created by changing the overall shape of the vocal tract. This change in
shape changes the resonant frequency of the vocal tract and produces
peaks of pressure at a number of different frequencies (Figure 14.2).
The frequencies at which these peaks occur are called formants. Each
vowel sound has a characteristic series of formants. The first formant
has the lowest frequency; the second formant is the next highest; and so
on. The formants for the vowel /ae/ (the vowel sound in the word had)
are shown on a sound spectrogram in Figure 14.3 (speech sounds are
indicated by setting them off with slashes). The sound spectrogram

Tongue

Frequency

/U/ Amplitude

The Acoustic Signal

Oral cavity Soft palate Pharynx

14.1 The Speech Stimulus Chapter 11 introduced pure tones---simple
sine-wave patterns with different amplitudes and frequencies, and then
complex tones---a number of pure tones, called harmonics, occurring
together, with frequencies that are multiples of the tone's fundamental
frequency. The sounds of speech increase the complexity one more level.
We can still describe speech in terms of frequencies, but a complete
description needs to take into account abrupt starts and stops,
silences, and noises that occur as speakers form words. It is these
words that enable speakers to create meaning by saying words and
stringing them together into sentences. These meanings, in turn,
influence our perception of the incoming stimuli, so that what we
perceive depends not only on the physical sound stimulus but also on
cognitive processes that help us interpret what we are hearing. We begin
by describing the physical sound stimulus, called the acoustic signal.

Hard palate

Frequency

Figure 14.2 Left: the shape of the vocal tract for the vowels /I/ (as in
zip) and /U/ (as in put). Right: the amplitude of the pressure changes
produced for each vowel. The peaks in the pressure changes are the
formants. Each vowel sound has a characteristic pattern of formants that
is determined by the shape of the vocal tract for that vowel. (From
Denes & Pinson, 1993)

Chapter 14  Perceiving Speech

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

your tongue away from the alveolar ridge (try it). As you produce the
sound /b/, you place your two lips together and then release a burst or
air. The way speech sounds are produced is described by characteristics
that include the manner of articulation and the place of articulation.
The manner of articulation describes how the articulators---the mouth,
tongue, teeth, and lips---interact when making a speech sound. For
example, /b/ is created by blocking the airflow and releasing it
quickly. The place of articulation describes the locations of the
articulation. Notice, for example, how the place of articulation moves
from the back to the front of the mouth as you say /g/, /d/, and /b/.
Movements of the tongue, lips, and other articulators create patterns of
energy in the acoustic signal that we can observe on the sound
spectrogram. For example, the spectrogram for the sentence "Roy read the
will," shown in Figure 14.4, shows aspects of the signal associated with
vowels and consonants. The three horizontal bands marked F1, F2, and F3
are the three formants associated with the /e/ sound of read. Rapid
shifts in frequency preceding or following formants are called formant
transitions and are associated with consonants. For example, T2 and T3
in Figure 14.4 are formant transitions associated with the /r/ of read.
We have described the physical characteristics of the acoustic signal.
To understand how this acoustic signal results in speech perception, we
need to consider the basic units of speech.

5,000

Frequency (Hz)

4,000

3,000 F3 2,000

F2

1,000 F1 0

/h/

/æ/

/d/ "Had"

Figure 14.3 Spectrogram of the word had. "Time" is on the horizontal
axis. The dark horizontal bands are the first (F1), second (F2), and
third (F3) formants associated with the sound of the vowel /ae/.
(Spectrogram courtesy of Kerry Green)

indicated by darkness, with darker areas indicating greater intensity.
From Figure 14.3 we can see that formants are concentrations of energy
at specific frequencies, with the sound /ae/ having formants at 500,
1,700, and 2,500 Hz, which are labeled F1, F2, and F3 in the figure. The
vertical lines in the spectrogram are pressure oscillations caused by
vibrations of the vocal cord. Consonants are produced by a constriction,
or narrowing, of the vocal tract. To illustrate how different consonants
are produced, let's focus on the sounds /g/, /d/, and /b/. Make these
sounds, and notice what your tongue, lips, and teeth are doing. As you
produce the sound /d/, you place your tongue against the ridge above
your upper teeth (the alveolar ridge of Figure 14.1) and then release a
slight rush of air as you move

Basic Units of Speech Our first task in studying speech perception is
separating the acoustic stream of speech into linguistic units that
reflect the listener's perceptual experience. What are these units? The
flow of a sentence? A particular word? A syllable? The sound of a
letter? A sentence is too large a unit for easy analysis, and some
letters have no sounds at all. Although there are arguments for the idea
that the syllable is the basic unit of speech (Mehler, 1981; Segui,
1984), most speech research has been

Figure 14.4 Spectrogram of the sentence "Roy read the will," showing
formants F1, F2, and F3 and formant transitions T2 and T3. (Spectrogram
courtesy of Kerry Green)

5,000

Frequency (Hz)

4,000

3,000 T3

F3

2,000 T2

F2

1,000 F1 0 R

o

y

r e a d

t h e

w

i

l l 14.1 The Speech Stimulus

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

337

based on a unit called the phoneme. A phoneme is the shortest segment of
speech that, if changed, would change the meaning of a word. Consider
the word bit, which contains the phonemes /b/, /i/, and /t/. We know
that /b/, /i/, and /t/ are phonemes because we can change the meaning of
the word by changing each phoneme individually. Thus, bit becomes pit if
/b/ is changed to /p/, bit becomes bat if /i/ is changed to /a/, and bit
becomes bid if /t/ is changed to /d/. The phonemes of American English,
listed in Table 14.1, are represented by phonetic symbols that stand for
speech sounds. This table shows phonemes for 13 vowel sounds and 24
consonant sounds. Your first reaction to this table may be that there
are more vowels than the standard set you learned in grade school (a, e,
i, o, u, and sometimes y). The reason there are more vowels is that some
vowels can have more than one pronunciation, so there are more vowel
sounds than vowel letters. For example, the vowel o sounds different in
boat and hot, and the vowel e sounds different in head and heed.
Phonemes, then, refer not to letters but to speech sounds that determine
the meaning of what people say. Because different languages use
different sounds, the number of phonemes varies across languages. There
are only 13 phonemes in Hawaiian, but as many as 47 have been identified
in American English and up to 60 in some African languages. Thus,
phonemes are defined in terms of the sounds that are used to create
words in a specific language. It might seem that if the phoneme is the
basic unit of speech, we could describe speech perception in terms of
strings of phonemes. According to this idea, we perceive a series of
sounds called phonemes, which create syllables that combine

to create words. These syllables and words appear strung together one
after another like beads on a string. For example, we perceive the
phrase "perception is easy" as the sequence of units
"per-sep-shun-iz-ee-zee." Although perceiving speech may seem to be just
a matter of processing a series of discrete sounds that are lined up one
after another, the actual situation is much more complex. Rather than
following one another, with the signal for one sound ending and then the
next beginning, like letters on a page, signals for neighboring sounds
overlap one another. A further complication is that the acoustic signal
for a particular word can vary greatly depending on whether the speaker
is male or female, young or old, speaks rapidly or slowly, or has an
accent. This creates the variability problem, which refers to the fact
that there is no simple relationship between a particular phoneme and
the acoustic signal. In other words, the acoustic signal for a
particular phoneme is variable. We will now describe a number of ways
that this variability occurs and an early attempt to deal with this
variability.

14.2 Variability of the Acoustic Signal The main problem facing
researchers trying to understand speech perception is that there is a
variable relationship between the acoustic signal and perception of that
signal. Thus, a particular phoneme can be associated with a number of
different acoustic signals. Let's consider some of the sources of this
variability.

Table 14.1 Major Consonants and Vowels of English

and Their Phonetic Symbols

Variability From Context VOWELS

p

pull

s

sip

i

heed

b

bull

z

zip

I

hid

m

man

r

rip

e

bait

w

will

š

should

f

fill

ž

pleasure

æ

had

v

vet

č

chop

u

who'd

u

thigh

gyp

U

put

ð

that

y

yip

\^

but

t

tie

k

kale

o

boat

d

die

g

gale

O

bought

n

near

h

hail

a

hot

l

lear

h

sing

e

sofa

head

There are other American English phonemes in addition to those shown
here, and specific symbols may vary depending on the source. 338

The acoustic signal associated with a phoneme changes depending on its
context. For example, look at Figure 14.5, which shows spectrograms for
the sounds /di/ and /du/. These are smoothed hand-drawn spectrograms
that show the two most important characteristics of the sounds: the
formants (shown in red) and the formant transitions (shown in blue).
Because formants are associated with vowels, we know that the 3,000

Frequency (Hz)

CONSONANTS

/di/

/du/

2,000

Formant transition Formant

1,000

0

0

300

0

300

Time (ms)

Figure 14.5 Hand-drawn spectrograms for /di/ and /du/. (From Liberman et
al., 1967)

Chapter 14  Perceiving Speech

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

the size of its image changes on our retina, page 250). Perceptual
constancy in speech perception is similar. We perceive the sound of a
particular phoneme as constant even when the phoneme appears in
different contexts that change its acoustic signal.

Variability in Pronunciation People say the same words in a variety of
different ways. There are between speaker differences---variations in
how different people say words. Some people's voices are high-pitched
and some are low-pitched; people speak with various accents; some talk
very rapidly and others speak e-x-t-r-e-m-e-l-y s-l-o-w-l-y. These wide
variations in speech mean that for different speakers, a particular
phoneme or word can have very different acoustic signals. Analysis of
how people actually speak has determined that there are 50 different
ways to produce the word the (Waldrop, 1988). There are also within
speaker differences---variations in how an individual speaker says
words. When talking to a friend "This was a best buy" might come out
"This was a bes buy," with the /t/ being omitted. Or "Did you go to the
store?" might come out "Didjoo go to the store?" However, you might
speak more slowly and formally if you were talking to a teacher,
pronouncing the /t/ in "best buy" and by saying "Did you" as two
separate words. That people do not usually articulate each word
individually in conversational speech is reflected in the spectrograms
in Figure 14.6. The spectrogram in Figure 14.6a is for the question
"What are you doing?" spoken slowly and distinctly; the spectrogram in
Figure 14.6b is for the same question taken from conversational speech,
in which "What are you doing?" becomes "Whad'aya doin'?" This difference
shows up clearly in the spectrograms. Although the first and last words
(what and doing) create similar patterns in the two spectrograms, the
pauses between words are absent or are much less obvious in the
spectrogram of Figure 14.6b, and the middle of this spectrogram is
completely changed, with a number of speech sounds missing.

5,000

5,000

4,000

4,000

Frequency (Hz)

Frequency (Hz)

formants at 200 and 2,600 Hz are the acoustic signal for the vowel /i/
in /di/ and that the formants at 200 and 600 Hz are the acoustic signal
for the vowel /u/ in /du/. Because the formants are associated with the
vowels, the formant transitions that precede the vowel-related
steady-state formants must be the signal for the consonant /d/. But
notice that the formant transitions for the second (higher-frequency)
formants of /di/ and /du/ are different. For /di/, the formant
transition starts at about 2,200 Hz and rises to about 2,600 Hz. For
/du/, the formant transition starts at about 1,100 Hz and falls to about
600 Hz. Thus, even though we perceive the same /d/ sound in /di/ and
/du/, the formant transitions, which are the acoustic signals associated
with these sounds, are very different. Thus, the context in which a
specific phoneme occurs can influence the acoustic signal that is
associated with that phoneme (McRoberts, 2020). This effect of context
occurs because of the way speech is produced. Because articulators are
constantly moving as we talk, the shape of the vocal tract associated
with a particular phoneme is influenced by the sounds that both precede
and follow that phoneme. This overlap between the articulation of
neighboring phonemes is called coarticulation. You can demonstrate
coarticulation to yourself by noting how you produce phonemes in
different contexts. For example, say bat and boot. When you say bat,
your lips are unrounded, but when you say boot, your lips are rounded,
even during the initial /b/ sound. Thus, even though the /b/ is the same
in both words, you articulate each differently. In this example, the
articulation of /oo/ in boot overlaps the articulation of /b/, causing
the lips to be rounded even before the /oo/ sound is actually produced.
The fact that we perceive the sound of a phoneme as the same even though
the acoustic signal is changed by coarticulation is an example of
perceptual constancy. This term may be familiar to you from our
observations of constancy phenomena in the sense of vision, such as
color constancy (we perceive an object's chromatic color as constant
even when the wavelength distribution of the illumination changes, page
215) and size constancy (we perceive an object's size as constant even
when

3,000 2,000

2,000 1,000

1,000

0 Whad aya

0 What (a)

3,000

are

you

doing

?

do

in

?

(b) 

Figure 14.6 (a) Spectrogram of "What are you doing?" pronounced slowly
and distinctly. (b) Spectrogram of "What are you doing?" as pronounced
in conversational speech. (Spectrograms courtesy of David Pisoni) 14.2
Variability of the Acoustic Signal

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

339

14.3 Some History: The Motor Theory of Speech Perception It was the
1960s, and researchers at the Haskins Laboratory in New York were
working to develop a reading machine for the blind. The idea behind this
machine was to capture the acoustic signals associated with the letters
in each word and to transform that signal into sounds to create words
(Whalen, 2019). As part of the project, the Haskins researchers had
developed a machine called the speech spectrograph, which created
records like the ones in Figures 14.3 and 14.4. The Haskins researchers
hoped to use the speech spectrograph to identify the acoustic signal
that went with each phoneme. However, much to their surprise, it turned
out that there wasn't a pattern because the same phoneme could have
different acoustic patterns in different contexts. The classic example
of this is the example of coarticulation, illustrated in Figure 14.5, in
which the /d/ sound in /di/ and /du/ have very different acoustic
signals, even though the /d/ sounds the same in both cases. The di/du
spectrograms are an example of the variability problem---the same
phoneme can have different acoustic patterns in different contexts.
Having discovered that there is no one-to-one correspondence between
acoustic signals and phonemes, the Haskins researchers shifted gears and
turned their attention to explaining the basis of speech perception. To
achieve this, they used the following reasoning: The acoustic signal
can't stand for phonemes, because of the variability problem. So what
property of speech is less variable? What property comes closer to a
one-to-one correspondence with phonemes? The answer to these questions
was described in papers by Alvin Liberman and coworkers titled "A Motor
Theory of Speech Perception" (1963) and "Perception of the Speech Code"
(1967). The motor theory of speech perception proposed that motor
commands are the property that avoids the variability problem because
they have a one-to-one relationship to phonemes.

speakers, so they felt that it is natural that producing and perceiving
would be related. One of the problems with this theory, in the eyes of
other researchers, was that it was unclear what or where the motor
commands were that stood for each phoneme. In a later version of motor
theory, it was stated that these motor commands are located in the brain
(Liberman & Mattingly, 1989). But exactly where are these commands and
what do they look like? The answer wasn't clear. Motor theory stimulated
a large number of experiments, some obtaining results that supported the
theory, but many obtaining results that argued against it. It is
difficult for motor theory to explain, for example, how people with
brain damage that disables their speech motor system can still perceive
speech (Lotto et al., 2009; Stasenko, 2013), or how young infants can
understand speech before they have learned to speak (Eimas et al.,
1987). Evidence such as this, plus the fact that the actual source of
the motor information was never made clear, led many speech perception
researchers to reject the idea that our perception of speech depends on
the activation of motor mechanisms (Lane, 1965; Whalen, 2019).

The Proposal That "Speech Is Special" Along with proposing the
production-perception link, motor theory also proposed that speech
perception is based on a special mechanism that is different from other
auditory mechanisms. This conclusion was based on experiments studying a
phenomenon called categorical perception, which occurs when stimuli that
exist along a continuum are perceived as divided into discrete
categories. The Haskins researchers demonstrated categorical perception
using a property of the speech stimulus called voice onset time (VOT),
the time delay between when a sound begins and when the vocal cords
begin vibrating. We can illustrate this delay by comparing the
spectrograms for the sounds /da/ and /ta/ in Figure 14.7. These
spectrograms show that the time 8 7 6 5 kHz

The variability in the acoustic signal caused by coarticulation and
differences between and within speakers creates a problem for the
listener, because there isn't a "standard" acoustic signal for each
phoneme. In the next section, we will consider an early attempt to solve
this problem.

4 3 2 1

The Proposed Connection Between Production and Perception According to
motor theory, hearing a sound triggers motor processes in the listener
associated with producing the sound. This connection between production
and perception made sense to the Haskins researchers, because perceivers
are also 340

0 17 ms d

91 ms a

t

a

Figure 14.7 Spectrograms for /da/ and /ta/. The voice onset time---the
time between the beginning of the sound and the onset of voicing---is
indicated at the beginning of the spectrogram for each sound.
(Spectrogram courtesy of Ron Cole)

Chapter 14  Perceiving Speech

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

between the beginning of the sound and the beginning of the vocal cord
vibrations (indicated by the presence of vertical stripes in the
spectrogram) is 17 ms for /da/ and 91 ms for /ta/. Thus, /da/ has a
short VOT, and /ta/ has a long VOT. The Haskins researchers created
sound stimuli in which the VOT was varied in small equal steps from
short to long. When they varied VOT, using stimuli like the ones shown
in Figure 14.7, and asked listeners to indicate what sound they heard,
the listeners reported hearing only one or the other of the two
phonemes, /da/ or /ta/, even though a large number of stimuli with
different VOTs were presented. This result is shown in Figure 14.8a
(Eimas & Corbit, 1973). At short VOTs, listeners heard /da/, and they
continued reporting this even when the VOT was increased. But when the
VOT reached about 35 ms, their perception abruptly changed, so they
heard /ta/ at VOTs above 40 ms. The phonetic boundary is the VOT at
which perception changes from one category to another. One key result of
the categorical perception experiment was that even though the VOT was
changed continuously across a wide range, the listeners had the
experience of perceiving only two categories: /da/ on one side of the
phonetic boundary and /ta/ on the other side. Another result was that
when listeners were presented with two sounds separated by a VOT of,
say, 25 ms that are on the same side of the phonetic boundary, such as
stimuli with VOTs of 0 and 25 ms, the listener says they sound the same
(Figure 14.8b). However, when

we present two stimuli that are separated by the same difference in VOT
but are on opposite sides of the phonetic boundary, such as stimuli with
VOTs of 25 and 50 ms, the listener says they sound different. The fact
that all stimuli on the same side of the phonetic boundary are perceived
as the same category is an example of perceptual constancy. You may
remember perceptual constancy from our discussion of color and lightness
constancy in Chapter 9. In color constancy, we perceive the color of
objects as staying the same even when the illumination is changed. In
lightness constancy, whites, grays, and blacks are perceived as staying
the same shade under different illuminations (a white dog looks white
under dim indoor illumination and intense outdoor sunlight). In the
perceptual constancy of speech sounds, we identify sounds as the same
phoneme, even as VOT is changed over a large range. Liberman and
coworkers' 1967 paper proposed that categorical perception provided
evidence for a special speech decoder that is different than the
mechanism involved in hearing non-speech sounds. However, other
researchers rejected this idea when categorical perception was
demonstrated for nonspeech sounds (Cutting & Rosner, 1974), preverbal
infants (Eimas et al., 1971), chinchillas (Kuhl & Miller, 1978), and
budgerigars (parakeets) (Dooling et al., 1989). Why was the motor theory
important? One reason is that it attempted to deal with the main problem
in understanding speech perception---the variable relationship between
the

"Same"

/da/

/da/ 100

100

Phonetic boundary

80

Percentage /da/ responses

Percentage /da/ responses

"Different"

60

40

80

60

40

20

20

/ta/

/ta/ 0

0 0 (a)

20

40

Voice onset time (ms)

60

0

80 (b)

20

40

60

80

Voice onset time (ms)

Figure 14.8 (a) The results of a categorical perception experiment
indicating a phonetic boundary, with /da/ perceived for VOTs to the left
and /ta/ perceived for VOTs to the right. (From Eimas & Corbit, 1973)
(b) In the discrimination part of a categorical perception experiment,
two stimuli are presented, and the listener indicates whether they are
the same or different. The typical result is that two stimuli with VOTs
on the same side of the phonetic boundary (VOT = 0 and 25 ms; solid
arrows) are judged to be the same, whereas two stimuli on different
sides of the phonetic boundary (VOT = 25 ms and 50 ms; dashed arrows)
are judged to be different. 14.3 Some History: The Motor Theory of
Speech Perception

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

341

acoustic signal and phonemes. While motor theory didn't solve the
problem of variability, it influenced later researchers to study the
problem, and also stimulated research on the connection between
production and perception. As it turns out, although most researchers do
not accept the idea that motor commands are the major mechanism
responsible for speech perception, there is evidence for some
connections between motor processes and speech perception, which we will
consider in the next section. Finally, what about categorical
perception? While it didn't end up supporting the idea of a special
speech mechanism, it is nonetheless an interesting and important
phenomenon because it helps explain how we can perceive speech sounds
that happen very quickly, reaching rates as high as 15--30 phonemes per
second (Liberman et al., 1967). Categorical perception helps simplify
things by transforming a long string of voice onset times into two
categories. So the system doesn't have to register a particular sound's
exact VOT; it just has to place the sound in a category that contains
many VOTs. This story of early speech perception research and theorizing
is important because it ushered in the beginning of modern research on
speech perception and stimulated a great deal of research, which
identified a wide range of information that listeners take into account
in perceiving speech. We will describe this information in the next
section.

TEST YOuRSELF 14.1

that the problem for vision was that an object's image on the retina is
ambiguous (see page 92). The solution proposed by Hermann von Helmholtz
involves a process called unconscious inference, in which we infer,
based on whatever information is available, what object is most likely
to have created a particular image on the retina (p. 107). Many
researchers have taken a similar approach to speech by proposing that
listeners use multiple sources of information to perceive the ambiguous
speech stimulus (Devlin & Aydelott, 2009; Skipper et al., 2017). One of
the sources of information is motor processes.

Motor Processes Although motor processes may not be the centerpiece of
speech perception as proposed by motor theory, modern researchers have
demonstrated some connections between motor processes and perceiving
speech sounds. Alessandro D'Ausilio and coworkers (2009) used the
technique of transcranial magnetic stimulation (TMS) (see Method:
Transcranial Magnetic Stimulation (TMS), page 185), to show that
stimulation of motor areas associated with making specific sounds can
aid in perception of these sounds. Figure 14.9 shows the sites of TMS
stimulation on the motor area of the cortex. Stimulation of the lip area
resulted in faster responding to labial phonemes (/b/ and /p/) and
stimulation of the tongue area resulted in faster responding to the
dental phonemes (/t/ and /d/). Based on these results, D'Ausilio
suggested that activity in the motor cortex can influence speech
perception.

1.  Describe the acoustic signal. Be sure you understand how speech
    sounds are represented on a speech spectrogram.
2.  What are phonemes? Why is it not possible to describe speech as a
    string of phonemes?

Faster response to /b/ and /p/

3.  What are two sources of variability that affect the relationship
    between the acoustic signals and the sounds we hear? Be sure you
    understand coarticulation.
4.  Describe the motor theory of speech perception. What is the proposed
    connection between production and perception?

Lip area

5.  What is categorical perception? Be sure you understand how it is
    measured and why proponents of the motor theory thought it was
    important.

Tongue area

6.  What is the present status of motor theory?

14.4 Information for Speech Perception We've seen that the variability
of the speech signal has made it difficult to determine exactly what
information a listener uses in order to perceive speech. This situation
is similar to the one we encountered for perceiving visual objects.
Remember 342

Faster response to /t/ and /d/

Figure 14.9 Sites of transcranial magnetic stimulation of the motor area
for lips and tongue. Stimulation of the lip area increases the speed of
responding to /b/ and /p/. Stimulation of the tongue area increases the
speed of responding to /t/ and /d/. (From D'Ausilio et al., 2009)

Chapter 14  Perceiving Speech

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Figure 14.10 Left hemispheres of the cortex, showing areas activated by
producing speech (red) and comprehending speech (yellow), and areas that
respond to both production and comprehension (blue). (From Silbert et
al., 2014)

The link between producing and perceiving speech has also been studied
using fMRI. Lauren Silbert and coworkers (2014) measured the fMRI
response to two 15-minute stories under two conditions: (1) when the
person in the scanner was telling the story (production condition) and
(2) when the person in the scanner was listening to the story
(comprehension condition). Figure 14.10 shows that Silbert found areas
that responded when a person was producing speech (red areas) and when a
person was comprehending speech (yellow areas). But she also found areas
that responded both when speech was being produced and when it was being
comprehended (blue areas). Just because the same brain regions respond
during both production and comprehension doesn't necessarily mean that
they share processing mechanisms. It is possible that different kinds of
processing could be going on within these regions for these two
different tasks. But the possibility that production and comprehension
share processing mechanisms is supported by Silbert's finding that the
brain's response to production and comprehension was "coupled."

Dorsal action network

Speech production network

Ventral visual network

That is, the time course of the neural responses to these two processes
was similar. Another argument for links between speech production and
perception is based on the interconnectedness of different circuits in
the brain. Mark Schomers and Freidman Pulvermüller (2016) use the
diagrams in Figure 14.11 to compare two theoretical positions regarding
how the brain is involved in speech perception. The diagram on the left
pictures the speech production and perception networks as separated. The
diagram on the right pictures the two networks as connected and also
being connected to the dorsal action network and the ventral vision
network (see Chapter 4, page 80). They then present evidence in favor of
the network-interaction model on the right. The idea that motor
processes are one source of information that is used to understand
speech is still being researched, with some researchers assigning motor
processes a small role in speech perception (Hickock, 2009; Stokes et
al., 2019) and others assigning it a greater role (Schomers &
Pulvermüller, 2016; Skipper et al., 2017; Wilson, 2009). Next, we will
consider evidence that the face and movements of the lips provide
information for speech perception.

The Face and Lip Movements Another property of speech perception is that
it is multimodal; that is, understanding speech can be influenced by
information from senses such as vision and touch. One illustration of
how speech perception can be influenced by visual information is shown
in Figure 14.12. The woman seen in the monitor is saying /ba-ba/, but
the woman's lip movements are those that would produce the sounds
/fa-fa/. The listener therefore hears the sound as /fa-fa/ to match the
lip movements he is seeing, even though the acoustic signal corresponds
to /ba-ba/. (Note that when the listener closes his eyes, his perception
is no longer influenced by what he is seeing and he hears /ba-ba/.) This
effect is called the McGurk effect, after Harry McGurk, who first
described it along with John MacDonald

Dorsal action network

Speech perception network

Speech production network

Speech perception network

Ventral visual network

Figure 14.11 Two ways of thinking about the relation between speech
production and perception. The diagram on the left pictures the networks
serving speech production and perception as separated. The diagram on
the right pictures the networks as connected and are also connected to
the dorsal action network and ventral visual networks (see page 81).
Schomers and Pulvermüller (2016) present evidence favoring the
interconnected network diagram on the right.

14.4 Information for Speech Perception

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

343

Perception

Fa-fa

Ba-ba

Sound from monitor

Lips fa-fa

Figure 14.12 The McGurk effect. The women is saying /ba-ba/ but her lip
movements correspond to /fa-fa/, so the listener reports hearing
/fa-fa/.

(McGurk & MacDonald, 1976). It illustrates that although auditory
information is the major source of information for speech perception,
visual information can also exert a strong influence on what we hear
(see "Something to Consider: Interactions Between Hearing and Vision" in
Chapter 12, page 306). This influence of vision on speech perception is
called audiovisual speech perception. The McGurk effect is one example
of audiovisual speech perception. Another example is the way people
routinely use information provided by a speaker's lip movements to help
understand speech in a noisy environment (also see Sumby & Pollack,
1954). The link between vision and speech has been shown to have a
physiological basis. Gemma Calvert and coworkers (1997) used fMRI to
measure brain activity as observers watched a silent videotape of a
person making mouth movements for saying numbers. Observers silently
repeated the numbers as they watched, so this task was similar to what
people do when they read lips. In a control condition, observers watched
a static face while silently repeating numbers. A comparison of the
brain activity in these two conditions showed that watching the lips
move activated an area in the auditory cortex that Calvert had shown in
another experiment to be activated when people are perceiving speech.
Calvert suggests that the fact that the same areas are activated for
lipreading and speech perception is evidence for a neural mechanism
behind the McGurk effect. The link between speech perception and face
perception was demonstrated in another way by Katharina von Kriegstein
and coworkers (2005), who measured fMRI activation as listeners were
carrying out a number of tasks involving sentences spoken by familiar
speakers (people who also worked in the laboratory) and unfamiliar
speakers (people they had never heard before). Just listening to speech
activated the superior temporal sulcus (STS; see Figure 5.42 page 111),
an area that had been associated in previous studies with speech
perception (Belin et al., 2000). But when listeners were asked to carry
out a task that involved paying attention to the sounds of familiar
voices, the 344

fusiform face area (FFA) was also activated. In contrast, paying
attention to the sounds of unfamiliar voices did not activate the FFA.
Apparently, when people hear a voice that they associate with a specific
person, this activates areas not only for perceiving speech but also for
perceiving faces. The link between perceiving speech and perceiving
faces, which has been demonstrated in both behavioral and physiological
experiments, provides information that helps us deal with the
variability of phonemes (for more on the link between observing someone
speaking and perceiving speech, see Hall et al., 2005; McGettigan et
al., 2012; van Wassenhove et al., 2005).

Knowledge of Language A large amount of research has shown that it is
easier to perceive phonemes that appear in a meaningful context. Philip
Rubin and coworkers (1976), for example, presented a series of short
words, such as sin, bat, and leg, or nonwords, such as jum, baf, and
teg, and asked listeners to respond by pressing a key as rapidly as
possible whenever they heard a sound that began with /b/. On average,
participants took 631 ms to respond to the nonwords and 580 ms to
respond to the real words. Thus, when a phoneme was at the beginning of
a real word, it was identified about 8 percent faster than when it was
at the beginning of a meaningless syllable. The effect of meaning on the
perception of phonemes was demonstrated in another way by Richard Warren
(1970), who had participants listen to a recording of the sentence "The
state governors met with their respective legislatures convening in the
capital city." Warren replaced the first /s/ in "legislatures" with the
sound of a cough and told his participants that they should indicate
where in the sentence the cough occurred. None of the participants
identified the correct position of the cough, and, even more
significantly, none noticed that the /s/ in "legislatures" was missing.
This effect, which Warren called the phonemic restoration effect, was
experienced even by students and staff in the psychology department who
knew that the /s/ was missing. Warren not only demonstrated the phonemic
restoration effect but also showed that it can be influenced by the
meaning of words following the missing phoneme. For example, the last
word of the phrase "There was time to *ave..." (where the* indicates the
presence of a cough or some other sound) could be "shave," "save,"
"wave," or "rave," but participants heard the word "wave" when the
remainder of the sentence had to do with saying good-bye to a departing
friend. Arthur Samuel (1990) also demonstrated top-down processing by
showing that longer words increase the likelihood of the phonemic
restoration effect. Apparently, participants used the additional context
provided by the long word to help identify the masked phoneme. Further
evidence for the importance of context is Samuel's finding that more
restoration occurs for a real word such as prOgress (where the capital
letter indicates the masked phoneme) than for a similar pseudoword such
as crOgress (Samuel, 1990; also see Samuel, 1997, 2001, for more
evidence that top-down processing is involved in phonemic restoration).

Chapter 14  Perceiving Speech

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

The Meaning of Words in Sentences

ers heard the stimuli in the presence of a background noise. For
example, at a moderately high level of background noise, accuracy was 63
percent for the normal sentences, 22 percent for the anomalous
sentences, and only 3 percent for the ungrammatical strings of words.
These results tell us that when words are arranged in a meaningful
pattern, we can perceive them more easily. But most people don't realize
it is their knowledge of the nature of their language that helps them
fill in sounds and words that might be difficult to hear. For example,
our knowledge of permissible word structures tells us that ANT, TAN, and
NAT are all permissible sequences of letters in English, but that TQN or
NQT cannot be English words. A similar effect of meaning on perception
also occurs because our knowledge of the rules of grammar tells us that
"There is no time to question" is a permissible English sentence, but
"Question, no time there is" is not permissible or, at best, is
extremely awkward (unless you are Yoda, who says this in Star Wars,
Episode III: Revenge of the Sith). Because we mostly encounter
meaningful words and grammatically correct sentences, we are continually
using our knowledge of what is permissible in our language to help us
understand what is being said. This becomes particularly important when
listening under less than ideal conditions, such as in a noisy
environment or when the speaker's voice quality or accent is difficult
to understand, as we will discuss later in the chapter (see also Salasoo
& Pisoni, 1985). Another example of the effect of meaning on perception
is that even though the acoustic signal for spoken sentences is
continuous, with either no physical breaks in the signal or breaks that
don't necessarily correspond to the breaks we perceive between words
(Figure 14.13), we usually have little trouble perceiving individual
words when conversing with another person. The perception of individual
words in a conversation is called speech segmentation. The fact that
there are often no spaces between words becomes obvious when you listen
to someone speaking a foreign language. To someone who is unfamiliar
with that language, the words seem to speed by in an unbroken string.
However, to a speaker of that language, the words seem separated, just
as the words of your native language seem separated to you. We somehow
solve the problem of speech segmentation and divide the continuous
stream of the acoustic signal into a series of individual words.

It has been said that "all language begins with speech" (Chandler,
1950), but we can also say that perceiving speech is aided by language.
One illustration of this is that when words are in a sentence, they can
be read even when they are incomplete, as in the following
demonstration. DEMONSTRATION

Perceiving Degraded Sentences

Read the following sentences: 1. M*R* H*D* L*TTL* L*MB I*S FL**C\* W*S
WH*T\* *S SN*W 2. TH\* S*N* S N*T SH*N*NG T*D** 3. S*M* W\*\*DS *R*
EA*I*R T\* U*D*R*T*N\* T*A* *T*E\*S

Your ability to read the sentences, even though half of the letters have
been eliminated, was aided by your knowledge of English words, how words
are strung together to form sentences, and perhaps in the first example,
your familiarity with the nursery rhyme (Denes & Pinson, 1993). A
similar effect of meaningfulness also occurs for spoken words. A classic
experiment by George Miller and Steven Isard (1963) demonstrated how
meaningfulness makes it easier to perceive spoken words by showing that
words are more intelligible when heard in the context of a grammatical
sentence than when presented as items in a list of unconnected words.
They demonstrated this by creating three kinds of stimuli: (1) normal
grammatical sentences, such as Gadgets simplify work around the house;
(2) anomalous sentences that follow the rules of grammar but make no
sense, such as Gadgets kill passengers from the eyes; and (3)
ungrammatical strings of words, such as Between gadgets highways
passengers the steal. Miller and Isard used a technique called
shadowing, in which they presented these sentences to participants
through earphones and asked them to repeat aloud what they were hearing.
The participants reported normal sentences with an accuracy of 89
percent, but their accuracy fell to 79 percent for the anomalous
sentences and 56 percent for the ungrammatical strings. The differences
among the three types of stimuli became even greater when the listen-

Figure 14.13 Sound energy for the words "speech segmentation." Notice
that it is difficult to tell from this record where one word ends and
the other begins. (Speech signal courtesy of Lisa Sanders)

S

P

EE

CHS

E G MEN T

A

T IO N 14.4 Information for Speech Perception

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

345

DEMONSTRATION

Organizing Strings of Sounds

Read the following words: Anna Mary Candy Lights out loud, speaking
rapidly and ignoring the spaces between the words. Now that you've read
the words, what do they mean?

If you succeeded in creating the phrase "An American Delights" from the
series of words, you did so by changing the perceptual organization of
the sounds, and this change was achieved by your knowledge of the
meaning of the sounds. Another example of how meaning and prior
knowledge or experience are responsible for organizing sounds into words
is provided by these two sentences: Jamie's mother said, "Be a big girl
and eat your vegetables." The thing Big Earl loved most in the world was
his car. "Big girl" and "Big Earl" are both pronounced the same way, so
hearing them differently depends on the overall meaning of the sentence
in which these words appear. (Slight differences in stress may also play
a role here.) This example is similar to the familiar "I scream, you
scream, we all scream for ice cream" that many people learn as children.
The sound stimuli for "I scream" and "ice cream" are identical, so the
different organizations must be achieved by the meaning of the sentence
in which these words appear. While segmentation is aided by knowing the
meanings of words and making use of the context in which these words
occur, listeners use other information as well to achieve segmentation.
As we learn a language, we learn that certain sounds are more likely to
follow one another within a word, and other sounds are more likely to be
separated by the space between two words.

Learning About Words in a Language Consider the words pretty baby. In
English it is likely that pre and ty will be in the same word (pre-tty)
and that ty and ba will be separated by a space so will be in two
different words (pretty baby). Thus, the space in the phrase prettybaby
is most likely to be between pretty and baby. Psychologists describe the
way sounds follow one another in a language in terms of transitional
probabilities---the chances that one sound will follow another sound.
Every language has transitional probabilities for different sounds, and
as we learn a language, we not only learn how to say and understand
words and sentences, but we also learn about the transitional
probabilities in that language. The process of learning about
transitional probabilities and about other characteristics 346

of language is called statistical learning. Research has shown that
infants as young as 8 months of age are capable of statistical learning.
Jennifer Saffran and coworkers (1996) carried out an early experiment
that demonstrated statistical learning in young infants. Figure 14.14a
shows the design of this experiment. During the learning phase of the
experiment, the infants heard strings of nonsense "words" such as
bidaku, padoti, golabu, and tupiro, which were combined in random order
to create 2 minutes of continuous sound. An example of part of a string
created by combining these words is bidakupadotigolabutu­
piropadotibidaku. ... In this string, every other word is printed in
boldface in order to help you pick out the words. However, when the
infants heard these strings, all the words were pronounced with the same
intonation, and there were no breaks between the words to indicate where
one word ended and the next one began. Because the words were presented
in random order and with no spaces between them, the 2-minute string of
words the infants heard sounds like a jumble of random sounds. However,
there was information within the string of words in the form of
transitional probabilities, which the infants could potentially use to
determine which groups of sounds were words. The transitional
probabilities between two syllables that appeared within a word was
always 1.0. For example, for the word bidaku, when /bi/ was presented,
/da/ always followed it. Similarly, when /da/ was presented, /ku/ always
followed it. In other words, these three sounds always occurred together
and in the same order, to form the word bidaku. However, the
transitional probabilities between the end of one word and the beginning
of another was only 0.33. For example, there was a

Listen to string of "words"--- 2 minutes

Listen to pairs of words---"whole" and "part"

Learning

Test

(a) 8.0 Listening time (sec)

The fact that we can perceive individual words in conversational speech,
even though there are few breaks in the speech signal, means that our
perception of words is not based only on the energy stimulating the
receptors. One thing that helps us tell when one word ends and another
begins is knowledge of the meanings of words. The link between speech
segmentation and meaning is illustrated in the following demonstration.

7.5 7.0 6.5

Whole word (b)

Part word

Stimulus

Figure 14.14 (a) Experimental design of the experiment by Saffran and
coworkers (1996), in which infants listened to a continuous string of
nonsense syllables and were then tested to see which sounds they
perceived as belonging together. (b) The results, indicating that
infants listened longer to the "part-word" stimuli.

Chapter 14  Perceiving Speech

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

33 percent chance that the last sound, /ku/ from bidaku, would be
followed by the first sound, /pa/, from padoti, a 33 percent chance that
it would be followed by /tu/ from tupiro, and a 33 percent chance it
would be followed by /go/ from golabu. If Saffran's infants were
sensitive to transitional probabilities, they would perceive stimuli
like bidaku or padoti as words, because the three syllables in these
words are linked by transitional probabilities of 1.0. In contrast,
stimuli like tibida (the end of padoti plus the beginning of bidaku)
would not be perceived as words, because the transitional probabilities
were much smaller. To determine whether the infants did, in fact,
perceive stimuli like bidaku and padoti as words, the infants were
tested by being presented with pairs of three-syllable stimuli. One of
the stimuli was a "word" that had been presented before, such as padoti.
This was the "whole-word" test stimulus. The other stimulus was created
from the end of one word and the beginning of another, such as tibida.
This was the "part-word" test stimulus. The prediction was that the
infants would choose to listen to the part-word test stimuli longer than
to the whole-word stimuli. This prediction was based on previous
research that showed that infants tend to lose interest in stimuli that
are repeated, and so become familiar, but pay more attention to novel
stimuli that they haven't experienced before (see habituation procedure,
page 225). Thus, if the infants perceived the whole-word stimuli as
words that had been repeated over and over during the 2-minute learning
session, they would pay less attention to these familiar stimuli than to
the more novel partword stimuli that they did not perceive as being
words. Saffran measured how long the infants listened to each sound by
presenting a blinking light near the speaker where the sound was coming
from. When the light attracted the infant's attention, the sound began,
and it continued until the infant looked away. Thus, the infants
controlled how long they heard each sound by how long they looked at the
light. Figure 14.14b shows that the infants did, as predicted, listen
longer to the part-word stimuli. These results are impressive,
especially because the infants had never heard the words before, they
heard no pauses between words, and they had only listened to the strings
of words for 2 minutes. From results such as these, we can conclude that
the ability to use transitional probabilities to segment sounds into
words begins at an early age. TEST YOuRSELF 14.2 1. How have speech
researchers been guided by Helmholtz's theory of unconscious inference?
2. How has transcranial magnetic stimulation (D'Ausilo experiment) and
fMRI (Silbert experiment) been used to demonstrate a connection between
speech production and perception? 3. What does measuring networks in the
brain indicate about the possible link between production and
perception?

4.  What is the McGurk effect, and what does it illustrate about how
    speech perception can be influenced by visual information? What
    physiological evidence demonstrates a link between visual processing
    and speech perception?
5.  Describe evidence that shows how perceiving phonemes is influenced
    by the context in which they appear. Describe the phonemic
    restoration effect and the evidence for both bottom-up and top-down
    processing in creating this effect.
6.  What is the evidence that meaning can influence word perception?
7.  What mechanisms help us perceive breaks between words?
8.  Describe the Saffran experiment and the basic principle behind
    statistical learning.

14.5 Speech Perception in Difficult Circumstances One thing you should
be convinced of by now is that although the starting point for
perceiving speech is the incoming acoustic signal, listeners also use
top-down processing, involving their knowledge of meaning and the
properties of language, to perceive speech. This additional information
helps listeners deal with the variability of speech produced by
different speakers. But in our everyday environment we have to deal with
more than just different ways of speaking. We also have to deal with
background noise, poor room acoustics, and smartphones under poor
reception conditions, all of which prevent a clear acoustic signal from
reaching our ears. How well can we understand speech heard under adverse
conditions? Research designed to answer this question has shown that
listeners can adapt to adverse conditions by using top-down processing
to "decode" the degraded acoustic signal. Matthew Davis and coworkers
(2005) tested participants to determine their ability to perceive speech
distorted by a process called noise vocoding. Noise-vocoded speech is
created by dividing the speech signal up into different frequency bands
and then adding noise to each band. This process transforms the
spectrogram of the original speech stimulus on the left in Figure 14.15
into the noisy spectrogram on the right. The loss of detail in the
frequency representation of the noise-vocoded signal transforms clear
speech into a harsh noisy whisper. Participants in Davis's experiment
listened to a vocoded sentence and then wrote down as much of the
sentence as they could. This was repeated for a total of 30 sentences.
Figure 14.16 shows the average proportion of words reported correctly by
six participants for each of the 30 sentences. Notice that performance
was near zero for the first three sentences, and then increases, until
by the 30th sentence, participants are reporting half or more of the
words. (The variability occurs because some vocoded sentences are more
difficult to hear than others.) The increase in performance shown in
Figure 14.16 is 14.5 Speech Perception in Difficult Circumstances

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

347

Frequency (kHz)

Original clear speech

Noise vocoded speech

8

8

6

6

4

4

2

2

0

0

1

2

0

3

0

Time (sec)

1

2

3

Time (sec)

Figure 14.15 How the speech signal was changed for the Davis et
al. (2005) noise vocoding experiment. The spectrogram of the original
speech stimulus is on the left, and the noise-vocoded version is on the
right. See text for details.

important because all the participants were doing is listening to one
sentence after the other. In another experiment, Davis's participants
first listened to a degraded sentence and wrote down what they heard, as
before, and then heard a clear, undistorted version of the sentence,
followed by the distorted sentence again (hear degraded sentence → hear
clear sentence → hear degraded sentence again). Participants reported
that when they listened to the second presentation of the degraded
sentence, they heard some words they hadn't heard the first time. Davis
calls this ability to hear previously unintelligible words the "pop-out"
effect. The pop-out effect shows that higher-level information such as
listeners' knowledge can improve speech perception. But this result
becomes even more interesting when we consider that after experiencing
the pop-out effect participants became better at understanding other
degraded sentences that

\% Words reported correctly

100 80 60 40 20 0

0

5

10

15

20

25

30

Sentence number

Figure 14.16 Perception of noise-vocoded words correctly identified for
a series of 30 different sentences. Each data point is the average
performance for the six subjects in Davis and coworkers' (2005)
experiment. 348

they were hearing for the first time. Even more interesting, the pop-out
effect and later improvement in performance also occurred in a group of
participants who read the sentence after hearing the degraded version
(hear degraded sentence → read written sentence → hear degraded sentence
again). What this means is that it wasn't listening to the clear sound
that was important, but knowing the content (the speech sounds and
words) of what they were hearing that helped with learning. Thus, this
experiment provides another demonstration of how listeners can use
information in addition to the acoustic signal to understand speech.
What information can listeners pick up from degraded sentences? One
possibility is the temporal pattern---the timing or rhythm of the
speech. Robert Shannon and coworkers (1995) used noise-vocoded speech to
demonstrate the importance of these slow temporal fluctuations. They
showed that when most of the pitch information was eliminated from a
speech signal, listeners were still able to recognize speech by focusing
on temporal cues such as the rhythm of the sentence. You can get a feel
for the information carried by temporal cues by imagining what speech
sounds like when you press your ears to a door and hear only muffled
voices. Although hearing-through-the-door speech is difficult to
understand, there is information in the rhythm of speaking that can lead
to understanding. Much of this information comes from your knowledge of
language, learned through years of experience. One example of learning
from experience is the learning of statistical regularities we discussed
in connection with Saffran's infant experiments on page 346. We also
discussed the idea of learning from experience in Chapter 5, when we
described how visual perception is aided by our knowledge of
regularities in the environment (p. 105). Remember the "multiple
personalities of a blob" experiment in which perception of a blob-like
shape depended on the type of scene in which it appeared (also see
Figure 5.37 and see page 323 for a similar

Chapter 14  Perceiving Speech

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

discussion related to music perception). Demonstrations such as this
illustrate how knowledge of what usually happens in the visual
environment influences what we see. Similarly, our knowledge of how
certain speech sounds usually follow one another can help us learn to
perceive words in sentences, even if the individual sounds are
distorted. The ability to determine what is being said, even when sounds
are distorted, is something you may have experienced if you have ever
listened to someone speaking with a foreign accent that was difficult to
understand at first but became easier to understand as you continued to
listen. If this has happened to you, it is likely that you were trying
to understand what the person were saying---the overall
meaning---without focusing on the sounds of individual words. But
eventually, listening to determine the overall message results in an
increased ability to understand individual words, which in turn makes it
easier to understand the overall message. Clearly, transforming "sound"
to "meaningful speech" involves a combination of bottom-up processing,
based on the incoming acoustic signal, and top-down processing, based on
knowledge of meanings and the nature of speech sounds.

14.6 Speech Perception and the Brain Investigation of the physiological
basis for speech perception stretches back to at least the 19th century,
but considerable progress has been made recently in understanding the
physiological foundations of speech perception and spoken word
recognition. Let's begin with the classic observations of Paul Broca
(1824--1880) and Carl Wernicke (1848--1905), who showed that damage to
specific areas of the brain causes language problems, called aphasias
(Figure 14.17). When Broca tested patients who had suffered strokes that
damaged their frontal lobe, in an area that came to be called Broca's
area, he found that their speech was slow and labored and often had
jumbled

Wernicke's area

Broca's area

Figure 14.17 Broca's and Wernicke's areas. Broca's area is in the
frontal lobe and Wernicke's is in the temporal lobe.

sentence structure. (See Chapter 2, page 31, and Chapter 13, page 327
for more on Broca.) Here is an example of the speech of a modern
patient, who is attempting to describe when he had his stroke, which
occurred when he was in a hot tub. Alright. ... Uh... stroke and un. ...
I... huh tawanna guy. ... H... h... hot tub and. ... And the. ... Two
days when uh. ... Hos... uh. ... Huh hospital and uh... amet... am...
ambulance. (Dick et al., 2001, p. 760) Patients with this
problem---slow, labored, ungrammatical speech caused by damage to
Broca's area, are diagnosed as having Broca's aphasia. Later research
showed that patients with Broca's aphasia not only have difficulty
forming complete sentences, they also have difficulty understanding some
types of sentences. Consider, for example, the following two sentences:
The apple was eaten by the girl. The boy was pushed by the girl.
Patients with Broca's aphasia have no trouble understanding the first
sentence but have difficulty with the second sentence. The problem they
have with the second sentence is deciding whether the girl pushed the
boy or the boy pushed the girl. While you may think it is obvious that
the girl pushed the boy, patients with Broca's aphasia have difficulty
processing connecting words such as "was" and "by," and this makes it
difficult to determine who was pushed. (Notice what happens to the
sentence when these two words are omitted.) You can see, however, that
the first sentence cannot be interpreted in two ways. It is clear that
the girl ate the apple, because it is not possible, outside of an
unlikely science fiction scenario, for the apple to eat the girl (Dick
et al., 2001; Novick et al., 2005). Taking into account the problems in
both producing and understanding speech experienced by Broca's patients,
modern researchers have concluded that damage to Broca's area in the
frontal lobe causes problems in processing the structure of sentences.
The patients studied by Wernicke, who had damage to an area in their
temporal lobe that came to be called Wernicke's area, produced speech
that was fluent and grammatically correct but tended to be incoherent.
Here is a modern example of the speech of a patient with Wernicke's
aphasia. It just suddenly had a feffort and all the feffort had gone
with it. It even stepped my horn. They took them from earth you know.
They make my favorite nine to severed and now I'm a been habed by the uh
stam of fortment of my annulment which is now forever. (Dick et al.,
2001, p. 761) Patients such as this not only produce meaningless speech
but are unable to understand speech and writing. While patients with
Broca's aphasia have trouble understanding sentences in which meaning
depends on word order, as in "The boy was pushed by the girl,"
Wernicke's patients have more widespread difficulties in understanding
and would be unable to understand "The apple was eaten by the girl" as
well. In the most extreme form of Wernicke's aphasia, the person has
14.6 Speech Perception and the Brain

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

349

Dorsal pathway Parietal lobe

High Activation Low

(a) 

Phoneme

Motor cortex

of the electrodes on the temporal lobe. Each dot is an electrode; the
darker colored dots indicate locations at which neurons responded most
strongly to speech when participants listened to 500 sentences spoken by
400 different people. Each column in Figure 14.19b shows the results for
a single electrode. Red and dark red indicate the neural response for
the first 0.4 seconds after the onset of each phoneme, listed on the
left. Each of these electrodes records responses to a group of phonemes.
For example, electrode 1 responds to consonants like /d/, /b/, /g/, /k/,
and /t/, and electrode 3 responded to vowels like /a/ and /ae/.

d b g p k t ∫ z s f θ u w ә r I о c ai a a æ ε ei j i i u

e1

e2

Electrodes e3

e4

e5

Ω

a condition called word deafness, in which he or she cannot recognize
words, even though the ability to hear pure tones remains intact (Kolb &
Whishaw, 2003). Modern research on speech and the brain has moved away
from focusing on Broca's and Wernicke's areas to consider how speech
perception involves multiple areas of the brain, some of which are
specialized for specific speech functions. For example, we saw in
Chapter 2 that Pascal Belin and coworkers (2000) used fMRI to locate a
"voice area" in the human superior temporal sulcus (STS; see Figure
2.17) that is activated more by human voices than by other sounds, and
Catherine Perrodin and coworkers (2011) recorded from neurons in the
monkey's temporal lobe that they called voice cells because they
responded more strongly to recordings of monkey calls than to calls of
other animals or to "nonvoice" sounds. The "voice area" and "voice
cells" are located in the temporal lobe, which is part of the what
processing stream for hearing that we described in Chapter 12 (see
Figure 12.12, page 299). In describing the cortical organization for
hearing in Chapter 12, we saw that the what pathway is involved in
identifying sounds and the where pathway is involved in locating sounds.
Piggybacking on this dual-stream idea for hearing, researchers have
proposed a dual-stream model of speech perception. One version of this
model is shown in Figure 14.18. The ventral stream supports speech
comprehension and the dorsal stream may be involved in linking the
acoustic signal to the movements used to produce speech (Hickock &
Poeppel, 2015; Rauschecker, 2011). In addition to considering the
possible functions of the ventral and dorsal streams, other research has
looked at how phonemes are represented in the brain. Nima Mesgarani and
coworkers (2014) took advantage of the "standard procedure" for brain
surgery for epilepsy, which involves using electrodes placed on the
brain to determine the functional layout of a particular person's brain.
Figure 14.19a shows the locations

Ω

Frontal lobe

AC

Posterior

Anterior Ventral pathway

Figure 14.18 Human cortex showing the ventral pathway (red arrows) that
is responsible for recognizing speech and the dorsal pathway (blue
arrows) that links the acoustic signal and motor movements. AC 5
auditory cortex. The ventral pathway sends signals from the anterior
auditory area to the frontal cortex. The dorsal pathway sends signals
from the posterior auditory area to the parietal lobe and motor areas.
(Adapted from Rauschecker, 2011) 350

v n m ŋ

0 0.2 0.4 (b)

0 0.2 0.4 0 0.2 0.4 0 0.2 0.4 Time from phoneme onset (sec)

0 0.2 0.4

Figure 14.19 (a) The red dots indicate electrode placements on the
temporal lobe for Mesgarani and coworkers' (2014) experiment. Darker
dots indicate larger responses to speech sounds. (b) Average neural
responses to the phonemes on the left, showing activity in red for 5
electrodes during the first 0.4 seconds after presentation of the
phonemes.

Chapter 14  Perceiving Speech

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

While Mesgarani and coworkers observed electrode responses corresponding
to single phonemes, they also found responses corresponding to phonetic
features, like manner of articulation, which describes how the
articulators interact while making a speech sound, and place of
articulation, which describes the location of articulation (p. 337).
Mesgarani and coworkers found electrodes that were linked to specific
phonetic features. For example, one electrode picked up responses to
sounds that involved place of articulation in the back of the mouth,
such as /g/, and another responded to sounds associated with places near
the front, such as /b/. Thus, neural responses can be linked both to
phonemes, which specify specific sounds, and to specific features, which
are related to the way these sounds are produced. If we consider the
response to a particular phoneme or feature across all of the
electrodes, we find that each phoneme or feature causes a pattern of
activity across these electrodes. The neural code for phonemes and
phonetic features therefore corresponds to population coding, which was
described in Chapter 2 (see Figure 2.13, page 30). What is important
about studies such as this one is that they go beyond just identifying
where speech is processed in the cortex. These studies, and many others,
have provided information about how basic units of speech such as
phonemes and the phonetic features associated with these phonemes are
represented by patterns of neural responding.

SOMETHING TO CONSIDER:

Cochlear Implants Our ability to hear speech depends, of course, on both
the brain, which we have just been discussing, and on the hair cells

3

inside the cochlea, which, as we saw in Chapter 11, generate the
electrical signals that are sent to the brain. Damage to the hair cells
causes sensorimotor hearing loss, which affects the ability to hear and
perceive speech. If hearing is not completely lost, hearing can be
partially restored by hearing aids, which amplify the sound that
remains. However, if hearing loss is extreme or even complete, hearing
aids can't help. Prior to 1957, people with severe sensorimotor hearing
loss were told that nothing could be done for their condition. But in
1957, Andre Djourno and Charles Eyries succeeded in eliciting sound
sensations by stimulating a person's hair cells in the cochlea with an
electrode placed in the inner ear. This was the first cochlear implant
(CI). Later work created multielectrode CIs, with modern CIs having 12
to 22 electrodes. There are now more than half a million people who have
been surgically fitted with cochlear implants (Svirsky, 2017). The basic
principle behind CIs is shown in Figure 14.20. The cochlear implant
consists of (1) a microphone that receives sound signals from the
environment; (2) a sound processor that divides the sound received by
the microphone into a number of frequency bands; (3) a transmitter that
sends these signals to (4) an array of 12--22 electrodes that are
implanted along the length of the cochlea. These electrodes stimulate
the cochlea at different places along its length, depending on the
intensities of the frequencies in the stimuli received by the
microphone. This stimulation activates auditory nerve fibers along the
cochlea, which send signals toward the brain. The placement of the
electrodes is based on the topographic layout of the cochlea described
by von Bekesy (p. 276), in which activation of hair cells near the base
of the cochlea is associated with high frequencies, and activation near
the apex is associated with low frequencies (see Figure 11.24, page
278).

4

1

2 Electrode

Cochlea

Figure 14.20 Cochlear implant. See text for details. Something to
Consider: Cochlear Implants

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

351

Figure 14.21a shows a speech spectrogram for the word Choice. Figure
14.21b shows the pattern of electrical pulses delivered by the cochlear
implant electrodes in response to the same word. Notice that "frequency"
on the vertical axis of the spectrogram is replaced by "electrode
number" on the vertical axis of the CI stimulation record. There are two
noteworthy things about the CI stimulation record. First, there is a
correspondence between the spectrogram and the CI record. The high
frequency stimulus between 3,500 and 5,000 Hz recorded on the
spectrogram, which corresponds to the sound \[Ch\], is signaled by
responses in electrodes 13--22. Also, frequencies below 3,000 that occur
between 0.55 and 0.8 seconds on the spectrogram, which corresponds to
the sound \[oice\], are signaled by responses in electrodes 1--11. This
correspondence between the signal indicated by the spectrogram and the
hair cells activated by the CI is what results in perception of a sound
corresponding to Choice. However, notice that the harmonics between 0.55
and 0.8 seconds in the spectrogram are blurred together in the CI
signal. Thus, there is a correspondence between the audio signal
indicated by the spectrogram and the CI signal, but it isn't perfect.
One reason for this lack of complete correspondence is that the CI
electrodes are separated from the hair cells by a wall of bone, which
spreads the stimulation. Thus,

(a) 

Frequency (Hz)

5,000

CH

O

I

0

CE

4,000

210

3,000

220 230

2,000

Intensity (dB)

Figure 14.21 (a) Spectrogram created in response to the spoken word
"choice." (b) Pattern of electrically stimulated pulses delivered to
electrodes of a cochlear implant in response to the same word. Notice
that the overall pattern of the O I record in (b) matches the
spectrogram in (a), but that details of the spectrogram, such as the
harmonic bands between 0.55 and 0.80 seconds, are missing. (Svirsky,
2017)

a particular CI electrode stimulates many neurons, which causes the
stimulation created by neighboring electrodes to overlap. Because of
these distortions, what a person with a CI hears is not the same as
normal hearing. For example, people who had experienced some hearing
before receiving their CI describe what they hear as a radio out of
tune, Minnie Mouse, Donald Duck, or (less frequently) Darth Vader.
Luckily, because the brain is plastic it can adapt to distorted input,
so while a person's spouse might initially sound like a chipmunk, the
quality and understandability of her voice typically improves over a
period of weeks or months (Svirsky, 2017). In clinical practice there is
a great deal of variability between people who have received CIs, so one
person might be able to perceive 100 percent of the words in sentences,
and might even be able to use a telephone, while another person
perceives nothing (Macherey & Carlyon, 2014). In one evaluation, people
with some preoperative hearing were tested on their ability to perceive
words in sentences (Parkinson et al., 2002). Their preoperative score
was 11 percent correct, and postoperative, 78 percent correct. However,
adding background noise made identifying words more difficult, and CI
users often report that music sounds distorted or "nonmusical" (Svirsky,
2017).

240 1,000 250 0 0.4

0.7 0.8 Time (s)

0.9

1.0

1.1

1.0

22 20 18 16 14 12 10 8 6 4 2

0.8 0.6 0.4 0.2 0 0.4

352

0.6

Stimulation current

Electrode number

(b) 

0.5

0.5

0.6

0.7 0.8 Time (s)

0.9

1.0

1.1

Chapter 14  Perceiving Speech

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

1 Change in blood oxygenation

Change in blood oxygenation

0.4 0.2 0 20.2 20.4 210

0

10

20 30 Time (s) (a) Before cochlear implant activated

40

50

0.5 0

20.5 210

0

10

20 Time (s) (b) Cochlear implant activated

30

40

50

Figure 14.22 (a) The red record indicates blood oxygenation (which
indicates neural activity), measured from the auditory cortex in a deaf
child in response to auditory stimulation (green line), delivered before
activation of the child's cochlear implant. Note that there is no
response to the stimulation. (b) The record in response to auditory
stimulation delivered after activation of the cochlear implant. (From
Bortfeld, 2019)

An extremely important application of CI is its use in children,
especially those who are born deaf. Using a special technique to measure
brain activity in infants, Heather Bortfeld (2019) showed that there was
no auditory cortex response in an infant before installing the CI, but a
response did appear after the CI was installed (Figure 14.22). One key
to a successful outcome for children is to implant the CI at an early
age. One study showed that if the CI is implanted by near the first
birthday, children's speech and language skills can be near normal by
the age of 4 1/2 and another study found that 75 percent of grade school
children with implants were in mainstream education (Geers & Nicholas,

2013; Sharma et al., 2020). Because of results such as these, installing
of CI is now standard clinical procedure for children who are born deaf
(Machery & Carlyon, 2014). This early implantation is possible because
the human cochlea is near adult size at birth, so children don't outgrow
their electrode array. The development of CI is an example of how basic
research, which helps us understand the workings of a sensory system---
like Bekesy's mapping of frequency along the cochlea---can have
meaningful practical applications. In this case, CIs transform a world
of silence into one in which speech can be heard and understood, which,
in turn, leads to being able to use speech to communicate with others.

DEVELOPMENTAL DIMENSION Infant-Directed Speech In the Developmental
Dimension in Chapter 11, we saw that newborns can hear (although not as
well as adults), and, because they can hear in the womb, newborns can
recognize their mother's voice (DeCasper & Fifer, 1980). But perceiving
and understanding speech goes beyond simply "hearing," because the
sounds that newborns experience need to be transformed into words and
then meaningful speech. So how does a newborn negotiate the journey from
hearing sounds to understanding someone talking? One thing that helps is
that infants between 1 and 4 months of age can discriminate between
different consonant sounds such as /ba/ and /ga/ or /ma/ and /na/ (Eimas
et al., 1971), and between different vowel sounds such as /a/ and /i/,
as in had versus hid (Trehub, 1973). But speech perception involves more
than discriminating between phonemes. It also involves learning both
isolated words and words as they speed by within phrases or sentences,
and also being able to understand the thoughts transmitted by strings of
words.

The key to achieving this learning is to have a teacher. Often the
primary teacher is the mother, but the father and others usually
participate as well. This teaching occurs as the infant listens to
people talk, but most importantly, as people talk directly to the
infant, which brings us to the title of this section: Infant-directed
speech. Infant-directed speech (IDS), which is also called "motherese"
(or more recently, "parentese,"), or "baby talk," has special
characteristics that both attract an infant's attention and make it
easier for the infant to recognize individual words. The characteristics
of IDS that make it different than adultdirected speech (ADS) are: (1)
higher in pitch, (2) larger range of pitches, (3) slower, (4) words are
more separated, or totally isolated, and (5) words are often repeated.
In addition, IDS often transmits positive affect. Research has shown
that the higher pitch and pitch range, and the positive affect help
capture the infant's attention, so from birth through at least 14
months, infants prefer to listen to IDS compared to ADS (Fernald & Kuhl,
1987; Continued Something to Consider: Cochlear Implants

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

353

TEST YOuRSELF 14.3 1. How did Davis use noise-vocoded speech to
demonstrate how listeners can use information other than the acoustic
signal to perceive speech? 2. Describe Robert Shannon's experiment on
the temporal pattern of speech. 3. What did Broca and Wernicke discover
about the physiology of speech perception? 4. Describe the "voice area"
and "voice cells." 5. Describe the dual-stream model of speech
perception.

IDS is magnified in infants with CI because these infants typically show
reduced attention to speech, compared to normalhearing infants (Horn et
al., 2007). In a study that measured the degree to which infants with
CIs responded to IDS, Yuayan Wang and coworkers (2017) found that IDS
increased these infants' attention to speech and words and was
associated with better language comprehension. Infant-directed speech,
it turns out, is a good way to catch the attention of infants whose
tendency to attend has been affected by their hearing loss.

/i/

3,000

F2 (Hz)

McRoberts et al., 2009; Soderstrom, 2007). The larger range of pitches
for IDS is pictured in the IDS "vowel triangle" shown in Figure 14.23.
When the first and second formants of the IDS sounds /i/ as in see, /a/
as in saw, and /u/ as in sue are plotted, they create a larger triangle
than do the same ADS sounds, and this larger range makes it easier to
tell the difference between the sounds (Golinkoff et al., 2015; Kuhl et
al., 1997). The greater separation between sounds and words helps
infants distinguish individual words, and another feature of
IDS---saying key words at the end of a phrase---helps highlight these
words. For example, saying "Can you see the doggie?" highlights "doggie"
more than "The doggie is eating a bone," so "doggie" in the last
position is more likely to be remembered (Liang, 2016). Saying words in
isolation also helps, so a substantial proportion of the infant's first
30 to 50 words have typically been spoken in isolation by the mother.
Examples are "bye bye," and "mommy" (Brendt & Siskind, 2001). When
parents are talking to children their goals may include "making a social
connection," "being affectionate," and "making a connection between
words and things." But whatever the goals, talking is always an
opportunity for teaching. These teaching opportunities vary a lot from
child to child, with the average child hearing 20,000 to 38,000 words a
day, but the range being from 2,000 to 50,000 words a day (Hart &
Risley, 1995; Shneidman et al., 2013; Weisleder & Fernald, 2013). This
range of words experienced by different children is important because
there is a correlation between the number of words heard early in
development, and later outcomes such as size of the vocabulary, learning
to read, and achievement in school (Montag et al., 2018; Rowe, 2012). So
talking to infants is good, and it's even better if they are paying
attention, which is enhanced by IDS. Finally, let's return to our
earlier discussion of cochlear implants (CI) in children, in which we
noted that it is important to install CIs early, so the infant can begin
learning how to perceive speech and understand language. The importance
of

2,000

/a/ /u/ 1,000 Adult-directed

Infant-directed

300

700

1,100

F1 (Hz)

Figure 14.23 A "vowel triangle" in which the frequencies of the first
formant (F1) and second formant (F2) are plotted for three vowel sounds,
/i/, /a/, and /u/. The blue record is for adult-directed speech. The red
record is for infant-directed speech. (From Golinkoff et al., 2015)

7.  Describe how cochlear implants work, and why the sounds they create
    are not the same as what normalhearing people hear.

8.  Why is it important to install CIs in deaf children at an early age?

9.  What is infant-directed speech? How is it different from
    adult-directed speech?

10. Describe how the characteristics of infant-directed speech help
    infants to learn to perceive speech.

11. Describe the Mesgarani electrode recording experiment. What did it
    demonstrate about neural responding to phonemes and to phonetic
    features?

354

Chapter 14  Perceiving Speech

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

THINK ABOUT IT 1. How well can computers recognize speech? You can
research this question by talking to Siri, Alexa, or some other
voice-recognition system, but instead of going out of your way to talk
slowly and clearly, talk in a normal conversational voice or a little
faster (but clearly enough that a human would still understand you), and
see whether

you can determine the limits of the computer's ability to understand
speech. (p. 335) 2. How do you think your perception of speech would be
affected if the phenomenon of categorical perception did not exist?
(p. 340)

KEY TERMS Acoustic signal (p. 336) Acoustic stimulus (p. 336)
Adult-directed speech (p. 353) Aphasia (p. 349) Articulator (p. 336)
Audiovisual speech perception (p. 344) Automatic speech recognition
(ASR) (p. 335) Broca's aphasia (p. 349) Broca's area (p. 349)
Categorical perception (p. 340) Coarticulation (p. 339) Cochlear implant
(p. 351)

Dual-stream model of speech perception (p. 350) Formant (p. 336) Formant
transitions (p. 337) Infant-directed speech (p. 353) Manner of
articulation (p. 337) McGurk effect (p. 343) Motor theory of speech
perception (p. 340) Multimodal (p. 343) Noise-vocoded speech (p. 347)
Phoneme (p. 338) Phonemic restoration effect (p. 344) Phonetic boundary
(p. 341) Phonetic feature (p. 351)

Place of articulation (p. 337) Sensorimotor hearing loss (p. 351)
Shadowing (p. 345) Sound spectrogram (p. 336) Speech segmentation
(p. 345) Speech spectrograph (p. 340) Statistical learning (p. 346)
Transitional probabilities (p. 346) Variability problem (p. 338) Voice
cells (p. 350) Voice onset time (VOT) (p. 340) Wernicke's aphasia
(p. 349) Wernicke's area (p. 349) Word deafness (p. 350)

Key Terms

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

355

This frog is stimulating receptors on the tip of the finger, which
create perceptions of touch, pressure, and temperature. Different
perceptions are created when the finger strokes the frog's skin. This
chapter describes the perceptions associated with stimulation of the
skin. Tim Wright/Documentary Value/Corbis

Learning Objectives After studying this chapter, you will be able to ...
■■ Describe the functions of the cutaneous senses. ■■ Describe the basic
anatomy and functioning of the parts of the

cutaneous system, ranging from skin to cortex. ■■ Describe the role of
tactile exploration in perceiving details, vi-

brations, texture, and objects. ■■ Understand how receptors in the skin,
brain connectivity and

knowledge a person brings to a situation are involved in social touch.

■■ Describe the different kinds of pain and the gate-control theory

of pain. ■■ Describe how top-down processes affect pain. ■■ Understand
the connection between the brain and pain. ■■ Describe how pain can be
affected by social touch and social

situations. ■■ Understand the connection between pain and brain
plasticity.

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

C hapter 1 5

The Cutaneous Senses

Chapter Contents Perception by the Skin and Hands

DEMONSTRATION: Perceiving Texture

With a Pen

Attention Emotions

15.1 Overview of the Cutaneous System

TEST YOURSELF 15.1

TEST YOURSELF 15.2

15.4 Perceiving Objects

15.8 The Brain and Pain

The Skin Mechanoreceptors Pathways From Skin to Cortex and Within the
Cortex Somatosensory Areas in the Cortex

DEMONSTRATION: Identifying Objects

Brain Areas Chemicals and the Brain

15.2 Perceiving Details

15.5 Social Touch

METHOD: Measuring Tactile Acuity

Sensing Social Touch The Social Touch Hypothesis Social Touch and the
Brain Top-Down Influences on Social Touch

Receptor Mechanisms for Tactile Acuity DEMONSTRATION: Comparing

Two-Point Thresholds

Cortical Mechanisms for Tactile Acuity

15.3 Perceiving Vibration and Texture Vibration of the Skin Surface
Texture

Identifying Objects by Haptic Exploration The Cortical Physiology of
Tactile Object Perception

Pain Perception 15.6 The Gate Control Model of Pain

Pain Reduction by Social Touch The Effect of Observing Someone Else's
Pain The "Pain" of Social Rejection SOMETHING TO CONSIDER: Plasticity

and the Brain

DEVELOPMENTAL DIMENSION: Social

Touch in Infants

TEST YOURSELF 15.3 THINK ABOUT IT

15.7 Top-Down Processes Expectation

Some Questions We Will Consider: ■■ Are there specialized receptors in
the skin for sensing dif-

ferent tactile qualities? (p. 358) ■■ What is the most sensitive part of
the body? (pp. 362, 364) ■■ Is it possible to reduce pain with your
thoughts? (p. 375) ■■ What is the evidence that holding hands can reduce
pain?

(p. 379)

W

15.9 Social Aspects of Pain

hen asked which sense they would choose to lose, if they had to lose
either vision, hearing, or touch, some people pick touch. This is
understandable given the high value we place on seeing and hearing, but
making a decision to lose the sense of touch would be a serious mistake.
Although people who are blind or deaf can get along

quite well, people with a rare condition that results in losing the
ability to feel sensations though the skin often suffer constant
bruises, burns, and broken bones in the absence of the warnings provided
by touch and pain (Melzack & Wall, 1988; Rollman, 1991; Wall & Melzack,
1994). But losing the sense of touch does more than increase the chance
of injury. It also makes it difficult to interact with the environment
because of the loss of feedback from the skin that accompanies many
actions. As I type this, I hit my computer keys with just the right
amount of force, because I can feel pressure when my fingers hit the
keys. Without this feedback, typing and other actions that receive
feedback from touch would become much more difficult. Experiments in
which participants have had their hands temporarily anesthetized have
shown that the resulting loss of feeling causes them to apply much more
force than necessary when carrying

357

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

out tasks with their fingers and hands (Avenanti et al., 2005; Monzée et
al., 2003). One of the most extreme examples of the effect of losing the
ability to sense with the skin is the case of Ian Waterman, which we
described in Chapter 7 (p. 162). As a result of an autoimmune reaction
that destroyed most of the neurons that transmitted signals from his
skin, joints, tendons, and muscles to his brain, he lost the ability to
feel skin sensations so he couldn't feel his body when lying in bed, and
he often used inappropriate force when grasping objects---sometimes
gripping too tightly, and sometimes dropping objects because he hadn't
gripped tightly enough. To make things even worse, destruction of the
nerves from his muscles, tendons, and joints eliminated Ian's ability to
sense the position of his arms, legs, and body, so the only way he could
carry out movements by visually monitoring the positions of his limbs
and body. Ian's problems were caused by a breakdown of his somatosensory
system, which includes (1) the cutaneous senses,

which are responsible for perceptions such as touch and pain that are
usually caused by stimulation of the skin; (2) proprioception, the
ability to sense the position of the body and limbs; and (3)
kinesthesis, the ability to sense the movement of the body and limbs. In
this chapter we will focus on the cutaneous senses, which are important
not only for activities like grasping objects and protecting against
damage to the skin, but also for motivating sexual activity (another
reason picking touch as the sense to lose would be a mistake). Not only
are the perceptions we experience through our skin crucial for carrying
out everyday activities and protecting ourselves from injury, but they
can, under the right conditions, create good feelings! These good
feelings come under the heading of social touch, which we will see can
have beneficial effects beyond being pleasant. Considering all of the
functions of the skin senses, we could make a good case for the idea
that perceptions felt through the skin are as important both for
day-to-day functioning and survival as are seeing and hearing.

Perception by the Skin and Hands The cutaneous senses refer to
everything we feel through the skin. Although touch and pain are the
most obvious feelings involving the skin, there are many others,
including pressure, vibration, tickle, temperature, and pleasure. We
begin our discussion of the cutaneous senses by first describing the
anatomy of the cutaneous system and then focusing on the sense of touch,
which enables us to perceive properties of surfaces and objects such as
details, vibrations, texture, and shape. In the second half of the
chapter we will focus on the perception of pain.

15.1 Overview of the Cutaneous System In this section we will describe
some basic facts about the anatomy and functioning of the various parts
of the cutaneous system.

The Skin M. Comèl (1953) called the skin the "monumental facade of the
human body" for good reason. It is the heaviest organ in the human body,
and, if not the largest (the surface areas of the gastrointestinal tract
and of the alveoli of the lungs exceed the surface area of the skin), it
is certainly the most obvious, especially in humans, whose skin is not
obscured by fur or large amounts of hair (Montagna & Parakkal, 1974). In
addition to its warning function, the skin also prevents body fluids
from escaping and at the same time protects us by keeping bacteria,
chemical agents, and dirt from penetrating our bodies. Skin maintains
the integrity of what's inside and protects 358

us from what's outside, but it also provides us with information about
the various stimuli that contact it. The sun's rays heat our skin, and
we feel warmth; a pinprick is painful; and when someone touches us, we
experience pressure or other sensations. On the surface of the skin is a
layer of tough dead skin cells. (Try sticking a piece of cellophane tape
onto your palm and pulling it off. The material that sticks to the tape
is dead skin cells.) This layer of dead cells is part of the outer layer
of skin, which is called the epidermis. Below the epidermis is another
layer, called the dermis (Figure 15.1). Within the skin are
mechanoreceptors, receptors that respond to mechanical stimulation such
as pressure, stretching, and vibration.

Mechanoreceptors Many of the tactile perceptions that we feel from
stimulation of the skin can be traced to mechanoreceptors that are
located in the epidermis and the dermis. Two mechanoreceptors, the
Merkel receptor and the Meissner corpuscle, are located close to the
surface of the skin, near the epidermis. Because they are located close
to the surface, these receptors have small receptive fields. (Just as a
visual receptive field is the area of retina which, when stimulated,
causes firing of a neuron (p. 55), a cutaneous receptive field is the
area of skin which, when stimulated, influences the firing of the
neuron.) Figure 15.1 shows the structure and firing of the Merkel and
Meissner receptors in response to a pressure stimulus that is presented
and then removed (blue line). Because the nerve fiber associated with
the slowly adapting Merkel receptor fires continuously, as long as the
stimulus is on, it is called a slowly adapting (SA1) fiber. Because the
nerve fiber associated with the rapidly adapting Meissner corpuscle
fires only when the stimulus is first applied and when it is removed, it
is called a

Chapter 15  The Cutaneous Senses

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Merkel receptors (SA1)

Meissner corpuscle (RA1) Small receptive fields Epidermis

Fires to continuous pressure

Fires to "on" and "off"

Perception

Perception

Fine details and texture

Dermis

Shape

Handgrip control Motion across skin

Figure 15.1 A cross section of glabrous (without hairs or projections)
skin, showing the layers of the skin and the structure, firing
properties, and perceptions associated with the Merkel receptor (SA1)
and Meissner corpuscle (RA1)---two mechanoreceptors near the surface of
the skin.

rapidly adapting (RA1) fiber. The types of perception associated with
the Merkel receptor/SA1 fiber are details, shape, and texture, and with
the Meissner corpuscle/RA1 fiber, controlling handgrip and perceiving
motion across the skin. Like the Merkel receptors, the Ruffini cylinder
is a slowly adapting (SA2) fiber, which responds continuously to
stimulation. Like the Meissner corpuscle, the Pacinian corpuscle is a
rapidly adapting fiber (RA2 or PC) which responds when the stimulus is
applied or removed. Both the Ruffini cylinder and Pacinian corpuscle are
located deep in the skin (Figure 15.2), so they have larger receptive
fields. The Ruffini cylinder is associated with perceiving stretching of
the skin, the Pacinian corpuscle with sensing rapid vibrations and fine
texture.1 Our description has associated each receptor/fiber type with
specific types of stimulation. However, when we consider how neurons
fire when fingers move across natural textures, we will see that the
perception of texture often involves the coordinated activity of
different types of neurons working together.

Pathways From Skin to Cortex and Within the Cortex The receptors for the
other senses are localized in one area--- the eye (vision), the ear
(hearing), the nose (olfaction), and the 1

Although Michael Paré and coworkers (2002) have reported that there are
no Ruffini receptors in the finger pads of monkeys, Ruffini cylinders
are still included in most lists of glabrous (nonhairy) skin receptors,
so they are included here.

mouth (taste)---but cutaneous receptors in the skin are distributed over
the whole body. This wide distribution, plus the fact that signals must
reach the brain before stimulation of the skin can be perceived, creates
a travel situation we might call "journey of the long-distance nerve
impulses," especially for signals that must travel from the fingertips
or toes to the brain. Signals from all over the body are conducted from
the skin to the spinal cord, which consists of 31 segments, each of
which receives signals through a bundle of fibers called the dorsal root
(Figure 15.3). After the signals enter the spinal cord, nerve fibers
transmit them to the brain along two major pathways: the medial
lemniscal pathway and the spinothalamic pathway. The lemniscal pathway
has large fibers that carry signals related to sensing the positions of
the limbs (proprioception) and perceiving touch. These large fibers
transmit signals at high speed, which is important for and reacting to
touch. The spinothalamic pathway consists of smaller fibers that
transmit signals related to temperature and pain. The case of Ian
Waterman illustrates this separation in function, because although he
lost the ability to feel touch and to sense the positions of his limbs
(lemniscal pathway), he was still able to sense pain and temperature
(spinothalamic pathway). Fibers from both pathways cross over to the
other side of the body during their upward journey and synapse in the
thalamus. (Remember that fibers from the retina and the cochlea also
synapse in the thalamus, in the lateral geniculate nucleus for vision
and the medial geniculate nucleus for hearing. Most of the fibers in the
cutaneous system synapse 15.1 Overview of the Cutaneous System

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

359

Ruffini cylinder (SA2)

Pacinian corpuscle (RA2 or PC)

Large receptive fields

Fires to "on" and "off"

Fires to continuous pressure

Perception

Perception

Vibration

Stretching

Fine texture by moving fingers

Figure 15.2 A cross section of glabrous skin, showing the structure,
firing properties, and perceptions associated with the Ruffini cylinder
(SA2) and the Pacinian corpuscle (RA2 or PC)---two mechanoreceptors that
are deeper in the skin.

in the ventrolateral nucleus of the thalamus.) Because the signals in
the spinal cord have crossed over to the opposite side of the body,
signals originating from the left side of the body reach the thalamus in
the right hemisphere of the brain, and signals from the right side of
the body reach the left hemisphere. Figure 15.3 The pathway from
receptors in the skin to the somatosensory receiving area of the cortex.
The fiber carrying signals from a receptor in the finger enters the
spinal cord through the dorsal root. The signals then travel up the
spinal cord along two pathways: the medial lemniscus and the
spinothalamic tract. These pathways synapse in the thalamus and then
send signals to the somatosensory cortex in the parietal lobe.

The idea of two pathways conducting cutaneous signals to the thalamus
and then to the somatosensory cortex supports the idea that different
pathways serve different sensations. But it is important to realize that
the cutaneous pathways and structures within the brain that are far more
complex than the picture in Figure 15.3. This complexity is illustrated
in

Somatosensory cortex

Somatosensory cortex

Thalamus Medial lemniscus Spinothalamic tract

Touch

360

Dorsal root

Spinal cord

Chapter 15  The Cutaneous Senses

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Figure 15.4, which shows multiple brain areas that are associated with
cutaneous functions (Bushnell et al., 2013).

and S2, that we will be encountering later in the chapter are the
insula, which is important for sensing light touch, and the anterior
cingulate cortex (ACC), which is involved in pain. An important
characteristic of the somatosensory cortex is that it is organized into
maps that correspond to locations on the body. The story behind the
discovery of these maps is an interesting one that begins in the 1860s,
when British neurologist Hughlings Jackson observed that in some cases
of epilepsy, his patients' seizures progressed over the body in an
orderly way, with a seizure in one body part being followed by a seizure
in a neighboring body part, and so on (Jackson, 1870). This sequence,
which came to be known as "the Jacksonian march,"

Somatosensory Areas in the Cortex Two of the areas that receive signals
from the thalamus are the primary somatosensory cortex (S1) in the
parietal lobe and the secondary somatosensory cortex (S2) (Rowe et al.,
1996; Turman et al., 1998). Signals also travel between S1 and S2 and to
a network of other areas in the brain in additional pathways that are
not shown here (Avanzini et al., 2016; Rullman et al., 2019). Two of the
structures in Figure 15.4, in addition to S1

S1

ACC S2 Insula

PFC BG

Thalamus

AMY Cerebellum PAG PB

Figure 15.4 Some of the brain structures associated with the cutaneous
system. We will be considering the structures that are labeled: primary
somatosensory cortex (S1), secondary somatosensory cortex (S2), anterior
cingulate cortex (ACC), and the insula. (Bushnell et al., 2013) 15.1
Overview of the Cutaneous System

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

361

suggested that the seizures reflected the spread of neural activity
across maps in the motor area of the brain (Berkowitz, 2018;
Harding-Forrester & Feldman, 2018). Sixty-seven years later, Wilder
Penfield and Edwin Boldrey (1937) measured the map of the somatosensory
cortex by stimulating points on the brain of awake patients who were
having brain surgery to relieve symptoms of epilepsy (Penfield &
Rasmussen, 1950). Note that there are no pain receptors in the brain, so
the patients cannot feel the surgery. When Penfield stimulated points on
the primary somatosensory cortex (S1) and asked patients to report what
they perceived, they reported sensations such as tingling and touch on
various parts of their body. Penfield found that stimulating the ventral
part of S1 (lower on the parietal lobe) caused sensations on the lips
and face, stimulating higher on S1 caused sensations in the hands and
fingers, and stimulating the dorsal S1 caused sensations in the legs and
feet. The resulting body map, shown in Figure 15.5, is called the
homunculus, Latin for "little man." The homunculus shows that adjacent
areas of the skin project to adjacent areas in the brain, and that some
areas on the skin are represented by a disproportionately large area of
the brain. The area devoted to the thumb, for example, is as large as
the area devoted to the entire forearm. This result is analogous to the
cortical magnification factor in vision (see page 75), in which
receptors in the fovea, which are responsible for perceiving visual
details, are allotted a disproportionate area on the visual cortex.
Similarly, parts of the body such as the fingers, which are used to
detect details through the sense of touch, are allotted

Figure 15.5 (a) The somatosensory cortex in the parietal lobe. The
primary somatosensory area, S1 (light purple), receives inputs from the
ventrolateral nucleus of the thalamus. The secondary somatosensory area,
S2 (dark purple), is partially hidden behind the temporal lobe. (b) The
sensory homunculus on the somatosensory cortex. Parts of the body with
the highest tactile acuity are represented by larger areas on the
cortex. (Adapted from Penfield & Rasmussen, 1950)

a disproportionate area on the somatosensory cortex (Duncan & Boynton,
2007). A similar body map also occurs in the secondary somatosensory
cortex (S2). This description in terms of S1 and S2 and the homunculus
is accurate but simplified. Recent research has shown that S1 is divided
into four interconnected areas, each with its own body map and its own
functions (Keysers et al., 2010). For example, the area in S1 involved
in perceiving touch is connected to another area that is involved in
haptics (exploring objects with the hand). Finally, there are other
areas that we will discuss when we consider pain later in the chapter.
The fact that the cutaneous system involves numerous areas of the brain
which communicate with each other over many pathways isn't surprising
when we consider the many different qualities that are sensed by the
skin. Now that we've described the cutaneous receptors and some of the
brain areas that are activated by signals arriving from the receptors,
we will consider how we perceive qualities such as details, vibration,
and texture.

15.2 Perceiving Details One of the most impressive examples of
perceiving details with the skin is provided by Braille, the system of
raised dots that enables blind people to read with their fingertips. A
Braille character consists of a cell made up of one to six dots.
Different arrangements of dots and blank spaces represent letters of the
alphabet, as shown in Figure 15.6; additional characters

Homunculus on S1

Hip

Trunk Neck Head Shoulder Arm Fo Elbow rea W r Ha rist m nd Li Ri ttle n
Mi g dd le

Dorsal

Leg

Foot Toes

Genitalia

S1

Dorsal

x de In mb u e Th Ey e s No ce Fa p r li pe s Up Lip ip er l Low

Teeth, gums, and jaw

S2

In

Tong

tra

Ventral

-a

bd

Ph

om

ue

ar

in a

(a) 

362

(b) 

yn

x

l

Ventral

Chapter 15  The Cutaneous Senses

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

a

b

c

d

e

f

g

h

i

j

k

l

m

n

o

p

q

r

s

t

u

v

w

x

y

z

represent numbers, punctuation marks, and common speech sounds and
words. Experienced Braille readers can read at a rate of about 100 words
per minute, slower than the rate for visual reading, which averages
about 250 to 300 words per minute, but impressive nonetheless when we
consider that a Braille reader transforms an array of raised dots into
information that goes far beyond simply feeling sensations on the skin.
The ability of Braille readers to identify patterns of small raised dots
based on the sense of touch depends on tactile detail perception. The
first step in describing research on tactile detail perception is to
consider how researchers have measured tactile acuity---the capacity to
detect details of stimuli presented to the skin.

METHOD

Figure 15.6 The Braille alphabet consists of raised dots in a 2 × 3
matrix. The large blue dots indicate the location of the raised dot for
each letter. Blind people read these dots by scanning them with their
fingertips.

(a) One point or two?

(b) Grating vertical or horizontal?

Figure 15.7 Methods for determining tactile acuity: (a) two-point
threshold; (b) grating acuity.

Measuring Tactile Acuity

Just as there are a number of different kinds of eye charts for
determining a person's visual acuity, there are a number of ways to
measure a person's tactile acuity. The classic method of measuring
tactile acuity is the two-point threshold, the minimum separation
between two points on the skin that when stimulated is perceived as two
points (Figure 15.7a). The twopoint threshold is measured by gently
touching the skin with two points, such as the points of a drawing
compass, and having the person indicate whether he or she feels one
point or two. The two-point threshold was the main measure of acuity in
most of the early research on touch. Recently, however, other methods
have been introduced. Grating acuity is measured by pressing a grooved
stimulus like the one in Figure 15.7b onto the skin and asking the
person to indicate the orientation of the grating. Acuity is measured by
determining the narrowest spacing for which orientation can be
accurately judged. Finally, acuity can also be measured by pushing
raised patterns such as letters onto the skin and determining the
smallest sized pattern or letter that can be identified (Cholewaik &
Collins, 2003; Craig & Lyle, 2001, 2002).

As we consider the role of both receptor mechanisms and cortical
mechanisms in determining tactile acuity, we will see that there are a
number of parallels between the cutaneous system and the visual system.

Receptor Mechanisms for Tactile Acuity The properties of the receptors
are one of the things that determine what we experience when the skin is
stimulated. We will illustrate this by first focusing on the connection
between the Merkel receptor and associated fibers and tactile acuity.
Figure 15.8a shows how the fiber associated with a Merkel receptor fires
in response to a grooved stimulus pushed into the skin. Notice that the
firing of the fiber reflects the pattern of the grooved stimuli. This
indicates that the firing of the Merkel receptor's fiber signals details
(Johnson, 2002; Phillips & Johnson, 1981). For comparison, Figure 15.8b
shows the firing of the fiber associated with the Pacinian corpuscle.
The lack of match between the grooved pattern and the firing indicates
that this receptor is not sensitive to the details of patterns that are
pushed onto the skin. 15.2 Perceiving Details

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

363

Impulses for 1-second presentation

Impulses for 1-second presentation

100

50

0 0.5 mm bar width

20

10

0

5.0 mm

(a) Merkel/SA1

0.5 mm bar width

5.0 mm

(b) Pacinian/RA2

Figure 15.8 Firing to the grooved stimulus pattern of (a) the fiber
associated with a Merkel receptor and (b) the fiber associated with a
Pacinian corpuscle receptor. The response to each groove width was
recorded during a 1-second indentation for each bar width, so these
graphs represent the results for a number of presentations. (Adapted
from Phillips & Johnson, 1981)

It is not surprising that there is a high density of Merkel receptors in
the fingertips, because the fingertips are the parts of the body that
are most sensitive to details (Vallbo & Johansson, 1978). The
relationship between locations on the body and sensitivity to detail has
been studied psychophysically by measuring the two-point threshold on
different parts of the body. Try this yourself by doing the following
demonstration.

Comparing Two-Point Thresholds

To measure two-point thresholds on different parts of the body, hold two
pencils side by side (or better yet, use a drawing compass) so that
their points are about 12 mm (0.5 in.) apart; then touch both points
simultaneously to the tip of your thumb and determine whether you feel
two points. If you feel only one, increase the distance between the
pencil points until you feel two; then note the distance between the
points. Now move the pencil points to the underside of your forearm.
With the points about 12 mm apart (or at the smallest separation you
felt as two points on your thumb), touch them to your forearm and note
whether you feel one point or two. If you feel only one, how much must
you increase the separation before you feel two?

A comparison of grating acuity on different parts of the hand shows that
better acuity is associated with less spacing between Merkel receptors
(Figure 15.9). But receptor spacing isn't the whole story, because the
cortex also plays a role in determining tactile acuity (Duncan &
Boynton, 2007).

Cortical Mechanisms for Tactile Acuity Just as there is a parallel
between tactile acuity and receptor density, there is also a parallel
between tactile acuity and the representation of the body in the brain.
Table 15.1 indicates the two-point threshold measured on different parts
of the male body. By comparing these two-point thresholds to how
different parts of the body are represented in the brain (Figure 15.5a),
we can see that regions of high acuity, like the 364

Palm 8.0 Tactile acuity (mm)

DEMONSTRATION

10.0

6.0 Base of finger

4.0 2.0 0

Fingertip 1.0

2.0

3.0

4.0

SA1 Receptor spacing (mm)

Figure 15.9 Correlation between density of Merkel receptors and tactile
acuity. (From Craig & Lyle, 2002)

fingers and lips, are represented by larger areas on the cortex. As we
mentioned earlier, when we described the homunculus, "magnification" of
the representation on the brain of parts of the body such as the
fingertips parallels the magnification Table 15.1 Two-Point Thresholds
on Different Parts of

the Male Body

PART OF BODY

THRESHOLD (mm)

Fingers

4

Upper lip

8

Big toe

9

Upper arm

46

Back

42

Thigh

44

Data from Weinstein, 1968.

Chapter 15  The Cutaneous Senses

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

factor in vision (p. 75). The map of the body on the brain is enlarged
to provide the extra neural processing that enables us to accurately
sense fine details with our fingers and other parts of the body. Another
way to demonstrate the connection between cortical mechanisms and acuity
is to determine the receptive fields of neurons in different parts of
the cortical homunculus. Figure 15.10, which shows the sizes of
receptive fields from cortical neurons that receive signals from a
monkey's fingers (Figure 15.10a), hand (Figure 15.10b), and arm (Figure
15.10c), indicates that cortical neurons representing parts of the body
with better acuity, such as the fingers, have smaller receptive fields.
This means that two points that are close together on the fingers might
fall on receptive fields that don't overlap (as indicated by the two
arrows in Figure 15.10a) and so would cause neurons that are separated
in the cortex to fire (Figure 15.10d). However, two points with the same
separation when applied to the arm are likely to fall on receptive
fields that overlap (see arrows in Figure 15.10c) and so could cause
neurons that are not separated in the cortex to fire (Figure 15.10d).
Thus, the small receptive fields of neurons receiving signals from the
fingers translates into more separation on the cortex, which enhances
the ability to feel two closetogether points on the skin as two separate
points.

15.3 Perceiving Vibration and Texture The skin is capable of detecting
not only spatial details of objects, but other qualities as well. When
you place your hands on mechanical devices that produce vibration, such
as a car, a lawnmower, or an electric toothbrush, you can sense these
vibrations with your fingers and hands.

Vibration of the Skin The mechanoreceptor that is primarily responsible
for sensing vibration is the Pacinian corpuscle. One piece of evidence
linking the Pacinian corpuscle to vibration is that recording from
fibers associated with the corpuscle shows that these fibers respond
poorly to slow or constant pushing but respond well to high rates of
vibration. Why do the Pacinian corpuscle fibers respond well to rapid
vibration? The answer to this question is that the presence of the
corpuscle surrounding the nerve fiber determines which pressure stimuli
actually reach the fiber. The corpuscle, which consists of a series of
layers, like an onion, with fluid between each layer, transmits rapidly
repeated pressure, like vibration, to the nerve fiber, as shown in
Figure 15.11a, but does not transmit continuous pressure, as shown in
Figure 15.11b. Thus, the corpuscle causes the fiber to receive rapid
changes in pressure, but not to receive continuous pressure. Because the
Pacinian corpuscle does not transmit continuous pressure to the fiber,
presenting continuous pressure to the corpuscle should cause no response
in the fiber. This is exactly what Werner Lowenstein (1960) observed in
a classic experiment, in which he showed that when pressure was applied
to the corpuscle (at A in Figure 15.11c), the fiber responded when the
pressure was first applied and when it was removed, but it did not
respond to continuous pressure. But when Lowenstein dissected away the
corpuscle and applied pressure directly to the fiber (at B in Figure
15.11c), the fiber fired to the continuous pressure. Lowenstein
concluded from this result that properties of the corpuscle cause the
fiber to respond poorly to continuous stimulation, such as sustained
pressure, but to respond well to changes in stimulation that occur at
the beginning and end of a pressure stimulus or when stimulation is
changing rapidly, as occurs in vibration. As we now consider the
perception of surface texture, we will see that vibration plays a role
in perceiving fine textures.

Activity from 2 points on arm

S1 cortex

rs

Fin

ge

Arm

Activity from 2 points on finger

(a) 
(b) 
(c) 
(d) 

Figure 15.10 Receptive fields of monkey cortical neurons that fire (a)
when the fingers are stimulated, (b) when the hand is stimulated, and
(c) when the arm is stimulated. (d) Stimulation of two nearby points on
the finger causes separated activation on the finger area of the cortex,
but stimulation of two nearby points on the arm causes overlapping
activation in the arm area of the cortex. (From Kandel & Jessell, 1991)
15.3 Perceiving Vibration and Texture

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

365

Rapid vibration Transmits rapid vibration to fiber

(a) Continuous pressure

Does not transmit continuous pressure to fiber

(b) Push at A

Push at B

A B

(c) 

Figure 15.11 (a) When a vibrating pressure stimulus is applied to the
Pacinian corpuscle, it transmits these pressure vibrations to the nerve
fiber. (b) When a continuous pressure stimulus is applied to the
Pacinian corpuscle, it does not transmit the continuous pressure to the
fiber. (c) Lowenstein determined how the fiber fired to stimulation of
the corpuscle (at A) and to direct stimulation of the fiber (at B).
(Adapted from Lowenstein, 1960)

Surface Texture Surface texture is the physical texture of a surface
created by peaks and valleys. As can be seen in Figure 15.12, visual
inspection can be a poor way of determining surface texture because
seeing texture depends on the light--dark pattern determined by the
angle of illumination. Thus, although the visually perceived texture of
the two sides of the post in Figure 15.12 looks

very different, moving the fingers across the two surfaces reveals that
their texture is the same. Research on texture perception tells an
interesting story, extending from 1925 to the present, that illustrates
how psychophysics can be used to understand perceptual mechanisms. In
1925, David Katz proposed what is now called the duplex theory of
texture perception, which states that our perception of texture depends
on both spatial cues and temporal cues (Hollins & Risner, 2000; Katz,
1925/1989). Spatial cues are provided by relatively large surface
elements, such as bumps and grooves, that can be felt both when the skin
moves across the surface elements and when it is pressed onto the
elements. These cues result in feeling different shapes, sizes, and
distributions of these surface elements. An example of spatial cues is
perceiving a coarse texture such as Braille dots or the texture you feel
when you touch the teeth of a comb. Temporal cues occur when the skin
moves across a textured surface like fine sandpaper. This type of cue
provides information in the form of vibrations that occur as a result of
the movement over the surface. Temporal cues are responsible for our
perception of fine texture that cannot be detected unless the fingers
are moving across the surface. Although Katz proposed that texture
perception is determined by both spatial and temporal cues, research on
texture perception has, until recently, focused on spatial cues.
However, Mark Hollins and Ryan Risner (2000) presented evidence for the
role of temporal cues by showing that when participants touched surfaces
without moving their fingers and judge "roughness" using the procedure
of magnitude estimation (see Chapter 1, page 16; Appendix B, page 418),
they sensed little difference between two fine textures (particle sizes
of 10 μm and 100 μm). However, when participants were allowed to move
their fingers across the surface, they could detect the difference
between the fine textures. Thus, movement, which generates vibration as
the skin scans a surface, makes it possible to sense the roughness of
fine surfaces. These results and the results of other behavioral
experiments (Hollins et al., 2002) support the duplex theory of

Courtesy of Bruce Goldstein

Courtesy of Bruce Goldstein

Figure 15.12 The post in (a) is illuminated from the left. The close-up
in (b) shows how the visual perception of texture is influenced by
illumination. Although the surface on the right side of the pole appears
rougher than on the left, the surface textures of the two sides are
identical.

(a) 

366

(b) 

Chapter 15  The Cutaneous Senses

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

perception---that the perception of coarse textures is determined by
spatial cues and of fine textures by temporal (vibration) cues (also see
Weber et al., 2013). Additional evidence for the role of temporal cues
in perceiving texture has been provided by research that shows that
vibrations are important for perceiving textures not only when people
explore a surface directly with their fingers, but also when they make
contact with a surface indirectly, through the use of tools. You can
experience this yourself by doing the following demonstration.
DEMONSTRATION

Perceiving Texture With a Pen

Turn your pen over (or cap it) so you can use it as a "probe" (without
writing on things). Hold the pen at one end and move the other end over
something smooth, such as your desk or a piece of paper. As you do this,
notice that you can sense the smoothness of the page, even though you
are not directly touching it. Then, try the same thing on a rougher
surface, such as a rug, fabric, or concrete.

Your ability to detect differences in texture by running a pen (or some
other "tool," such as a stick) over a surface is determined by
vibrations transmitted through the tool to your

Cortical Responses to Surface Texture Justin Lieber and Sliman Bensmaia
(2019) studied how textures are represented in the brain by training
monkeys to place their fingers on a rotating drum like the one in Figure
15.13a. Textures ranged from very fine (micorsuede) to coarse (dots
spaced 5 mm apart. Figure 15.13b shows a monkey having a texture scanned
across its fingertip. Figure 15.13c shows the responses of five neurons
in the somatosensory cortex to six different textures. These patterns
show that (1) different textures caused different firing patterns in an
individual neuron (compare records from left to right, across textures,
for a particular neuron), and (2) different neurons responded
differently to the same texture (compare records from top to bottom for
a specific texture). These results showed that texture is represented in
the cortex by the pattern of firing of many neurons. In addition, Lieber
and Bensmaia found that cortical neurons that fired best to coarse
textures received input from SA1 neurons in the skin (Merkel receptors)
and neurons that fired best to fine textures received input from PC
receptors (Pacinian corpuscles). (b)

Sliman Bensmaia

(a) 

skin (Klatzky et al., 2003). The most remarkable thing about perceiving
texture with a tool is that what you perceive is not the vibrations but
the texture of the surface, even though you are feeling the surface
remotely, with the tip of the tool (Carello & Turvey, 2004).

(c) 

Microsuede

Satin

Nylon

Chiffon

Denim

Dots (5mm)

1 2 3 4 5

250 ms Figure 15.13 (a) Experimental apparatus used by Lieber and
Bensmaia (2019), which, when rotated, scanned different textures across
fingertips. (b) A monkey having its finger scanned. (c) Firing patterns
of five different neurons (numbers on the left) to six textures. 15.3
Perceiving Vibration and Texture

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

367

TEST YOuRSELF 15.1 1. Describe the four types of mechanoreceptors in the
skin, indicating (a) their appearance, (b) where they are located, (c)
how they respond to pressure, (d) the sizes of their receptive fields,
(e) the type of perception associated with each receptor, and (f) the
type of fiber associated with each receptor. 2. Where is the cortical
receiving area for touch, and what does the map of the body on the
cortical receiving area look like? How can this map be changed by
experience? 3. How is tactile acuity measured, and what are the receptor
and cortical mechanisms that serve tactile acuity?

ribs reflected backward. "It's a Harpa," I replied tentatively. "It must
be Harpa major." Right so far. "How about this one?" inquired Boell, as
another fine shell changed hands. Smooth, sleek, channeled suture,
narrow opening; could be any olive. "It's an olive. I'm pretty sure it's
Oliva sayana, the common one from Florida, but they all look alike."
Both men were momentarily speechless. They had planned this little
exercise all along to call my bluff. Now that I had passed, Boell had
undergone an instant metamorphosis. Beaming with enthusiasm and warmth,
he promised me his full support. (pp. 79--80)

5.  What is the duplex theory of texture perception? Describe the series
    of behavioral experiments that led to the conclusion that vibration
    is responsible for perceiving fine textures and observations that
    have been made about the experience of exploring an object with a
    probe.

Vermeij received his PhD from Yale and is now a worldrenowned expert on
marine mollusks. His ability to identify objects and their features by
touch is an example of active touch---touch in which a person actively
explores an object, usually with fingers and hands. In contrast, passive
touch occurs when touch stimuli are applied to the skin, as when two
points are pushed onto the skin to determine the twopoint threshold. The
following demonstration compares the ability to identify objects using
active touch and passive touch.

6.  Describe the experiment which showed how monkey cortical neurons
    respond to texture. What do the results indicate about how texture
    is represented in the cortex?

DEMONSTRATION

4.  Which receptor is primarily responsible for the perception of
    vibration? Describe the experiment that showed that the presence of
    the receptor structure determines how the fiber fires.

15.4 Perceiving Objects Imagine that you and a friend are at the
seashore. Your friend knows something about shells from the small
collection he has accumulated over the years, so as an experiment you
decide to determine how well he can identify different types of shells
by using his sense of touch alone. When you blindfold your friend and
hand him a snail shell and a crab shell, he has no trouble identifying
the shells as a snail and a crab. But when you hand him shells of
different types of snails that are very similar, he finds that
identifying the different types of snails is much more difficult. Geerat
Vermeij, blind at the age of 4 from a childhood eye disease and
currently Distinguished Professor of Marine Ecology and Paleoecology at
the University of California at Davis, describes his experience when
confronted with a similar task. This experience occurred when he was
being interviewed by Edgar Boell, who was considering Vermeij's
application for graduate study in the biology department at Yale.
Wondering whether Geerat's blindness would disqualify him from graduate
study, Boell took Vermeij to the museum, introduced him to the curator,
and handed him a shell. Here is what happened next, as told by Vermeij
(1997): "Here's something. Do you know what it is?" Boell asked as he
handed me a specimen. My fingers and mind raced. Widely separated ribs
parallel to outer lip; large aperture; low spire; glossy; 368

Identifying Objects

Ask another person to select five or six small objects for you to
identify. Close your eyes and have the person place an object in your
hand. In the active condition your job is to identify the object by
touch alone, by moving your fingers and hand over the object. As you do
this, be aware of what you are experiencing: your finger and hand
movements, the sensations you are feeling, and what you are thinking. Do
this for three objects. Then, in the passive condition, hold out your
hand, keeping it still, with fingers outstretched, and let the person
move each of the remaining objects around on your hand, moving their
surfaces and contours across your skin. Your task is the same as before:
to identify the object and to pay attention to what you are experiencing
as the object is moved across your hand.

You may have noticed that in the active condition, in which you moved
your fingers across the object, you were much more involved in the
process than in the passive condition, and you had more control over
what parts of the objects to which you were exposed. The active part of
the demonstration involved haptic perception---perception in which
three-dimensional objects are explored with the fingers and hand.

Identifying Objects by Haptic Exploration Haptic perception is an
example of a situation in which a number of different systems are
interacting with each other. As you manipulated the objects in the first
part of the demonstration above, you were using three distinct systems
to arrive at your goal of identifying the objects: (1) the sensory
system, which was involved in detecting cutaneous sensations such as
touch,

Chapter 15  The Cutaneous Senses

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

temperature, and texture and the movements and positions of your fingers
and hands; (2) the motor system, which was involved in moving your
fingers and hands; and (3) the cognitive system, which was involved in
thinking about the information provided by the sensory and motor
systems. Haptic perception is an extremely complex process because the
sensory, motor, and cognitive systems must all work together. For
example, the motor system's control of finger and hand movements is
guided by cutaneous feelings in the fingers and the hands, by your sense
of the positions of the fingers and hands, and by thought processes that
determine what information is needed about the object in order to
identify it. These processes working together create an experience of
active touch that is quite different from the experience of passive
touch. J. J. Gibson (1962), who championed the importance of movement in
perception (see Chapter 7, page 150, and Chapter 8, page 181), compared
the experience of active and passive touch by noting that we tend to
relate passive touch to the sensation experienced in the skin, whereas
we relate active touch to the object being touched. For example, if
someone pushes a pointed object into your skin, you might say, "I feel a
pricking sensation on my skin"; if, however, you push on the tip of the
pointed object yourself, you might say, "I feel a pointed object"
(Kruger, 1970). Thus, for passive touch you experience stimulation of
the skin, and for active touch you experience the objects you are
touching. Psychophysical research has shown that people can accurately
identify most common objects within 1 or 2 seconds using active touch
(Klatzky et al., 1985). When Susan Lederman and Roberta Klatzky (1987,
1990) observed participants' hand movements as they made these
identifications, they found that people use a number of distinctive
movements, which they called exploratory procedures (EPs), and that the
types of EPs used depend on the object qualities the participants are
asked to judge. Figure 15.14 shows four of the EPs observed by Lederman
and Klatzky. People tend to use just one or two EPs to determine a
particular quality. For example, people use mainly lateral motion and
contour following to judge texture, and they use enclosure and contour
following to judge exact shape.

Lateral motion

Enclosure

Pressure

Contour following

Figure 15.14 Some of the exploratory procedures (EPs) observed by
Lederman and Klatzky as participants identified objects. (From Lederman
& Klatzky, 1987)

In the cortex, we find some neurons with center-surround receptive
fields and others that respond to more specialized stimulation of the
skin. Figure 15.16 shows stimuli that cause neurons in the monkey's
somatosensory cortex to fire. There are neurons that respond to specific
orientations (Figure 15.16a) and neurons that respond to movement across
the skin in a specified direction (Figure 15.16b; Hyvärinen & Poranen,
1978; also see Bensmaia et al., 2008; Pei et al., 2011; Yau et al.,
2009). There are also neurons in the monkey's somatosensory cortex that
respond when the monkey grasps a specific object (Sakata & Iwamura,
1978). For example, Figure 15.17 shows the response of one of these
neurons. This neuron responds when the monkey grasps the ruler but does
not respond when the monkey grasps a cylinder or a sphere (see also
Iwamura, 1998).

The Cortical Physiology of Tactile Object Perception Inhibitory surround

Exploring objects with our fingers and hands activates mechanoreceptors
that send signals toward the cortex. When these signals reach the
cortex, they eventually activate specialized neurons.

Cortical Neurons Are Specialized Moving from mechanoreceptor fibers in
the fingers toward the brain, neurons become more specialized. This is
similar to what occurs in the visual system. Neurons in the ventral
posterior nucleus, which is the tactile area of the thalamus, have
center-surround receptive fields that are similar to the center-surround
receptive fields in the lateral geniculate nucleus, which is the visual
area of the thalamus (Mountcastle & Powell, 1959; Figure 15.15).

Excitatory center

Figure 15.15 An excitatory-center, inhibitory-surround receptive field
of a neuron in a monkey's thalamus. 15.4 Perceiving Objects

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

369

(a)

(b) 

Figure 15.16 Receptive fields of neurons in the monkey's somatosensory
cortex. (a) The records to the right of the hand show nerve firing to
stimulation of the hand with the orientations shown on the hand. This
neuron responds best when a horizontally oriented edge is presented to
the monkey's hand. (b) The records on the right indicate nerve firing
for movement of a stimulus across the fingertip from right to left (top)
and from left to right (bottom). This neuron responds best when a
stimulus moves across the fingertip from right to left. (From Hyvärinen
& Poranen, 1978)

Cortical Responding Is Affected by Attention Cortical neurons are
affected not only by the properties of an object but also by whether the
perceiver is paying attention. Steven Hsiao and coworkers (1993, 1996)
recorded the

50 25 --6

--4

--2

0

2

4

8

10

Firing rate

0 --8

response of neurons in areas S1 and S2 to raised letters that were
scanned across a monkey's finger. In the tactile-attention condition,
the monkey had to perform a task that required focusing its attention on
the letters being presented to its fingers. In the visual-attention
condition, the monkey had to focus its attention on an unrelated visual
stimulus. The results, shown in Figure 15.18, show that even though the
monkey is receiving exactly the same stimulation on its fingertips in
both conditions, the response is larger for the tactile-attention
condition. Thus, stimulation of the receptors may trigger a response,
but the size of the response can be affected by processes such as
attention, thinking, and other actions of the perceiver. If the idea
that events other than stimulation of the receptors can affect
perception sounds familiar, it is because similar situations occur in
vision (see pages 133, 164) and hearing (p. 344). A person's active
participation makes a difference in perception, not just by influencing
what stimuli stimulate the receptors but also by influencing the
processing that occurs

Firing rate

40

50 25 0 --8

20

Visualattention condition

10 --6

--4

--2 0 2 Time in seconds

4

8

10

Figure 15.17 The response of a neuron in a monkey's parietal cortex that
fires when the monkey grasps a ruler but that does not fire when the
monkey grasps a cylinder. The monkey grasps the objects at time = 0.
(From Sakata & Iwamura, 1978) 370

30

Tactileattention condition

0 Stimulus position

Figure 15.18 Firing rate of a neuron in area S1 of a monkey's cortex to
a letter being rolled across the fingertips. The neuron responds only
when the monkey is paying attention to the tactile stimulus. (From Hsiao
et al., 1993)

Chapter 15  The Cutaneous Senses

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Cultural anthropology Do different cultures, genders, social classes,
and age populations have different touch behaviors and different ways of
interpreting touch?

Figure 15.19 The disciplines concerned with research on social touch and
some of the questions being posed by each discipline. (Adapted from
Gallace & Spence, 2010)

Cognitive sciences How can different tactile sensations be classified?
What kinds of stimuli are perceived as pleasant and unpleasant? What are
the more perceptual aspects of touch relevant to communicative
functions?

Social psychology How can touch influence a person's attitude toward
other people and his/her social behavior?

Interpersonal touch

Can touch communicate distinct emotions?

once the receptors are stimulated. Later, we will see that this is
clearly demonstrated for the experience of pain, which is strongly
affected by processes in addition to stimulation of the receptors.

15.5 Social Touch What happens when you are touched by another person?
One person touching another person, which is called interpersonal
touching or social touch, has recently become a "hot topic" for
researchers in a number of different areas (Gallace & Spence, 2010).
Figure 15.19 indicates the kinds of questions that are being asked about
social touch. We will focus on the questions in the "cognitive sciences"
and "neurosciences" boxes: "What kinds of stimuli are perceived as
pleasant and unpleasant?" and "Which receptors and brain areas are
responsible for the social aspects of touch?"

Neurosciences Which receptors and brain areas are responsible for the
social aspects of touch?

As recently as 2002, the function of CT afferents was not known.
However, Hakan Olausson and coworkers (2002) took a step toward solving
this mystery by studying patient G.L., a 54-year-old woman who
contracted a disease that destroyed all of her myelinated fibers, which
she reported had caused her to lose her sensation of touch. However,
careful testing revealed that she could detect light brush strokes
presented to the hairy part of her forearm, which contains CT afferents.
In addition, her sensations of light touch were accompanied by
activation of the insula, which we will see receives signals from CT
afferents. These results led Olausson to propose that CT afferents are
involved in "caress-like skin to skin contact between individuals"---
the type of stimulation that came to be called social touch.

The Social Touch Hypothesis Research that followed the study of G.L. led
to the social touch hypothesis, which is that CT afferents and their
central

Sensing Social Touch At the beginning of the chapter, we described four
types of receptors that occur in glabrous (non-hairy) skin. We now focus
on hairy skin, which contains nerve fibers called CT afferents, where CT
stands for C-tactile. These fibers are unmyelinated, which means that
they are not covered by the myelin sheath that covers the fibers
associated with the receptors in glabrous skin. Unmyelinated fibers
conduct nerve impulses much more slowly than myelinated fibers, a
property which we will see is reflected in the type of stimuli they
respond to. The activity of these slow-conducting CT fibers was first
recorded using a technique called microneurography, which involves
inserting a metal electrode with a very fine tip just under the skin
(Figure 15.20) (Vallbo & Hagbarth, 1968; Vallbo et al., 1993).

Recording electrode Reference electrode

Figure 15.20 Microneurography, which involves inserting mental
electrodes with very fine tips just under the skin, has been used to
record from cutaneous fibers. When the skin on the forearm is stroked,
the electrodes pick up signals as they are transmitted in nerve fibers
conducting signals toward the brain. 15.5 Social Touch

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

371

projections are responsible for social touch. This was recognized as a
new touch system that is different from the systems we described earlier
in the chapter, which sense the discriminative functions of
touch---sensing details, texture, vibration, and objects. The CT system,
in contrast, is the basis of the affective function of touch---sensing
pleasure and therefore often eliciting positive emotions. Line Loken and
coworkers (2009) focused on the pleasant aspect of social touch by using
microneurography to record how fibers in the skin responded to stroking
the skin with a soft brush. Loken found that the stroking caused firing
in CT afferents and also in the SA1 and SA2 myelinated fibers associated
with the discriminative functions, but with an important difference.
Whereas the response of SA1 and SA2 fibers continued to increase as
stroking velocity increased all the way to 30 cm per second (Figure
15.21a), the response of the CT afferents peaked at 3--10 cm per second
and then decreased (Figure 15.21b). CT afferents are therefore
specialized for slow stroking. And perhaps as important, Loken also had
participants rate the pleasantness of the sensation caused by this slow
stroking and found a relationship between pleasantness and CT afferent
firing (Figure 15.21c). Further research showed that maximum
pleasantness ratings occurred at stroking speeds associated with optimal
CT firing (Pawling et al., 2017).

Social Touch and the Brain

150 100 Force 0.2 N 0.4 N

50 0

0.1 0.3

1

3

10 30

Velocity (cm/sec) (a)

50

Top-Down Influences on Social Touch The theme of our discussion so far
is that slow stroking of the arm (and other parts of the body) is
pleasant. But the effects of slow stroking can be influenced by factors
in addition to the location and rate of stroking. For example, knowledge
of who is doing the stroking can determine whether the stroking is
perceived as pleasant or unpleasant. Dan-Mikael Ellingsen and coworkers
(2016) demonstrated this effect by having heterosexual male participants
rate the pleasantness on a scale of 1 (very unpleasant) to 20 (very
pleasant) of a sensual caress to their arm. They were led to believe
that the caress was delivered by a female or a male, and although the
stroking was the same in both cases, the pleasantness rating was 9.2 if
they thought they were being stroked by a male, and 14.2 if they thought
they were being stroked by a female. Results such as this demonstrate
that although slow stroking is often pleasant, evaluation of the
situation can turn a pleasant interaction into a less pleasant
interaction or even a negative one. The fact that people's thoughts
about who is touching them can influence their perception of
pleasantness is an example of how top-down processing (also called
knowledge-based processing) (see Chapter 1, page 10) can influence the
perception of social touch. When we discuss pain, in the next section,
we will describe many examples of how pain can be influenced by top-down
processes.

CT

25

0

CT and psychophysics 3 Pleasantness

SAI

200

Mean firing frequency (impulses per s)

Mean firing frequency (impulses per s)

As important as CT afferents are for social touch, the perception of
touch doesn't happen until the signals from the CT afferents reach the
brain. The main area that receives this input is the insula (see Figure
15.4), which had been known to be involved in positive emotions. Monika
Davidovic and coworkers (2019) determined the functional connectivity
between different part of the insula caused by

pleasant touch (see page 33 to review functional connectivity) and found
that slow stroking creates connections between the back of the insula,
which receives sensory information, and the front of the insula, which
is connected to emotional areas of the brain. Apparently, this
connection to emotional areas helps create the pleasurable response to
social touch.

1 0 --1

0.1 0.3 1 3 10 30 Velocity (cm/sec)

(b) 

2

0 25 50 Mean firing frequency (impulses per s)

(c) 

Figure 15.21 Line Loken and coworkers (2009) used microneurography to
record firing rates of (a) SA1 fibers (associated with Merkel receptors)
and (b) CT afferents, as a function of the velocity a soft brush was
moved across the skin for two different brush pressures, low (blue) and
high (red). The firing rate of the SA1 fibers continues to increase as
velocity increases, but the firing of the CT afferent peaks at about 3
cm/ second and then decreases. (c) The relationship between pleasantness
ratings and velocity firing frequency of CT afferents, showing that
higher firing rates are associated with higher pleasantness ratings. 372

Chapter 15  The Cutaneous Senses

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Pain Perception As we mentioned at the beginning of this chapter, pain
functions to warn us of potentially damaging situations and therefore
helps us avoid or deal with cuts, burns, and broken bones. People born
without the ability to feel pain might become aware that they are
leaning on a hot stove burner only when they smell burning flesh, or
might be unaware of broken bones, infections, or internal
injuries---situations that could easily be life-threatening (Watkins &
Maier, 2003). The signaling function of pain is reflected in the
following definition, from the International Association for the Study
of Pain: "Pain is an unpleasant sensory and emotional experience
associated with actual or potential tissue damage, or described in terms
of such damage" (Merskey, 1991). Joachim Scholz and Clifford Woolf
(2002) distinguish three different types of pain. Inflammatory pain is
caused by damage to tissue or inflammation of joints or by tumor cells.
Neuropathic pain is caused by lesions or other damage to the nervous
system. Examples of neuropathic pain are carpal tunnel syndrome, which
is caused by repetitive tasks such as typing; spinal cord injury; and
brain damage due to stroke. Nociceptive pain is pain caused by
activation of receptors in the skin called nociceptors, which are
specialized to respond to tissue damage or potential damage (Perl,
2007). A number of different kinds of nociceptors respond to different
stimuli---heat, chemical, severe pressure, and cold (Figure 15.22). We
will focus on nociceptive pain. Our discussion will include not only
pain that is caused by stimulation of nociceptors in the skin, but also
mechanisms that affect the perception of nociceptive pain, and even some
examples of pain that can occur when the skin is not stimulated at all.

15.6 The Gate Control Model of Pain We begin our discussion of pain by
considering how early researchers thought about pain, and how these
early ideas

began changing in the 1960s. In the 1950s and early 1960s, pain was
explained by the direct pathway model of pain. According to this model,
pain occurs when nociceptor receptors in the skin are stimulated and
send their signals directly from the skin to the brain (Melzack & Wall,
1965). But in the 1960s, some researchers began noting situations in
which pain was affected by factors in addition to stimulation of the
skin. One example was the report by Beecher (1959) that most American
soldiers wounded at the Anzio beachhead in World War II "entirely denied
pain from their extensive wounds or had so little that they did not want
any medication to relieve it" (p. 165). One reason for this was that the
soldiers' wounds had a positive aspect: they provided escape from a
hazardous battlefield to the safety of a behind-the-lines hospital.
Another example, in which pain occurs without any transmission from
receptor to brain, is the phenomena of phantom limbs, in which people
who have had a limb amputated continue to experience the limb (Figure
15.23). This perception is so convincing that amputees have been known
to try stepping off a bed onto phantom feet or legs, or to attempt to
lift a cup with a phantom hand. For many, the limb moves with the body,
swinging while walking. But perhaps most interesting of all, it is not
uncommon for amputees to experience pain in the phantom limb (Jensen &
Nikolajsen, 1999; Katz & Gagliese, 1999; Melzack, 1992; Ramachandran &
Hirstein, 1998). One idea about what causes pain in the phantom limb is
that signals are sent from the part of the limb that remains after
amputation. However, researchers noted that cutting the nerves that used
to transmit signals from the limb to the brain does not eliminate either
the phantom limb or the pain and concluded that the pain must originate
not in the skin but in the brain. In addition, examples such as not
perceiving the pain from serious wounds or perceiving pain when no
signals are being sent to the brain could not be explained Figure 15.22
Nociceptive pain is created by activation of nociceptors in the skin
that respond to different types of stimulation. Signals from the
nociceptors are transmitted to the spinal cord and then up the spinal
cord in pathways that lead to the brain.

Heat

Chemical To brain

Pressure

Cold

Spinal cord

15.6 The Gate Control Model of Pain

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

373

Dorsal root Dorsal horn (a) Central control Signals from
mechanoreceptors +

--

Gate closes

- 

Gate opens

- Signals from nociceptors

- 

- -- Transmission cell

- 

PAIN

- 

Gate control system

(b) 

Figure 15.23 The light part of the right arm represents the phantom
limb---an extremity that is not physically present, but which the person
perceives as existing.

by the direct pathway model. This led Ronald Melzak and Patrick Wall
(1965, 1983, 1988) to propose the gate control model of pain. The gate
control model begins with the idea that pain signals enter the spinal
cord from the body and are then transmitted from the spinal cord to the
brain. In addition, the model proposes that there are additional
pathways that influence the signals sent from the spinal cord to the
brain. The central idea behind the theory is that signals from these
additional pathways can act to open or close a gate, located in the
spinal cord, which determines the strength of the signal leaving the
spinal cord. Figure 15.24 shows the circuit that Melzack and Wall (1965)
proposed. The gate control system consists of cells in the dorsal horn
of the spinal cord (Figure 15.24a). These cells in the dorsal horn are
represented by the red and green circles in the gate control circuit in
Figure 15.24b. We can understand how this circuit functions by
considering how input to the gate control system occurs along three
pathways: Nociceptors. Fibers from nociceptors activate a circuit
consisting entirely of excitatory synapses, and therefore send
excitatory signals to the transmission cells. Excitatory signals from
the (+) neurons in the dorsal horn "open the gate" and increase the
firing of the transmission cells. Increased activity in the transmission
cells results in more pain. ■■ Mechanoreceptors. Fibers from
mechanoreceptors carry information about nonpainful tactile stimulation.
An example of this type of stimulus would be signals sent from rubbing
the skin. When activity in the mechanoreceptors reaches the (--) neurons
in the dorsal horn, inhibitory signals sent to the transmission cells
"close the gate" and ■■

374

Figure 15.24 (a) Cross section of the spinal cord showing fibers
entering through the dorsal root. (b) The circuit proposed by Melzack
and Wall (1965, 1988) for their gate control model of pain perception.
See text for details.

decrease the firing of the transmission cells. This decrease in firing
decreases the intensity of pain. ■■ Central control. These fibers, which
contain information related to cognitive functions such as expectation,
attention, and distraction, carry signals down from the cortex. As with
the mechanoreceptors, activity coming down from the brain also closes
the gate, decreases transmission cell activity, and decreases pain.
Since the introduction of the gate control model in 1965, researchers
have determined that the neural circuits that control pain are much more
complex than what was proposed in the original model (Perl & Kruger,
1996; Sufka & Price, 2002). Nonetheless, the idea proposed by the
model---that the perception of pain is determined by a balance between
input from nociceptors in the skin and nonnociceptive activity from the
skin and the brain---stimulated research that provided a great deal of
additional evidence for the idea that the perception of pain is
influenced by more than just stimulation of the skin (Fields & Basbaum,
1999; Sufka & Price, 2002; Turk & Flor, 1999; Weissberg, 1999). We will
now consider some examples of how cognition can influence the perception
of pain.

15.7 Top-Down Processes Modern research has shown that pain can be
influenced by what a person expects, how the person directs his or her
attention, the type of distracting stimuli that are present, and
suggestions made under hypnosis (Rainville et al., 1999; Wiech et al.,
2008).

Chapter 15  The Cutaneous Senses

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Expectation In a hospital study in which surgical patients were told
what to expect and were instructed to relax to alleviate their pain, the
patients requested fewer painkillers following surgery and were sent
home 2.7 days earlier than patients who were not provided with this
information (Egbert et al., 1964). Studies have also shown that a
significant proportion of patients with pathological pain get relief
from taking a placebo, a pill that they believe contains painkillers but
that, in fact, contains no active ingredients (Finniss & Benedetti,
2005; Weisenberg, 1977). This decrease in pain from a substance that has
no pharmacological effect is called the placebo effect. The key to the
placebo effect is that the patient believes that the substance is an
effective therapy. This belief leads the patient to expect a reduction
in pain, and this reduction does, in fact, occur. Many experiments have
shown that expectation is one of the more powerful determinants of the
placebo effect (Colloca & Benedetti, 2005). Ulrike Bingel and coworkers
(2011) demonstrated the effect of expectation on painful heat
stimulation presented by an electrode on the calf of a person's leg. The
heat was adjusted so the participant reported a pain rating of 70, where
0 corresponds to "no pain," and 100 to "unbearable pain." Participants
then rated the pain in a condition in which a saline solution was
presented by infusion (baseline) and three conditions in which the
analgesic drug remifentanil was presented, but the participants were
told (1) that they were still receiving the saline solution (no
expectation); (2) that the drug was being presented (positive
expectation); and (3) that the drug was going to be discontinued in
order to investigate the possible increase in pain that would occur
(negative expectation). The results, shown in Table 15.2, indicate that
pain was reduced slightly, from 66 to 65, in the no expectation
condition when the drug infusion began, but dropped to 39 in the
positive expectation condition, then increased to 64 in the negative
expectation condition. The important thing about these results is that
after the saline baseline condition, the participant was continuously
receiving the same dose of the drug. What was being changed was their
expectation, and this change in expectation changed their experience of
pain. The decrease in pain experienced in the positive expectation
condition is a placebo effect, in which the positive expectation
instructions function as the placebo. Conversely, the negative effect
caused by the negative expectation instructions

Table 15.2 Effect of Expectation on Pain Ratings CONDITION

DRUG?

Baseline

No

66

No expectation

Yes

55

Positive expectation

Yes

39

Negative expectation

Yes

64

Source: Bingel et al., 2011.

PAIN RATING

is called a nocebo effect, a negative placebo effect (see Tracey, 2010,
for a review of placebo and nocebo effects). This study also measured
the participants' brain activity and found that the placebo effect was
associated with increases in a network of areas associated with pain
perception, and the nocebo effect was associated with increases in
activity in the hippocampus. A person's expectation therefore affects
both perception and physiological responding.

Attention When we described perceiving textures by the fingers, we saw
that the response of cortical neurons can be influenced by attention
(Figure 15.18). Similar effects occur for pain perception. Examples of
the effect of attention on pain were noted in the 1960s by Melzack and
Wall (1965) as they were developing their gate control theory of pain.
Here is a recent description of this effect, as reported by a student in
my class: I remember being around five or six years old, and I was
playing Nintendo when my dog ran by and pulled the wire out of the game
system. When I got up to plug the wire back in I stumbled and banged my
forehead on the radiator underneath the living room window. I got back
up and staggered over to the Nintendo and plugged the controller back
into the port, thinking nothing of my little fall. ... As I resumed
playing the game, all of a sudden I felt liquid rolling down my
forehead, and reached my hand up to realize it was blood. I turned and
looked into the mirror on the closet door to see a gash running down my
forehead with blood pouring from it. All of a sudden I screamed out, and
the pain hit me. My mom came running in, and took me to the hospital to
get stitches. (Ian Kalinowski) The important message of this description
is that Ian's pain occurred not when he was injured but when he realized
he was injured. One conclusion that we might draw from this example is
that one way to decrease pain would be to distract a person's attention
from the source of the pain. This technique has been used in hospitals
with virtual reality techniques as a tool to distract attention from a
painful stimulus. Consider, for example, the case of James Pokorny, who
received third-degree burns over 42 percent of his body when the fuel
tank of the car he was repairing exploded. While having his bandages
changed at the University of Washington Burn Center, he wore a black
plastic helmet with a computer monitor inside, on which he saw a virtual
world of multicolored three-dimensional graphics. This world placed him
in a virtual kitchen that contained a virtual spider, and he was able to
chase the spider into the sink so he could grind it up with a virtual
garbage disposal (Robbins, 2000). The point of this "game" was to reduce
Pokorny's pain by shifting his attention from the bandages to the
virtual reality world. Pokorny reports that "you're concentrating on
different things, rather than your pain. The pain level went down 15.7
Top-Down Processes

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

375

significantly." Studies of other patients indicate that burn patients
using this virtual reality technique experienced much less pain when
their bandages were being changed than patients in a control group who
were distracted by playing video games (Hoffman et al., 2000) or who
were not distracted at all (Hoffman et al., 2008; also see Buhle et al.,
2012).

Emotions A great deal of evidence shows that pain perception can be
influenced by a person's emotional state, with many experiments showing
that positive emotions are associated with decreased pain (Bushnell et
al., 2013). Two ways this has been demonstrated is by having people look
at pictures and having them listen to music. Minet deWied and Marinis
Verbaten (2001) had participants look at pictures that had been
previously rated as being positive (sports pictures and attractive
females), neutral (household objects, nature, and people), or negative
(burn victims and accidents). The participants looked at the pictures
while one of their hands was immersed in cold (2°C/35.6°F) water, and
they were told to keep the hand immersed for as long as possible but to
withdraw the hand when it began to hurt. The results indicated that
participants who were looking at the positive pictures kept their hands
immersed for an average of 120 seconds, but participants in the other
groups removed their hands more quickly (80 seconds for neutral
pictures; 70 seconds for negative pictures). Because their ratings of
the intensity of their pain---made immediately after removing their
hands from the water---was the same for all three groups, deWied and
Verbaten concluded that the content of the pictures influenced the time
it took to reach the same pain level in the three groups. In another
experiment, Jaimie Rhudy and coworkers (2005) found that participants
gave lower ratings to pain caused by an electric shock when they were
looking at pleasant pictures than when they were looking at unpleasant
pictures. They concluded from this result that positive or negative
emotions can affect the experience of pain. Music is another way to
elicit emotions, both positive and negative (Altenmüller et al., 2014;
Fritz et al., 2009; Koelsch, 2014). These emotional effects are one of
the primary reasons we listen to music, but there is also evidence that
the positive emotions associated with music can decrease pain. Mathieu
Roy and coworkers (2008) measured how music affected the perception of a
thermal heat stimulus presented to the forearm by having participants
rate the intensity and unpleasantness of the pain on a scale of 0 (no
pain) to 100 (extremely intense or extremely unpleasant), under one of
these three conditions: listening to unpleasant music (example: Sonic
Youth, Pendulum Music), listening to pleasant music (example: Rossini,
William Tell Overture), and silence. The results of Roy's experiment for
the highest temperature used (48°C/119°F), shown in Table 15.3,
indicates that listening to unpleasant music didn't affect pain,
compared to silence, but that listening to pleasant music decreased both
the 376

Table 15.3 Effect of Pleasant and Unpleasant Music

on Pain

INTENSITY RATING

UNPLEASANTNESS RATING

Silence

69.7

60.0

Unpleasant music

68.6

60.1

Pleasant music

57.7

47.8

CONDITION

Source: Roy et al., 2008.

intensity and the unpleasantness of pain. In fact, the pain relief
caused by the pleasant music was comparable to the effects of common
analgesic drugs such as ibuprofen. TEST YOuRSELF 15.2 1. What processes
are involved in identifying objects by haptic exploration? 2. Describe
the specialization of cortical areas for touch and how cortical
responding to touch is affected by attention. 3. What is social touch?
4. Which receptors in the skin are responsible for social touch? 5. What
is the social touch hypothesis? 6. What brain areas are involved in
social touch? 7. How do "situational influences" affect social touch? 8.
Describe the three types of pain. 9. What is the direct pathway model of
pain? What evidence led researchers to question this model of pain
perception? 10. What is the gate control model? Be sure you understand
the roles of the nociceptors, mechanoreceptors, and central control. 11.
Describe evidence that supports the conclusions that pain is influenced
by expectation, attention, and emotion.

15.8 The Brain and Pain Research on the physiology of pain has focused
on identifying areas of the brain and the chemicals that are involved in
pain perception.

Brain Areas A large number of studies support the idea that the
perception of pain is accompanied by activity that is widely distributed
throughout the brain. Figure 15.25 shows a number of the structures that
become activated by pain. They include subcortical structures, such as
the hypothalamus, the amygdala, and the thalamus, and areas in the
cortex, including

Chapter 15  The Cutaneous Senses

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

The sensory and affective components of pain can be distinguished by
asking participants who are experiencing painful stimuli to rate
subjective pain intensity (sensory component) and unpleasantness
(affective component), as was done in the music study described in the
previous section. When R. K. Hofbauer and coworkers (2001) used hypnotic
suggestion to increase or decrease these components separately, they
found that changes in the sensory component were associated with
activity in the somatosensory cortex and changes in the affective
component were associated with changes in the anterior cingulate cortex.
Figure 15.26 shows these two areas and some other areas that have been
determined from other experiments to be associated with sensory (blue)
and affective (green) pain experiences (Eisenberger, 2015).

S1

ACC Thalamus Insula

PFC Amygdala, Hippocampus

Figure 15.25 The perception of pain is accompanied by activation of a
number of different areas of the brain. ACC is the anterior cingulate
cortex; PFC is the prefrontal cortex; S1 is the somatosensory cortex.
The positions of the structures are approximate, with some, such as the
amygdala, hypothalamus, and insula, located deep within the cortex, and
others, such as S1 and PFC, located at the surface. Lines indicate
connections between the structures.

Chemicals and the Brain Another important development in our
understanding of the relationship between brain activity and pain
perception is the discovery of a link between chemicals called opioids
and pain perception. This can be traced back to research that began in
the 1970s on opiate drugs, such as opium and heroin, which have been
used since the dawn of recorded history to reduce pain and induce
feelings of euphoria. By the 1970s, researchers had discovered that
opiate drugs act on receptors in the brain that respond to stimulation
by molecules with specific structures. The importance of the molecule's
structure for exciting these "opiate receptors" explains why injecting a
drug called naloxone into a person who has overdosed on heroin can
almost immediately revive the victim. Because naloxone's structure is
similar to heroin's, it attaches to the same receptor sites, thereby
preventing from binding to those receptors (Figure 15.27a). Why are
there opiate receptor sites in the brain? After all, they certainly have
been present since long before people started taking heroin. Researchers
concluded that there must be naturally occurring substances in the body
that act on these sites, and in 1975 neurotransmitters were discovered
that act on the same receptors that are activated by opium and heroin.
One group of these transmitters is called endorphins, for endogenous
(naturally occurring) morphine.

the somatosensory cortex (S1), the anterior cingulate cortex (ACC), the
prefrontal cortex (PFC), and the insula (Chapman, 1995; Derbyshire et
al., 1997; Price, 2000; Rainville, 2002; Tracey, 2010). Although pain is
associated with the overall pattern of firing in the many structures,
there is also evidence that certain areas are responsible for specific
components of the pain experience. In the definition of pain on page
373, we stated that pain is "an unpleasant sensory and emotional
experience." This reference to both sensory and emotional experience
reflects the multimodal nature of pain, which is illustrated by how
people describe pain. When people describe their pain with words like
throbbing, prickly, hot, or dull, they are referring to the sensory
component of pain. When they use words like torturing, annoying,
frightful, or sickening, they are referring to the affective (or
emotional) component of pain (Melzack, 1999).

Figure 15.26 Two views of the brain showing the areas involved in the
affective and sensory components of pain. Green 5 Affective component:
ACC 5 anterior cingulate cortex; AI 5 anterior insula; Blue 5 Sensory
component: S1, S2 5 somatosensory areas; PI 5 posterior insula. (Adapted
from Eisenberger, 2015,

S1 S2

dACC

Fig 1, p. 605)

(a) 
(b) 

PI

AI

15.8 The Brain and Pain

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

377

Since the discovery of endorphins, researchers have accumulated a large
amount of evidence linking endorphins to pain reduction. For example,
pain can be decreased by stimulating sites in the brain that release
endorphins (Figure 15.27b), and pain can be increased by injecting
naloxone, which blocks endorphins from reaching their receptor sites
(Figure 15.27c). In addition to decreasing the analgesic effect of
endorphins, naloxone also decreases the analgesic effect of placebos
(see page 375). This finding, along with other evidence, led to the
conclusion that the pain reduction effect of placebos occurs because
placebos cause the release of endorphins. As it turns out, there are
some situations in which the placebo effect can occur without the
release of endorphins, but we will focus on the endorphin-based placebo
effect by considering the following question, raised by Fabrizio
Benedetti and coworkers (1999): Where are placebo-related endorphins
released in the nervous system? Benedetti wondered whether expectation
caused by placebos triggered the release of endorphins throughout the
brain, therefore creating a placebo effect for the entire body, or
whether expectation caused the release of endorphins only at specific
places in the body. To answer this question, Benedetti injected
participants with the chemical capsaicin just under the skin at four
places on the body: the left hand, the right hand, the left foot, and
the right foot. Capsaicin, which is the active component in chili
peppers, causes a burning sensation where it is injected.

Heroin

Opiate site

Naloxone

Revive from heroin overdose

(a) Brain stimulation

Endorphin Less pain

(b) Endorphin Naloxone

Increases pain by blocking endorphins

of the Body

PAIN RATINGS AT DIFFERENT BODY LOCATIONS CONDITION

LEFT HAND

RIGHT HAND

LEFT FOOT

RIGHT FOOT

No placebo

6.6

5.5

6.0

5.4

Placebo cream on left hand

3.0

6.4

5.3

6.0

Placebo cream on right hand and left foot

5.4

3.0

3.8

6.3

Source: Benedetti et al., 1999.

One group of participants rated the pain at each part of the body on a
scale of 0 (no pain) to 10 (unbearable pain) every minute for 15 minutes
after the injection. The "No placebo" row in Table 15.4 shows that the
participants in this group reported pain at all the locations (ratings
between 5.4 and 6.6). Another group of participants also received the
injection, but just before the injections, the experimenter rubbed a
cream at one or two of the locations and told participants that the
cream was a potent local anesthetic that would relieve the burning
sensation of the capsaicin. The cream was actually a placebo treatment;
it had no pain-reducing ingredients. The second row of Table 15.4 shows
that the pain rating for the left hand decreased to 3.0 for a
participant who received the cream on the left hand, and the third row
shows that the pain ratings for the right and left foot decreased for a
participant who received the cream on the right hand and left foot.
These results are striking because the placebo effect occurred only
where the cream was applied. To demonstrate that this placebo effect was
associated with endorphins, Benedetti showed that injecting naloxone
abolished the placebo effect. What this means, according to Benedetti,
is that when participants direct their attention to specific places
where they expect pain will be reduced, pathways are activated that
release endorphins at specific locations. The mechanism behind
endorphin-related analgesia is therefore much more sophisticated than
simply chemicals being released into the overall circulation. The mind,
as it turns out, can not only reduce pain by causing the release of
chemicals, it can literally direct these chemicals to the locations
where the pain would be occurring. Research such as this, which links
the placebo effect to endorphins, provides a physiological basis for
what had previously been described in strictly psychological terms.

15.9 Social Aspects of Pain

(c) 

Figure 15.27 (a) Naloxone, which has a structure similar to heroin,
reduces the effect of heroin by occupying a receptor site normally
stimulated by heroin. (b) Stimulating sites in the brain that cause the
release of endorphins can reduce pain by stimulating opiate receptor
sites. (c) Naloxone decreases the pain reduction caused by endorphins by
keeping the endorphins from reaching the receptor sites. 378

Table 15.4 Effect of Placebo Cream on Different Parts

We've described social aspects of touch, in which activation of CT
afferents is associated with the pleasurable sensation that often
accompanies slow stroking of the skin. In this section we will describe
three connections between "social" and pain: (1) how social touch can
reduce pain; (2) how observing someone else feeling pain can affect the
observer; and (3) possible connections between the pain of social
rejection and physical pain.

Chapter 15  The Cutaneous Senses

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Pain Reduction by Social Touch

The Effect of Observing Someone Else's Pain

Figure 15.28 Setup for the Goldstein et al. (2018) experiment. See text
for details.

Woman

empathy, the ability to share and vicariously experience someone else's
feeling. In the hand-holding experiment, the person receiving the
empathy experienced pain analgesia. We can also look at empathy from
another point of view, by considering what is happening for the
"empathizer," the person who is feeling empathy for the person who is in
pain. In Chapter 7 we introduced the idea that observing an action can
cause activity related to that action in the observer's brain. This was
demonstrated in experiments studying mirror neurons in the monkey's
premotor cortex, which fired both when the monkey picking up an object,
such as food, and also when the monkey saw someone else picking up the
food. Research on the somatosensory system has revealed similar
phenomena. To introduce this phenomenon, let's first consider an
experiment involving touch. Keysers and coworkers (2004) measured the
fMRI response of a person's cortex to being touched on the leg. They
also measured the

Man

(a) 

Man

Woman

Goldstein's experiment demonstrated that holding someone's hand can
reduce their pain. This has been described as effect of

Man

We've seen that being a recipient of social touch is often perceived as
pleasant (p. 372). We now describe an experiment by Pascal Goldstein and
coworkers (2018) that was inspired by Goldstein's observation that
holding his wife's hand during the delivery of his daughter decreased
his wife's pain. Figure 15.28 shows the position of the two participants
in the experiment that studied this observation in the laboratory.
Romantically involved couples wore electrode arrays on their heads to
record electroencephalogram (EEG), which is the response of thousands of
neurons under the electrodes. They faced each other, but were not
allowed to talk to each other. The woman received a heat stimulus on her
arm that was moderately painful and was instructed to rate her pain
level just before the heat was turned off. On no-touch trials, the man
and woman just looked at each other without touching; on touch trials,
the man held the woman's hand. There were also trials in which the man
was absent. The results showed that the woman's pain ratings were lower
when her partner was holding her hand (rating 5 25.0), compared to when
he wasn't (37.8) or when he was absent (52.4). This decrease in pain in
the hand-holding condition replicates Goldstein's wife's experience in
the delivery room. But comparing the woman's and man's EEG responses
revealed something that wasn't obvious in the delivery room. The woman's
and man's brains were strongly "coupled" or synchronized when holding
hands (Figure 15.29a), but were not as synchronized when not holding
hands (Figure 15.29b). The authors suggest that the support provided by
hand holding causes synchronized brain waves, which are translated into
reduced pain. In addition to this synchronization effect, other research
has shown that hand holding reduces activity in brain areas associated
with pain (Lopez-Sola et al., 2019). These experiments have two
important messages: Providing support by being there for someone who is
experiencing pain can reduce their pain, and making physical contact by
holding hands reduces the pain even further.

Woman

Heat stimulator

(b) 

Figure 15.29 Coupling between EEG brain waves of the woman and man when
the woman was experiencing pain in the Goldstein et al. (2018)
experiment. The orange lines represent synchronized responses between
two brain areas. (a) Holding-hands condition. (b) Not holding-hands
condition. 15.9 Social Aspects of Pain

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

379

response that occurred when that person viewed a film of another person
being touched on the leg. Being touched caused a response in the
person's primary somatosensory area (S1) and secondary somatosensory
area (S2). Watching someone else being touched caused a response in S2,
part of which overlapped with the response to being touched. Keysers
concluded from the overlap in the areas that the brain transforms
watching someone being touched into an activation of brain areas
involved in our own experience of touch (see also Keysers et al., 2010).
Another way to describe Keysers's results is, when we witness someone
else being touched, we don't just see touch, but we have an empathic
response in which we understand the other person's response to touch
through a link with our own experience of touch. This idea that
observing someone else being touched triggers brain mechanisms that
might help us understand the other person's response to touch is
important because there is also evidence that similar mechanisms operate
for pain. Tania Singer and coworkers (2004) demonstrated the connection
between brain responses to pain and empathy by bringing romantically
involved couples into the laboratory and having the woman, whose brain
activity was being measured by an fMRI scanner, either receive shocks
herself or watch her male partner receive shocks. The results, shown in
Figure 15.30,

show that a number of brain areas were activated when the woman received
the shocks (Figure 15.30a), and that some of the same areas were
activated when she watched her partner receive shocks (Figure 15.30b).
The main two areas activated in common were the anterior cingulate
cortex (ACC) and the anterior insula (AI), both of which are associated
with the affective component of pain (see Figure 15.26). To show that
the brain activity caused by watching their partner was related to
empathy, Singer had the women fill out "empathy scales" designed to
measure their tendency to empathize with others. As predicted, women
with higher empathy scores showed higher activation of their ACC. In
another experiment, Olga Klimecki and coworkers (2014) had participants
undergo training designed to increase their empathy for others and then
showed them videos depicting other people experiencing suffering due to
injury or natural disasters. Participants in the empathy-training group
showed more empathy compared to a control group that hadn't received the
training and greater activation of the ACC. Thus, although the pain
associated with watching someone else experience pain may be caused by
stimulation that is very different from physical pain, these two types
of pain apparently share some physiological mechanisms. (Also see
Avenanti et al., 2005; Lamm et al., 2007; Singer & Klimecki, 2014.)

Figure 15.30 Singer and coworkers (2004) used fMRI to determine the
areas of the brain activated by (a) receiving painful stimulation and
(b) watching another person receive the painful stimulation. Singer
proposes that the activation in (b) is related to empathy for the other
person. Empathy did not activate the somatosensory cortex but did
activate other areas that are activated by pain, such as the insula
(tucked between the parietal and temporal lobes) and anterior cingulate
cortex (see Figures 15.25 and 15.26). (Adapted from Holden, 2004)

(a) Receive painful stimulation

(b) Watch partner receive painful stimulation

380

Chapter 15  The Cutaneous Senses

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

The "Pain" of Social Rejection Our discussion of the connection between
"social" and "pain" has so far considered effects associated with
physical pain--- pain caused by heating or shocking the skin. We will
now describe something very different---the pain caused by social
situations such as social rejection. The question we will be considering
is, "What does this social pain---pain caused by social
interactions---have in common with physical pain?" The idea that social
rejection hurts is well known. When describing emotional responses to
negative social experiences, it is common for people to use words
associated with physical pain, such as broken hearts, hurt feelings, or
emotional scars (Eisenberger, 2012, 2015). In 2003, Naomi Eisenberger
and coworkers published a paper titled "Does Rejection Hurt? An fMRI
Study of Social Exclusion," which concluded that the dorsal anterior
cingulate cortex (dACC; see Figure 15.26) is activated by feelings of
social exclusion. They demonstrated this by having participants
participate in a video game called "Cyberball," in which they were told
that they would be playing a ball-tossing game with two other
participants, who were indicated by the two figures at the top of the
computer screen, with the participant being indicated by a hand at the
bottom of the screen (Figure 15.31).

Including the third player

Initially, the two other players included the participant in their ball
tossing (Figure 15.31a), but then they suddenly excluded the participant
and just tossed the ball between themselves (Figure 15.31b). This
exclusion caused activity in the participant's dACC, as shown in Figure
15.31c, and this dACC activity was related to the degree of social
distress the participant reported feeling, with greater distress
associated with greater dACC activity (Figure 15.31d). Other studies
provided more evidence for similar physiological responses to negative
social experiences and physical pain. Activation of the dACC and
anterior insula (AI) occurred in response to a threat of negative social
evaluation (Eisenberger et al., 2011) and when remembering a romantic
partner who had recently rejected the person (Kross et al., 2011). Also,
taking a pain reliever such as Tylenol not only reduces physical pain
but also reduces hurt feelings and dACC and AI activity (DeWall et al.,
2010). Results such as these have led to the physical-social pain
overlap hypothesis, which proposes that pain resulting from negative
social experiences is processed by some of the same neural circuitry
that processes physical pain (Eisenberger, 2012, 2015; Eisenberger &
Lieberman, 2004). This idea has not gone unchallenged, however. One line
of

Figure 15.31 The "Cyberball" experiment. (a) The participant is told
that the two characters shown on the top of the screen are being
controlled by two other participants. These two characters throw the
ball to the participant in the first part of the experiment. (b) The
participant is excluded from the game in the second part of the
experiment. (c) Exclusion results in activity in the ACC, shown in
orange. (d) Participant's rating of social distress (y-axis) is related
to ACC activation (x-axis). (From Eisenberger & Lieberman, 2004)

Excluding the third player

(a) 
(b) 4.0

Source: Elsevier Ltd.

Social distress

3.5 3.0 2.5 20.06 20.03

1.5

Anterior cingulate (c)

0

0.03

0.06

0.09

0.12

0.15

2.0 r 5 0.88

1.0

(d) 

Anterior cingulate activity 15.9 Social Aspects of Pain

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

381

criticism has focused on the idea that activity in the ACC may be
reflecting things other than pain. For example, it has been suggested
that the ACC may respond to many types of emotional and cognitive tasks,
rather than being specialized for pain (Krishnan et al., 2016; Wager et
al., 2016), or that the ACC responds to salience---how much a stimulus
stands out from its surroundings (Iannetti et al., 2013). Another
question that has been raised is whether activation of the ACC by both
social and physical pain means that the same neural circuits are being
activated. This question is similar to a question we considered in
Chapter 13, when we considered whether music and language share neural
mechanisms. One of the points made in that discussion, which is also
relevant here, is that just because two functions activate the same area
of the brain, doesn't mean that the two functions are activating the
same neurons within that area. Look back at Figure 13.24 for an
illustration of the idea that activation within a particular area can
involve different neural networks. Choong-Wan Woo and coworkers (2014)
used a technique called multivoxel pattern analysis (MVPA) to look at
what is happening inside the brain structures involved in social and
physical pain. MVPA was used for the neural mind reading experiment
described in Chapter 5 in which the pattern of voxel responses to
oriented lines was determined to create computer image decoders for
visual stimuli (see Method: Neural Mind Reading, page 114). Woo found
that the pattern of voxel responses generated by recalling social
rejection by a romantic partner was different from the pattern generated
by painful heat presented to the forearm, which why his paper is titled
"Separate Neural Representations for Physical Pain and Social
Rejection." So which idea is correct? Do social pain and physical pain
share neural mechanisms, or are they two separate phenomena that both
use the word "pain"? There is evidence supporting the physical-social
pain overlap hypothesis, but there is also evidence that argues against
this hypothesis. Because social pain and physical pain are certainly
different---it's easy to tell the difference between the feeling of
being rejected and the feeling from burning your finger---it is unlikely
that mechanisms overlap completely. The physical-social pain overlap
hypothesis proposes that there is some overlap. But how much is "some"?
A little or a lot? Research to answer this question is continuing.

SOMETHING TO CONSIDER:

Plasticity and the Brain We've seen that there are orderly maps of the
body on the somatosensory cortex, in which parts of the body that are
more sensitive and are used more, like the lips, hands, and fingers, are
represented by large areas on the brain (Figure 15.5). But these maps
can change based both on how much a body part is used and in response to
injury. An early study 382

Before stimulation of fingertip

5

4

3

2

1

1 mm (a)

(b) 

After stimulation of fingertip

5

4

3

2

1

1 mm (c)

Figure 15.32 (a) Each numbered zone represents the area in the
somatosensory cortex that corresponds to one of a monkey's five fingers.
The shaded area on the zone for finger 2 is the part of the cortex that
represents the small area on the tip of the finger shown in (b). (c) The
shaded region shows how the area representing the fingertip increased in
size after this area was heavily stimulated over a three-month period.
(From Merzenich et al., 1988)

showing that the map changes with use was done by William Jenkins and
Michael Merzenich (1987), who began by measuring the cortical areas
devoted to each of a monkey's fingers (Figure 15.32a). They gave the
monkey a task that heavily stimulated the tip of finger 2 over a
three-month period (Figure 15.32b) and then remeasured the areas devoted
to the fingers (Figure 15.32c). Comparison of the "before" and "after"
cortical maps showed that the area representing the stimulated fingertip
was greatly expanded after the training. This change in the brain's map
is an example of experience-dependent plasticity, introduced in Chapter
4, when we saw that raising a kitten in an environment consisting only
of vertically oriented stripes caused the cat's orientation-sensitive
neurons to respond mainly to verticals (p. 74). An effect of
experience-dependent plasticity has been demonstrated in humans by
measuring the brain maps of musicians. Consider, for example, players of
stringed instruments. A right-handed violin player bows with the right
hand and uses the fingers

Chapter 15  The Cutaneous Senses

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Chapter 4: Perceiving orientation (p. 74) ■■ Chapter 5: Sensitivity to
regularities in the environment (p. 105) ■■ Chapter 6: Attentional
control by scene schemas (p. 131)

Chapter 7: Affordances (p. 152) Chapter 7: Cognitive maps (p. 157) ■■
Chapter 7: London taxi drivers' brains (p. 158) ■■ Chapter 8: Motion
response to still pictures (p. 190) ■■ Chapter 9: Color constancy
(p. 215) ■■ Chapter 10: Sizes of familiar objects (p. 231) ■■ Chapter
11: Babies recognizing their mother's voice (p. 287) ■■ Chapter 12:
Auditory scene analysis (p. 302) ■■ Chapter 12: Echolocation in blind
people (p. 307) ■■ Chapter 13: Music rewires the brain (p. 313) ■■
Chapter 13: Expectancy in music (p. 323) ■■ Chapter 14: Speech
segmentation (p. 345) ■■ Chapter 14: Statistical properties of speech
stimuli (p. 346) ■■ ■■

As we continue our discussion of perception in the chapter on the
chemical senses, we will yet again encounter perceptions that involve
brain plasticity, as we discuss how we smell and how our perception of
flavor is influenced by cognition.

(a) 

Normal participants

D5 D1

Participants with dystonia (b)

of the hand

American Psychological Association (APA)

of his or her left hand to finger the strings. One result of this
tactile experience is that these musicians have a greater than normal
cortical representation for the fingers on their left hand (Elbert et
al., 1995). Just as in the monkeys, plasticity has created more cortical
area for parts of the body that are used more. Changes in the cortical
map can also occur when part of the body is damaged. For example, when a
monkey loses a finger, the brain area representing that finger no longer
receives input from that finger, so over a period of time that area is
taken over by the fingers next to the one that is missing (Byl et al.,
1996). A particularly interesting example of a possible change in
mapping associated with dysfunction is the case of worldfamous concert
pianist Leon Fleisher, who, at the age of 36, began experiencing hand
dystonia, a condition which caused the fingers on his right hand to curl
into his palm, making it impossible to play the piano with his right
hand. Fleisher developed a repertoire of left-handed piano compositions,
and eventually, after 30 years of therapy, regained the use of his right
hand and was able to resume his career as a two-handed pianist.
Fleisher's dystonia could have been due to a number of causes. Fleischer
came to believe that his problem was caused by overpracticing, which he
described as "seven or eight hours a day of pumping ivory" (Kozinn,
2020). One possible mechanism associated with this practicing is that
using his fingers often and in close conjunction with each other could
have changed the locations of their representation in the cortex. This
effect of hand dystonia was demonstrated by William Bara-Jimenez and
coworkers (1998), who showed that the map of the fingers in area S1 is
abnormally organized in some patients with dystonia. Figure 15.33 shows
the results of their experiment, in which they measured the location of
the areas in S1 representing the thumb and little finger in a group of
six normal participants (Figure 15.33a) and six participants with
dystonia of the hand (Figure 15.33b). The locations for these two
fingers, which was determined by stimulating the fingers and measuring
the brain activity with scalp electrodes, are separated in the normal
participants but are close together in the patients with dystonia. The
beauty of somatosensory maps is that the plastic changes that occur due
to stimulation or injury are easy to visualize. But these
easy-to-visualize changes are but another example of a principle of
brain functioning that we have encountered over and over throughout this
book: The brain's structure and functioning are shaped by our
experiences and by the nature of our environment. And the outcome of
these changes is our ability to understand stimuli we encounter in the
environment and to take action within the environment. A few examples
that involve plasticity that we have encountered in previous chapters
are: ■■

Figure 15.33 Locations of place of representation on the brain of finger
D5 (little finger) and D1 (the thumb) of the left hand for (a) control
participants and (b) patients with dystonia of the hand. (Adapted from
Bar-Jimeniz et al., 1998)

Something to Consider: Plasticity and the Brain

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

383

DEVELOPMENTAL DIMENSION Social Touch in Infants showed, using ultrasound
films, that in the third trimester, the fetus responds when the mother's
abdomen is touched. Even more interesting is what happens with twin
fetuses. They are, of course, in close proximity in the womb, so could
touch each other accidentally. But ultrasound films have captured a
fetus not only moving its hands to its mouth, but also "caressing" the
head of its sibling (Castiello et al., 2010) (Figure 15.34). The
importance of the infant's ability to touch and sense touch becomes
magnified when it is born, because this is when touch becomes social.
Sixty-five percent of face-to-face interaction between caregiver and
infant involves touch (Cascio et al., 2019). And there is evidence
linking touch felt by infants to the social touch experienced by adults
that involves CT afferents (p. 371). For example, Merle Fairhurst and
coworkers (2014) found that 9-month-old infants respond to movement of a
brush along their arm with a decrease in heart rate (indicating a
decrease in arousal) if the brush is moved across the arm at 3 cm per
second, which is in the range that activates CT afferents. Lower (0.3
cm/sec) or higher (30 cm/sec) rates did not cause this effect.

PLOS ONE

PLOS ONE

Social touch is important to adults because it not only feels good
(p. 372), but it has the important function of being able to reduce pain
(p. 379). It could be argued that social touch is also an important part
of an infant's experience, and can have far-reaching effects on
development that extend into childhood and adulthood. Touch is the
earliest sensory modality to develop, emerging just 8 weeks after
gestation, then developing and becoming functional within the womb, and
being ready for action at birth (Cascio et al., 2019). Touch and speech
are the earliest forms of parent--child interaction, but in contrast to
speech, which is a one-way interaction at the beginning, touch is
twoway. This is illustrated by the automatic hand closure response to
objects (like a parent's finger, for example) placed in the infant's
palm (Bremner & Spence, 2017). What makes the story of infant touch even
more interesting is that it begins in the womb. From 26 weeks after
gestation, the fetus begins responding to vibration with heart rate
acceleration. Later, the fetus begins bringing a hand to the face, and
in the last 4 to 5 weeks before birth, begins touching the feet. Viola
Marx and Emese Nagy (2017)

(a) 
(b) 

Figure 15.34 Frames from ultrasound videos of fetuses in the womb. (a)
Fetus that has moved the hand to the mouth. (b) Interaction between
twins showing a fetus reaching toward and "caressing" the back of the
sibling. (From Castiello et al., 2010)

384

Chapter 15  The Cutaneous Senses

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Evidence that CT afferents may become involved just after birth is
provided by Jetro Tuulari and coworkers (2019), who found that
presenting soft brush strokes to the legs of 11- to 16-day-old infants
activates the posterior insula (Figure 15.26), which is associated with
social touch in adults. And just as social touch can reduce pain in
adults, the skin-to-skin contact that occurs when newborn infants are
held close by the mother (sometimes called kangaroo care) has been shown
to cause an 82 percent decrease in crying in response to a medical heel
lance procedure (Gray et al., 2000; also see Ludington-Hoe & Husseini,
2005). The most important outcome of an infant's experience of social
touch is how it shapes social, communicative, and cognitive development
in the months and years that follow (Cascio et al., 2019). A dramatic
demonstration of this effect of social

TEST YOuRSELF 15.3 1. What does it mean to say that pain is multimodal?
Describe the hypnosis experiments that identified areas involved in the
sensory component of pain and the emotional component of pain. 2.
Describe the role of chemicals in the perception of pain. Be sure you
understand how endorphins and naloxone interact at receptor sites, and a
possible mechanism that explains why pain is reduced by placebos. 3.
Describe the experiment which demonstrated that a placebo effect can
operate on local parts of the body. 4. Describe the experiment which
shows that social touch can cause a decrease in pain. 5. Describe the
experiments which showed how observing someone being touched or
observing someone

touch is provided by premature infants who are deprived of early social
touch when they are separated from their mothers and placed in
incubators. When these premature infants are massaged, they have more
weight gain, better cognitive development, better motor skills, and
better sleep than premature infants who are not massaged (Field, 1995;
Wang et al., 2013). At the beginning of this Developmental Dimension we
noted that touch and speech are the earliest forms of parent--child
interaction. We saw in Chapter 14 that infant-directed speech (IDS) has
many beneficial effects on the developing infant (p. 353). We've seen
here that social touch has its own positive effects. Clearly, using
infantdirected speech in conjunction with social touch is a powerful
combination for enhancing the course of an infant's development.

experiencing pain can affect activity in the observer's brain. What do
these results tell us about empathy? 6. What is the evidence supporting
the idea that social and physical pain share some mechanisms? What
evidence questions this idea? 7. How has the plasticity of the
somatosensory cortex been demonstrated in monkeys? In humans? What is
hand dystonia, and how is it related to brain plasticity? 8. What are
some examples of brain plasticity in vision and hearing that were
described in previous chapters? 9. When does touch develop in infants,
and what is the evidence that social touch has an impact on later
development?

THINK ABOUT IT 1. One of the themes in this book is that it is possible
to use the results of psychophysical experiments to suggest the
operation of physiological mechanisms or to link physiological
mechanisms to perception. Cite an example of how psychophysics has been
used in this way for each of the senses we have considered so
far---vision, hearing, and the cutaneous senses. 2. Some people report
situations in which they were injured but didn't feel any pain until
they became aware of their injury. How would you explain this kind of
situation in terms of top-down and bottom-up processing? How could

you relate this situation to the studies we have discussed? (p. 374) 3.
Even though the senses of vision and cutaneous perception are different
in many ways, there are a number of parallels between them. Cite
examples of parallels between vision and cutaneous sensations (touch and
pain) for the following: "tuned" receptors, mechanisms of detail
perception, receptive fields, and top-down processing. Also, can you
think of situations in which vision and touch interact with one another?

Think About It

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

385

KEY TERMS Active touch (p. 368) Affective (or emotional) component of
pain (p. 377) Affective function of touch (p. 372) CT afferents (p. 371)
Cutaneous receptive field (p. 358) Cutaneous senses (p. 358) Dermis
(p. 358) Direct pathway model of pain (p. 373) Discriminative function
of touch (p. 372) Duplex theory of texture perception (p. 366) Empathy
(p. 379) Endorphin (p. 377) Epidermis (p. 358) Exploratory procedures
(EPs) (p. 369) Gate control model (p. 374) Grating acuity (p. 363) Hand
dystonia (p. 383) Haptic perception (p. 368) Homunculus (p. 362)
Inflammatory pain (p. 373) Interpersonal touching (p. 371)

386

Kinesthesis (p. 358) Knowledge-based processing (p. 372) Mechanoreceptor
(p. 358) Medial lemniscal pathway (p. 359) Meissner corpuscle (RA1)
(p. 358) Merkel receptor (SA1) (p. 358) Microneurography (p. 371)
Multimodal nature of pain (p. 377) Naloxone (p. 377) Neuropathic pain
(p. 373) Nocebo effect (p. 375) Nociceptive pain (p. 373) Nociceptor
(p. 373) Opioid (p. 377) Pacinian corpuscle (RA2 or PC) (p. 359) Passive
touch (p. 368) Phantom limb (p. 373) Physical-social pain overlap
hypothesis (p. 381) Placebo (p. 375) Placebo effect (p. 375) Primary
somatosensory cortex (S1) (p. 361)

Proprioception (p. 358) RA1 fiber (p. 359) RA2 fiber (p. 359) Rapidly
adapting (RA1) fiber (p. 359) Ruffini cylinder (SA2) (p. 359) SA1 fiber
(p. 358) SA2 fiber (p. 359) Secondary somatosensory cortex (S2) (p. 361)
Sensory component of pain (p. 377) Slowly adapting (SA) fiber (p. 358)
Social touch (p. 371) Social touch hypothesis (p. 371) Social pain
(p. 381) Somatosensory system (p. 358) Spatial cue (p. 366)
Spinothalamic pathway (p. 359) Surface texture (p. 366) Tactile acuity
(p. 363) Temporal cue (p. 366) Top-down processing (p. 372) Transmission
cell (p. 374) Two-point threshold (p. 363) Ventrolateral nucleus
(p. 359)

Chapter 15  The Cutaneous Senses

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

These people are enjoying not only the social experience of eating with
others, but also the sensory experiences created by taste and smell.
Taste is created by receptors on the tongue, smell by receptors in the
nose, and taste and smell work together to create flavor, which is the
dominant perception we experience when eating or drinking.
iStock.com/SeventyFour

Learning Objectives After studying this chapter, you will be able to ...
■■ Describe the structure of the taste system and how activity in

this system is related to taste quality. ■■ Describe genetic research on
individual differences in taste. ■■ Describe the following aspects of
basic olfactory abilities: detecting

odors, identifying odors, individual differences in olfaction, and how
olfaction is affected by COVID-19 and Alzheimer's disease. ■■ Describe
how olfactory quality is analyzed by the mucosa and

olfactory bulb.

■■ Understand how odors are represented in the cortex. ■■ Understand the
connection between olfaction and memory. ■■ Describe what flavor is and
how it is related to taste, olfaction,

cognition, and satiation. ■■ Describe multimodal interactions between
the senses. ■■ Describe how researchers have measured infant chemical

sensitivity.

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

C h a pter 1 6

The Chemical Senses

Chapter Contents 16.1 Some Properties of the Chemical Senses 16.2 Taste
Quality Basic Taste Qualities Connections Between Taste Quality and a
Substance's Effect

16.3 The Neural Code for Taste Quality Structure of the Taste System
Population Coding Specificity Coding

16.4 Individual Differences in Taste TEST YOURSELF 16.1

16.5 The Importance of Olfaction 16.6 Olfactory Abilities Detecting
Odors Identifying Odors Demonstration: Naming and Odor

Individual Differences in Olfaction Loss of Smell in COVID-19 and
Alzheimer's Disease

16.9 The Perception of Flavor

16.7 Analyzing Odorants: The Mucosa and Olfactory Bulb

Taste and Olfaction Meet in the Mouth and Nose Taste and Olfaction Meet
in the Nervous System Flavor Is Influenced by Cognitive Factors Flavor
Is Influenced by Food Intake: Sensory-Specific Satiety

The Puzzle of Olfactory Quality The Olfactory Mucosa How Olfactory
Receptor Neurons Respond to Odorants Method: Calcium Imaging

The Search for Order in the Olfactory Bulb TEST YOURSELF 16.2

Demonstration: Tasting With

and Without the Nose

SOMETHING TO CONSIDER:

Community of the Senses Correspondences Influences

16.8 Representing Odors in the Cortex How Odorants Are Represented in
the Piriform Cortex How Odor Objects Are Represented in the Piriform
Cortex How Odors Trigger Memories

DEVELOPMENTAL DIMENSION:

Infant Chemical Sensitivity TEST YOURSELF 16.3 THINK ABOUT IT

Identification

Some Questions We Will Consider: ■■ Are there differences in the way
different people experi-

ence the taste of food? (p. 396) ■■ How is the sense of smell affected
by other senses like

vision and hearing? (p. 412) ■■ How does what a pregnant woman eats
affect the taste

preferences of her baby? (p. 414)

K

atherine Hansen always had such an acute sense of smell that she could
recreate any restaurant dish at home without the recipe, just by
recalling the scents and flavors (Rabin, 2021). But in March of 2020,
her sense of smell vanished, a condition called anosmia, followed by her
sense of taste, followed by the onset of coronavirus (COVID-19). Over

half of people who get COVID experience partial or total loss of smell
(olfaction) and taste (Parma et al., 2020; Pinna et al., 2020), caused
by mechanisms we will discuss later in the chapter (see page 399). Most
COVID patients regain their senses of smell and taste within a short
time, but a few, like Katherine, remain without these senses for long
periods. In Katherine's case, her senses of smell and taste were still
absent 10 months after they had vanished. Although smell and taste are
often thought of as "minor" senses, the effect of losing smell and taste
argues otherwise, as this loss is associated with dramatic effects on a
person's quality of life (Croy et al., 2013). In a study of the
experiences of 9,000 COVID patients who had lost their senses of smell
and taste, many said that they not only lost the pleasure of eating, but
also the pleasure of socializing, and reported feeling isolated and
detached from reality. One person put it this way:

389

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

"I feel alien from myself. It's also a kind of loneliness in the world,
like a part of me is missing, as I can no longer smell and experience
the emotions of everyday basic living" (Rabin, 2021). People who have
lost smell and taste, both from COVID and other causes, not only become
unmotivated to eat, which can lead to health problems (Beauchamp &
Mennella, 2011), but also become more prone to hazardous events, such as
food poisoning or failure to detect fire or leaking natural gas. In one
study, 45 percent of people with anosmia had experienced at least one
such hazardous event, compared to 19 percent of people with normal
olfactory function (Cameron, 2018; Santos et al., 2004). Molly Birnbaum
(2011), who lost her sense of smell after being hit by a car while
crossing the street, also noted the loss of everyday smells she had
taken for granted. She described New York City without smell as "a blank
slate without the aroma of car exhaust, hot dogs or coffee" and when she
gradually began to regain some ability to smell she reveled in every new
odor. "Cucumber!" she writes, "Their once common negligible scent had
returned---intoxicating, almost ambrosial. The scent of melon could
bring me to tears" (Birnbaum, 2011, p. 110). These descriptions help us
see that olfaction is more important in our lives than most of us
realize. Although it may not be essential to our survival, life is often
enhanced by our ability to smell and becomes a little more dangerous if
we lose the olfactory warning system that can alert us to danger.

16.1 Some Properties of the Chemical Senses The chemical senses involve
three components: taste, which occurs when molecules---often associated
with food---enter the mouth in solid or liquid form and stimulate
receptors on the tongue (Figure 16.1); olfaction, which occurs when
airborne molecules enter the nose and stimulate receptor neurons in the
olfactory mucosa, located on the roof of the nasal cavity; and flavor,
which is the impression we experience from the combination of taste and
olfaction. One property that distinguishes the chemical senses from
vision, hearing, and the cutaneous senses occurs right at the beginning
of the systems, when the receptors are being stimulated. For vision,
light stimulates rod and cone receptors inside the eyeball. For hearing,
pressure changes are transmitted to hair cells located deep inside the
cochlea. For the cutaneous senses, stimuli applied to the skin are
transmitted to receptors or nerve endings hidden under the skin. But for
taste and smell, molecules stimulate receptors that are exposed to the
environment. Because the receptors that serve taste and smell are
constantly exposed not only to the chemicals they are designed to sense
but also to harmful materials such as bacteria and dirt, they undergo a
cycle of birth, development, and death over 5--7 weeks for olfactory
receptors and 1--2 weeks for taste receptors. This constant renewal of
the receptors, called neurogenesis, is unique to these senses. 390

Olfactory mucosa Nasal cavity Olfactory stimuli

Nasal pharynx Oral cavity Tongue Taste stimuli

Retronasal route Pharynx

Figure 16.1 Odorant molecules released by food in the oral cavity and
pharynx can travel through the nasal pharynx (dashed arrow) to the
olfactory mucosa in the nasal cavity. This is the retronasal route to
the olfactory receptors, which will be discussed later in the chapter.

Because the stimuli responsible for tasting and smelling are taken into
the body, these senses are often seen as "gatekeepers" that (1) identify
things that the body needs for survival and that should therefore be
consumed and (2) detect things that would be bad for the body and that
should therefore be rejected. The gatekeeper function of taste and smell
is aided by a large affective, or emotional, component---things that are
bad for us often taste or smell unpleasant, and things that are good for
us generally taste or smell good. In addition to creating "good" and
"bad" affect, smelling an odor associated with a past place or event can
trigger memories, which in turn may create emotional reactions. In this
chapter, we will first consider taste and then olfaction. We will
describe the psychophysics and anatomy of each system and then how
different taste and smell qualities are coded in the nervous system.
Finally, we consider flavor, which results from the interaction of taste
and olfaction.

16.2 Taste Quality Everyone is familiar with taste. We experience it
every time we eat. (Although later in the chapter we will see that what
we usually experience when we eat is actually "flavor," which is a
combination of taste and olfaction.) Taste occurs when molecules enter
the mouth in solid or liquid form and stimulate taste receptors on the
tongue. The perceptions resulting from this stimulation have been
described in terms of five basic taste qualities. Most taste researchers
describe taste quality in terms of five basic taste sensations: salty,
sour, sweet, bitter, and umami (which has been described as meaty,
brothy, or savory, and is

Chapter 16  The Chemical Senses

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

often associated with the flavor-enhancing properties of MSG, monosodium
glutamate).

Basic Taste Qualities In an early experiment on taste quality, before
umami became the fifth basic taste, Donald McBurney (1969) presented
taste solutions to participants and asked them to make magnitude
estimates of the intensity of each of the four taste qualities for each
solution (see page 16 and Appendix B for descriptions of the magnitude
estimation procedure). He found that some substances have a predominant
taste quality and that other substances result in combinations of the
four taste qualities. For example, sodium chloride (salty), hydrochloric
acid (sour), sucrose (sweet), and quinine (bitter) are compounds that
come the closest to having only one of the four basic tastes. However,
the compound potassium chloride (KCl) has substantial salty and bitter
components, whereas sodium nitrate (NaNO3) results in a taste consisting
of a combination of salty, sour, and bitter (Figure 16.2). Results such
as these have led most researchers to accept the idea of basic tastes.
As you will see when we discuss the neural code for taste quality, most
of the research on this problem takes the idea of basic tastes as the
starting point. (Although Erickson, 2000, presents some arguments
against the idea of basic tastes.)

Connections Between Taste Quality and a Substance's Effect We noted that
taste and olfaction can be thought of as "gatekeepers." This is
especially true for taste because we often use KCI

20

16.3 The Neural Code for Taste Quality One of the central concerns in
taste research has been identifying the physiological code for taste
quality. We will first describe the structure of the taste system and
then will describe two proposals regarding how taste quality is coded in
this system.

10

Structure of the Taste System 0

Sour

Salty

0

Bitter

NaNO3

10

Sweet

Magnitude estimates

taste to choose which foods to eat and which to avoid (Breslin, 2001).
Taste accomplishes its gatekeeper function by the connection between
taste quality and a substance's effect. Sweetness is often associated
with compounds that have nutritive or caloric value and that are,
therefore, important for sustaining life. Sweet compounds cause an
automatic acceptance response and also trigger anticipatory metabolic
responses that prepare the gastrointestinal system for processing these
substances. Bitter compounds have the opposite effect---they trigger
automatic rejection responses to help the organism avoid harmful
substances. Examples of harmful substances that taste bitter are the
poisons strychnine, arsenic, and cyanide. Salty tastes often indicate
the presence of sodium. When people are deprived of sodium or lose a
great deal of sodium through sweating, they often seek out foods that
taste salty in order to replenish the salt their body needs. Although
there are many examples of connections between a substance's taste and
its function in the body, this connection is not perfect. People have
often made the mistake of eating good-tasting poisonous mushrooms, and
there are artificial sweeteners, such as saccharine and sucralose, that
have no metabolic value. There are also bitter foods that are not
dangerous and do have metabolic value. People can also learn to modify
their responses to certain tastes, as when they develop a taste for
foods they may have initially found unappealing, such as the bitter
tastes in beer and coffee.

Figure 16.2 The contribution of each of the four basic tastes to the
tastes of KCl and NaNO3, determined by the method of magnitude
estimation. The height of the line indicates the size of the magnitude
estimate for each basic taste. (From McBurney, 1969)

The process of tasting begins with the tongue (Figure 16.3a and Table
16.1). The surface of the tongue contains many ridges and valleys caused
by the presence of structures called papillae, which fall into four
categories: (1) filiform papillae, which are shaped like cones and are
found over the entire surface of the tongue, giving it its rough
appearance; (2) fungiform papillae, which are shaped like mushrooms and
are found at the tip and sides of the tongue (see Figure 16.4); (3)
foliate papillae, which are a series of folds along the back of the
tongue on the sides; and (4) circumvallate papillae, which are shaped
like flat mounds surrounded by a trench and are found at the back of the
tongue. All of the papillae except the filiform papillae contain taste
buds (Figures 16.3b and 16.3c), and the whole tongue contains about
10,000 taste buds (Bartoshuk, 1971). Because 16.3 The Neural Code for
Taste Quality

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

391

Circumvallate Bitter

Foliate

Sweet

Sour H+

Filiform

Salty Na+

- 

H Fungiform

(a) Tongue

Taste bud

Na+

(e) Receptor sites on tip of taste cell

(f) Fungiform papilla Taste pore

Taste cell

Nerve fibers (c) Taste bud

(d) Taste cell

Figure 16.3 (a) The tongue, showing the four different types of
papillae. (b) A fungiform papilla on the tongue; each papilla contains a
number of taste buds. (c) Cross section of a taste bud showing the taste
pore where the taste stimulus enters. (d) The taste cell; the tip of the
taste cell is positioned just under the pore. (e) Close-up of the
membrane at the tip of the taste cell, showing the receptor sites for
bitter, sweet, sour, and salty substances. Stimulation of these receptor
sites, as described in the text, triggers a number of different
reactions within the cell (not shown) that lead to movement of charged
molecules across the membrane, which creates an electrical signal in the
receptor.

the filiform papillae contain no taste buds, stimulation of the central
part of the tongue, which contains only these papillae, causes no taste
sensations. However, stimulation of the back or perimeter of the tongue
results in a broad range of taste sensations. Each taste bud contains 50
to 100 taste cells, which have tips that protrude into the taste pore
(Figure 16.3c). Transduction occurs when chemicals contact receptor
sites located on the tips of these taste cells (Figure 16.3d and 16.3e).
Electrical 392

signals generated in the taste cells are transmitted from the tongue
toward the brain in a number of different nerves: (1) the chorda tympani
nerve (from taste cells on the front and sides of the tongue); (2) the
glossopharyngeal nerve (from the back of the tongue); (3) the vagus
nerve (from the mouth and throat); and (4) the superficial petrosal
nerve (from the soft palette---the top of the mouth). The fibers from
the tongue, mouth, and throat make connections in the brain stem in the
nucleus of the solitary tract

Chapter 16  The Chemical Senses

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Table 16.1 Structures in the Taste System Structure

Description

Tongue

The receptor sheet for taste. Contains papillae and all of the other
structures described below.

Papillae

The structures that give the tongue its rough appearance. There are four
kinds, each with a different shape.

Taste buds

Contained on the papillae. There are about 10,000 taste buds.

Taste cells

Cells that make up a taste bud. There are a number of cells for each
bud, and the tip of each one sticks out into a taste pore. One or more
nerve fibers are associated with each cell.

Receptor sites

Sites located on the tips of the taste cells. There are different types
of sites for different chemicals. Chemicals contacting the sites cause
transduction by affecting ion flow across the membrane of the taste
cell.

Thalamus Frontal operculum Insula

Science Source

Temporal lobe

Figure 16.4 The surface of the tongue. The red dots are fungiform
papillae. (From Shahbake, 2008)

(Figure 16.5). From there, signals travel to the thalamus and then to
two areas in the frontal lobe that are considered to be the primary
taste cortex---the insula and the frontal operculum---which are
partially hidden behind the temporal lobe (Finger, 1987; Frank & Rabin,
1989).

Population Coding In Chapter 2 we distinguished between two types of
coding: specificity coding, the idea that quality is signaled by the
activity in individual neurons that are tuned to respond to specific
qualities; and population coding, the idea that quality is signaled by
the pattern of activity distributed across many neurons. In that
discussion, and in others throughout the book, we have generally favored
population coding. The situation for taste, however, is not clear-cut,
and there are arguments in favor of both types of coding (Frank et al.,
2008). Let's consider some evidence for population coding. Robert
Erickson (1963) conducted one of the first experiments that demonstrated
this type of coding by presenting a number of different taste stimuli to
a rat's tongue and recording the response of the chorda tympani nerve.
Figure 16.6 shows how 13 nerve fibers responded to ammonium chloride
(NH4Cl), potassium chloride (KCl), and sodium chloride (NaCl). Erickson

Chorda tympani nerve Glossopharyngeal nerve Vagus nerve

Nucleus of the solitary tract

Figure 16.5 The central pathway for taste signals, showing the nucleus
of the solitary tract, where nerve fibers from the tongue and the mouth
synapse in the medulla at the base of the brain. From the nucleus of the
solitary tract, these fibers synapse in the thalamus and then the insula
and frontal operculum, which are the cortical areas for taste. (From
Frank & Rabin, 1989)

called these patterns the across-fiber patterns, which is another name
for population coding. The red and green lines show that the
across-fiber patterns for ammonium chloride and potassium chloride are
similar to each other but different from the pattern for sodium
chloride, indicated by the open circles. Erickson reasoned that if the
rat's perception of taste quality depends on the across-fiber pattern,
then two substances with similar patterns should taste similar. Thus,
the electrophysiological results would predict that ammonium chloride
and potassium chloride should taste similar and that both should taste
different from sodium chloride. To test this 16.3 The Neural Code for
Taste Quality

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

393

NH 4 CI KCI NaCI

60 40 30

Normal mouse 0 220 240

Mouse with PTC receptor

260 280

2100

20

0

10

100 PTC concentration

10 0

A

B C D E F G H I J K Chorda tympani fibers (rat)

L M

Figure 16.6 Across-fiber patterns of the response of fibers in the rat's
chorda tympani nerve to three salts. Each letter on the horizontal axis
indicates a different single fiber. (Based on Erickson, 1963)

hypothesis, Erickson shocked rats while they were drinking potassium
chloride and then gave them a choice between ammonium chloride and
sodium chloride. If potassium chloride and ammonium chloride taste
similar, the rats should avoid the ammonium chloride when given a
choice. This is exactly what they did. And when the rats were shocked
for drinking ammonium chloride, they subsequently avoided the potassium
chloride, as predicted by the electrophysiological results. But what
about the perception of taste in humans? When Susan Schiffman and Robert
Erickson (1971) asked humans to make similarity judgments between a
number of different solutions, they found that substances that were
perceived to be similar were related to patterns of firing for these
same substances in the rat. Solutions judged more similar
psychophysically had similar patterns of firing, as population coding
would predict.

Specificity Coding Most of the evidence for specificity coding comes
from research that has recorded neural activity early in the taste
system. We begin at the receptors by describing experiments that have
revealed receptors for sweet, bitter, and umami. The evidence supporting
the existence of receptors that respond specifically to a particular
taste has been obtained by using genetic cloning, which makes it
possible to add or eliminate specific receptors in mice. Ken Mueller and
coworkers (2005) did a series of experiments using a chemical compound
called PTC that tastes bitter to humans but is not bitter to mice. The
lack of bitter PTC taste in mice is inferred from the fact that mice do
not avoid even high concentrations of PTC in behavioral tests (blue
curve in Figure 16.7). Because 394

20 Change in licking (%)

Impulses in first second of activity

0.1M 0.3M 0.1M

Figure 16.7 Mouse behavioral response to PTC. The blue curve indicates
that a normal mouse will consume PTC even in high concentrations. The
red curve indicates that a mouse that has a human bitter-PTC receptor
avoids PTC, especially at high concentrations. (Adapted from Mueller et
al., 2005)

a specific receptor in the family of bitter receptors had been
identified as being responsible for the bitter taste of PTC in humans,
Mueller decided to see what would happen if he used genetic cloning
techniques to create a strain of mice that had this human bitter-PTC
receptor. When he did this, the mice with this receptor avoided high
concentrations of PTC (red curve in Figure 16.7; see Table 16.2a). In
another experiment, Mueller created a strain of mice that lacked a
bitter receptor that responds to a compound called cyclohexamide (Cyx).
Mice normally have this receptor, so they avoid Cyx. But the mice
lacking this receptor did not avoid Cyx (Table 16.2b). In addition, Cyx
no longer caused any firing in nerves receiving signals from the tongue.
Therefore, when the taste receptor for a substance is eliminated, this
is reflected in both nerve firing and the animal's behavior. It is
important to note that in all these experiments, adding or eliminating
bitter receptors had no effect on neural firing or behavior to sweet,
sour, salty, or umami stimuli. Other research using similar techniques
has identified receptors for sugar and umami (Zhao et al., 2003). The
results of these experiments in which adding a receptor makes an animal
sensitive to a specific quality and eliminating a receptor makes an
animal insensitive to a specific quality have been cited as support for
specificity coding---that there Table 16.2 Results of Mueller's
Experiments Chemical

Normal Mouse

Cloned Mouse

(a) PTC

No PTC receptor

Has PTC receptor

Doesn't avoid PTC

Avoids PTC

Has Cyx receptor

No Cyx receptor

Avoids Cyx

Doesn't avoid Cyx

(b) Cyx

Chapter 16  The Chemical Senses

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

are receptors that are specifically tuned to sweet, bitter, and umami
tastes. However, not all researchers agree that the picture is so
clear-cut. For example, Eugene Delay and coworkers (2006) showed that
with different behavioral tests, mice that appeared to have been made
insensitive to sugar by eliminating a "sweet" receptor can actually
still show a preference for sugar. Based on this result, Delay suggests
that perhaps there are a number of different receptors that respond to
specific substances like sugar. Another line of evidence for speciﬁcity
coding in taste has come from research on how single neurons respond to
taste stimuli. Recordings from neurons at the beginning of the taste
systems of animals, ranging from rats to monkeys, have revealed neurons
that are specialized to respond to specific stimuli, as well as neurons
that respond to a number of different types of stimuli (Lundy &
Contreras, 1999; Sato et al., 1994; Spector & Travers, 2005). Figure
16.8 shows how three neurons in the rat taste system respond to sucrose
(sweet to humans), sodium chloride (NaCl; salty), hydrochloric acid
(HCl; sour in low concentrations), and quinine (QHCl; bitter) (Lundy &
Conteras, 1999). The neuron in Figure 16.8a responds selectively to
sucrose, the one in Figure 16.8b responds selectively to NaCl, and the
neuron in Figure 16.8c responds to NaCl, HCl, and QHCl. Neurons like the
ones in Figures 16.8a and 16.8b, which respond selectively to stimuli
associated with sweetness (sucrose) and saltiness (NaCl), provide
evidence for specificity coding. Neurons have also been found that
respond selectively to sour (HCl) and bitter (QHCl) (Spector & Travers,
2005). Another ﬁnding in line with speciﬁcity theory is the effect of
presenting a substance called amiloride, which blocks the ﬂow of sodium
into taste receptors. Applying amiloride to the tongue causes a decrease
in the responding of neurons in the rat's brainstem (nucleus of the
solitary tract) that respond best to salt (Figure 16.9a) but has little
effect on neurons that respond best to a combination of salty and bitter
tastes (Figure 16.9b; Scott & Giza, 1990). Thus, eliminating the ﬂow of
sodium across the membrane selectively eliminates responding of
salt-best neurons but does not affect the response of neurons that
respond best to other tastes. As it turns out, the sodium channel that
is blocked by amiloride is important for determining saltiness in rats
and other animals, but not in humans. More recent research has identiﬁed
another channel that serves the salty taste in humans (Lyall et al.,
2004, 2005). What does all of this mean? The results of the experiments
involving cloning, recording from single neurons, and the effect of
amiloride seem to be shifting the balance in the population versus
speciﬁcity argument toward speciﬁcity (Chandrashekar et al., 2006).
However, the issue is still not settled. For example, David Smith and
Thomas Scott (2003) argue for population coding based on the finding
that at more central locations in the taste system, neurons are tuned
broadly, with many neurons responding to more than one taste quality.
Smith and coworkers (2000) point out that just because there are neurons
that respond best to one compound like salty or sour, this doesn't mean
that these tastes are signaled by just

Sucrose-selective neuron Sucrose

NaCl

HCl

QHCl

(a) NaCl-selective neuron Sucrose

NaCl

HCl

QHCl

(b) Neuron responds to NaCl, HCl, and QHCl Sucrose

NaCl

HCl

QHCl

(c) 

Figure 16.8 Responses of three neurons recorded from the cell bodies of
chorda tympani nerve fibers in the rat. Solutions of sucrose, salt
(NaCl), hydrochloric acid (HCl), and quinine hydrochloride (QHCl) were
flowed over the rat's tongue for 15 seconds, as indicated by the
horizontal lines below the firing records. Vertical lines are individual
nerve impulses. (a) Neuron responds selectively to sweet stimulus; (b)
neuron responds selectively to salt; (c) neuron responds to salty, sour,
and bitter stimuli. (From Lundy & Contreras, 1999)

one type of neuron. They illustrate this by drawing an analogy between
taste perception and the mechanism for color vision. Even though
presenting a long-wavelength light that appears red may cause the
highest activation in the long-wavelength cone pigment (see Figure 9.15
page 206), our perception of red still depends on the combined response
of both the longand medium-wavelength pigments. Similarly, salt stimuli
may cause high firing in neurons that respond best to salt, but other
neurons are probably also involved in creating saltiness. Because of
arguments such as this, some researchers believe that even though there
is good evidence for specific taste receptors, population coding is
involved in determining taste as well, especially at higher levels of
the system. One suggestion is that basic taste qualities might be
determined by a specific code, but population coding could determine
subtle differences 16.3 The Neural Code for Taste Quality

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

395

This interesting fact about cats has something to tell us about human
taste perception, because it turns out that there are genetic
differences that affect people's ability to sense the taste of certain
substances. One of the best-documented effects involves people's ability
to taste the bitter substance phenylthiocarbamide (PTC), which we
discussed earlier in connection with Mueller's experiments on
specificity coding (see page 394). The discovery of the PTC effect has
been described as follows:

60 Spikes in 5 sec

50

Salt response

40 30 20 10 0 N1 N2 N3 N4 L M K Ca Q Cl H S G Sa P X

(a) 70

Spikes in 5 sec

60

Salt response

50 40 30 20 10 0 N1 N2 N3 N4 L M K Ca Q Cl H S G Sa P X

(b) 

Salt

Bitter/Sour

Sweet

Figure 16.9 The blue dashed lines show how two neurons in the rat's
nucleus of the solitary tract respond to a number of different taste
stimuli (along the horizontal axis). The neuron in (a) responds strongly
to compounds associated with salty tastes. The neuron in (b) responds to
a wide range of compounds. The solid red lines show how these two
neurons fire after the sodium-blocker amiloride is applied to the
tongue. This compound inhibits the responses of the neuron that responds
to salt (a) but has little effect on neuron (b). (Adapted from Scott &
Giza, 1990)

between tastes within a category (Pfaffmann, 1974; Scott &
Plata-Salaman, 1991). This would help explain why not all substances in
a particular category have the same taste. For example, the taste of all
sweet substances is not identical (Lawless, 2001).

16.4 Individual Differences in Taste The "taste worlds" of humans and
animals are not necessarily the same. For example, domestic cats, unlike
most mammals, don't prefer the sweetness of sugar, even though they
display human-like taste behavior to other compounds, such as avoiding
compounds that taste bitter or very sour to humans. Genetic research has
shown that this "sweet blindness" occurs because cats lack a functional
gene for formation of a sweet receptor and so, lacking a sweet receptor,
have no mechanism for detecting sweetness (Li et al., 2005). 396

The different reactions to PTC were discovered accidentally in 1932 by
Arthur L. Fox, a chemist working at the E. I. DuPont deNemours Company
in Wilmington, Delaware. Fox had prepared some PTC, and when he poured
the compound into a bottle, some of the dust escaped into the air. One
of his colleagues complained about the bitter taste of the dust, but
Fox, much closer to the material, noticed nothing. Albert F. Blakeslee,
an eminent geneticist of the era, was quick to pursue this observation.
At a meeting of the American Association for the Advancement of Science
(AAAS) in 1934, Blakeslee prepared an exhibit that dispensed PTC
crystals to 2,500 of the conferees. The results: 28 percent of them
described it as tasteless, 66 percent as bitter, and 6 percent as having
some other taste. (Bartoshuk, 1980, p. 55) People who can taste PTC are
described as tasters, and those who cannot are called nontasters.
Additional experiments have also been done with a substance called
6-n-propylthiouracil, or PROP, which has properties similar to those of
PTC (Lawless, 1980, 2001). Researchers have found that about one-third
of Americans report that PROP is tasteless and two-thirds can taste it.
What causes these differences in people's ability to taste PROP? One
explanation for these differences is that people who can taste PROP have
higher densities of taste buds than those who can't taste it (Bartoshuk
& Beauchamp, 1994) (Figure 16.10). A factor that determines individual
differences in taste, in addition to receptor density, is the presence
of specialized receptors. Advances in genetic techniques have made it
possible to determine the locations and identities of genes on human
chromosomes that are associated with taste and smell receptors. These
studies have shown that PROP and PTC tasters have specialized receptors
that are absent in nontasters (Bufe et al., 2005; Kim et al., 2003).
What does this mean for everyday taste experience? If PROP tasters also
perceived other compounds as being more bitter than nontasters, then
certain foods might taste more bitter to the tasters. The evidence on
this question, however, has been mixed. Some studies have reported
differences between how tasters and nontasters rate the bitterness of
other compounds (Bartoshuk, 1979; Hall et al., 1975), and others have
not observed this difference (Delwiche et al., 2001b). However, it does
appear that people who are especially sensitive to PROP, called
supertasters, may actually be more sensitive to most bitter substances,
as if the amplification in the bitter

Chapter 16  The Chemical Senses

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Linda Bartoshuk

(a) 
(b) 

Figure 16.10 (a) Video micrograph of the tongue showing the fungiform
papillae of a "supertaster"---a person who is very sensitive to the
taste of PROP. (b) Papillae of a "nontaster"---someone who cannot taste
PROP. The supertaster has both more papillae and more taste buds than
the nontaster. (Courtesy of Linda Bartoshuk)

taste system is turned up for all bitter compounds (Delwiche et al.,
2001a). But the research on PROP nontasters and supertasters has turned
out to be just the tip of the iceberg with regard to individual
differences. For example, genetic differences between individuals have
also been linked to differences in the perception of the sweetness of
sucrose (Fushan et al., 2009). Thus, the next time you disagree with
someone about the taste of a particular food, don't automatically assume
that your disagreement is simply a reflection of your individual
preferences. It may reflect not a difference in preference (you like
sweet things more than John does) but a difference in perception (you
perceive sweet tastes as more intense than John does), which could be
caused by differences in the types and numbers of taste receptors on the
tongue or other differences in your taste systems. TEST YOuRSELF 16.1 1.
What is anosmia? How does anosmia change people's life experience? 2.
What are some ways that the chemical senses differ from vision, touch,
and the cutaneous senses? 3. What is neurogenesis, and what function
does it serve? 4. What are the five basic taste qualities? 5. How is
taste quality linked to a substance's physiological effect? 6. Describe
the anatomy of the taste system, including the receptors and central
destinations. 7. What is the evidence for population coding and
specificity coding in taste? Is it possible to choose between the two?
8. What kinds of evidence support the idea that different people may
have different taste experiences? What mechanisms may be responsible for
these differences?

16.5 The Importance of Olfaction At the beginning of the chapter we
noted that taste and olfaction have often been described as being less
important than vision and hearing. The importance of olfaction has also
been minimized in many textbooks, which describe human olfaction as
being microsmatic (having a poor sense of smell that is not crucial to
survival), while describing olfaction in other animals, and especially
dogs, as macrosmatic---having a welldeveloped sense of smell (McGann,
2017). But recent measurements of the sensitivity of humans and animals
to different odors indicates that humans are more sensitive to many
odors than a wide range of animals, including mice, monkeys, rabbits,
and seals. And although dogs are far more sensitive than humans to some
odors, human's sensitivity equals dog's for others (Laska, 2017).
Caroline Bushdid and coworkers (2014) tested participants to determine
how many components of a substance they could change before they could
detect the difference between two substances. Based on their results,
plus an estimate of the number of possible odors, they proposed that
humans can discriminate the difference in the smells of more than 1
trillion olfactory stimuli. Other researchers have questioned this
number, saying it is too high (Gerkin & Castro, 2015; Meister, 2015).
But even if this is an overestimate, human olfaction is extremely
impressive, especially when compared to vision (we can discriminate
several million different colors) and hearing (we can discriminate
almost half a million different tones), and to make the case for human
olfaction more even convincing, it has been shown that humans, like
dogs, can track scents in a field (Porter et al., 2007). This recent
evidence has led one olfaction researcher to state that "...contrary to
traditional textbook wisdom, humans are not generally inferior in their
olfactory sensitivity 16.5 The Importance of Olfaction

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

397

compared to animals" (Laska, 2017), and another one to state that "our
sense of smell is much more important than we think" (McGann, 2017). In
the next section we will look further at some of our olfactory
abilities.

16.6 Olfactory Abilities How well can we smell? We've already noted that
human sensitivity to odors can rival that of many animals. We will now
look at sensitivity in more detail and then will consider how well we
can identify odors.

Detecting Odors Our sense of smell enables us to detect extremely low
concentrations of some odorants. The detection threshold for odors is
the lowest concentration at which an odorant can be detected. One method
for measuring detection thresholds is the forced-choice method, in which
participants are presented with blocks of two trials---one trial
contains a weak odorant and the other, no odorant. The participant's
task is to indicate which trial has a stronger smell. The threshold is
determined by measuring the concentration that results in a correct
response on 75 percent of the trials (50 percent would be chance
performance). Table 16.3 lists thresholds for a number of substances. It
is notable that there is a very large range of thresholds. T-butyl
mercaptan, the odorant that is added to natural gas to warn people of
gas leaks, can be detected in very small concentrations of less than 1
part per billion in air. In contrast, to detect the vapors of acetone
(the main component of nail polish remover), the concentration must be
15,000 parts per billion, and for the vapor of methanol, the
concentration must be 141,000 parts per billion.

Identifying Odors One of the more intriguing facts about odors is that
even though humans can discriminate millions or perhaps trillions of
different odors, they often find it difficult to accurately identify
specific odors. For example, when people are presented Table 16.3 Human
Odor Detection Thresholds Compound

Odor Threshold in Air (parts per billion)

Methanol

141,000

Acetone

15,000

Formaldehyde

870

Menthol

40

T-butyl mercaptan

0.3

Source: Devos et al., 1990.

398

with the odors of familiar substances such as mint, bananas, and motor
oil, they can easily tell the difference between them. However, when
they are asked to identify the substance associated with the odor, they
are successful less than half the time (Desor & Beauchamp, 1974; Engen &
Pfaffmann, 1960). However, when people are trained in identifying odors
by being told the names of substances when they first smell them and
then being reminded of the correct names if they fail to name them
correctly on subsequent presentations, they can eventually identify 98
percent of the substances (Desor & Beauchamp, 1974). One of the amazing
things about odor identification is that knowing the correct label for
the odor actually seems to transform our perception into that odor. I
had this experience a number of years ago when sampling the drink
aquavit with some friends. Aquavit has a very interesting but difficult
to identify smell. Odors such as "anise," "orange," and "lemon" were
proposed as we tried to identify its smell, but it wasn't until someone
turned the bottle around and read the label on the back that the truth
became known: "Aquavit (Water of Life) is the Danish national drink---a
delicious, crystal-clear spirit distilled from grain, with a slight
taste of caraway." When we heard the word caraway, the previous
hypotheses of anise, orange, and lemon were transformed into caraway.
Thus, when we have trouble identifying odors, this trouble may occur not
because of a deficiency in our olfactory system, but from an inability
to retrieve the odor's name from our memory (Cain, 1979, 1980).
DEMONSTRATION

Naming and Odor Identification

To demonstrate the effect of naming substances on odor identification,
have a friend collect a number of familiar objects for you and, without
your looking, try to identify the odors your friend presents. You will
find that you can identify some but not others, but when your friend
tells you the answers for the ones you were unable to identify
correctly, you will wonder how you could have failed to identify such a
familiar smell. Don't blame your mistakes on your nose; blame them on
your memory.

Individual Differences in Olfaction We noted earlier that there are
people with anosmia who have lost their sense of smell (p. 389). There
are also genetic conditions which cause selective losses of some smells.
For example, a section of the human chromosome is associated with
receptors that are sensitive to the chemical β-ionone, which is often
added to foods and beverages to add a pleasant floral note. Individuals
sensitive to β-ionone describe paraffin with low concentrations of
β-ionone added as "fragrant" or "floral," whereas individuals with less
sensitivity to β-ionone describe the same stimulus as "sour," "pungent,"
or "acid" (Jaeger et al., 2013). Genetically caused variation in
sensitivity occurs for many other chemicals, as well, leading to the
idea that everyone experiences his or her own unique "flavor world"
(McRae et al., 2013).

Chapter 16  The Chemical Senses

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Loss of Smell in COVID-19 and Alzheimer's Disease The recent COVID-19
pandemic, which as I'm writing this in the fall of 2020 is still raging
throughout the world, has, among its symptoms, a loss of taste and smell
in a majority of patients (Joffily et al., 2020; Sutherland, 2020). The
reason for this loss is under intensive investigation. One explanation
that has been proposed is that COVID molecules attach to an enzyme
called ACE2 that is found in the intestines, lungs, arteries, and heart,
and which has recently been found in the nose. Figure 16.11 shows that
ACE2 is found on the surface of sustentacular cells, which provide
metabolic and structural support to the olfactory sensory neurons
(Bilinska et al., 2020). It has been proposed, therefore, that COVID-19
causes loss of smell not by directly attacking sensory neurons but by
affecting their supporting cells. Exactly how this causes a loss of
smell is still being investigated. The importance of the COVID-induced
loss of smell for diagnostic purposes is that it is so common in people
with COVID that some researchers have recommended loss of smell

Nasal cavity

Coronavirus

Olfactory cilia

Olfactory epithelium

ACE2

ACE2

Sustentacular cells

No ACE2

No ACE2

as a diagnostic test because it may be a more reliable indicator of the
disease than fever or other symptoms (Sutherland, 2020). Loss of smell
is also associated with Alzheimer's disease (AD), a serious loss of
memory and other cognitive functions that is often preceded by mild
cognitive impairment (MCI), which has affected 50 million people
worldwide (Bathini et al., 2019). But an important difference between
the olfactory losses associated with AD and COVID-19 is that in AD the
loss of olfaction begins occurring decades before the occurrence of
clinical symptoms such as memory loss and difficulties in reasoning
(Bathini et al., 2019; Devanand et al., 2015). This is shown in Figure
16.12, which tracks the progression of the loss of cognition that is the
primary symptom of AD (purple curve) and the progression of "biomarkers"
associated with AD (Bathini et al., 2019). Notice that the curves for
the biomarkers start rising much earlier than the curve for the loss of
cognition. The fastest biomarker curve is for Amyloid B (red curve),
which is associated with formation of plaques in the brain, and the
next-fastest curve is for loss of olfaction (dashed curve). Abnormal
olfactory functioning rises very rapidly during the preclinical phase
(yellow shading), and is very high before symptoms of MCI and
Alzheimer's begin appearing. This property has led to the proposal that
measuring olfactory function is a way to achieve early diagnosis of
Alzheimer's, which, in turn, would make it possible to start treatment
earlier. (Although there is no cure for AD, treatments are being
developed that may slow the development of clinical symptoms.) Another
difference between olfactory loss in AD and in COVID-19 is that the
olfactory system may suffer from more widespread attack in AD. There is
evidence that AD attacks not only the olfactory bulb, but more central
structures, as well. The key conclusion is that the olfactory system
appears to be much more sensitive than the visual system or auditory
system to neural dysfunction. Thus, although some visual loss precedes
AD symptoms, the loss of olfactory function is the key sensory biomarker
for predicting development of AD.

Olfactory nerves

Smell sensation

Another example of individual differences in smell is that the smell of
the steroid androsterone, which is derived from testosterone, is
described negatively ("sweaty," "urinous") by some people, positively by
some people ("sweet," "floral"), and as having no odor by others (Keller
et al., 2007). Or consider the fact that after eating asparagus some
people's urine takes on a smell that has been described as sulfurous,
much like cooked cabbage (Pelchat et al., 2011). Some people, however,
can't detect this smell. As noted at the beginning of the chapter,
decreases in olfaction is one of the symptoms of the viral infection
COVID-19. In addition, it is a predictor of Alzheimer's disease.

Olfactory neurons

Figure 16.11 Current research on the coronavirus indicates that it
attaches to an enzyme, ACE2, which is found in sustentacular cells,
which support the olfactory neurons. 16.6 Olfactory Abilities

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

399

Abnormal

Amyloid b Olfaction Synaptic transmission Tau ND Brain volume Cognition

Normal

Figure 16.12 Biomarkers in the progression of dementia associated with
Alzheimer's disease. The purple line plots the progression of cognitive
decline over time. The dashed line plots the progression of loss of
olfaction, which precedes cognitive decline. The area shaded yellow
indicates the preclinical stage, in which there are no symptoms of
cognitive decline. (From Bathini et al., 2019)

MCI

Preclinical

A related finding is that loss of olfaction is also associated with a
higher risk of death. Jayant Pinto and coworkers (2014) found that in a
group of older adults (57--87 years old), who were representative of the
general U.S. population, people with anosmia (loss of smell) were three
times more likely to die within five years than people with normal
smell.

16.7 Analyzing Odorants: The Mucosa and Olfactory Bulb We have, so far,
been describing the functions of olfaction and the experiences that
occur when olfactory stimuli, molecules in the air, enter the nose. We
now consider the question of how the olfactory system knows what
molecules are entering the nose. The first step toward answering this
question is to consider some of the difficulties facing researchers who
are searching for connections between molecules and perception.

chemical α-ionone, they usually say that it smells like violets. This
description, it turns out, is fairly accurate, but if you compare
α-ionone to real violets, they smell different. The perfume industry's
solution is to use names such as "woody violet" and "sweet violet" to
distinguish between different violet smells, but this hardly solves the
problem we face in trying to determine how olfaction works. Another
difficulty in relating odors to molecular properties is that some
molecules that have similar structures can smell different (Figure
16.13a), and molecules that have very different structures can smell
similar (Figure 16.13b). But things really become challenging when we
consider the kinds of odors we routinely encounter in the environment,
which consist of mixtures of many chemicals. Consider, for example, that
when you walk into the kitchen and smell freshly brewed coffee, the
coffee aroma is created by more than 100 different molecules. Although

The Puzzle of Olfactory Quality Although we know that we can
discriminate among a huge number of odors, research to determine the
neural mechanisms behind this ability is complicated by difficulties in
establishing a system to bring some order to our descriptions of odor
quality. Such systems exist for other senses. We can describe visual
stimuli in terms of their colors and can relate our perception of color
to the physical property of wavelength. We can describe sound stimuli as
having different pitches and relate these pitches to the physical
property of frequency. Creating a way to organize odors and to relate
odors to physical properties of molecules, however, has proven extremely
difficult. One reason for the difficulty is that we lack a specific
language for odor quality. For example, when people smell the 400

Dementia

Musk

C

O

No odor

CH3

CH2 CH3

(a) OH

O

CH3

CH3

O

O O

Both pineapple (b)

Figure 16.13 (a) Two molecules that have the same structure, but one
smells like musk and the other is odorless. (b) Two molecules with
different structures but similar odors.

Chapter 16  The Chemical Senses

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

individual molecules may have their own odors, we don't perceive the
odors of individual molecules; we perceive "coffee." The feat of
perceiving "coffee" becomes even more amazing when we consider that
odors rarely occur in isolation. Thus, the coffee odor from the kitchen
might be accompanied by the smells of bacon and freshly squeezed orange
juice. Each of these has its own tens or hundreds of molecules, yet
somehow the hundreds of different molecules that are floating around in
the kitchen become perceptually organized into smells that refer to
three different sources: coffee, bacon, and orange juice (Figure 16.14).
Sources of odors such as coffee, bacon, and orange juice, as well as
nonfood sources such as rose, dog, and car exhaust, are called odor
objects. Our goal, therefore, is to explain not just how we smell
different odor qualities, but how we identify different odor objects.
Perceiving odor objects involves olfactory processing that occurs in two
stages. The first stage, which takes place at the beginning of the
olfactory system in the olfactory mucosa and olfactory bulb, involves
analyzing. In this stage, the olfactory system analyzes the different
chemical components of odors and transforms these components into neural
activity at specific places in the olfactory bulb (Figure 16.15). The
second stage, which takes place in the olfactory cortex and beyond,
involves synthesizing.

"Coffee" "OJ"

"Bacon"

In this stage, the olfactory system synthesizes the information about
chemical components received from the olfactory bulb into
representations of odor objects. As we will see, it has been proposed
that this synthesis stage involves learning and memory. But let's start
at the beginning, when odorant molecules enter the nose and stimulate
receptors on the olfactory mucosa.

The Olfactory Mucosa The olfactory mucosa is a dime-sized region located
on the roof of the nasal cavity just below the olfactory bulb (Figure
16.15a). Odorant molecules are carried into the nose in an air stream
(blue arrows), which brings these molecules into contact with the
mucosa. Figure 16.15b shows the olfactory receptor neurons (ORNs) that
are located in the mucosa (colored parts) and the supporting cells (tan
area). Just as the rod and cone receptors in the retina contain visual
pigment molecules that are sensitive to light, the olfactory receptor
neurons in the mucosa are dotted with molecules called olfactory
receptors that are sensitive to chemical odorants (Figure 16.15c). One
parallel between visual pigments and olfactory receptors is that they
are both sensitive to a specific range of stimuli. Each type of visual
pigment is sensitive to a band of wavelengths in a particular region of
the visible spectrum (see Figure 9.13 page 205), and each type of
olfactory receptor is sensitive to a narrow range of odorants. However,
an important difference between the visual system and the olfactory
system is that while there are only four different types of visual
pigments (one rod pigment and three cone pigments), there are about 400
different types of olfactory receptors, each sensitive to a particular
group of odorants. The discovery that there are 350 to 400 types of
olfactory receptors in the human and 1,000 types in the mouse was made
by Linda Buck and Richard Axel (1991), who received the 2004 Nobel Prize
in Physiology and Medicine for their research on the olfactory system
(also see Buck, 2004). The large number of olfactory receptor types
increases the challenges in understanding how olfaction works. One thing
that makes things slightly simpler is another parallel with vision: Just
as a particular rod or cone receptor contains only one type of visual
pigment, a particular olfactory receptor neuron (ORN) contains only one
type of olfactory receptor.

How Olfactory Receptor Neurons Respond to Odorants

Figure 16.14 Hundreds of molecules from the coffee, orange juice, and
bacon are mixed together in the air, but the person just perceives
"coffee," "orange juice," and "bacon." This perception of three odor
objects from hundreds of intermixed molecules is a feat of perceptual
organization.

Figure 16.16a shows the surface of part of the olfactory mucosa. The
circles represent ORNs, with two types of ORNs highlighted in red and
blue. Remember that there are 400 different types of ORNs in the mucosa
in humans. There are about 10,000 of each type of ORN, so the mucosa
contains millions of ORNs. The first step in understanding how we
perceive different odorants is to ask how this array of millions of ORNs
that blanket the olfactory mucosa respond to different odorants. One way
this question has been answered is by using a technique called calcium
imaging. 16.7 Analyzing Odorants: The Mucosa and Olfactory Bulb

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

401

Olfactory bulb Olfactory mucosa

Olfactory bulb

Bone

(f) Signals sent to higher cortical areas

Olfactory receptor neurons

(e) Signals sent to glomeruli in olfactory bulb

Olfactory mucosa

(d) Olfactory receptor neurons activated

Glomerulus

(a) Odorants enter nose

(b) Odorants flow over mucosa

Olfactory receptor neuron Olfactory receptor

(c) Olfactory receptors stimulated Air with odorant molecules

Figure 16.15 The structure of initial structures in the olfactory
system. (a) Odorant molecules enter the nose, and then (b) flow over the
olfactory mucosa, which contains 350 different types of olfactory
receptor neurons (ORNs). (c) Stimulation of receptors in the ORNs (d)
activates the ORNs. Three types of ORNs are shown here, indicated by
different colors. Each type has its own specialized receptors. (e)
Signals from the ORNs are then sent to glomeruli in the olfactory bulb,
and then (f) to higher cortical areas.

METHOD

Calcium Imaging

When an olfactory receptor responds, the concentration of calcium ions
(Ca11) increases inside the OR. Calcium imaging measures this increase
in calcium ions by soaking olfactory neurons in a chemical that causes
the ORN to fluoresce with a green glow when exposed to ultraviolet (380
nm) light. This green glow can be used to measure how much Ca11 had
entered the neuron because increasing Ca11 inside the neuron decreases
the glow. Thus, measuring the decrease in fluorescence indicates how
strongly the ORN is activated.

402

Bettina Malnic and coworkers (1999) determined the response to a large
number of odorants using calcium imaging. The results for a few of her
odorants are shown in Figure 16.17, which indicates how 10 different
ORNs are activated by each odorant. (Remember that each ORN contains
only one type of olfactory receptor.) The response of individual
receptors is indicated by the circles in each column. Reading down the
columns indicates that each of the receptors, except 19 and 41, responds
to some odorants but not to others. The pattern of activation for each
odorant, which is indicated by reading across each

Chapter 16  The Chemical Senses

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Glomerulus

Olfactory receptor neuron (ORN)

(a) Olfactory mucosa

(b) Olfactory bulb

Figure 16.16 (a) A portion of the olfactory mucosa. The mucosa contains
400 types of ORNs and about 10,000 of each type. The red circles
represent 10,000 of one type of ORN, and the blue circles, 10,000 of
another type. (b) All ORNs of a particular type send their signals to
one or two glomeruli in the olfactory bulb.

row, is called the odorant's recognition profile. For example, the
recognition profile of octanoic acid is weak firing of ORN 79 and strong
firing of ORNs 1, 18, 19, 41, 46, 51, and 83, whereas the profile for
octanol is strong firing of ORNs 18, 19, 41, and 51. From these
profiles, we can see that each odorant causes a different pattern of
firing across ORNs. Also, odorants that have similar structures (shown
on the right in Figure 16.17), such as octanoic acid and nonanoic acid,
often have similar profiles. We can also see, however, that this doesn't
always occur (compare the patterns for bromohexanoic acid and
bromooctanoic acid, which also have similar structures). Olfactory
Receptors

Odorants

1

18

19

41

46

51

79

83

85

86

Octanoic acid

O

Nonanoic acid

O

OH

OH

Octanol

OH

Bromohexanoic acid Bromooctanoic acid

BR

O OH

BR

O OH

Figure 16.17 Recognition profiles for some odorants. Large dots indicate
that the odorant causes a high firing rate for the receptor listed along
the top; a small dot indicates a lower firing rate for the receptor. The
structures of the compounds are shown on the right. (Adapted from Malnic
et al., 1999)

Remember that one of the puzzling facts about odor perception is that
some molecules have similar structures but smell different (Figure
16.13a). When Malnic compared such molecules, she found that these
molecules had different recognition profiles. For example, octanoic acid
and octanol differ only by one oxygen molecule, but the smell of octanol
is described as "sweet," "rose," and "fresh," whereas the smell of
octanoic acid is described as "rancid," "sour," and "repulsive." This
difference in perception is reflected in their different profiles.
Although we still can't predict which smells result from specific
patterns of response, we do know that when two odorants smell different,
they usually have different profiles. The idea that an odorant's smell
can be related to different response profiles is similar to the
trichromatic code for color vision that we described in Chapter 9 (see
page 204). Remember that each wavelength of light is coded by a
different pattern of firing of the three cone receptors, and that a
particular cone receptor responds to many wavelengths. The situation for
odors is similar---each odorant is coded by a different pattern of
firing of ORNs, and a particular ORN responds to many odorants. What's
different about olfaction is that there are 350--400 different types of
ORNs, compared to just three cone receptors for vision.

The Search for Order in the Olfactory Bulb Activation of receptors in
the mucosa causes electrical signals in the ORNs that are distributed
across the mucosa. These ORNs send signals to structures called
glomeruli in the olfactory bulb. Figure 16.16b illustrates a basic
principle of the relationship between ORNs and glomeruli: All of the
ORNs of a particular type send their signals to just one or two
glomeruli, so each glomerulus collects information about the firing of a
particular type of ORN. This targeting of specific areas of the
olfactory bulb by certain receptors creates different patterns of
olfactory bulb activation for different odorants. Figures 16.18 and
16.19, which are based on measurements of rat olfactory bulb activation
using two different techniques (which we won't describe), both show that
different chemicals result in different patterns of activity in the
olfactory bulb. Figure 16.18 shows that two different types of
compounds, carboxylic acids and aliphatic alcohols, activate different
areas on the olfactory bulb, and that as the length of the carbon chain
for each compound increases, the area of activation moves to the left.
Figure 16.19 also shows that different odorants cause different patterns
of activation. The different patterns for different odorants create a
map of odorants in the olfactory bulb that it is based on molecular
features of odorants such as carbon chain length or functional groups.
These maps have been called chemotopic maps (Johnson & Leon, 2007;
Johnson et al., 2010; Murthy, 2011), odor maps (Restrepo et al., 2009;
Soucy et al., 2009; Uchida et al., 2000), and odotoptic maps (Nikonov et
al., 2005). 16.7 Analyzing Odorants: The Mucosa and Olfactory Bulb

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

403

O O O O O

OH

OH

OH

OH

OH

OH

OH

OH

OH

OH O

OH

OH O

OH OH

(a) Carboxylic acids

(b) Aliphatic alcohols

Figure 16.18 Areas in the rat olfactory bulb that are activated by
various chemicals: (a) a series of carbolic acids; (b) a series of
aliphatic alcohols. (From Uchida et al., 2000)

The idea that odorants with different properties create a map on the
olfactory bulb is similar to the situation we have described for the
other senses. There is a retinotopic map for vision, in which locations
on the retina are mapped on the visual cortex (p. 75); a tonotopic map
for hearing, in which frequencies are mapped onto various structures in
the auditory system (p. 278); and a somatotopic map for the cutaneous
senses, in which locations on the body are mapped onto the somatosensory
cortex (p. 362). Research on the olfactory map has just begun, however,
and much remains to be learned about how odors are represented in the
olfactory bulb. Based on what has been discussed so far, it is clear
that odorants are at least crudely mapped on the olfactory bulb based on
their chemical properties. However, we are far from creating a map based
on perception. This map, if it exists, will be a map of different odor
experiences arranged on the olfactory bulb (Arzi & Sobel, 2011). But the
olfactory bulb represents an early stage of olfactory processing and is
not where perception occurs. To understand olfactory

perception, we need to follow the output of the olfactory bulb to the
olfactory cortex. TEST YOuRSELF 16.2 1. Why is it inaccurate to describe
human olfaction as microsmatic? 2. What is the difference between
detecting odors and identifying odors? 3. How well can people identify
odors? What is the role of memory in identifying odors? 4. Describe some
genetically determined individual differences in odor perception. What
are some of the consequences of losing the ability to smell? 5. How is
olfaction affected by COVID-19 and Alzheimer's disease? Why is it
important that loss of olfactory function precedes the main symptoms of
AD by many years? 6. Why has it been difficult to organize odors and
relate odors to physical properties of molecules? 7. What is an odor
object? What are the two stages for perceiving an odor object? 8.
Describe the following components of the olfactory system: the olfactory
receptors, the olfactory receptor neurons, the olfactory bulb, and the
glomeruli. Be sure you understand the relation between olfactory
receptors and olfactory receptor neurons, and between olfactory receptor
neurons and glomeruli.

Figure 16.19 Patterns of activation in the rat olfactory bulb for five
different odorants. Yellow and red areas indicate areas of high
activation compared to activation caused by exposure to air. Each
odorant causes a distinctive pattern of activation. (Courtesy of Michael
Leon)

404

9.  How do olfactory receptor neurons respond to different odorants, as
    determined by calcium imaging? What is an odorant's recognition
    profile?
10. Describe the evidence that there is a chemotopic map on the
    olfactory bulb. What is the difference between a chemotopic map and
    a perceptual map?

Chapter 16  The Chemical Senses

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

16.8 Representing Odors in the Cortex To begin our discussion of how
odors are represented in the cortex, let's look at where signals are
transmitted when they leave the olfactory bulb. Figure 16.20a shows the
location of the two main olfactory areas: (1) the piriform cortex (PC),
which is the primary olfactory area, and (2) the orbitofrontal cortex,
which is the secondary olfactory area. Figure 16.20b shows the olfactory
system as a flow diagram and adds the amygdala, which is involved in
determining emotional reactions not only to smell but also to faces and
pain. We begin by considering the piriform cortex.

imagery, which, like fMRI, determines brain activation by measuring
changes in blood flow. Figure 16.21a shows that hexanal and pentyl
acetate cause different patterns of activity in the rat olfactory bulb.
Figure 16.21b shows that hexanal and pentyl acetate cause activity
throughout the entire PC. This widespread activity in the PC has also
been demonstrated by recording from single neurons. Figure 16.22 shows
the results of an experiment by Robert Rennaker and coworkers (2007),
who used multiple electrodes to measure neural responding in the PC.
Figure 16.22 shows that isoamyl acetate causes activation across the
cortex. Other compounds also cause widespread activity, and there is
substantial overlap between the patterns of activity for different
compounds. (b) Piriform cortex Osmanski et al., 2014; Parts of Fig. 3B
and 3C, p. 180; Parts of Fig. 4B and 4C, p. 181

(a) Olfactory bulb

How Odorants Are Represented in the Piriform Cortex So far in our
journey through the olfactory system, progressing from the olfactory
neurons to the olfactory bulb, order has prevailed. Odors that smell
different cause different patterns of firing of olfactory receptors
(Figure 16.17). Moving to the olfactory bulb, different chemicals cause
activity in specific areas, which has led to the proposal of odotopic
maps (Figures 16.18 and 16.19). But when we move up to the piriform
cortex (PC), something surprising happens: The map vanishes! Odorants
that caused activity in specific locations in the olfactory bulb now
cause widespread activity in the PC, and there is overlap between the
activity caused by different odorants. This shift in organization from
the olfactory bulb to the PC is illustrated in a study by B. F. Osmanski
and coworkers (2014), who used a technique called functional ultrasound

Hexanal

Pentyl acetate

Figure 16.21 Response of the rat's (a) olfactory bulb and (b) piriform
cortex to hexanal and pentyl acetate measured by functional ultrasound
scanning. See text for details. (From Osmanski et al., 2014)

Frontal lobe Orbitofrontal cortex (Secondary olfactory area)

Olfactory bulb

Amygdala

Temporal lobe Piriform cortex (Primary olfactory area)

(a) 

Olfactory mucosa

Olfactory bulb

Piriform cortex

Orbitofrontal cortex

Primary olfactory area

Secondary olfactory area

(b) 

Figure 16.20 (a) The underside of the brain, showing the neural pathways
for olfaction. On the left side, the temporal lobe has been deflected to
expose the olfactory area. (b) Flow diagram of the pathways for
olfaction. \[(a) Adapted from Frank & Rabin, 1989; (b) adapted from
Wilson & Stevenson, 2006\]

16.8 Representing Odors in the Cortex

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

405

(a) Electrode placements

(b) Activation by isoamyl acetate

Figure 16.22 (a) Recording sites used by Rennaker and coworkers (2007)
to determine activity of neurons in the piriform cortex of the rat. (b)
The pattern of activation caused by isoamyl acetate.

These results show that the orderly activation pattern in the olfactory
bulb no longer exists in the piriform cortex. This occurs because the
projection from the olfactory bulb is scattered, so activity associated
with a single chemical is spread out over a large area. Things become
even more interesting when we ask what the activation pattern might look
like for an odor object such as coffee.

How Odor Objects Are Represented in the Piriform Cortex We can
appreciate how complicated things become for odor objects by imagining
what the pattern of activation would be for coffee, which contains a
hundred different chemical components. Not only will the pattern be very
complicated, but if you are smelling a particular odor for the first
time, this raises the question of how the olfactory system is able to
determine the identity of this "mystery odor" based on the information
in this first-time response. Some researchers have answered this
question by drawing a parallel between recognizing odors and
experiencing memories. Figure 16.23 indicates what happens when a memory
is formed. When a person witnesses an event, a number of neurons are
activated (Figure 16.23a). At this point, the memory for the event isn't
completely formed in the brain; it is fragile and can be easily
forgotten or can be disrupted by trauma, such as a blow to the head. But
connections begin forming between the neurons that were activated by the
event (Figure 16.23b), and after these connections are formed (Figure
16.23c), the memory is stronger and more resistant to disruption.
Formation of stable memories thus involves a process in which linkages
are formed between a number of neurons. Applying this idea to odor
perception, it has been proposed that formation of odor objects involves
learning, which links together the scattered activations that occur for
a Figure 16.23 A model of how memories are formed in the cortex. (a)
Initially, incoming information activates a number of areas in the
cortex. The rectangles are different cortical areas. Red circles are
activated areas. (b) As time passes, the neural activity is replayed,
which creates connections between activated areas. (c) Eventually, the
activated areas for a particular memory are linked, which stabilizes the
memory. 406

particular object. We can see how this works by imagining that you are
smelling the odor of a flower for the first time. The odor of this
flower, just like the odors of coffee and other substances, is created
by a large number of chemical compounds (Figure 16.24a). These chemical
components first activate the olfactory receptors in the mucosa and then
create a pattern of activation on the olfactory bulb that is shaped by
the chemotopic map. This pattern occurs any time the flower's odor is
presented (Figure 16.24b). From the research described above, we know
that signals from the olfactory bulb are transformed into a scattered
pattern of activation in the piriform cortex (Figure 16.24c). Because
this is the first time you have ever experienced the flower's odor, the
activated neurons aren't associated with each other. This is like the
neurons that represent a new memory, which aren't yet linked (see Figure
16.23a). At this point you are likely to have trouble identifying the
odor and might confuse it with other odors. But after a number of
exposures to the flower, which cause the same activation pattern to
occur over and over, neural connections form, and the neurons become
associated with each other (Figure 16.24d). Once this occurs, a pattern
of activation has been created that represents the flower's odor. Thus,
just as a stable memory becomes established when neurons become linked,
odor objects become formed when experience with an odor causes neurons
in the piriform cortex to become linked. According to this idea, when
the person in Figure 16.14 walks into the kitchen, the activation caused
by the hundreds of molecules in the air become three linked networks of
activation in the PC that stand for coffee, orange juice, and bacon. The
idea that learning plays an important role in perceiving odors is
supported by research. For example, Donald Wilson (2003) measured the
response of neurons in the rat's piriform cortex to two odorants: (1) a
mixture of isoamyl acetate, which has a banana-like odor, and peppermint
and (2) the component isoamyl acetate alone. Wilson was interested in
how well the rat's neurons could tell the difference between the mixture
and the component after the rat had been exposed to the mixture. Wilson
presented the mixture to the rat for either a brief exposure (10 seconds
or about 20 sniffs) or a longer exposure (50 seconds or about 100
sniffs) and, after a short pause, measured the response to the mixture
and to the component. Following 10 seconds of sniffing, the piriform
neurons responded similarly to the mixture and to the component.
However, following 50 seconds of sniffing, the neurons fired more
rapidly to the component. Thus, after 100 sniffs of the mixture, the
neurons became able to tell the difference between the mixture and the
component. Similar experiments

Areas in cortex

(a) 
(b) 
(c) 

Chapter 16  The Chemical Senses

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Odorant molecules

Courtesy of Bruce Goldstein

1

2

3

(a) Odor object

4

5

Chemotopic map activated

Scattered activation

Pattern for odor object

(b) Olfactory bulb

(c) Piriform cortex

(d) Piriform cortex after learning

measuring responses of neurons in the olfactory bulb did not show this
effect. Wilson concluded from these results that, given enough time,
neurons in the piriform cortex can learn to discriminate between
different odors, and that this learning may be involved in our ability
to tell the difference between different odors in the environment.
Numerous other experiments support the idea that a mechanism involving
experience and learning is involved in associating patterns of piriform
cortex firing with specific odor objects (Choi et al., 2011; Gottfried,
2010; Sosulski et al., 2011; Wilson, 2003; Wilson et al., 2004, 2014;
Wilson & Sullivan, 2011).

How Odors Trigger Memories While memory is involved in identifying odor
objects, there is another, different, connection between olfaction and
memory. Olfaction can, under some circumstances, create memories. This
connection between chemical senses and memory was noted by the French
author Marcel Proust (1871--1922) in his description of an experience
after eating a small lemon cookie called a madeleine: The sight of the
little madeleine had recalled nothing to my mind before I tasted it...
as soon as I had recognized the taste of the piece of madeleine soaked
in her decoction of lime-blossom which my aunt used to give me...
immediately the old grey house upon the street, where her room was, rose
up like a stage set to attach itself to the little pavilion opening on
to the garden which had been built out behind it for my parents... and
with the house the... square where I used to be sent before lunch, the
streets along which I used to run errands, the country roads we took
when it was fine. (Marcel Proust, Remembrance of Things Past, 1913)
Proust's rather dramatic description of how tasting a cookie unlocked
memories he hadn't thought of for years is called the Proust effect.
This passage captures some characteristics of Proustian memories: (1)
Memories were realized not by seeing the cookie but by tasting it; (2)
the memory was vivid and transported Proust back to a number of places
from his past; and (3) the memory was from Proust's early childhood.

Figure 16.24 Memory mechanism for forming representations of the
flower's odor. See text for details.

Modern researchers call these Proustian memories odorevoked
autobiographical memories (OEAMs), because they are elicited by odors
and are memories about events from a person's life story. But OEAMs
aren't simply a literary observation. In an early experiment that
confirmed and elaborated on the properties of OEAMs hinted at by
Proust's observations, Rachel Herz and Jonathan Schooler (2002) had
participants describe a personal memory associated with items like
Crayola crayons, Coppertone suntan lotion, and Johnson's baby powder.
After describing their memory associated with the objects, participants
were presented with an object either in visual form (a color photograph)
or in odor form (smelling the object's odor) and were asked to think
about the event they had described and to rate it on a number of scales.
The result was that participants who smelled the odor rated their
memories as more emotional than participants who saw the picture. They
also had a stronger feeling than the visual group of "being brought
back" to the time the memory occurred (also see Willander & Larsson,
2007). The fact that Proust described a memory from his childhood is no
coincidence, because an experiment that collected autobiographical
memories from 65- to 80-year-old participants elicited by odors, words,
or pictures yielded the results shown in Figure 16.25 (Larsson &
Willander, 2009). Memories elicited by odors were most likely to be for
events that occurred in the first decade of life, whereas memories
elicited by words were more likely to be for events from the second
decade of life. The participants in this experiment also described their
odor-evoked memories as being associated with strong emotions and
feelings of being brought back in time. What's happening in the brain
during OEAMs? A clue to the answer is that the amygdala, which is
involved in creating emotions and emotional memories, is only two
synapses from the olfactory nerve, and the hypothalamus, which is
involved in storing and retrieving memories, is only three synapses
away. It therefore isn't surprising that fMRI brain scans have revealed
that odor-evoked memories cause higher activity in the amygdala than
word-evoked memories (Arshamian et al., 2013; Herz et al., 2004). So
Proust was onto something when he described how the flavor of a cookie
transported him back in time. The research inspired by Proust's
observation indicates that there is something special about olfactory
memories. It is also important to note that although Proust described
"tasting" the madeleine 16.8 Representing Odors in the Cortex

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

407

The reason you may have found it difficult to determine what you were
drinking or eating when you were holding your nose is that your
experience of flavor depends on a combination of taste and olfaction,
and by holding your nose, you eliminated the olfactory component of
flavor. This interaction between taste and olfaction occurs at two
levels: first in the mouth and nose, and then in the cortex.

0.6

Proportion of memories

0.5

0.4

0.3

Taste and Olfaction Meet in the Mouth and Nose

0.2

0.1

--8 0 71

--7 0 61

--6 0 51

--5 0 41

--4 0 31

--3 0 21

--2 0 11

0-- 10

0.0

Age decade

Figure 16.25 Distribution of events associated with odor-cued memories
(dashed curve) and word-cued memories (solid curve) over the lifespan.
The events retrieved from odor-cued memories peak during the first
decade of life, whereas the events retrieved from word-cued memories
peak during the second decade.

cookie, he was really describing flavor, which is determined by a
combination of taste and olfaction.

16.9 The Perception of Flavor What most people refer to as "taste" when
describing their experience of food ("That tastes good, Mom") is usually
a combination of taste, from stimulation of the receptors in the tongue,
and olfaction, from stimulation of the receptors in the olfactory
mucosa. This combination, which is called flavor, is defined as the
overall impression that we experience from the combination of nasal and
oral stimulation (Lawless, 2001; Shepherd, 2012). You can demonstrate
how smell affects flavor with the following demonstration.

DEMONSTRATION

Tasting With and Without the Nose

While pinching your nostrils shut, drink a beverage with a distinctive
taste, such as grape juice, cranberry juice, or coffee. Notice both the
quality and the intensity of the taste as you are drinking it. (Take
just one or two swallows because swallowing with your nostrils closed
can cause a buildup of pressure in your ears.) After one of the
swallows, open your nostrils, and notice whether you perceive a flavor.
Finally, drink the beverage normally with nostrils open, and notice the
flavor. You can also do this demonstration with fruits or cooked foods
or try eating a jellybean with your eyes closed (so you can't see its
color) while holding your nose.

408

Chemicals in food or drink cause taste when they activate taste
receptors on the tongue. But in addition, food and drink release
volatile chemicals that reach the olfactory mucosa by following the
retronasal route, from the mouth through the nasal pharynx, the passage
that connects the oral and nasal cavities (Figure 16.1). Although
pinching the nostrils shut does not close the nasal pharynx, it prevents
vapors from reaching the olfactory receptors by eliminating the
circulation of air through this channel (Murphy & Cain, 1980). The same
thing happens when you have a cold---less airflow means the flavor of
foods will be greatly reduced. The fact that olfaction is a crucial
component of flavor may be surprising because the flavors of food seem
to be centered in the mouth. It is only when we keep molecules from
reaching the olfactory mucosa that the importance of olfaction is
revealed. One reason this localization of flavor occurs is because food
and drink stimulate tactile receptors in the mouth, which creates oral
capture, in which the sensations we experience from both olfactory and
taste receptors are referred to the mouth (Small, 2008). Thus, when you
"taste" food, you are usually experiencing flavor, and the fact that it
is all happening in your mouth is an illusion created by oral capture
(Todrank & Bartoshuk, 1991). The importance of olfaction in the sensing
of flavor has been demonstrated experimentally by using both chemical
solutions and typical foods. In general, solutions are more difficult to
identify when the nostrils are pinched shut (Mozell et al., 1969) and
are often judged to be tasteless. For example, Figure 16.26a shows that
the chemical sodium oleate has a strong soapy flavor when the nostrils
are open but is judged tasteless when they are closed. Similarly,
ferrous sulfate (Figure 16.26b) normally has a metallic flavor but is
judged predominantly tasteless when the nostrils are closed (Hettinger
et al., 1990). However, some compounds are not influenced by olfaction.
For example, monosodium glutamate (MSG) has about the same flavor
whether or not the nose is clamped (Figure 16.26c). In this case, the
sense of taste predominates.

Taste and Olfaction Meet in the Nervous System Although taste and
olfactory stimuli occur in close proximity in the mouth and nose, our
perceptual experience of their combination is created when they interact
in the cortex.

Chapter 16  The Chemical Senses

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Sodium oleate Clamped Sweet Salty Sour Bitter Soapy Metallic Sulfurous
Tasteless Other

Open

x x

xx

xxxxxxxx x

Ferrous sulfate Clamped

MSG

Open

Clamped

Open

xxxxxxxxx xxx xx

xxxxxxxx xxx x

x

xx

x

xxx

x

x xxxxxxxxx xx

xx x x x

x

xxxxxx x

(a) 

x xx xxxxxxxxx x x

(b) 
(c) 

Figure 16.26 How people described the flavors of three different
compounds when they tasted them with their nostrils clamped shut and
with their nostrils open. Each X represents the judgment of one person.
(From Hettinger et al., 1990)

Figure 16.27 is the diagram of the olfactory pathway from Figure 16.16b
(in blue) with the taste pathway added (in orange), showing connections
between olfaction and taste (Rolls et al., 2010; Small, 2012). In
addition, vision and touch contribute to flavor by sending signals to
the amygdala (vision), structures in the taste pathway (touch), and the
orbitofrontal cortex (vision and touch). All of these interactions among
taste, olfaction, vision, and touch underscore the multimodal nature of
our experience of flavor. Flavor includes not only what we typically
call "taste," but also perceptions such as the texture and temperature
of food (Verhagen et al., 2004), the color of food (Spence, 2015; Spence
et al., 2010), and the sounds of "noisy" foods such as potato chips and
carrots that crunch when we eat them (Zampini & Spence, 2010). We will
have more to say about the

Vision signals sent to the OFC and the amygdala. Touch signals sent to
the OFC and the insula.

multimodal nature of flavor in the Something to Consider section at the
end of the chapter. Because of this convergence of neurons from
different senses, the orbitofrontal cortex contains many bimodal
neurons, neurons that respond to more than one sense. For example, some
bimodal neurons respond to both taste and smell, and others respond to
taste and vision. An important property of these bimodal neurons is that
they often respond to similar qualities. Thus, a neuron that responds to
the taste of sweet fruits would also respond to the smell of these
fruits. This means that neurons are tuned to respond to qualities that
occur together in the environment. Because of these properties, it has
been suggested that the orbitofrontal cortex is a cortical center for
detecting flavor and for the perceptual representation of foods (Rolls &
Baylis, 1994; Rolls et al., 2010). Other

Amygdala

Hypothalamus

Olfactory mucosa

Tongue

Olfactory bulb

Nucleus of the solitary tract

Piriform cortex

Orbitofrontal cortex (OFC)

Primary olfactory area

Secondary olfactory area

Thalamus

Insula

Figure 16.27 Flavor is created by interactions among taste, olfaction,
vision, and touch. The olfactory pathway (blue) and taste pathway (red)
interact, as signals are sent between these two pathways. In addition,
both taste and olfactory pathways send signals to the orbitofrontal
cortex (OFC), signals from touch are sent to the taste pathway and the
OFC, and signals from vision are sent to the OFC. Also shown are the
amygdala, which is responsible for emotional responses and has many
connections to structures in both the taste and olfaction pathways and
also receives signals from vision, and the hypothalamus, which is
involved in determining hunger.

Primary taste area 16.9 The Perception of Flavor

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

409

research has shown that the insula, the primary taste cortex, is also
involved in the perception of flavor (de Araujo et al., 2012; Veldhuizen
et al., 2010). But flavor isn't a fixed response that is automatically
determined by the chemical properties of food. Although the chemicals in
a particular food may always activate the same pattern of ORNs in the
mucosa, by the time the signals reach the cortex they can be affected by
many different factors, including cognitive factors and the amount of a
particular food the person has consumed.

Flavor Is Influenced by Cognitive Factors

Pleasantness rating

What you expect can influence both what you experience and neural
responding. This was demonstrated by Hilke Plassmann and coworkers
(2008) by having participants in a brain scanner judge the "taste
pleasantness" of different samples of wine. Participants were asked to
indicate how much they liked five different wines, which were identified
by their price. In reality, there were only three wines; two of them
were presented twice, with different price labels. The results, for a
wine that was labeled either \$10 or \$90, are shown in Figure 16.28.
When the wines are presented without labels, the taste pleasantness
judgments are the same (Figure 16.28a, left bars), but when tasting is
preceded by a price label, the "\$90 wine" gets a much higher

Flavor Is Influenced by Food Intake: Sensory-Specific Satiety 4 3 2 1 No
price

\$10

\$90

(a) \$90 wine

OFC response

0.4 0.2 0

--5 --2

1

4

7

10

13

16

19

--0.2 \$10 wine

--0.4 (b)

Time (sec)

Figure 16.28 Effect of expectation on flavor perception, as indicated by
the results of Hilke Plassman and coworkers' (2008) experiment. (a) The
red and blue bars indicate ratings given to two presentations of the
same wine (although participants didn't know they were the same). The
two bars on the left indicate ratings when there were no price labels on
the wines. The two bars on the right indicate that the participants give
higher "taste pleasantness" ratings when the wine is labeled \$90,
compared to when it is labeled \$10. (b) Responses of the OFC when
tasting the wines labeled \$10 and \$90. 410

taste rating than the "\$10 wine." In addition to influencing the
person's judgments, the labels also influence the response of the
orbitofrontal cortex, with the \$90 wine causing a much large response
(Figure 16.28b). What's happening here is that the response of the
orbitofrontal cortex is being determined both by signals that begin with
stimulation of the taste and olfactory receptors and by signals created
by the person's expectations. In another experiment, participants rated
the same odor as more pleasant when it was labeled "cheddar cheese" than
when it was called "body odor," and the orbitofrontal cortex response
was larger for the cheddar cheese label (de Araujo et al., 2005). Many
other experiments have shown that flavor is influenced by factors in
addition to the actual food that is being consumed. The taste of a red
frozen strawberry dessert was judged to be 10 percent sweeter and 15
percent more flavorful when it was presented on a white plate compared
to on a black plate (Piqueras-Fiszman et al., 2012). The sweetness of
café latte was almost doubled when consumed from a blue mug compared to
a white mug (Van Doorn et al., 2014). And returning to wine, experiments
have shown that perception of the flavor of wine can be influenced not
only by information about its price but also by the shape of the wine
glass (Hummel et al., 2003).

Have you ever experienced the first few forkfuls of a particular food as
tasting much better than the last? Food consumed to satiety (when you
don't want to eat any more) is often considered less pleasurable than
food consumed when hungry. John O'Doherty and coworkers (2000) showed
that both the pleasantness of a food-related odor and the brain's
response to the odor can be influenced by satiety. Participants were
tested under two conditions: (1) when hungry and (2) after eating
bananas until satiety. Participants in a brain scanner judged the
pleasantness of two food-related odors: banana and vanilla. The
pleasantness ratings for both were similar before they had consumed any
food. However, after eating bananas until satiety, the pleasantness
rating for vanilla decreased slightly (but was still positive), but the
rating for banana decreased much more and became negative (Figure
16.29a). This larger effect on the odor associated with the food eaten
to satiety, called sensory-specific satiety, also occurred in the
response of the orbitofrontal cortex. The orbitofrontal cortex response
decreased for the banana odor but remained the same for the vanilla odor
(Figure 16.29b). Similar effects also occurred in the amygdala and
insula for some (but not all) participants. The finding that
orbitofrontal cortex activity is related to the pleasantness of an odor
or flavor can also be stated in another way: The orbitofrontal cortex is
involved in determining the reward value of foods. Food is more
rewarding when you are hungry and becomes less rewarding as food is
consumed, until eventually---at satiety---the reward is gone and eating
stops.

Chapter 16  The Chemical Senses

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Pre-satiety

Pleasantness ratings

Post-satiety

0.3 Vanilla

0.2 --0.7

Odor

Percent change in OFC activation

0.3 0.2 0.1 0

Vanilla

--1.1 --1.2

SOMETHING TO CONSIDER:

The Community of the Senses

Banana

(a) 

order to live, and our experience of flavor helps motivate that eating.
(Unfortunately, it should be added, the shutoff mechanisms are sometimes
overridden by manufactured foods that are rich in sugar and fat and by
other factors, with obesity as an outcome---but that's another story.)

Banana

--1.3 Odor

(b) 

Figure 16.29 Sensory-specific satiety. Results of the O'Doherty et
al. (2000) experiment. (a) Pleasantness rating for banana and vanilla
odor before eating (left bars) and after eating bananas to satiety
(right bars). (b) Response of the orbitofrontal cortex to banana and
vanilla odors before and after eating bananas.

These changes in the reward value of flavors are important because just
as taste and olfaction are important for warning of danger, they are
also important for regulating food intake. Also note in Figure 16.27
that the orbitofrontal cortex sends signals to the hypothalamus, where
neurons are found that respond to the sight, taste, and smell of food if
hunger is present (Rolls et al., 2010). What we've learned by
considering each of the stages of the systems for taste, olfaction, and
flavor is that the purpose of the chemical senses extends beyond simply
creating experiences of taste, smell, and flavor. Their purpose is to
help guide behavior---avoiding potentially harmful substances, seeking
out nutrients, and helping control the amount of food consumed. In fact,
even neurons in the olfactory bulb are sensitive to signals the body
creates about hunger and satiety. Thus they respond to food odors in the
context of signals from the body about how hungry you are. Does this
description of a sense being concerned with behavior sound familiar? You
may remember that Chapter 7, "Taking Action," presented a similar
message for vision: Although early researchers saw the visual system as
being concerned primarily with creating visual experiences, later
researchers have argued that the ultimate goal of the visual system is
to support taking actions that are necessary for survival (see page
149). The chemical senses have a similar ultimate purpose of guiding and
motivating actions required for survival. We eat in

We live in a world that is not organized as separate senses, like the
chapters in a sensation and perception textbook, but exists as a rich
tapestry of moving and stationary objects, spaces, sounds, smells, and
potentials for action, among other things. This tapestry is decorated
with properties like color and shape, pitch and rhythm, rough and smooth
textures. All of these things together combine to create our experience
of the environment. For example, consider the bird that just flew past
me. It was small, gray with dotted smoothly textured feathers, and a
series of rapid flying movements placed it on a shaded tree branch,
where it began its song---a series of rapid high-pitched tweets. I
perceived this constellation of appearances, behaviors, and sounds as
characteristic of this particular bird, and knowing the properties of
this bird led me to predict certain things about it. If it had emitted a
low-pitched "caw, caw" sound that is normally associated with much
larger birds, I would have been extremely surprised. Certain
properties---size, sound, movements---go together in certain situations.
But enough about birds. How about something really important, like
baseball. You see the pitcher throw the ball. The batter swings and from
the loud crack of the bat you know that the ball is most probably going
out of the park. But a swing without that sound, followed by the
"thwunk" of the ball hitting the catcher's glove, signals a strike. You
may not be conscious of it as you're watching the game, but seeing and
hearing are working together to provide information about what is
happening. Examples of multimodal interactions---interactions that
involve more than one sense or quality---are endless, because they are
all around us. You propose a toast, clink wine glasses, and the contact
between the glasses is signaled by both vision and hearing. As you are
talking to someone in a noisy room, hearing them becomes easier if you
can see their lips moving as they are speaking to you. You see people
dancing to music on TV or on your phone or computer. When you turn off
the sound you still see them dancing, but something's missing. With the
sound on, their movement not only seems more synchronized, but you may
feel like moving yourself. And if you were the one dancing, the
modalities of touch and pressure would be added to the music, vision,
and action. Two visual-auditory interactions we discussed in this book
are the ventriloquism effect, in which the perceived location of the
sound source is determined by vision (see page 306), and the Something
to Consider: The Community of the Senses

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

411

McGurk effect, in which seeing a speaker's lips move can affect what
sound the listener hears (McGurk effect: see page 343). But what about
taste and smell? They are multimodal when they combine to create flavor.
But taste and smell also interact with non-chemical senses in numerous
ways. This makes sense, because when we smell something, there's an
object involved, such as food on a plate or liquid in a glass. Smell and
taste also occur within a particular situation---cooking smells in a
kitchen, a smoke alarm signaling smoke in a house, the smell of hot dogs
being grilled outside in a park. Here are a few examples of experiments
that studied interactions between the chemical senses and the other
senses, divided into two types, correspondences and influences.

Correspondences Correspondences refer to how a property of a chemical
sense--- taste, olfaction, or flavor---is associated with properties of
other senses.

Odors and Tastes Are Associated With Different Pitches and Instruments
When participants were presented with odors like almond, cedar, lemon,
raspberry, and vanilla and were asked to pick the auditory pitch that
matched the odor, pitches were matched to different odors. Figure 16.30a
indicates that fruits were matched by high pitches, and smells such as
smoked, musk, and dark chocolate were matched by lower pitches (Crisinel
& Spence, 2012). In a study using taste stimuli, the tastes of citric
acid and sucrose were matched to high tones, and coffee and MSG were
matched to lower tones (Crisinel & Spence, 2010). This study was titled
"As Bitter as a Trombone," because when also asked to match tastes and
musical instruments, bitter substances, like caffeine, were more likely
to be matched to brass instrument sounds, and sweet substances, like
sugar, were more likely to be matched to piano or string sounds (Figure
16.30b). Odors Are Associated With Different Colors When participants
sniffed a wide range of odors and picked

Influences Influences occur when stimuli from one sense affect our
perception or performance associated with another sense.

Music Can Influence Flavor Often, the experience of eating at a
restaurant or bar is accompanied by background music. While this music
may be creating a relaxing mood in a "fine dining" restaurant, or a more
upbeat mood in a bar, it may also be affecting the flavor of the food.
Felipe Reinoso Carvalho and coworkers (2017) demonstrated this by having
participants taste samples of chocolate while listening to two different
soundtracks. The soft/smooth track consisted of long, consonant notes
(where consonant refers to notes that go well together), whereas the
hard/rough track had staccato dissonant notes. The results were
clear-cut---when participants had eaten the chocolate while listening to
the soft/smooth soundtrack they rated it as creamier and sweeter than if
eaten during the hard/rough track. This research and other research has
shown that music can affect our perception of a food's flavor (Crisinel
et al., 2012; Wang & Spence, 2018). 40

C6

C4

50

C3

40

20 Type of instrument Piano Strings Woodwind Brass

10 0 Caffeine

C2 Smoked Musk Dark chocolate Cut hay Cedar Honey Liquorice Pepper
Mushroom Caramel Green pepper Vanilla Violet Blackberry Almond Pineapple
Raspberry Apricot Lemon Apple

30

Sucrose

60

Western musical scale

C5

70

(a) 

412

Odors Are Associated With Different Textures When participants judged
the texture of fabric while smelling different odors, they judged the
fabrics to be slightly softer when smelling a pleasant lemon odor than
when smelling an unpleasant animal-like odor (Dematte et al., 2006).
Other research has shown that different odors are associated with
specific textures. For example, cinnamon and onion odors are associated
with rough textures, whereas violet and peppermint are associated with
smooth textures (Spector & Maurer, 2012).

80

Pitch (MIDI)

Figure 16.30 (a) Pitches matched to a range of different odors. The
vertical axis indicates the western musical scale. There is one octave
between C3 and C4 and between C4 and C5. (From Crisinel & Spence, 2012)
(b) The heights of the bars indicate the number of participants who
matched different instruments to caffeine and sucrose. Note that brass
instruments were the main choice as a match for caffeine, whereas piano
was the choice for sucrose. (From Crisinel & Spence, 2010)

colors that matched them, they matched odors to specific colors (Maric &
Jacquot, 2013). For example, pineapple was associated with red, yellow,
pink, orange, and purple, whereas caramel was associated with brown,
orange, and pale orange. Wild strawberry odor was matched by red, pink,
and purple; "smoked" odor by brown, dark red, black, and gray.

(b) 

Chapter 16  The Chemical Senses

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Color Can Influence Flavor Participants perceive a cherry-flavored drink
as orange-flavored if it is colored orange (DuBose et al., 1980) and
rate a strawberry-flavored drink as less pleasant if it is colored
orange rather than red (Zellner et al., 1991). Charles Spence (2020), in
a review of "wine psychology," notes that there is a large amount of
evidence that color influences the aroma, taste, and flavor of wine. He
also notes that even wine experts can be fooled by deliberately
miscoloring wine. This was demonstrated in a paper titled "Drinking
Through Rosé-Colored Glasses," by Qian Wang and Spence (2019) in which
wine experts were asked to rate the aroma and flavor of a white wine, a
rosé wine, and "fake rosé" wine, which was the white wine dyed with food
coloring so it matched the color of the real rosé wine. Magically, the
food coloring caused the experts to describe the aroma and flavor of the
fake rosé as being very similar to the real rosé and very different from
the white wine. Most notably, the fake rosé and the real rosé received
high ratings for "red fruit" aroma and taste, while the red fruit rating
for the white wine was near zero. Odors Can Influence Attention and
Performance Participants sat in a cubicle, and were given the task of
determining, as quickly as possible, whether a string of letters was a
real word (like bicycle) or a non-word (like poetsen). Six of the real
words were related to cleaning (like hygiene). When the smell of citrus,
which is often associated with cleaning products, was infused into the
cubicle, participants responded faster to the cleaning words, but the
smell had no effect on the words that

weren't related to cleaning (Holland et al., 2005). In another
experiment, participants expected to perform better and actually did
perform better on an analytical reasoning task when the testing room
smelled like coffee, compared to when they took the test in an unscented
environment (Madzharov et al., 2018). (Why do you think smelling coffee
caused this effect? See page 415 for the answer.) Results in both the
"correspondences" and "influences" categories show that taste, smell,
and flavor do not operate in isolation. They share correspondences with
other senses, interact with them, affect them, and are affected by them.
But why do these effects occur? One answer is "learning." Many
associations are formed from everyday experiences. An obvious example is
associating lemon flavor and yellow, strawberry and red. Also, odors of
edible substances are likely to be associated with yellow, whereas odors
that seem "inedible" are likely to be associated with blue, since blue
is less likely to be associated with food. Paralleling the birdsong
example at the beginning of this section, our experience tells us that a
large dog is likely to have a lower pitched bark than a small dog. Some
correspondences can be explained by pleasure or emotions. Bright colors,
often associated with happiness, are associated with pleasant odors.
Similarly, pleasant odors are associated with the pleasant feelings from
stroking soft fabrics. Although learning from everyday experience and
taking emotions into account can't explain all correspondences and
influences, there is no question that much of perception is multimodal,
and that it is accurate to describe the different senses as all part of
a "community."

DEVELOPMENTAL DIMENSION Infant Chemical Sensitivity Do newborn infants
perceive odors and tastes? One way researchers have answered this
question is to measure newborn facial expressions. Figure 16.31 shows a
newborn's response to

the sweet taste of sucrose (left) and the bitter taste of quinine
(right) (Rosenstein & Oster, 1988). It has also been shown that 3to
7-day-old infants respond to banana extract or vanilla extract

Figure 16.31 Newborn facial expressions to the sweet taste of sucrose
(left) and to the bitter taste of quinine (right), presented
approximately 2 hours after birth and before the first feeding. (From
Rosenstein & Oster, 1988)

Continued

Something to Consider: The Community of the Senses

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

413

Table 16.4 Effect of What the Mother Consumes

with sucking and facial expressions that are similar to smiles, and they
respond to concentrated shrimp odor and an odor resembling rotten eggs
with rejection or disgust (Steiner, 1974, 1979). Research studying how
newborns and young infants respond to salt indicates that there is a
shift toward greater acceptance of salty solutions between birth and 4
to 8 months of age that continues into childhood (Beauchamp et al.,
1994). One explanation for this shift is that it reflects the
development of receptors sensitive to salt during infancy. But there is
also evidence that infants' preferences are shaped by experience that
occurs both before birth and during early infancy. Figure 16.32 shows a
number of ways that experience can shape the response to flavors, from
pregnancy to weaning (Forestell, 2017). What the mother eats during
pregnancy changes the flavor profile of the amniotic fluid, which
affects the developing fetus, because by the last trimester, the taste
and olfactory receptors are functioning and the fetus swallows between
500 and 1,000 ml of amniotic fluid a day (Forestell, 2017; Ross &
Nijland, 1997). An experiment by Julie Mennella and coworkers (2001)
provides evidence that the flavor of the amniotic fluid can influence an
infant's preferences. Mennella's experiment involved three groups of
pregnant women, as shown in Table 16.4. Group 1 drank carrot juice
during their final trimester of pregnancy and water during the

on Infant Preferences

Group

Last Trimester

During BREASTFEEDING

Intake of Carrot Flavor

1

Carrot juice

Water

0.62

2

Water

Carrot juice

0.57

3

Water

Water

0.51

Note: Intake score above 0.50 indicates preference for carrot-flavored
cereal.

first two months of lactation, when they were breast-feeding their
infants. Group 2 drank water during pregnancy and carrot juice during
the first two months of lactation, and Group 3 drank water during both
periods. The infants' preference for carrot-flavored cereal versus plain
cereal was tested four weeks after they had begun eating cereal but
before they had experienced any food or juice containing a carrot
flavor. The results, shown in the right column of Table 16.4, indicate
that the infants who had experienced carrot flavor either in utero or in
the mother's milk showed a preference for the carrotflavored cereal
(indicated by a score above 0.5), whereas the infants whose mothers had
consumed only water showed no preference.

Pregnancy: last trimester

Pregnancy Infants learn about the changing flavor profile of the
amniotic fluid, which reflects the mothers' dietary choices during
pregnancy

Birth 6 months

Breastfeeding Infants learn about the changing flavor profile of
breastmilk, which reflects the mothers' dietary choices during lactation

Bottle feeding Infants learn about the flavor profile of the milk they
are fed, which is invariant and does not reflect the dietary choices of
the mother

1 year

Weaning Infants begin to learn about the flavor profile of the family's
cuisine through repeated exposure to a variety of foods

Figure 16.32 What infants learn during different stages of feeding. See
text for explanation. (From Forestell, 2017)

414

Chapter 16  The Chemical Senses

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Returning to Figure 16.32, notice the contrast between breast-feeding
and bottle feeding. The advantage of breastfeeding is that the taste of
mother's milk is influenced by what she eats. So if a mother eats a lot
of vegetables, the infant is drinking "vegetable flavored milk" and
becomes familiar with that flavor. This translates into increased
acceptance of vegetables when the child is older, which is a healthy
food choice not always made by infants. Bottle feeding, in contrast,
teaches infants about the flavor of whatever milk is in the formula, so
the infant is not sharing the mothers' dietary choices.

TEST YOuRSELF 16.3 1. What are the main structures in the olfactory
system past the olfactory bulb? 2. How are odors represented in the
piriform cortex? How does this representation differ from the
representation in the olfactory bulb? 3. How has formation of the
representation of odor objects in the cortex been described as being
caused by experience? How is this similar to the process of forming
memories? 4. What is the Proust effect? What are some properties of
Proustian memories? 5. What is flavor perception? Describe how taste and
olfaction meet in the mouth and nose and then later in the nervous
system.

Finally, when the child is weaned to solid food, its preferences are
influenced first by what was experienced in the womb, then during
nursing, and finally by exposure to the solid foods chosen by the
child's family. Infants' responses to tastes, odors, and flavors are,
therefore, determined both by innate factors, indicated by the fact that
most newborns respond positively to sweet and negatively to bitter, and
by experience, indicated by the way the mother's diet can influence the
child's preferences. Thus, the first step toward insuring that young
children develop good eating habits is for mothers to eat healthy foods,
both when pregnant and while nursing.

6.  Describe the experiment that showed how expectations about a wine's
    taste can influence taste judgments and brain responding.
7.  Describe the experiment that demonstrates sensoryspecific satiety.
8.  What does it mean to say that there is a "community" of senses?
9.  Give examples of connections between chemical senses and (a) pitches
    and instruments, (b) colors, (c) texture,

<!-- -->

(d) attention and performance.

<!-- -->

10. What is the evidence that newborns can detect different taste and
    smell qualities? Describe the carrot juice experiment and how it
    demonstrates that what a mother consumes can influence infant taste
    preferences.

THINK ABOUT IT 1. Consider the kinds of food that you avoid because you
don't like the taste. Do these foods have anything in common that might
enable you to explain these taste preferences in terms of the activity
of specific types of taste receptors? (p. 396) 2. Can you think of
situations in which you have encountered a smell that triggered memories
about an event or

place that you hadn't thought about in years? Do you think your
experience was a "Proustian" memory? (p. 407) Answer to question on page
413: One possible explanation for why coffee odor would cause higher
expectations and better performance is that people often associate being
in a coffee-scented environment, like a coffee shop, with the
physiological arousal caused by drinking coffee.

KEY TERMS Across-fiber patterns (p. 393) Alzheimer's disease (p. 399)
Amiloride (p. 395) Amygdala (p. 405) Anosmia (p. 389)

Bimodal neuron (p. 409) Calcium imaging (p. 402) Chemotopic map (p. 403)
COVID-19 (p. 399) Detection threshold (p. 398)

Flavor (p. 390) Forced-choice method (p. 398) Frontal operculum (p. 393)
Glomeruli (p. 403) Insula (p. 393) Key Terms

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

415

Macrosmatic (p. 397) Microsmatic (p. 397) Mild cognitive impairment
(p. 399) Multimodal interactions (p. 411) Nasal pharynx (p. 408)
Neurogenesis (p. 390) Nucleus of the solitary tract (p. 392) Odor map
(p. 403) Odor object (p. 401) Odor-evoked autobiographical memory
(p. 407)

416

Odotoptic map (p. 403) Olfaction (p. 390) Olfactory bulb (p. 401)
Olfactory mucosa (p. 401) Olfactory receptor neurons (ORNs) (p. 401)
Olfactory receptors (p. 401) Oral capture (p. 408) Orbitofrontal cortex
(p. 405) Papillae (p. 391) Piriform cortex (PC) (p. 405) Primary
olfactory area (p. 405)

Proust effect (p. 407) Recognition profile (p. 403) Retronasal route
(p. 408) Secondary olfactory area (p. 405) Sensory-specific satiety
(p. 410) Sustentacular cell (p. 399) Taste (p. 390) Taste bud (p. 391)
Taste cell (p. 392) Taste pore (p. 392)

Chapter 16  The Chemical Senses

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

appendix

a

W

The Difference Threshold

hen Fechner published Elements of Psychophysics, he not only described
his methods for measuring the absolute threshold but also described the
work of Ernst Weber (1795--1878), a physiologist who, a few years before
the publication of Fechner's book, measured another type of threshold,
the difference threshold: the minimum difference that must exist between
two stimuli before we can tell the difference between them. This just
detectible difference is the difference threshold (also called DL from
the German Differenze Limen, which is translated as "difference
threshold"). Measuring instruments, such as an old-fashioned balance
scale, can detect very small differences. For example, imagine that a
scale is balanced when four 50-penny rolls are placed on each pan. When
just one additional penny is placed on one side, the scale succeeds in
detecting this very small difference between the two weights. The human
sensory system is not as sensitive to weight differences as this type of
scale, so a human comparing the weight of 201 pennies to 200 pennies
would not be able to tell the difference. The difference threshold for
weight is about 2 percent, which means that under ideal conditions, we
would have to add 4 pennies to one side before the difference could be
detected by the human. The idea that the difference threshold is a
percentage of the weights being compared was discovered by Weber, who
proposed that the ratio of the DL to the standard is constant. This
means that if we doubled the number of pennies to 400, the

DL would also double, becoming 8. The ratio DL/Standard for lifting
weights is 0.02, which is called the Weber fraction, and the fact that
the Weber fraction remains the same as the standard is changed is called
Weber's law. Modern investigators have found that Weber's law is true
for most senses, as long as the stimulus intensity is not too close to
the absolute threshold (Engen, 1972; Gescheider, 1976). The Weber
fraction remains relatively constant for a particular sense, but each
type of sensory judgment has its own Weber fraction. For example, from
Table A.1 we can see that people can detect a 1 percent change in the
intensity of an electric shock but that light intensity must be
increased by 8 percent before they can detect a difference. Table A.1
Weber Fractions for a Number of Different

Sensory Dimensions

Electric shock

0.01

Lifted weight

0.02

Sound intensity

0.04

Light intensity

0.08

Taste (salty)

0.08

Source: Teghtsoonian (1971).

417

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

appendix

B

Magnitude Estimation and the Power Function

T

Magnitude estimate

he procedure for a magnitude estimation experiment was described in
Chapter 1 (p. 16). Figure B.1 shows a graph that plots the results of a
magnitude estimation experiment in which participants assigned numbers
to indicate their perception of the brightness of lights. This graph,
which presents the average magnitude estimates made by a number of
participants, indicates that doubling the intensity does not necessarily
double the perceived brightness. For example, when the intensity is 20,
perceived brightness is 28. If we double the intensity to 40, perceived
brightness does not double to 56, but instead increases only to 36. This
result, in which the increase in perceived magnitude is smaller than the
increase in stimulus intensity, is called response compression. Figure
B.1 also shows the results of magnitude estimation experiments for the
experience caused by an electric shock presented to the finger and for
the perception of length of a line. The electric shock curve bends up,
indicating that doubling the strength of a shock more than doubles

the perceived magnitude of the shock. Increasing the intensity from 20
to 40 increases perception of shock magnitude from 6 to 49. This is
called response expansion. As intensity is increased, perceptual
magnitude increases more than intensity. The curve for estimating line
length is straight, with a slope of close to 1.0, meaning that the
magnitude of the response almost exactly matches increases in the
stimulus, so if the line length is doubled, an observer says it appears
to be twice as long. The beauty of the relationships derived from
magnitude estimation is that the relationship between the intensity of a
stimulus and our perception of its magnitude follows the same general
equation for each sense. These functions, which are called power
functions, are described by the equation P 5 KSn. Perceived magnitude,
P, equals a constant, K, times the stimulus intensity, S, raised to a
power, n. This relationship is called Stevens's power law. For example,
if the exponent, n, is 2.0 and the constant, K, is 1.0, the perceived
magnitude, P, for intensities 10 and 20 would be calculated as follows:

80

Intensity 10: P 5 (1.0) 3 (10)2 5 100

70

Intensity 20: P 5 (1.0) 3 (20)2 5 400

60 50 40 30 Brightness

20

Line length Electric shock

10 0 0

10

20

30

40

50

60

70

80

90

100

Stimulus intensity

Figure B.1 The relationship between perceived magnitude and stimulus
intensity for electric shock, line length, and brightness. (Adapted from
Stevens, 1962)

In this example, doubling the intensity results in a fourfold increase
in perceived magnitude, an example of response expansion. The exponent
of the power function, n, tells us something important about the way
perceived magnitude changes as intensity is increased. Exponents less
than 1.0 are associated with response compression (as occurs for the
brightness of a light), and exponents greater than 1.0 are associated
with response expansion (as occurs for sensing shocks). Response
compression and expansion illustrate how the operation of each sense is
adapted to how organisms function in their environment. Consider, for
example, your experience of brightness. Imagine you are inside reading a
book, when you turn to look out the window at a sidewalk bathed in
intense sunlight. Your eyes may be receiving thousands of times more
light from the sidewalk than from the page of your book, but because of
response compression, the sidewalk does not appear thousands

418

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

of times brighter than the page. It does appear brighter, but not so
much that you are blinded by the sunlit sidewalk.1 1

Another mechanism that keeps you from being blinded by high-intensity
lights is the process of adaptation, which adjusts the eye's sensitivity
in response to different light levels (see Chapter 3, page 46).

The opposite situation occurs for electric shock, which has an exponent
of 3.5, so small increases in shock intensity cause large increases in
pain. This rapid increase in pain associated with response expansion
serves to warn us of impending danger, and we therefore tend to withdraw
even from weak shocks.

Magnitude Estimation and the Power Function

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

419

appendix

C

The Signal Detection Approach

I

100

Lucy Percent "yes" responses

n Chapter 1 we saw that by randomly presenting stimuli of different
intensities, we can use the method of constant stimuli to determine a
person's threshold---the intensity to which the person reports "I see
the light" or "I hear the tone" 50 percent of the time (p. 14). What
determines this threshold intensity? Certainly, the physiological
workings of the person's eye and visual system are important. But some
researchers have pointed out that perhaps other characteristics of the
person may also influence the determination of threshold intensity. To
illustrate this idea, let's consider a hypothetical experiment in which
we use the method of constant stimuli to measure Lucy's and Cathy's
thresholds for seeing a light. We pick five different light intensities,
present them in random order, and ask Lucy and Cathy to say "yes" if
they see the light and "no" if they don't see it. Lucy thinks about
these instructions and decides that she wants to be sure she doesn't
miss any presentations of the light. Because Lucy decides to say "yes"
if there is even the slightest possibility that she sees the light, we
could call her a liberal responder. Cathy, however, is a conservative
responder. She wants to be totally sure that she sees the light before
saying "yes" and so reports that she sees the light only if she is
definitely sure she saw it. The results of this hypothetical experiment
are shown in Figure C.1. Lucy gives many more "yes" responses than Cathy
does and therefore ends up with a lower threshold. But given what we
know about Lucy and Cathy, should we conclude that Lucy's visual system
is more sensitive to the lights than Cathy's? It could be that their
actual sensitivity to the lights is exactly same, but Lucy's apparently
lower threshold occurs because she is more willing than Cathy to report
that she sees a light. A way to describe this difference between these
two people is that each has a different response criterion. Lucy's
response criterion is low (she says "yes" if there is the slightest
chance a light is present), whereas Cathy's response criterion is high
(she says "yes" only when she is sure that she sees the light). What are
the implications of the fact that people may have different response
criteria? If we are interested in how one person responds to different
stimuli (for example, measuring how a person's threshold varies for
different colors of light), then we don't need to take response
criterion into account because we are comparing responses within the
same person. Response

Cathy

50

0 Low

High Light intensity

Figure C.1 Data from experiments in which the threshold for seeing a
light is determined for Lucy (green points) and Cathy (red points) by
means of the method of constant stimuli. These data indicate that Lucy's
threshold is lower than Cathy's. But is Lucy really more sensitive to
the light than Cathy, or does she just appear to be more sensitive
because she is a more liberal responder?

criterion is also not very important if we are testing many people and
averaging their responses. However, if we wish to compare two people's
responses, their differing response criteria could influence the
results. Luckily, an approach called the signal detection approach can
be used to take differing response criteria into account. We will first
describe a signal detection experiment and then describe the theory
underlying the experiment.

A Signal Detection Experiment Remember that in a psychophysical
procedure such as the method of constant stimuli, at least ﬁve different
stimulus intensities are presented and a stimulus is presented on

420

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

The Basic Experiment A signal detection experiment differs from a
classical psychophysical experiment in two ways: (1) only one stimulus
intensity is presented, and (2) on some of the trials, no stimulus is
presented. Let's consider the results of such an experiment, using Lucy
as our subject. We present the tone for 100 trials and no tone for 100
trials, mixing the tone and no-tone trials at random. Lucy's results are
as follows. When the tone is presented, Lucy Says "yes" on 90 trials.
This correct response---saying "yes" when a stimulus is present---is
called a hit in signal detection terminology. ■■ Says "no" on 10 trials.
This incorrect response---saying "no" when a stimulus is present---is
called a miss. ■■

When no tone is presented, Lucy Says "yes" on 40 trials. This incorrect
response---saying "yes" when there is no stimulus---is called a false
alarm. ■■ Says "no" on 60 trials. This correct response---saying "no"
when there is no stimulus---is called a correct rejection. ■■

These results are not very surprising, given that we know Lucy has a low
criterion and likes to say "yes" a lot. This gives her a high hit rate
of 90 percent but also causes her to say "yes" on many trials when no
tone is present, so her 90 percent hit rate is accompanied by a 40
percent false-alarm rate. If we do a similar experiment on Cathy, who
has a higher criterion and therefore says "yes" much less often, we ﬁnd
that she has a lower hit rate (say, 60 percent) but also a lower
false-alarm rate (say, 10 percent). Note that although Lucy and Cathy
say "yes" on numerous trials on which no stimulus is presented, that
result would not be predicted by classical threshold theory. Classical
theory would say "no stimulus, no response," but that is clearly not the
case here. By adding the following new wrinkle to our signal detection
experiment, we can obtain another result that would not be predicted by
classical threshold theory.

Hit: Correct rejection: False alarm: Miss:

Win \$100 Win \$10 Lose \$10 Lose \$10

What would you do if you were in Cathy's position? You realize that the
way to make money is to say "yes" more. You can lose \$10 if a "yes"
response results in a false alarm, but this small loss is more than
counterbalanced by the \$100 you can win for a hit. Although you decide
not to say "yes" on every trial---after all, you want to be honest with
the experimenter about whether you heard the tone---you decide to stop
being so conservative. You decide to change your criterion for saying
"yes." The results of this experiment are interesting. Cathy becomes a
more liberal responder and says "yes" a lot more, responding with 98
percent hits and 90 percent false alarms. This result is plotted as data
point L (for "liberal" response) in Figure C.2, a plot of the percentage
of hits versus the percentage of false alarms. The solid curve going
through point L is called a receiver operating characteristic (ROC)
curve. We will see why the ROC curve is important in a moment, but ﬁrst
let's see how we determine the other points on the curve. Doing this is
simple: all we have to do is to change the payoffs. We can make Cathy
raise her criterion and therefore respond more conservatively by means
of the following payoffs. Hit: Correct rejection: False alarm: Miss:

Win \$10 Win \$100 Lose \$10 Lose \$10

100 N9

L L9

N Percentage hits

every trial. In a signal detection experiment studying the detection of
tones, we use only a single low-intensity tone that is difﬁcult to hear,
and we present this tone on some of the trials and present no tone at
all on the rest of the trials.

50

C9

Payoffs Without changing the tone's intensity at all, we can cause Lucy
and Cathy to change their percentages of hits and false alarms. We do
this by manipulating each person's motivation by means of payoffs. Let's
look at how payoffs might inﬂuence Cathy's responding. Remember that
Cathy is a conservative responder who is hesitant to say "yes." But
being clever experimenters, we can make Cathy say "yes" more frequently
by adding some ﬁnancial inducements to the experiment. We tell Cathy
that we are going to reward her for making correct responses and are
going to penalize her for making incorrect responses by using the
following payoffs.

C 0 0

50 Percentage false alarms

100

Figure C.2 A receiver operating characteristic (ROC) curve determined by
testing Lucy (green data points) and Cathy (red data points) under three
different criteria: liberal (L and L9), neutral (N and N9), and
conservative (C and C9). The fact that Cathy's and Lucy's data points
all fall on this curve means that they have the same sensitivity to the
tone. The triangles indicate the results for Lucy and Cathy for an
experiment that did not use payoffs. A Signal Detection Experiment

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

421

This schedule of payoffs offers a great inducement to respond
conservatively because there is a big reward for saying "no" when no
tone is presented. Cathy's criterion is therefore shifted to a much
higher level, so Cathy now returns to her conservative ways and says
"yes" only when she is quite certain that a tone is presented; otherwise
she says "no." The result of this newfound conservatism is a hit rate of
only 10 percent and a minuscule false-alarm rate of 1 percent, indicated
by point C (for "conservative" response) on the ROC curve. We should
note that although Cathy hits on only 10 percent of the trials in which
a tone is presented, she scores a phenomenal 99 percent correct
rejections on trials in which a tone is not presented. (If there are 100
trials in which no tone is presented, then correct rejections 1 false
alarms 5 100. Because there was 1 false alarm, there must be 99 correct
rejections.) Cathy, by this time, is rich and decides to put a down
payment on the electric car she's been dreaming about. (So far she's won
\$8,980 in the ﬁrst experiment and \$9,090 in the second experiment, for
a total of \$18,070! To be sure you understand how the payoff system
works, check this calculation yourself. Remember that the signal was
presented on 100 trials and was not presented on 100 trials.) However,
we point out that she may need a little extra cash to buy that new
laptop computer she's been thinking about, so she agrees to stick around
for one more experiment. We now use the following neutral schedule of
payoffs. Hit: Correct rejection: False alarm: Miss:

Win \$10 Win \$10 Lose \$10 Lose \$10

With this schedule, we obtain point N (for "neutral") on the ROC curve:
75 percent hits and 20 percent false alarms. Cathy wins \$1,100 more and
becomes the proud owner of a new laptop computer and we are the proud
owners of the world's most expensive ROC curve. (Do not, at this point,
go to the psychology department in search of the nearest signal
detection experiment. In real life, the payoffs are quite a bit less
than in our hypothetical example.)

What Does the ROC Curve Tell Us? Cathy's ROC curve shows that factors
other than sensitivity to the stimulus determine a person's response.
Remember that in all of our experiments the intensity of the tone has
remained constant. Even though we changed only the person's criterion,
we succeeded in drastically changing the person's responses. Other than
demonstrating that people will change how they respond to an unchanging
stimulus, what does the ROC curve tell us? Remember, at the beginning of
this discussion, we said that a signal detection experiment can tell us
whether Cathy and Lucy are equally sensitive to the tone. The beauty of
signal detection theory is that the person's sensitivity is indicated by
the shape of the ROC curve, so if experiments on two people result in
identical ROC curves, their sensitivities must be equal. (This
conclusion is not obvious from our discussion 422

so far. We will explain below why the shape of the ROC curve is related
to the person's sensitivity.) If we repeat the above experiments on
Lucy, we get the following results (data points L9, N9, and C9 in Figure
C.2): Liberal Payoff Hits = 99 percent False alarms = 95 percent Neutral
Payoff Hits = 92 percent False alarms = 50 percent Conservative Payoff
Hits = 50 percent False alarms = 6 percent The data points for Lucy's
results are shown by the green circles in Figure C.2. Note that although
these points are different from Cathy's, they fall on the same ROC curve
as do Cathy's. We have also plotted the data points for the ﬁrst
experiments we did on Lucy (open triangle) and Cathy (ﬁlled triangle)
before we introduced payoffs. These points also fall on the ROC curve.
That Cathy's and Lucy's data both fall on the same ROC curve indicates
their equal sensitivity to the tones. This conﬁrms our suspicion that
the method of constant stimuli misled us into thinking that Lucy is more
sensitive, when the real reason for her apparently greater sensitivity
is her lower criterion for saying "yes." Before we leave our signal
detection experiment, it is important to note that signal detection
procedures can be used without the elaborate payoffs that we described
for Cathy and Lucy. Much briefer procedures, which we will describe
shortly, can be used to determine whether differences in the responses
of different persons are due to differences in threshold or to
differences in response criteria. What does signal detection theory tell
us about functions such as the spectral sensitivity curve (Figure 3.15,
page 50) and the audibility curve (Figure 11.8, page 269), which are
usually determined using one of the classical psychophysical methods?
When the classical methods are used to determine these functions, it is
usually assumed that the person's criterion remains constant throughout
the experiment, so that the function measured is due not to changes in
response criterion but to changes in the wavelength or some other
physical property of the stimulus. This is a good assumption because
changing the wavelength of the stimulus probably has little or no effect
on factors such as motivation, which would shift the person's criterion.
Furthermore, experiments such as the one for determining the spectral
sensitivity curve usually use highly experienced people who are trained
to give stable results. Thus, even though the idea of an "absolute
threshold" may not be strictly correct, classical psychophysical
experiments run under well-controlled conditions have remained an
important tool for measuring the relationship between stimuli and
perception.

APPENDIX C  The Signal Detection Approach

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

We will now discuss the theoretical basis for the signal detection
experiments we have just described. Our purpose is to explain the
theoretical bases underlying two ideas: (1) the percentage of hits and
false alarms depends on a person's criterion, and (2) a person's
sensitivity to a stimulus is indicated by the shape of the person's ROC
curve. We will begin by describing two key concepts of signal detection
theory (SDT): signal and noise. (See Swets, 1964.)

Signal and Noise The signal is the stimulus presented to the person.
Thus, in the signal detection experiment we just described, the signal
is the tone. The noise is all the other stimuli in the environment, and
because the signal is usually very faint, noise can sometimes be
mistaken for the signal. Seeing what appears to be a ﬂicker of light in
a completely dark room is an example of visual noise. Seeing light where
there is none is what we have been calling a false alarm, according to
signal detection theory. False alarms are caused by the noise. In the
experiment we just described, hearing a tone on a trial in which no tone
was presented is an example of auditory noise. Let's now consider a
typical signal detection experiment, in which a signal is presented on
some trials and no signal is presented on the other trials. Signal
detection theory describes this procedure not in terms of presenting a
signal or no signal, but in terms of presenting signal plus noise (S 1
N) or noise (N). That is, the noise is always present, and on some
trials, we add a signal. Either condition can result in the perceptual
effect of hearing a tone. A false alarm occurs when the person says
"yes" on a noise trial, and a hit occurs when the person says "yes" on a
signal-plus-noise trial. Now that we have deﬁned signal and noise, we
introduce the idea of probability distributions for noise and signal
plus noise.

Probability Distributions Figure C.3 shows two probability
distributions. The one on the left represents the probability that a
given perceptual effect will be caused by noise (N), and the one on the
right represents the probability that a given perceptual effect will be
caused by signal plus noise (S 1 N). The key to understanding these
distributions is to realize that the value labeled "Perceptual effect
(loudness)" on the horizontal axis is what the person experiences on
each trial. Thus, in an experiment in which the person is asked to
indicate whether a tone is present, the perceptual effect is the
perceived loudness of the tone. Remember that in an SDT experiment the
tone always has the same intensity. The loudness of the tone, however,
can vary from trial to trial. The person perceives different loudnesses
on different trials, because of either trial-to-trial changes in
attention or changes in the state of the person's auditory system. The
probability distributions tell us what the chances are that a given
loudness of tone is due to (N) or to (S 1 N). For

Probability that a loudness on the horizontal axis is due to N or S + N

Signal Detection Theory

N

10

S+N

20

30

Perceptual effect (loudness)

Figure C.3 Probability distributions for noise alone (N, red curve), and
for signal plus noise (S 1 N, green curve). The probability that any
given perceptual effect is caused by the noise (no signal is presented)
or by the signal plus noise (signal is presented) can be determined by
ﬁnding the value of the perceptual effect on the horizontal axis and
extending a vertical line up from that value. The place where that line
intersects the (N) and (S 1 N) distributions indicates the probability
that the perceptual effect was caused by (N) or by (S 1 N).

example, let's assume that a person hears a tone with a loudness of 10
on one of the trials of a signal detection experiment. By extending a
vertical dashed line up from 10 on the "Perceptual effect" axis in
Figure C.3, we see that the probability that a loudness of 10 is due to
(S 1 N) is extremely low, because the distribution for (S 1 N) is
essentially zero at this loudness. There is, however, a fairly high
probability that a loudness of 10 is due to (N), because the (N)
distribution is fairly high at this point. Let's now assume that, on
another trial, the person perceives a loudness of 20. The probability
distributions indicate that when the tone's loudness is 20, it is
equally probable that this loudness is due to (N) or to (S 1 N). We can
also see from Figure C.3 that a tone with a perceived loudness of 30
would have a high probability of being caused by (S 1 N) and only a
small probability of being caused by (N). Now that we understand the
curves of Figure C.3, we can appreciate the problem confronting the
person. On each trial, she has to decide whether no tone (N) was present
or whether a tone (S 1 N) was present. However, the overlap in the
probability distributions for (N) and (S 1 N) means that for some
perceptual effects this judgment will be difﬁcult. As we saw before, it
is equally probable that a tone with a loudness of 20 is due to (N) or
to (S 1 N). So, on a trial in which the person hears a tone with a
loudness of 20, how does she decide whether the signal was presented?
According to signal detection theory, the person's decision depends on
the location of her criterion.

The Criterion We can see how the criterion affects the person's response
by looking at Figure C.4. In this ﬁgure, we have labeled three different
criteria: liberal (L), neutral (N), and conservative (C). Remember that
we can cause people to adopt these different criteria by means of
different payoffs. According to signal detection theory, once the person
adopts a criterion, he or she Signal Detection Theory

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

423

L

N N

Shanna

C

L L9

S+N N9

Cathy

Loudness

Figure C.4 The same probability distributions from Figure C.3, showing
three criteria: liberal (L), neutral (N), and conservative (C). When a
person adopts a criterion, he or she uses the following decision rule:
Respond "yes" ("I detect the stimulus") when the perceptual effect is
greater than the criterion, and respond "no" ("I do not detect the
stimulus") when the perceptual effect is less than the criterion.

uses the following rule to decide how to respond on a given trial: If
the perceptual effect is greater than (to the right of) the criterion,
say "Yes, the tone was present"; if the perceptual effect is less than
(to the left of) the criterion, say "No, the tone was not present."
Let's consider how different criteria inﬂuence the person's hits and
false alarms. To determine how the criterion affects the person's hits
and false alarms, we will consider what happens when we present (N) and
when we present (S 1 N) under three different criteria. Liberal
Criterion 1. Present (N): Because most of the probability distribution
for (N) falls to the right of the criterion, the chances are good that
presenting (N) will result in a loudness to the right of the criterion.
This means that the probability of saying "yes" when (N) is presented is
high; therefore, the probability of a false alarm is high. 2. Present (S
1 N): Because the entire probability distribution for (S 1 N) falls to
the right of the criterion, the chances are excellent that presenting (S
1 N) will result in a loudness to the right of the criterion. Thus, the
probability of saying "yes" when the signal is presented is high;
therefore, the probability of a hit is high. Because criterion L results
in high false alarms and high hits, adopting that criterion will result
in point L on the ROC curve in Figure C.5. Neutral Criterion 1. Present
(N): The person will answer "yes" only rarely when (N) is presented
because only a small portion of the (N) distribution falls to the right
of the criterion. The false-alarm rate, therefore, will be fairly low.
2. Present (S 1 N): The person will answer "yes" frequently when (S 1 N)
is presented because most of the (S 1 N) distribution falls to the right
of the criterion. The hit rate, therefore, will be fairly high (but not
as high as for the L criterion). Criterion N results in point N on the
ROC curve in Figure C.5. 424

Percentage hits

N

C C9 Percentage false alarms

Figure C.5 ROC curves for Cathy (solid curve) and Shanna (dashed curve)
determined using liberal (L, L9), neutral (N, N9), and conservative (C,
C9) criteria.

Conservative Criterion 1. Present (N): False alarms will be very low
because none of the (N) curve falls to the right of the criterion. 2.
Present (S 1 N): Hits will also be low because only a small portion of
the (S 1 N) curve falls to the right of the criterion. Criterion C
results in point C on the ROC curve in Figure C.5. You can see that
applying different criteria to the probability distributions generates
the solid ROC curve in Figure C.5. But why are these probability
distributions necessary? After all, when we described the experiment
with Cathy and Lucy, we determined the ROC curve simply by plotting the
results of the experiment. The reason the (N) and (S 1 N) distributions
are important is that, according to signal detection theory, the
person's sensitivity to a stimulus is indicated by the distance (d9)
between the peaks of the (N) and (S 1 N) distributions, and this
distance affects the shape of the ROC curve. We will now consider how
the person's sensitivity to a stimulus affects the shape of the ROC
curve.

The Effect of Sensitivity on the ROC Curve We can understand how the
person's sensitivity to a stimulus affects the shape of the ROC curve by
considering what the probability distributions would look like for
Shanna, a person with supersensitive hearing. Shanna's hearing is so
good that a tone barely audible to Cathy sounds very loud to Shanna. If
presenting (S 1 N) causes Shanna to hear a loud tone, this means that
her (S 1 N) distribution should be far to the right, as shown in Figure
C.6. In signal detection terms, we would say that Shanna's high
sensitivity is indicated by the large separation (d9) between the (N)
and the (S 1 N) probability distributions. To see how this greater
separation between the probability distributions will affect her ROC
curve, let's see how she would respond when adopting liberal, neutral,
and conservative criteria.

APPENDIX C  The Signal Detection Approach

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

d9

Shanna L

N N

C S+N

the neutral criterion, whereas less of Cathy's does (Figure C.4). The
neutral criterion, therefore, results in point N9 on the ROC curve in
Figure C.5. Conservative Criterion 1. Present (N): low false alarms. 2.
Present (S 1 N): low hits.

Loudness

Figure C.6 Probability distributions for Shanna, a person who is
extremely sensitive to the signal. The noise distribution (red) remains
the same, but the (S 1 N) distribution (green) is shifted to the right
compared to the curves in Figure C.4. Liberal (L), neutral (N), and
conservative (C) criteria are shown.

Liberal Criterion 1. Present (N): high false alarms. 2. Present (S 1 N):
high hits. The liberal criterion, therefore, results in point L9 on the
ROC curve of Figure C.5. Neutral Criterion 1. Present (N): low false
alarms. It is important to note that Shanna's false alarms for the
neutral criterion will be lower than Cathy's false alarms for the
neutral criterion because only a very small portion of Shanna's (N)
distribution falls to the right of the criterion, whereas more of
Cathy's (N) distribution falls to the right of the neutral criterion
(Figure C.4). 2. Present (S 1 N): high hits. In this case, Shanna's hits
will be higher than Cathy's because almost all of Shanna's (S 1 N)
distribution falls to the right of

The conservative criterion, therefore, results in point C9 on the ROC
curve. The difference between the two ROC curves in Figure C.5 is
obvious because Shanna's curve is more "bowed." But before you conclude
that the difference between these two ROC curves has anything to do with
where we positioned Shanna's L, N, and C criteria, see whether you can
get an ROC curve like Shanna's from the two probability distributions of
Figure C.4. You will ﬁnd that, no matter where you position the
criteria, there is no way that you can get a point like point N9 (with
very high hits and very low false alarms) from the curves of Figure C.4.
In order to achieve very high hits and very low false alarms, the two
probability distributions must be spaced far apart, as in Figure C.6.
Thus, increasing the distance (d9) between the (N) and the (S 1 N)
probability distributions changes the shape of the ROC curve. When the
person's sensitivity (d9) is high, the ROC curve is more bowed. In
practice, d9 can be determined by comparing the experimentally
determined ROC curve to standard ROC curves (see Gescheider, 1976), or
d9 can be calculated from the proportions of hits and false alarms that
occur in an experiment by means of a mathematical procedure we will not
discuss here. This mathematical procedure for calculating d9 enables us
to determine a person's sensitivity by determining only one data point
on an ROC curve, thus using the signal detection procedure without
running a large number of trials.

Signal Detection Theory

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

425

Glossary The number in parentheses at the end of each entry indicates
the chapter in which the term is first used. #TheDress The online
address for a picture of a dress that is seen as alternating blue and
black stripes by some people and as alternating white and gold stripes
by others. (9) Aberrations Imperfections on the eye's cornea and lens
that distort light on its way to the retina. (9) Ablation Removal of an
area of the brain. This is usually done in experiments on animals to
determine the function of a particular area. Also called lesioning. (4)
Absolute disparity See Angle of disparity. (10) Absolute threshold The
smallest stimulus level that can just be detected. (1) Absorption
spectrum A plot of the amount of light absorbed by a visual pigment
versus the wavelength of light. (3) Accommodation In vision, bringing
objects located at different distances into focus by changing the shape
of the lens. (3) Accretion A cue that provides information about the
relative depth of two surfaces. Occurs when the farther object is
uncovered by the nearer object due to sideways movement of an observer
relative to the objects. See also Deletion. (10) Achromatic color Color
without hue. White, black, and all the grays between these two extremes
are achromatic colors. (9) Acoustic shadow The shadow created by the
head that decreases the level of high-frequency sounds on the opposite
side of the head. The acoustic shadow is the basis of the localization
cue of interaural level difference. (12) Acoustic signal The pattern of
frequencies and intensities of the sound stimulus. (14) Acoustic
stimulus See Acoustic signal. (14) Across-fiber patterns The pattern of
nerve firing that a stimulus causes across a number of neurons. Also
referred to as distributed coding. (16) Action Motor activities in
response to a stimulus. (1) Action affordance A response to an object
that involves both its affordance (what it is for) and the action
associated with it. (7) Action pathway See Dorsal pathway. (4) Action
potential Rapid increase in positive charge in a nerve fiber that
travels down the fiber. Also called the nerve impulse. (2)
Action-specific perception hypothesis Hypothesis that people perceive
their environment in terms of their ability to act on it. (7) Active
touch Touch in which the observer plays an active role in touching and
exploring an object, usually with his or her hands. (15) Adaptive
optical imaging A technique that makes it possible to look into a
person's eye and take pictures of the receptor array in the retina. (9)
Additive color mixture See Color mixture, additive. (9)

Adjustment, method of A psychophysical method in which the experimenter
or the observer adjusts the stimulus intensity in a continuous manner
until the observer detects the stimulus. (1) Adult-directed speech
Speech that is directed toward an adult. (14) Affective (emotional)
component of pain The emotional experience associated with pain---for
example, pain described as torturing, annoying, frightful, or sickening.
See also Sensory component of pain. (15) Affective function of touch The
eliciting of emotions by touch. (15) Affordance The information
specified by a stimulus pattern that indicates how the stimulus can be
used. An example of an affordance would be seeing a chair as something
to sit on or a flight of stairs as something to climb. (7) Agnosia See
Visual form agnosia. (1) Akinetopsia A condition in which damage to an
area of the cortex involved in motion perception causes blindness to
motion. (8) Alzheimer's disease Serious loss of memory and other
cognitive functions that is often preceded by mild cognitive impairment
(MCI). (16) Amacrine cell A neuron that transmits signals laterally in
the retina. Amacrine cells synapse with bipolar cells and ganglion
cells. (3) Ames room A distorted room, first built by Adelbert Ames,
that creates an erroneous perception of the sizes of people in the room.
The room is constructed so that two people at the far wall of the room
appear to stand at the same distance from an observer. In actuality, one
of the people is much farther away than the other. (10) Amiloride A
substance that blocks the flow of sodium into taste receptors. (16)
Amplitude In the case of a repeating sound wave, such as the sine wave
of a pure tone, amplitude represents the pressure difference between
atmospheric pressure and the maximum pressure of the wave. (11)
Amplitude modulation Adjusting the level (or intensity) of a sound
stimulus so it fluctuates up and down. (11) Amplitude-modulated noise A
noise sound stimulus that is amplitude modulated. (11) Amygdala A
subcortical structure that is involved in emotional responding and in
processing olfactory signals. (16) Angle of disparity The visual angle
between the images of an object on the two retinas. When images of an
object fall on corresponding points, the angle of disparity is zero.
When images fall on noncorresponding points, the angle of disparity
indicates the degree of noncorrespondence. (10) Angular size contrast
theory An explanation of the moon illusion that states that the
perceived size of the moon is determined by the sizes of the objects
that surround it. According to this idea, the moon appears small when it
is surrounded by large objects, such as the expanse of the sky when the
moon is overhead. (10)

426

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Anomalous trichromatism A type of color deficiency in which a person
needs to mix a minimum of three wavelengths to match any other
wavelength in the spectrum but mixes these wavelengths in different
proportions than a trichromat. (9) Anosmia Loss of the ability to smell
due to injury or infection. (16) Anterior belt area The front of the
posterior belt in the temporal lobe, which is involved in perceiving
sound. (12) Aperiodic sound Sound waves that do not repeat. See Periodic
sound. (11) Aperture problem Occurs when only a portion of a moving
stimulus can be seen, as when the stimulus is viewed through a narrow
aperture or through the "field of view" of a neurons' receptive field.
This can result in misleading information about the direction in which
the stimulus is moving. (8) Apex (of the cochlea) The end of the cochlea
farthest from the middle ear. (11) Aphasia Difficulties in speaking or
understanding speech due to brain damage. (14) Apparent distance theory
An explanation of the moon illusion that is based on the idea that the
horizon moon, which is viewed across the filled space of the terrain,
should appear farther away than the zenith moon, which is viewed through
the empty space of the sky. This theory states that because the horizon
and zenith moons have the same visual angle but are perceived to be at
different distances, the farther appearing horizon moon should appear
larger. (10) Apparent motion See Apparent movement. (8) Apparent
movement An illusion of movement that occurs when two objects separated
in space are presented rapidly, one after another, separated by a brief
time interval. (5) Arch trajectory The rise and then fall in pitch
commonly found in music. (13) Architectural acoustics The study of how
sounds are reflected in rooms. An important concern of architectural
acoustics is how these reflected sounds change the quality of the sounds
we hear. (12) Area V1 The visual receiving area of the brain, called
area V1 to indicate that it is the first visual area in the cortex. Also
called the striate cortex. (4) Articulator Structure involved in speech
production, such as the tongue, lips, teeth, jaw, and soft palate. (14)
Atmospheric perspective A depth cue. Objects that are farther away look
more blurred and bluer than objects that are closer because we look
through more air and particles to see them. (10) Attack The buildup of
sound energy that occurs at the beginning of a tone. (11) Attention The
process of focusing on some objects while ignoring others. Attention can
enhance the processing of the attended object. (6) Attentional capture
Occurs when stimulus salience causes an involuntary shift of attention.
For example, attention can be captured by movement. (6) Audibility curve
A curve that indicates the sound pressure level (SPL) at threshold for
frequencies across the audible spectrum. (11) Audiogram Plot of hearing
loss versus frequency. (11) Audiovisual mirror neuron Neuron that
responds to actions that produce sounds. These neurons respond when a
monkey performs a hand action and when it hears the sound associated
with this action. See also Mirror neuron. (7) Audiovisual speech
perception A perception of speech that is affected by both auditory and
visual stimulation, as when a person sees a video of someone making the
lip movements for /fa/ while hearing the sound /ba/ and perceives /fa/.
Also called the McGurk effect. (14)

Auditory canal The canal through which air vibrations travel from the
environment to the tympanic membrane. (11) Auditory localization The
perception of the location of a sound source. (12) Auditory response
area The psychophysically measured area that defines the frequencies and
sound pressure levels over which hearing functions. This area extends
between the audibility curve and the curve for the threshold of feeling.
(11) Auditory scene The sound environment, which includes the locations
and qualities of individual sound sources. (12) Auditory scene analysis
The process by which the sound stimuli produced by different sources in
an auditory scene become perceptually organized into sounds at different
locations and into separated streams of sound. (12) Auditory space
Perception of where sounds are located in space. Auditory space extends
around a listener's head in all directions, existing wherever there is a
sound. (12) Auditory stream segregation The effect that occurs when a
series of sounds that differ in pitch or timbre are played so that the
tones become perceptually separated into simultaneously occurring
independent streams of sound. (12) Automatic speech recognition (ASR)
Using computers to recognize speech. (14) Axial myopia Myopia
(nearsightedness) in which the eyeball is too long. See also Refractive
myopia. (3) Axon The part of the neuron that conducts nerve impulses
over distances. Also called the nerve fiber. (2) Azimuth In hearing,
specifies locations that vary from left to right relative to the
listener. (12) Base (of the cochlea) The end of the cochlea nearest the
middle ear. (11) Basilar membrane A membrane that stretches the length
of the cochlea and controls the vibration of the cochlear partition.
(11) Bayesian inference A statistical approach to perception in which
perception is determined by taking probabilities into account. These
probabilities are based on past experiences in perceiving properties of
objects and scenes. (5) Beat In music, equally spaced intervals of time,
which occurs even if there are no notes. When you tap your feet to
music, you are tapping on the beat. (13) Bimodal neuron A neuron that
responds to stimuli associated with more than one sense. (16) Binaural
cue Sound localization cue that involves both ears. Interaural time
difference and interaural level difference are the primary binaural
cues. (12) Binding The process by which features such as color, form,
motion, and location are combined to create our perception of a coherent
object. Binding can also occur across senses, as when sound and vision
are associated with the same object. (6) Binocular depth cell A neuron
in the visual cortex that responds best to stimuli that fall on points
separated by a specific degree of disparity on the two retinas. Also
called a disparity-selective cell. (10) Binocular disparity Occurs when
the retinal images of an object fall on disparate points on the two
retinas. (10) Binocular rivalry A situation in which one image is
presented to the left eye, a different image is presented to the right
eye, and perception alternates back and forth between the two images.
(5) Binocularly fixate Directing the two foveas to exactly the same
spot. (10) Biological motion Motion produced by biological organisms.
Most of the experiments on biological motion have used walking humans
with lights attached to their joints and limbs as stimuli. See also
Point-light walker. (8) Glossary

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

427

Bipolar cell A retinal neuron that receives inputs from the visual
receptors and sends signals to the retinal ganglion cells. (3) Blind
spot The small area where the optic nerve leaves the back of the eye.
There are no visual receptors in this area, so small images falling
directly on the blind spot cannot be seen. (3) Border ownership When two
areas share a border, as occurs in figure--ground displays, the border
is usually perceived as belonging to the figure. (5) Bottom-up
processing Processing that is based on the information on the receptors.
Also called data-based processing. (1) Brain imaging Procedures that
make it possible to visualize areas of the human brain that are
activated by different types of stimuli, tasks, or behaviors. The most
common technique used in perception research is functional magnetic
resonance imaging (f MRI). (2) Broca's aphasia Language problems
including labored and stilted speech and short sentences, caused by
damage to Broca's area in the frontal lobe. (13) Broca's area An area in
the frontal lobe that is important for language perception and
production. One effect of damage is difficulty in speaking. (2) Calcium
imaging A method of measuring receptor activity by using fluorescence to
measure the concentration of calcium inside the receptor. This technique
has been used to measure the activation of olfactory receptor neurons.
(16) Categorical perception In speech perception, perceiving one sound
at short voice onset times and another sound at longer voice onset
times. The listener perceives only two categories across the whole range
of voice onset times. (14) Categorize Placing objects into categories,
such as "tree," "bird," "car." (1) Cell body The part of a neuron that
contains the neuron's metabolic machinery and that receives stimulation
from other neurons. (2) Center-surround antagonism The competition
between the center and surround regions of a center-surround receptive
field, caused by the fact that one is excitatory and the other is
inhibitory. Stimulating center and surround areas simultaneously
decreases responding of the neuron, compared to stimulating the
excitatory area alone. (3) Center-surround receptive field A receptive
field that has a centersurround organization. (3) Cerebral achromatopsia
A loss of color vision caused by damage to the cortex. (9) Cerebral
cortex The 2-mm-thick layer that covers the surface of the brain and
contains the machinery for creating perception, as well as for other
functions, such as language, memory, and thinking. (1) Change blindness
Difficulty in detecting differences between two visual stimuli that are
presented one after another, often with a short blank stimulus
interposed between them. Also occurs when part of a stimulus is changed
very slowly. (6) Characteristic frequency The frequency at which a
neuron in the auditory system has its lowest threshold. (11) Chemotopic
map The pattern of activation in the olfactory system in which chemicals
with different properties create a "map" of activation based on these
properties. For example, there is evidence that chemicals are mapped in
the olfactory bulb based on carbon-chain length. Also called odor map.
(16) Chevreul illusion Occurs when areas of different lightness are
positioned adjacent to one another to create a border. The illusion is
the perception of a light band on the light side of the border and a
dark band on the dark side of the border, even though these bands do not
exist in the intensity distribution. (3) 428

Chromatic adaptation Exposure to light in a specific part of the visible
spectrum. This adaptation can cause a decrease in sensitivity to light
from the area of the spectrum that was presented during adaptation. (9)
Chromatic color Color with hue, such as blue, yellow, red, or green. (9)
Classical psychophysical methods The methods of limits, adjustment, and
constant stimuli, described by Fechner, that are used for measuring
thresholds. (1) Cloze probability task Task in which a listener is
presented with a melody, which suddenly stops. The listener's task is to
sing the note they think comes next. This task is also used in the study
of language, in which case part of a sentence is presented and the
listener predicts the word that will come next. (13) Coarticulation The
overlapping articulation that occurs when different phonemes follow one
another in speech. Because of these effects, the same phoneme can be
articulated differently depending on the context in which it appears.
For example, articulation of the /b/ in boot is different from
articulation of the /b/ in boat. (14) Cochlea The snail-shaped,
liquid-filled structure that contains the structures of the inner ear,
the most important of which are the basilar membrane, the tectorial
membrane, and the hair cells. (11) Cochlear amplifier Expansion and
contraction of the outer hair cells in response to sound sharpens the
movement of the basilar membrane to specific frequencies. This
amplifying effect plays an important role in determining the frequency
selectivity of auditory nerve fibers. (11) Cochlear implant A device in
which electrodes are inserted into the cochlea to create hearing by
electrically stimulating the auditory nerve fibers. This device is used
to restore hearing in people who have lost their hearing because of
damaged hair cells. (14) Cochlear nucleus The nucleus where nerve fibers
from the cochlea first synapse. (11) Cochlear partition A partition in
the cochlea, extending almost its full length, that separates the scala
tympani and the scala vestibuli. The organ of Corti, which contains the
hair cells, is part of the cochlear partition. (11) Cocktail party
effect The ability to focus on one stimulus while filtering out other
stimuli, so called because at noisy parties people are able to focus on
what one person is saying even though there are many conversations
happening at the same time. (6) Cognitive map A mental map of the
spatial layout of an area of the environment. (7) Cognitivist approach
(to musical emotion) Approach to describing the emotional response to
music which proposes that listeners can perceive the emotional meaning
of a piece of music, but that they don't actually feel the emotions.
(13) Coherence In research on movement perception in which arrays of
moving dots are used as stimuli, the degree of correlation between the
direction of the moving dots. Zero percent coherence means all of the
dots are moving independently; 100 percent coherence means all of the
dots are moving in the same direction. (8) Coincidence detectors Neurons
in the Jeffress neural coincidence model, which was proposed to explain
how neural firing can provide information regarding the location of a
sound source. A neural coincidence detector fires when signals from the
left and right ears reach the neuron simultaneously. Different neural
coincidence detectors fire to different values of interaural time
difference. See also Jeffress model. (12) Color blindness A condition in
which a person perceives no chromatic color. This can be caused by
absent or malfunctioning cone receptors or by cortical damage. (9)

Glossary

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Color circle Perceptually similar colors located next to each other and
arranged in a circle. (9) Color constancy The effect in which the
perception of an object's hue remains constant even when the wavelength
distribution of the illumination is changed. Partial color constancy
occurs when our perception of hue changes a little when the illumination
changes, though not as much as we might expect from the change in the
wavelengths of light reaching the eye. (9) Color deficiency Condition
(sometimes incorrectly called color blindness) in which people see fewer
colors than people with normal color vision and need to mix fewer
wavelengths to match any other wavelength in the spectrum. (9) Color
mixture, additive The creation of colors that occurs when lights of
different colors are superimposed. (9) Color mixture, subtractive The
creation of colors that occurs when paints of different colors are mixed
together. (9) Color matching A procedure in which observers are asked to
match the color in one field by mixing two or more lights in another
field. (9) Color solid A solid in which colors are arranged in an
orderly way based on their hue, saturation, and value. (9) Common fate,
principle of A Gestalt principle of perceptual organization that states
that things that are moving in the same direction appear to be grouped
together. (5) Common region, principle of A modern Gestalt principle
that states that elements that are within the same region of space
appear to be grouped together. (5) Comparator A structure hypothesized
by the corollary discharge theory of movement perception. The corollary
discharge signal and the sensory movement signal meet at the comparator
to determine whether movement will be perceived. (6) Complex cell A
neuron in the visual cortex that responds best to moving bars with a
particular orientation. (4) Cone of confusion A surface in the shape of
a cone that extends out from the ear. Sounds originating from different
locations on this surface all have the same interaural level difference
and interaural time difference, so location information provided by
these cues is ambiguous. (12) Cone mosaic Arrangement of short-,
medium-, and long-wavelength cones in a particular area of the retina.
(9) Cone spectral sensitivity curve A plot of visual sensitivity versus
wavelength for cone vision. Often measured by presenting a small spot of
light to the fovea, which contains only cones. Can also be measured when
the eye is light adapted, so cones are the most sensitive receptors. (3)
Cones Cone-shaped receptors in the retina that are primarily responsible
for vision in high levels of illumination and for color vision and
detail vision. (3) Conflicting cues theory A theory of visual illusions
proposed by R. H. Day, which states that our perception of line length
depends on an integration of the actual line length and the overall
figure length. (10) Congenital amusia A condition in which a person
doesn't recognize tones as tones and therefore does not experience
sequences of tones as music. (13) Conjunction search A visual search
task in which it is necessary to search for a combination (or
conjunction) of two or more features on the same stimulus to find the
target. An example of a conjunction search would be looking for a
horizontal green line among vertical green lines and horizontal red
lines. (6) Consonance The positive sound quality created when two or
more pitches are played together. (13) Constant stimuli, method of A
psychophysical method in which a number of stimuli with different
intensities are presented repeatedly in a random order. (1)

Contextual modulation Change in response to a stimulus presented within
a neuron's receptive field caused by stimulation outside of the
receptive field. (4) Continuity error Mismatch, usually involving
spatial position or objects, that occurs from one film shot to another.
(6) Contralateral Side of the body opposite to the side on which a
particular condition occurs. (4) Contrast threshold The intensity
difference between two areas that can just barely be seen. This is often
measured using gratings with alternating light and dark bars. (4)
Convergence (depth cue) See Perspective convergence. (10) Convergence
(neural) When many neurons synapse onto a single neuron. (3) Cornea The
transparent focusing element of the eye that is the first structure
through which light passes as it enters the eye. The cornea is the eye's
major focusing element. (3) Corollary discharge signal (CDS) A copy of
the motor signal that is sent to the eye muscles to cause movement of
the eye. The copy is sent to the hypothetical comparator of corollary
discharge theory. (6) Corollary discharge theory The theory that
explains motion perception as being determined both by movement of the
image on the retina and by signals that indicate movement of the eyes.
See also Corollary discharge signal. (6) Correct rejection In a signal
detection experiment, saying "No, I don't detect a stimulus" on a trial
in which the stimulus is not presented (a correct response). (Appendix
C) Correspondence problem The problem faced by the visual system, which
must determine which parts of the images in the left and right eyes
correspond to one another. Another way of stating the problem is: How
does the visual system match up the images in the two eyes? This
matching of the images is involved in determining depth perception using
the cue of binocular disparity. (10) Corresponding retinal points The
points on each retina that would overlap if one retina were slid on top
of the other. Receptors at corresponding points send their signals to
the same location in the brain. (10) Cortical magnification Occurs when
a disproportionately large area on the cortex is activated by
stimulation of a small area on the receptor surface. One example of
cortical magnification is the relatively large area of visual cortex
that is activated by stimulation of the fovea. An example in the
somatosensory system is the large area of somatosensory cortex activated
by stimulation of the lips and fingers. (4) Cortical magnification
factor The size of the cortical magnification effect. (4) Covert
attention Attention without looking. Seeing something "out of the corner
of your eye" is an example of covert attention. (6) COVID-19 An acute
respiratory illness in humans caused by a coronavirus, originally
identified in China in 2019, and becoming a pandemic in 2020. (16)
Crossed disparity Disparity that occurs when one object is being
fixated, and is therefore on the horoptor, and another object is located
in front of the horoptor, closer to the observer. (10) CT afferents
Unmyelinated nerve fibers found in hairy skin, which have been shown to
be involved in social touch. (15) Cue approach to depth perception The
approach to explaining depth perception that focuses on identifying
information in the retinal image that is correlated with depth in the
scene. Some of the depth cues that have been identified are overlap,
relative height, relative size, atmospheric perspective, convergence,
and accommodation. (10) Cutaneous receptive field Area of skin that,
when stimulated, influences the firing of a neuron. (15) Glossary

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

429

Cutaneous senses The ability to perceive sensations, such as touch and
pain, that are based on the stimulation of receptors in the skin. (15)
Dark adaptation Visual adaptation that occurs in the dark, during which
the sensitivity to light increases. This increase in sensitivity is
associated with regeneration of the rod and cone visual pigments. (3)
Dark adaptation curve The function that traces the time course of the
increase in visual sensitivity that occurs during dark adaptation. (3)
Dark-adapted sensitivity The sensitivity of the eye after it has
completely adapted to the dark. (3) Data-based processing Another name
for bottom-up processing. Refers to processing that is based on incoming
data, as opposed to top-down, or knowledge-based, processing, which is
based on prior knowledge. (1) Decay The decrease in the sound signal
that occurs at the end of a tone. (11) Decibel (dB) A unit that
indicates the pressure of a sound stimulus relative to a reference
pressure: dB 5 20 log (p/po) where p is the pressure of the tone and po
is the reference pressure. (11) Decoder A computer program that can
predict the most likely stimulus based on the voxel activation patterns
that were previously observed in the calibration phase of neural mind
reading. (5) Delay unit A component of the Reichardt detector proposed
to explain how neural firing occurs to different directions of movement.
The delay unit delays the transmission of nerve impulses as they travel
from the receptors toward the brain. (8) Deletion A cue that provides
information about the relative depth of two surfaces. Deletion occurs
when a farther object is covered by a nearer object due to sideways
movement of an observer relative to the objects. See also Accretion.
(10) Dendrites Nerve processes on the cell body that receive stimulation
from other neurons. (2) Depolarization When the inside of a neuron
becomes more positive, as occurs during the initial phases of the action
potential. Depolarization is often associated with the action of
excitatory neurotransmitters. (2) Dermis The layer of skin below the
epidermis. (15) Desaturated Low saturation in chromatic colors as would
occur when white is added to a color. For example, pink is not as
saturated as red. (9) Detached retina A condition in which the retina is
detached from the back of the eye. (3) Detection threshold For olfaction
the detection threshold is the lowest concentration at which an odorant
can be detected. (16) Deuteranopia A form of dichromatism in which a
person is missing the medium-wavelength pigment. A deuteranope perceives
blue at short wavelengths, sees yellow at long wavelengths, and has a
neutral point at about 498 nm. (9) Dichotic listening Attention
experiment technique involving hearing where dichotic refers to
presenting different stimuli to the left and right ears. (6) Dichromat A
person who has a form of color deficiency. Dichromats can match any
wavelength in the spectrum by mixing two other wavelengths. (9)
Dichromatism A form of color deficiency in which a person has just two
types of cone pigment and so can see chromatic colors but confuse some
colors that trichromats can distinguish. Difference threshold The
minimum difference that must exist between two stimuli before we can
tell the difference between them. (1) Direct pathway model of pain The
idea that pain occurs when nociceptor receptors in the skin are
stimulated and send their signals to

430

the brain. This model does not account for the fact that pain can be
affected by other factors in addition to stimulation of the skin. (15)
Direct sound Sound that is transmitted directly from a sound source to
the ears. (12) Discriminative function of touch Functions of the touch
system such as sensing details, texture, vibration, and objects. (15)
Dishabituation An increase in responding that occurs when a stimulus is
changed. This response is used in testing infants to see whether they
can differentiate two stimuli. (9) Disparity-selective cell See
Binocular depth cell. (10) Disparity tuning curve A plot of a neuron's
response versus the degree of disparity of a visual stimulus. The
disparity to which a neuron responds best is an important property of
disparityselective cells, which are also called binocular depth cells.
(10) Dissonance The negative sound quality created when two or more
pitches are played together. (13) Distal stimulus The stimulus "out
there," in the external environment. (1) Distance How far a stimulus is
from the observer. In hearing, the distance coordinate specifies how far
the sound source is from the listener. (12) Distributed representation
Occurs when a stimulus causes neural activity in a number of different
areas of the brain, so the activity is distributed across the brain. (2)
Dopamine Neurotransmitter that is involved in reward-motivated behavior.
Dopamine has been associated with the rewarding effects of music. (13)
Dorsal pathway Pathway that conducts signals from the striate cortex to
the parietal lobe. The dorsal pathway has also been called the where,
the how, or the action pathway by different investigators. (4) Double
dissociation In brain damage, when function A is present and function B
is absent in one person, and function A is absent and function B is
present in another. Presence of a double dissociation means that the two
functions involve different mechanisms and operate independently of one
another. (4) Dual-stream model of speech perception Model that proposes
a ventral stream starting in the temporal lobe that is responsible for
recognizing speech, and a dorsal stream starting in the parietal lobe
that is responsible for linking the acoustic signal to the movements
used to produce speech. (14) Duple meter In Western music, meter in
which accents are in multiples of two, such as 12 12 12 or 1234 1234
1234, like a march. (13) Duplex theory of texture perception The idea
that texture perception is determined by both spatial and temporal cues
that are detected by two types of receptors. Originally proposed by
David Katz and now called the "duplex theory." (15) Eardrum Another term
for the tympanic membrane, the membrane located at the end of the
auditory canal that vibrates in response to pressure changes. This
vibration is transmitted to the bones of the middle ear. (11) Early
right anterior negativity (ERAN) Physiological "surprise response"
experienced by listeners, occurring in the right hemisphere of the
brain, in reaction to violations of linguistic or musical syntax. (13)
Echolocation Locating objects by sending out high-frequency pulses and
sensing the echo created when these pulses are reflected from objects in
the environment. Echolocation is used by bats and dolphins. (10)
Ecological approach to perception This approach focuses on specifying
the information in the environment that is used for perception,
emphasizing the study of moving observers to determine

Glossary

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

how their movement results in perceptual information that both creates
perception and guides further movement. (7) Edge enhancement An increase
in perceived contrast at borders between regions of the visual field.
(3) Effect of the missing fundamental Removing the fundamental frequency
and other lower harmonies from a musical tone does not change the tone's
pitch. (11) Electromagnetic spectrum Continuum of electromagnetic energy
that extends from very-short-wavelength gamma rays to longwavelength
radio waves. Visible light is a narrow band within this spectrum. (1)
Elevation In hearing, sound locations that are up and down relative to
the listener. (12) Emmert's law A law stating that the size of an
afterimage depends on the distance of the surface against which the
afterimage is viewed. The farther away the surface, the larger the
afterimage appears. (10) Emotivist approach (to musical emotion)
Approach to describing the emotional response to music which proposes
that a listener's emotional response to music involves actually feeling
the emotions. (13) Empathy The ability to share and vicariously
experience what someone else is feeling. (15) Endorphin Chemical that is
naturally produced in the brain and that causes analgesia. (15)
End-stopped cell A cortical neuron that responds best to lines of a
specific length that are moving in a particular direction. (4) Epidermis
The outer layers of the skin, including a layer of dead skin cells. (15)
Equal loudness curve A curve that indicates the sound pressure levels
that result in a perception of the same loudness at frequencies across
the audible spectrum. (11) Event A segment of time at a particular
location that is perceived by observers to have a beginning and an
ending. (8) Event boundary The point in time when one event ends and
another begins. (8) Event-related potential (ERP) The brain's response
to a specific event, such as flashing an image or presenting a tone, as
measured with small disc electrodes placed on a person's scalp. (13)
Evolutionary adaptation A function which evolved specifically to aid in
survival and reproduction. (13) Excitatory area Area of a receptive
field that is associated with excitation. Stimulation of this area
causes an increase in the rate of nerve firing. (3) Excitatory response
The response of a nerve fiber in which the firing rate increases. (2)
Excitatory-center, inhibitory-surround receptive field A centersurround
receptive field in which stimulation of the center area causes an
excitatory response and stimulation of the surround causes an inhibitory
response. (3) Experience-dependent plasticity A process by which neurons
adapt to the specific environment within which a person or animal lives.
This is achieved when neurons change their response properties so they
become tuned to respond best to stimuli that have been repeatedly
experienced in the environment. See also Neural plasticity; Selective
rearing. (4) Experience sampling Technique used to measure the thoughts,
feelings, and behaviors of people at various random points in time
during the day. This technique has been used to measure the frequency of
mind-wandering. (6) Expertise hypothesis The idea that human proficiency
in perceiving certain things can be explained by changes in the brain
caused by long exposure, practice, or training. (5)

Exploratory procedures (EPs) People's movements of their hands and
fingers while they are identifying three-dimensional objects by touch.
(15) Extinction A condition associated with brain damage in which there
is a lack of awareness of what is happening in one side of the visual
field. (6) Extrastriate body area (EBA) An area of the temporal lobe
that is activated by pictures of bodies and parts of bodies. (5)
Extrastriate cortex Collective term for visual areas in the occipital
lobe and beyond known as V2, V3, V4, and V5. (4) Eye The eyeball and its
contents, which include focusing elements, the retina, and supporting
structures. (3) Falling phase of the action potential In the axon, or
nerve fiber, the increase in negativity from 140 mV back to 270 mV (the
resting potential level) that occurs during the action potential. This
increase in negativity is associated with the flow of positively charged
potassium ions (K1) out of the axon. (2) False alarm In a signal
detection experiment, saying "Yes, I detect the stimulus" on a trial in
which the stimulus is not presented (an incorrect response). (Appendix
C) Familiar size A depth cue in which judgment of distance is based on
knowledge of the sizes of objects. Epstein's coin experiment illustrated
the operation of the cue of familiar size by showing that the relative
sizes of the coins influenced perception of the coins' distances. (10)
Farsightedness See Hyperopia. (3) Feature detector A neuron that
responds selectively to a specific feature of the stimulus such as
orientation or direction of motion. (4) Feature integration theory (FIT)
A theory proposed by Anne Treisman to explain how an object is broken
down into features and how these features are recombined to result in a
perception of the object. (6) Feature search A visual search task in
which a person can find a target by searching for only one feature. An
example would be looking for a horizontal green line among vertical
green lines. (6) Figural cue Visual cue that determines how an image is
segregated into figure and ground. (5) Figure When an object is seen as
separate from the background (the "ground"), it is called a figure. See
also Figure--ground segregation. (5) Figure--ground segregation The
perceptual separation of an object from its background. (5) First
harmonic See Fundamental frequency. (11) Fixation The brief pause of the
eye that occurs between eye movements as a person scans a scene. (6)
Flavor The perception that occurs from the combination of taste and
olfaction. (16) Focus of expansion (FOE) The point in the flow pattern
caused by observer movement in which there is no expansion. According to
J. J. Gibson, the focus of expansion always remains centered on the
observer's destination. (7) Focused attention meditation Common form of
meditation in which a person focuses on a specific object, which can be
the breath, a sound, a mantra (a syllable, word, or group of words), or
a visual stimulus. (6) Focused attention stage (of perceptual
processing) The stage of processing in feature integration theory in
which the features are combined. According to Treisman, this stage
requires focused attention. (6) Forced-choice method Method in which two
choices are given, and the subject has to pick one. For example, a
subject is presented with a weak odorant on one trial, and no odorant on
another trial, and has to pick the trial on which the odorant was
presented. (16)

Glossary

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

431

Formant Horizontal band of energy in the speech spectrogram associated
with vowels. (14) Formant transition In the speech stimulus, the rapid
shift in frequency that precedes a formant. (14) Fovea A small area in
the human retina that contains only cone receptors. The fovea is located
on the line of sight, so that when a person looks at an object, the
center of its image falls on the fovea. (3) Frequency The number of
times per second that pressure changes of a sound stimulus repeat.
Frequency is measured in Hertz, where 1 Hertz is one cycle per second.
(11) Frequency spectrum A plot that indicates the amplitudes of the
various harmonics that make up a complex tone. Each harmonic is
indicated by a line that is positioned along the frequency axis, with
the height of the line indicating the amplitude of the harmonic. (11)
Frequency tuning curve Curve relating frequency and the threshold
intensity for activating an auditory neuron. (11) Frontal eyes Eyes
located in front of the head, so the views of the two eyes overlap. (10)
Frontal lobe Receiving signals from all of the senses, the frontal lobe
plays an important role in perceptions that involve the coordination of
information received through two or more senses. It also serves
functions such as language, thought, memory, and motor functioning. (1)
Frontal operculum cortex An area in the frontal lobe of the cortex that
receives signals from the taste system. (16) Functional connectivity
Neural connectivity between two areas of the brain that are activated
when carrying out a specific function. (2) Functional magnetic resonance
imaging (fMRI) A brain imaging technique that indicates brain activity
in awake, behaving organisms. The fMRI response occurs when the response
to a magnetic field changes in response to changes in blood flow in the
brain. (2) Fundamental A pure tone with frequency equal to the
fundamental frequency of a complex tone. See also Fundamental frequency.
(11) Fundamental frequency The first harmonic of a complex tone; usually
the lowest frequency in the frequency spectrum of a complex tone. The
tone's other components, called higher harmonics, have frequencies that
are multiples of the fundamental frequency. (11) Fusiform face area
(FFA) An area in the human inferotemporal (IT) cortex that contains
neurons that are specialized to respond to faces. (5) Ganglion cell A
neuron in the retina that receives inputs from bipolar and amacrine
cells. The axons of the ganglion cells are the nerve fibers that travel
out of the eye in the optic nerve. (3) Gap fill In music, when after a
large jump from one note to another, the next notes of the melody turn
around, progressing in the opposite direction, to fill the gap. (13)
Gate control model Melzack and Wall's idea that perception of pain is
controlled by a neural circuit that takes into account the relative
amount of activity in nociceptors, mechanoreceptors, and central
signals. This model has been used to explain how pain can be influenced
by factors other than stimulation of receptors in the skin. (15) Geons
According to recognition by components (RBC) theory, individual
geometric components that comprise objects. (5) Gestalt psychology An
approach to psychology that developed as a reaction to structuralism.
The Gestalt approach proposes principles of perceptual organization and
figure--ground segregation and states that "the whole is different than
the sum of its parts." (5) 432

Gist of a scene General description of a scene. People can identify most
scenes after viewing them for only a fraction of a second, as when they
flip rapidly from one TV channel to another. It takes longer to identify
the details within the scene. (5) Global image features Information that
may enable observers to rapidly perceive the gist of a scene. Features
associated with specific types of scenes include degree of naturalness,
degree of openness, degree of roughness, degree of expansion, and color.
(5) Global optic flow Information for movement that occurs when all
elements in a scene move. The perception of global optic flow indicates
that it is the observer that is moving and not the scene. (8) Glomeruli
Small structures in the olfactory bulb that receive signals from similar
olfactory receptor neurons. One function of each glomerulus is to
collect information about a small group of odorants. (16) Good
continuation, principle of A Gestalt principle of perceptual
organization that states that points that, when connected, result in
straight or smoothly curving lines are seen as belonging together, and
that lines tend to be seen in such a way as to follow the smoothest
path. (5) Good figure, principle of See Pragnanz, principle of. (5)
Gradient of flow In an optic flow pattern, a gradient is created by
movement of an observer through the environment. The "gradient" refers
to the fact that the optic flow is rapid in the foreground and becomes
slower as distance from the observer increases. (7) Grandmother cell A
highly specific type of neuron that fires in response to a specific
stimulus, such as a person's grandmother. (2) Grating acuity (cutaneous)
The narrowest spacing of a grooved surface on the skin for which
orientation can be accurately judged. See also Two-point threshold. (15)
Grating acuity (visual) The smallest width of lines for which the
orientation of a black and white striped stimulus can be accurately
judged. (1) Grid cells Cells in the entorhinal cortex that fire when an
animal is in a particular place in the environment, and which have
multiple place fields arranged in a gridlike pattern. (7) Ground In
object perception, the background is called the ground. See also Figure.
(5) Grouping In perceptual organization, the process by which visual
events are "put together" into units or objects. (5) Habituation
procedure Procedure in which a person pays less attention when the same
stimulus is presented repeatedly. For example, infants look at a
stimulus less and less on each successive trial. See also
Dishabituation. (9) Hair cells Neurons in the cochlea that contain small
hairs, or cilia, that are displaced by vibration of the basilar membrane
and fluids inside the inner ear. There are two kinds of hair cells:
inner and outer. (11) Hair cells, inner Auditory receptor cells in the
inner ear that are primarily responsible for auditory transduction and
the perception of pitch. (11) Hair cells, outer Auditory receptor cells
in the inner ear that amplify the response of inner hair cells by
amplifying the vibration of the basilar membrane. (11) Hand dystonia A
condition which causes the fingers to curl into the palm. (15) Haptic
perception The perception of three-dimensional objects by touch. (15)
Harmonics Pure-tone components of a complex tone that have frequencies
that are multiples of the fundamental frequency. (11)

Glossary

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Harmony The qualities of sound (positive or negative) created when two
or more pitches are played together. (13) Head-mounted eye tracking Eye
tracking technique in which the perceiver is fitted with two devices:
(1) a head-mounted scene camera, which indicates the orientation of the
perceiver's head and their general field of view, and (2) an eye camera,
which indicates the precise location where the person is looking within
that field of view. (6) Hering's primary colors The colors red, yellow,
green, and blue in the color circle. (9) Hertz (Hz) The unit for
designating the frequency of a tone. One Hertz equals one cycle per
second. (11) Hidden hearing loss Hearing loss that occurs at high sound
levels, even though the person's thresholds, as indicated by the
audiogram, are normal. (11) Higher harmonics Pure tones with frequencies
that are whole-number (2, 3, 4, etc.) multiples of the fundamental
frequency. See also Fundamental; Fundamental frequency; Harmonics. (11)
Hippocampus Subcortical structure in the brain that is associated with
forming and storing memories. (4) Hit In a signal detection experiment,
saying "Yes, I detect a stimulus" on a trial in which the stimulus is
present (a correct response). (Appendix C) Homunculus Latin for "little
man"; refers to the topographic map of the body in the somatosensory
cortex. (15) Horizontal cell A neuron that transmits signals laterally
across the retina. Horizontal cells synapse with receptors and bipolar
cells. (3) Horopter An imaginary surface that passes through the point
of fixation. Images caused by a visual stimulus on this surface fall on
corresponding points on the two retinas. (10) How pathway See Dorsal
pathway. (4) Hue The experience of a chromatic color, such as red,
green, yellow, or blue, or combinations of these colors. (9) Hue
cancellation Procedure in which a subject is shown a monochromatic
reference light and is asked to remove, or "cancel," the one of the
colors in the reference light by adding a second wavelength. This
procedure was used by Hurvich and Jameson in their research on
opponent-process theory. (9) Hue scaling Procedure in which participants
are given colors from around the hue circle and told to indicate the
proportions of red, yellow, blue, and green that they perceive in each
color. (9) Hypercolumn In the striate cortex, unit proposed by Hubel and
Wiesel that combines location, orientation, and ocular dominance columns
that serve a specific area on the retina. (4) Hyperopia A condition
causing poor vision in which people can see objects that are far away
but do not see near objects clearly. Also called farsightedness. (3)
Hyperpolarization When the inside of a neuron becomes more negative.
Hyperpolarization is often associated with the action of inhibitory
neurotransmitters. (2) Illumination edge The border between two areas
created by different light intensities in the two areas. (9) Illusory
conjunction Illusory combination of features that are perceived when
stimuli containing a number of features are presented briefly and under
conditions in which focused attention is difficult. For example,
presenting a red square and a blue triangle could potentially create the
perception of a red triangle. (6) Illusory contour Contour that is
perceived even though it is not present in the physical stimulus. (5)
Illusory motion Perception of motion when there actually is none. See
also Apparent motion. (8)

Image displacement signal (IDS) In corollary discharge theory, the
signal that occurs when an image moves across the visual receptors. (6)
Implied motion When a still picture depicts an action that involves
motion, so that an observer could potentially extend the action depicted
in the picture in his or her mind based on what will most likely happen
next. (8) Inattentional blindness A situation in which a stimulus that
is not attended is not perceived, even though the person is looking
directly at it. (6) Incus The second of the three ossicles of the middle
ear. It transmits vibrations from the malleus to the stapes. (11)
Indirect sound Sound that reaches a listener's ears after being
reflected from a surface such as a room's walls. (12) Induced motion The
illusory movement of one object that is caused by the movement of
another object that is nearby. (8) Infant-directed speech (IDS) Also
called "motherese" (or more recently, "parentese"), or "baby talk," a
patter of speech that has special characteristics that both attract an
infant's attention and make it easier for the infant to recognize
individual words. (13) Inferior colliculus A nucleus in the hearing
system along the pathway from the cochlea to the auditory cortex. The
inferior colliculus receives inputs from the superior olivary nucleus.
(11) Inferotemporal (IT) cortex An area of the brain outside Area V1
(the striate cortex), involved in object perception and facial
recognition. (4) Inflammatory pain Pain caused by damage to tissues,
inflammation of joints, or tumor cells. This damage releases chemicals
that create an "inflammatory soup" that activates nociceptors. (15)
Inhibitory area Area of a receptive field that is associated with
inhibition. Stimulation of this area causes a decrease in the rate of
nerve firing. (3) Inhibitory response Occurs when a neuron's firing rate
decreases due to inhibition from another neuron. (2) Inhibitory-center,
excitatory-surround receptive field A centersurround receptive field in
which stimulation of the center causes an inhibitory response and
stimulation of the surround causes an excitatory response. (3) Inner ear
The innermost division of the ear, containing the cochlea and the
receptors for hearing. (11) Inner hair cells See Hair cells, inner. (11)
Insula An area in the frontal lobe of the cortex that receives signals
from the taste system and is also involved in the affective component of
the perception of pain. (16) Interaural level difference (ILD) The
difference in the sound pressure (or level) between the left and right
ears. This difference creates an acoustic shadow for the far ear. The
ILD provides a cue for sound localization for high-frequency sounds.
(12) Interaural time difference (ITD) When a sound is positioned closer
to one ear than to the other, the sound reaches the close ear slightly
before reaching the far ear, so there is a difference in the time of
arrival at the two ears. The ITD provides a cue for sound localization.
(12) Inter-onset interval In music, the time between the onset of each
note. (13) Interpersonal touching One person touching another person.
See also social touch. (15) Interval In music, the spacing between
notes. (13) Invariant information Environmental properties that do not
change as the observer moves relative to an object or scene. For
example, the spacing, or texture, of the elements in a homogenous
texture gradient does not change as the observer moves on the gradient.
The texture of the gradient therefore supplies invariant information for
depth perception. (7) Glossary

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

433

Inverse projection problem The idea that a particular image on the
retina could have been caused by an infinite number of different
objects. This means that the retinal image does not unambiguously
specify a stimulus. (5) Ions Charged molecules. Sodium (Na1), potassium
(K1), and chlorine (Cl2) are the main ions found within nerve fibers and
in the liquid that surrounds nerve fibers. (2) Ishihara plate A display
of colored dots used to test for the presence of color deficiency. The
dots are colored so that people with normal (trichromatic) color vision
can perceive numbers in the plate, but people with color deficiency
cannot perceive these numbers or perceive different numbers than someone
with trichromatic vision. (9) Isolated congenital anosmia (ICA) A
condition in which a person is born without a sense of smell. (16)
Isomerization Change in shape of the retinal part of the visual pigment
molecule that occurs when the molecule absorbs a quantum of light.
Isomerization triggers the enzyme cascade that results in transduction
from light energy to electrical energy in the retinal receptors. (3) ITD
detector Interaural time difference detector. Neurons in the Jeffress
neural coincidence model that fire when signals reach them from the left
and right ears. Each ITD detector is tuned to respond to a specific time
delay between the two signals, and so provides information about
possible locations of a sound source. (12) ITD tuning curve A plot of
the neuron's firing rate against the ITD (interaural time difference).
(12) Jeffress model The neural mechanism of auditory localization that
proposes that neurons are wired to each receive signals from the two
ears, so that different neurons fire to different interaural time
differences (ITD). (12) Kinesthesis The sense that enables us to feel
the motions and positions of the limbs and body. (15) Knowledge Any
information that the perceiver brings to a situation. See also Top-down
processing. (1) Knowledge-based processing See Top-down processing. (15)
Landmark Object on a route that serves as a cue to indicate where to
turn; a source of information for wayfinding. (7) Landmark
discrimination problem The behavioral task used in Ungerleider and
Mishkin's experiment in which they provided evidence for the dorsal, or
where, visual processing stream. Monkeys were required to respond to a
previously indicated location. (4) Lateral eyes Eyes located on opposite
sides of an animal's head, as in the pigeon and the rabbit, so the views
of the two eyes do not overlap or overlap only slightly. (10) Lateral
geniculate nucleus (LGN) The nucleus in the thalamus that receives
inputs from the optic nerve and, in turn, communicates with the cortical
receiving area for vision. (4) Lateral inhibition Inhibition that is
transmitted laterally across a nerve circuit. In the retina, lateral
inhibition is transmitted by the horizontal and amacrine cells. (3)
Lateral occipital complex (LOC) Area of the brain that is active when a
person views any kind of object---such as an animal, face, house, or
tool---but not when they view a texture, or an object with the parts
scrambled. (5) Leisure noise Noise associated with leisure activities
such as listening to music, hunting, and woodworking. Exposure to high
levels of leisure noise for extended periods can cause hearing loss.
(11)

434

Lens The transparent focusing element of the eye through which light
passes after passing through the cornea and the aqueous humor. The
lens's change in shape to focus at different distances is called
accommodation. (3) Level Short for sound pressure level or sound level.
Indicates the decibels or sound pressure of a sound stimulus. (11)
Light-adapted sensitivity The sensitivity of the eye when in the
light-adapted state. Usually taken as the starting point for the dark
adaptation curve because it is the sensitivity of the eye just before
the lights are turned off. (3) Light-from-above assumption The
assumption that light usually comes from above, which influences our
perception of form in some situations. (5) Lightness The perception of
shades ranging from white to gray to black. (9) Lightness constancy The
constancy of our perception of an object's lightness under different
intensities of illumination. (9) Likelihood (Bayes) In Bayesian
inference, the extent to which the available evidence is consistent with
a particular outcome. (5) Likelihood principle (Helmholtz) The idea
proposed by Helmholtz that we perceive the object that is most likely to
have caused the pattern of stimuli we have received. (5) Limits, method
of A psychophysical method for measuring threshold in which the
experimenter presents sequences of stimuli in ascending and descending
order. (1) Local disturbance in the optic array Occurs when one object
moves relative to the environment, so that the stationary background is
covered and uncovered by the moving object. This local disturbance
indicates that the object is moving relative to the environment. (8)
Location column A column in the visual cortex that contains neurons with
the same receptive field locations on the retina. (4) Location cues In
hearing, characteristics of the sound reaching the listener that provide
information regarding the location of a sound source. (12) Loudness The
quality of sound that ranges from soft to loud. For a tone of a
particular frequency, loudness usually increases with increasing
decibels. (11) Mach bands Light and dark bands perceived at light--dark
borders. (3) Macrosmatic Having a keen sense of smell; usually important
to an animal's survival. (16) Macular degeneration A clinical condition
that causes degeneration of the macula, an area of the retina that
includes the fovea and a small surrounding area. (3) Magnetic resonance
imaging (MRI) Brain scanning technique that makes it possible to create
images of structures within the brain. (2) Magnitude estimation A
psychophysical method in which the subject assigns numbers to a stimulus
that are proportional to the subjective magnitude of the stimulus. (1)
Malleus The first of the ossicles of the middle ear. Receives vibrations
from the tympanic membrane and transmits these vibrations to the incus.
(11) Manner of articulation How a speech sound is produced by
interaction of the articulators---the mouth, tongue, and lips---during
production of the sound. (14) McGurk effect See Audiovisual speech
perception. (14) Mechanoreceptor Receptor that responds to mechanical
stimulation of the skin, such as pressure, stretching, or vibration.
(15) Medial geniculate nucleus An auditory nucleus in the thalamus that
is part of the pathway from the cochlea to the auditory cortex. The
medial geniculate nucleus receives inputs from the inferior colliculus
and transmits signals to the auditory cortex. (11)

Glossary

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Medial lemniscal pathway A pathway in the spinal cord that transmits
signals from the skin toward the thalamus. (15) Meditation A practice
that originated in Buddhist and Hindu cultures, which involves different
ways of engaging the mind. See Focused-attention meditation. (6)
Meissner corpuscle (RA1) A receptor in the skin, associated with RA1
mechanoreceptors. It has been proposed that the Meissner corpuscle is
important for perceiving tactile slip and for controlling the force
needed to grip objects. (15) Melodic channeling See Scale illusion. (12)
Melody The experience of a sequence of pitches as belonging together.
Usually refers to the way notes follow one another in a song or musical
composition. (13) Melody schema A representation of a familiar melody
that is stored in a person's memory. Existence of a melody schema makes
it more likely that the tones associated with a melody will be
perceptually grouped. (12) Memory color The idea that an object's
characteristic color influences our perception of that object's color.
(9) Merkel receptor (SA1) A disk-shaped receptor in the skin associated
with slowly adapting fibers and the perception of fine details. (15)
Metamerism The situation in which two physically different stimuli are
perceptually identical. In vision, this refers to two lights with
different wavelength distributions that are perceived as having the same
color. (9) Metamers Two lights that have different wavelength
distributions but are perceptually identical. (9) Meter In music,
organization of beats into bars or measures, with the first beat in each
bar often being accented. There are two basic kinds of meter in Western
music: duple meter, in which accents are in multiples of two, such as 12
12 12 or 1234 1234 1234, like a march; and triple meter, in which
accents are in groups of three, such as 123 123 123, as in a waltz. (13)
Method of adjustment See Adjustment, method of. (1) Method of constant
stimuli See Constant stimuli, method of. (1) Method of limits See
Limits, method of. (1) Metrical structure The pattern of beats indicated
by a musical time signature like 2:4, 4:4, or 3:4. Musicians often
accentuate initial notes of a measure by using a stronger attack or by
playing them louder or longer. (13) Microneurography Technique used to
record neural signals that involves inserting a metal electrode with a
very fine tip just under the skin. (15) Microsmatic Having a weak sense
of smell. This usually occurs in animals, such as humans, in which the
sense of smell is not crucial for survival. (16) Microspectrophotometry
A technique in which a narrow beam of light is directed into a single
visual receptor. This technique makes it possible to determine the
pigment absorption spectra of single receptors. (9) Microstimulation A
procedure in which a small electrode is inserted into the cortex and an
electrical current passed through the electrode activates neurons near
the tip of the electrode. This procedure has been used to determine how
activating specific groups of neurons affects perception. (8) Middle ear
The small air-filled space between the auditory canal and the cochlea
that contains the ossicles. (11) Middle-ear muscles Muscles attached to
the ossicles in the middle ear. The smallest skeletal muscles in the
body, they contract in response to very intense sounds and dampen the
vibration of the ossicles. (11) Middle temporal (MT) area Brain region
in the temporal lobe that contains many directionally selective neurons.
(8)

Mild cognitive impairment Cognitive impairments that extend beyond those
associated with normal aging, but which often do not interfere with
activities of daily living. Often is a precursor to more serious
conditions such as Alzheimer's disease. (16) Mind--body problem One of
the most famous problems in science: How do physical processes such as
nerve impulses or sodium and potassium molecules flowing across
membranes (the body part of the problem) become transformed into the
richness of perceptual experience (the mind part of the problem)? (2)
Mind wandering Non-task-oriented mental activity. Also called
daydreaming. (6) Mirror neuron Neuron in the premotor area of the
monkey's cortex that responds when the monkey grasps an object and also
when the monkey observes someone else (another monkey or the
experimenter) grasping the object. There is also evidence for
mirror-neuron-like activity in the human brain. See also Audiovisual
mirror neuron. (7) Mirror neuron system Network of neurons hypothesized
to play a role in creating mirror neurons. (7) Misapplied size constancy
scaling A principle, proposed by Richard Gregory, that when mechanisms
that help maintain size constancy in the three-dimensional world are
applied to twodimensional pictures, an illusion of size sometimes
results. (10) Miss In a signal detection experiment, saying "No, I don't
detect a stimulus" on a trial in which the stimulus is present (an
incorrect response). (Appendix C) Modularity The idea that specific
areas of the cortex are specialized to respond to specific types of
stimuli. (2) Module A structure that processes information about a
specific behavior or perceptual quality. Often identified as a structure
that contains a large proportion of neurons that respond selectively to
a particular quality, such as the fusiform face area, which contains
many neurons that respond selectively to faces. (2) Monochromat A person
who is completely color-blind and therefore sees everything as black,
white, or shades of gray. A monochromat can match any wavelength in the
spectrum by adjusting the intensity of any other wavelength.
Monochromats generally have only one type of functioning receptors,
usually rods. (9) Monochromatic light Light that contains only a single
wavelength. (3) Monochromatism Rare form of color blindness in which the
absence of cone receptors results in perception only of shades of
lightness (white, gray, and black), with no chromatic color present. (9)
Monocular cue Depth cue---such as overlap, relative size, relative
height, familiar size, linear perspective, movement parallax, and
accommodation---that can work when we use only one eye. (10) Moon
illusion An illusion in which the moon appears to be larger when it is
on or near the horizon than when it is high in the sky. (10) Motion
aftereffect An illusion that occurs after a person views a moving
stimulus and then sees movement in the opposite direction when viewing a
stationary stimulus immediately afterward. See also Waterfall illusion.
(8) Motion parallax A depth cue. As an observer moves, nearby objects
appear to move rapidly across the visual field whereas far objects
appear to move more slowly. (10) Motor signal (MS) In corollary
discharge theory, the signal that is sent to the eye muscles when the
observer moves or tries to move his or her eyes. (6) Motor theory of
speech perception A theory that proposes a close link between how speech
is perceived and how it is produced. The idea behind this theory is that
when we hear a particular speech sound, this activates the motor
mechanisms that are responsible for producing that sound, and it is the
activation of these motor mechanisms that enable us to perceive the
sound. (14) Glossary

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

435

Müller-Lyer illusion An illusion in which two lines of equal length
appear to be of different lengths because of the addition of "fins" to
the ends of the lines. (10) Multimodal The involvement of a number of
different senses in determining perception. For example, speech
perception can be influenced by information from a number of different
senses, including audition, vision, and touch. (14) Multimodal
interactions Interactions that involve more than one sense or quality.
(16) Multimodal nature of pain The fact that the experience of pain has
both sensory and emotional components. (15) Multisensory interaction Use
of a combination of senses. An example for vision and hearing is seeing
a person's lips move while listening to the person speak. (12)
Multivoxel pattern analysis (MVPA) In neural mind reading, a technique
in which the pattern of activated voxels is used to determine what a
person is perceiving or thinking. (5) Munsell color system Depiction of
hue, saturation, and value developed by Albert Munsell in the early
1900s in which different hues are arranged around the circumference of a
cylinder with perceptually similar hues placed next to each other. (9)
Music Sound organized in a way that, in traditional Western music,
creates a melody. (13) Music-evoked autobiographical memory (MEAM)
Memory triggered by listening to music. MEAMs are often associated with
strong emotions like happiness and nostalgia, but can also be associated
with sad emotions. (13) Musical phrases How notes are perceived as
forming segments like phrases in language. (13) Musical syntax Rules
that specify how notes and chords are combined in music. (13) Myopia An
inability to see distant objects clearly. Also called nearsightedness.
(3) Naloxone A substance that inhibits the activity of opiates. It is
hypothesized that naloxone also inhibits the activity of endorphins and
therefore can have an effect on pain perception. (15) Nasal pharynx A
passageway that connects the mouth cavity and the nasal cavity. (16)
Nearsightedness See Myopia. (3) Nerve fiber In most sensory neurons, the
long part of the neuron that transmits electrical impulses from one
point to another. Also called the axon. (2) Neural circuit A number of
neurons that are connected by synapses. (3) Neural convergence Synapsing
of a number of neurons onto one neuron. (3) Neural mind reading Using a
neural response, usually brain activation measured by fMRI, to determine
what a person is perceiving or thinking. (5) Neural plasticity The
capacity of the nervous system to change in response to experience.
Examples are how early visual experience can change the orientation
selectivity of neurons in the visual cortex and how tactile experience
can change the sizes of areas in the cortex that represent different
parts of the body. See also Experience-dependent plasticity; Selective
rearing. (4) Neural processing Operations that transform electrical
signals within a network of neurons or that transform the response of
individual neurons. (1) Neurogenesis The cycle of birth, development,
and death of a neuron. This process occurs for the receptors for
olfaction and taste. (16) Neuron The structure that transmits electrical
signals in the body. Key components of neurons are the cell body,
dendrites, and the axon or nerve fiber. (2)

436

Neuropathic pain Pain caused by lesions or other damage to the nervous
system. (15) Neuropsychology The study of the behavioral effects of
brain damage in humans. (2) Neurotransmitter A chemical stored in
synaptic vesicles that is released in response to a nerve impulse and
has an excitatory or inhibitory effect on another neuron. (2) Neutral
point The wavelength at which a dichromat perceives gray. (9) Nocebo
effect A negative placebo effect, characterized by a negative response
to negative expectations. (15) Nociceptive pain This type of pain, which
serves as a warning of impending damage to the skin, is caused by
activation of receptors in the skin called nociceptors. (15) Nociceptor
A fiber that responds to stimuli that are damaging to the skin. (15)
Noise A sound stimulus that contains many random frequencies. (11) Noise
In signal detector theory, noise is all of the stimuli in the
environment other than the signal. (Appendix C) Noise-induced hearing
loss A form of sensorineural hearing loss that occurs when loud noises
cause degeneration of the hair cells. (11) Noise-vocoded speech A
procedure in which the speech signal is divided into different frequency
bands and then noise is added to each band. (14) Noncorresponding points
Two points, one on each retina, that would not overlap if the retinas
were slid onto each other. Also called disparate points. (10)
Nonspectral colors Colors that do not appear in the spectrum because
they are mixtures of other colors. An example is magenta, which is a
mixture of red and blue. (9) Novelty-preference procedure A procedure
used to study infant color vision in which two side-by-side squares of
different colors are presented and the infant's looking time to the two
squares is measured to determine whether they can tell the difference
between them. (9) Nucleus accumbens Brain structure closely associated
with the neurotransmitter dopamine, which is released into the nucleus
accumbens in response to rewarding stimuli. (13) Nucleus of the solitary
tract The nucleus in the brain stem that receives signals from the
tongue, the mouth, and the larynx transmitted by the chorda tympani,
glossopharyngeal, and vagus nerves. (16) Object discrimination problem
The behavioral task used in Ungerleider and Mishkin's experiment in
which they provided evidence for the ventral, or what, visual processing
stream. Monkeys were required to respond to an object with a particular
shape. (4) Object recognition The ability to identify objects. (5)
Oblique effect Enhanced sensitivity to vertically and horizontally
oriented visual stimuli compared to obliquely oriented (slanted)
stimuli. This effect has been demonstrated by measuring both perception
and neural responding. (1) Occipital lobe A lobe at the back of the
cortex that is the site of the cortical receiving area for vision. (1)
Occlusion Depth cue in which one object hides or partially hides another
object from view, causing the hidden object to be perceived as being
farther away. A monocular depth cue. (10) Octave Tones that have
frequencies that are binary multiples of each other (2, 4, etc.). For
example, an 800-Hz tone is one octave above a 400-Hz tone. (11)
Oculomotor cue Depth cue that depends on our ability to sense the
position of our eyes and the tension in our eye muscles. Accommodation
and convergence are oculomotor cues. (10)

Glossary

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Odor map. See Chemotopic map. (16) Odor object The source of an odor,
such as coffee, bacon, a rose, or car exhaust. (16) Odor-evoked
autobiographical memory Memories about events from a person's life that
are elicited by odors. (16) Odotoptic map. See Chemotopic map. (16)
Olfaction The sense of smell. Usually results from stimulation of
receptors in the olfactory mucosa. (16) Olfactory bulb The structure
that receives signals directly from the olfactory receptors. The
olfactory bulb contains glomeruli, which receive these signals from the
receptors. (16) Olfactory mucosa The region inside the nose that
contains the receptors for the sense of smell. (16) Olfactory receptor A
protein string that responds to odor stimuli. (16) Olfactory receptor
neurons (ORNs) Sensory neurons located in the olfactory mucosa that
contain the olfactory receptors. (16) Ommatidium A structure in the eye
of the Limulus that contains a small lens, located directly over a
visual receptor. The Limulus eye is made up of hundreds of these
ommatidia. The Limulus eye has been used for research on lateral
inhibition because its receptors are large enough so that stimulation
can be applied to individual receptors. (3) Operant conditioning A type
of learning in which behavior is controlled by rewards, called
reinforcements, that follow behaviors. (6) Opioid A chemical such as
opium, heroin, and other molecules with related structures that reduce
pain and induce feelings of euphoria. (15) Opponent neuron A neuron that
has an excitatory response to wavelengths in one part of the spectrum
and an inhibitory response to wavelengths in the other part of the
spectrum. (9) Opponent-process theory of color vision A theory
originally proposed by Hering, which claimed that our perception of
color is determined by the activity of two opponent mechanisms: a
blue--yellow mechanism and a red--green mechanism. The responses to the
two colors in each mechanism oppose each other, one being an excitatory
response and the other an inhibitory response. In addition, this theory
also includes a black-- white mechanism, which is concerned with the
perception of brightness. See also Opponent neuron. (9) Optic array The
structured pattern of light created by the presence of objects,
surfaces, and textures in the environment. (8) Optic chiasm An x-shaped
bundle of fibers on the underside of the brain, where nerve fibers
activated by stimulation of one side of the visual field cross over to
the opposite side of the brain. (4) Optic flow The flow of stimuli in
the environment that occurs when an observer moves relative to the
environment. Forward movement causes an expanding optic flow, whereas
backward movement causes a contracting optic flow. Some researchers use
the term optic flow field to refer to this flow. (7) Optic nerve Bundle
of nerve fibers that carry impulses from the retina to the lateral
geniculate nucleus and other structures. Each optic nerve contains about
1 million ganglion cell fibers. (3) Oral capture The condition in which
sensations from both olfaction and taste are perceived as being located
in the mouth. (16) Orbitofrontal cortex An area in the frontal lobe,
near the eyes, that receives signals originating in the olfactory
receptors. Also known as the secondary olfactory cortex. (16) Organ of
Corti The major structure of the cochlear partition, containing the
basilar membrane, the tectorial membrane, and the receptors for hearing.
(11) Orientation column A column in the visual cortex that contains
neurons with the same orientation preference. (4)

Orientation tuning curve A function relating the firing rate of a neuron
to the orientation of the stimulus. (4) Ossicles Three small bones in
the middle ear that transmit vibrations from the outer to the inner ear.
(11) Outer ear The pinna and the auditory canal. (11) Outer hair cells
See Hair cells, outer. (11) Outer segment Part of the rod and cone
visual receptors that contains the light-sensitive visual pigment
molecules. (3) Output unit A component of the Reichardt detector that
compares signals received from two or more neurons. According to
Reichardt's model, activity in the output unit is necessary for motion
perception. (8) Oval window A small, membrane-covered hole in the
cochlea that receives vibrations from the stapes. (11) Overt attention
Attention that involves looking directly at the attended object. (6)
Pacinian corpuscle (RA2 or PC) A receptor with a distinctive elliptical
shape associated with RA2 mechanoreceptors. It transmits pressure to the
nerve fiber inside it only at the beginning or end of a pressure
stimulus and is responsible for our perception of vibration and fine
textures when moving the fingers over a surface. (15) Papillae Ridges
and valleys on the tongue, some of which contain taste buds. There are
four types of papillae: filiform, fungiform, foliate, and circumvallate.
(16) Parahippocampal place area (PPA) An area in the temporal lobe that
is activated by indoor and outdoor scenes. (5) Parietal lobe A lobe at
the top of the cortex that is the site of the cortical receiving area
for touch and is the termination point of the dorsal (where or how)
stream for visual processing. (1) Parietal reach region (PRR) A network
of areas in the parietal cortex that contains neurons that are involved
in reaching behavior. (7) Partial color constancy A type of color
constancy that occurs when changing an object's illumination causes a
change in perception of the object's hue, but less change than would be
expected based on the change in the wavelengths of light reaching the
eye. Note that in complete color constancy, changing an object's
illumination causes no change in the object's hue. (9) Passive touch A
situation in which a person passively receives tactile stimulation. See
also Active touch. (15) Payoffs A system of rewards and punishments used
to influence a participant's motivation in a signal detection
experiment. (Appendix C) Perceived contrast The perceived difference in
the appearance of light and dark bars. (6) Perceived magnitude A
perceptual measure of stimuli, such as light or sound, that indicates
the magnitude of experience. (1) Perception Conscious sensory
experience. (1) Perceptual organization The process by which small
elements become perceptually grouped into larger objects. (5) Perceptual
process A sequence of steps leading from the environment to perception
of a stimulus, recognition of the stimulus, and action with regard to
the stimulus. (1) Periodic sound A sound stimulus in which the pattern
of pressure changes repeats. (11) Periodic waveform For the stimulus for
hearing, a pattern of repeating pressure changes. (11) Peripheral retina
The area of retina outside the fovea. (3) Permeability A property of a
membrane that refers to the ability of molecules to pass through it. If
the permeability to a molecule is high, the molecule can easily pass
through the membrane. (2) Glossary

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

437

Persistence of vision A phenomenon in which perception of any stimulus
persists for about 250 ms after the stimulus is physically terminated.
(5) Perspective convergence The perception that parallel lines in the
distance converge as distance increases. (10) Phantom limb A person's
continued perception of a limb, such as an arm or a leg, even though the
limb has been amputated. (15) Phase locking Firing of auditory neurons
in synchrony with the phase of an auditory stimulus. (11)
Phenomenological report Method of determining the relationship between
stimuli and perception in which the observer describes what he or she
perceives. (1) Phoneme The shortest segment of speech that, if changed,
changes the meaning of a word. (14) Phonemic restoration effect An
effect that occurs in speech perception when listeners perceive a
phoneme in a word even though the acoustic signal of that phoneme is
obscured by another sound, such as white noise or a cough. (14) Phonetic
boundary The voice onset time when perception changes from one speech
category to another in a categorical perception experiment. (14)
Phonetic feature Cues associated with how a phoneme is produced by the
articulators. (14) Photoreceptors The receptors for vision. (3)
Phrenology Belief that different mental faculties could be mapped onto
different brain areas based on the bumps and contours on a person's
skull. (2) Physical regularities Regularly occurring physical properties
of the environment. For example, there are more vertical and horizontal
orientations in the environment than oblique (angled) orientations. (5)
Physical-social pain overlap hypothesis Proposal that pain resulting
from negative social experiences is processed by some of the same neural
circuitry that processes physical pain. (15) Physiology--behavior
relationship Relationship between physiological responses and behavioral
responses. (1) Pictorial cue Monocular depth cue, such as overlap,
relative height, and relative size, that can be depicted in pictures.
(10) Pinna The part of the ear that is visible on the outside of the
head. (11) Piriform cortex (PC) An area under the temporal lobe that
receives signals from glomeruli in the olfactory bulb. Also called the
primary olfactory area. (16) Pitch The quality of sound, ranging from
low to high, that is most closely associated with the frequency of a
tone. (11) Pitch neuron A neuron that responds to stimuli associated
with a specific pitch. These neurons fire to the pitch of a complex tone
even if the first harmonic or other harmonics of the tone are not
present. (11) Place cells Neurons that fire only when an animal is in a
certain place in the environment. (7) Place field Area of the
environment within which a place cell fires. (7) Place of articulation
In speech production, the locations of articulation. See Manner of
articulation. (14) Place theory of hearing The proposal that the
frequency of a sound is indicated by the place along the organ of Corti
at which nerve firing is highest. Modern place theory is based on
Békésy's traveling wave theory of hearing. (11) Placebo A substance that
a person believes will relieve symptoms such as pain but that contains
no chemicals that actually act on these symptoms. (15) Placebo effect A
relief from symptoms resulting from a substance that has no
pharmacological effect. See also Placebo. (15) 438

Point-light walker A biological motion stimulus created by placing
lights at a number of places on a person's body and having an observer
view the moving-light stimulus that results as the person moves in the
dark. (8) Ponzo illusion An illusion of size in which two objects of
equal size that are positioned between two converging lines appear to be
different in size. Also called the railroad track illusion. (10)
Population coding Representation of a particular object or quality by
the pattern of firing of a large number of neurons. (2) Posterior belt
area Posterior (toward the back of the brain) area of the belt area,
which is an area in the temporal lobe involved in auditory processing.
(12) Power function A mathematical function of the form P 5 KSn, where P
is perceived magnitude, K is a constant, S is the stimulus intensity,
and n is an exponent. (Appendix B) Pragnanz, principle of A Gestalt
principle of perceptual organization that states that every stimulus
pattern is seen in such a way that the resulting structure is as simple
as possible. Also called the principle of good figure or the principle
of simplicity. (5) Preattentive processing Hidden processing that
happens within a fraction of a second, below one's level of awareness.
(6) Preattentive stage (of perceptual processing) An automatic and rapid
stage of processing, proposed by Treisman's feature integration theory,
during which a stimulus is decomposed into individual features. (6)
Precedence effect When two identical or very similar sounds reach a
listener's ears separated by a time interval of less than about 50 to
100 ms, the listener hears the first sound that reaches his or her ears.
(12) Precueing A procedure in which a cue stimulus is presented to
direct an observer's attention to a specific location where a test
stimulus is likely to be presented. This procedure was used by Posner to
show that attention enhances the processing of a stimulus presented at
the cued location. (6) Predictive coding A theory that describes how the
brain uses our past experiences to predict what we will perceive. (5)
Predictive remapping of attention Process in which attention begins
shifting toward a target just before the eye begins moving toward it,
enabling the perceiver to experience a stable, coherent scene. (6)
Preferential looking technique A technique used to measure perception in
infants. Two stimuli are presented, and the infant's looking behavior is
monitored for the amount of time the infant spends viewing each
stimulus. (3) Presbycusis A form of sensorineural hearing loss that
occurs as a function of age and is usually associated with a decrease in
the ability to hear high frequencies. Since this loss also appears to be
related to exposure to environmental sounds, it is also called
sociocusis. (11) Presbyopia The inability of the eye to accommodate due
to a hardening of the lens and a weakening of the ciliary muscles. It
occurs as people get older. (3) Primary auditory cortex (A1) An area of
the temporal lobe that receives signals via nerve fibers from the medial
geniculate nucleus in the thalamus. (12) Primary olfactory area A small
area under the temporal lobe that receives signals from glomeruli in the
olfactory bulb. Also called the piriform cortex. (16) Primary receiving
area Area of the cerebral cortex that first receives most of the signals
initiated by a sense's receptors. For example, the occipital cortex is
the site of the primary receiving area for vision, and the temporal lobe
is the site of the primary receiving area for hearing. (1) Primary
somatosensory cortex (SI) Area of the cortex in the parietal lobe that
receives signals that originate from the body and stimulation of the
skin. (15)

Glossary

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Principle of common fate See Common fate, principle of. (5) Principle of
common region See Common region, principle of. (5) Principle of good
continuation See Good continuation, principle of. (5) Principle of good
figure See Pragnanz, principle of. (5) Principle of pragnanz See
Pragnanz, principle of. (5) Principle of proximity (nearness) See
Proximity, principle of. (5) Principle of representation See
Representation, principle of. (1) Principle of similarity See
Similarity, principle of. (5) Principle of simplicity See Pragnanz,
principle of. (5) Principle of transformation See Transformation,
principle of. (1) Principle of uniform connectedness See Uniform
connectedness, principle of. (5) Principle of univariance See
Univariance, principle of. (9) Principles of perceptual organization
Principles that describe how elements in a scene become grouped
together. Many of these principles were originally proposed by the
Gestalt psychologists, but new principles have also been proposed by
recent researchers. (5) Prior probability (or prior) In Bayesian
inference, a person's initial estimate of the probability of an outcome.
See also Bayesian inference. (5) Propagated response A response, such as
a nerve impulse, that travels all the way down the nerve fiber without
decreasing in amplitude. (2) Proprioception The sensing of the position
of the limbs. (7) Prosopagnosia A form of visual agnosia in which the
person can't recognize faces. (5) Protanopia A form of dichromatism in
which a protanope is missing the long-wavelength pigment, and perceives
shortwavelength light as blue and long-wavelength light as yellow. (9)
Proust effect The elicitation of memories through taste and olfaction.
Named for Marcel Proust, who described how the taste and smell of a
tea-soaked madeleine cake unlocked childhood memories. (16) Proximal
stimulus The stimulus on the receptors. In vision, this would be the
image on the retina. (1) Proximity, principle of A Gestalt principle of
perceptual organization that states that things that are near to each
other appear to be grouped together. Also called the principle of
nearness. (5) Psychophysics Traditionally, the term psychophysics refers
to quantitative methods for measuring the relationship between
properties of the stimulus and the subject's experience. In this book,
all methods that are used to determine the relationship between stimuli
and perception will be broadly referred to as pychophysical methods. (1)
Pupil The opening through which light reflected from objects in the
environment enters the eye. (3) Pure tone A tone with pressure changes
that can be described by a single sine wave. (11) Purkinje shift The
shift from cone spectral sensitivity to rod spectral sensitivity that
takes place during dark adaptation. See also Spectral sensitivity. (3)
RA1 fiber Fiber in the skin associated with Meissner corpuscles that
adapts rapidly to stimuli and fires only briefly when a tactile stimulus
is presented. (15) RA2 fiber Fiber in the skin associated with Pacinian
corpuscle receptors that is located deeper in the skin than RA1 fibers.
(15) Random-dot stereogram A pair of stereoscopic images made up of
random dots. When one section of this pattern is shifted slightly in one
direction, the resulting disparity causes the shifted section to appear
above or below the rest of the pattern when the patterns are viewed in a
stereoscope. (10)

Rapidly adapting (RA) fiber Fiber in the cutaneous system that adapts
rapidly to a stimulus and so responds briefly to tactile stimulation.
(15) Ratio principle A principle stating that two areas that reflect
different amounts of light will have the same perceived lightness if the
ratios of their intensities to the intensities of their surroundings are
the same. (9) Rat--man demonstration The demonstration in which
presentation of a "ratlike" or "manlike" picture influences an
observer's perception of a second picture, which can be interpreted
either as a rat or as a man. This demonstration illustrates an effect of
top-down processing on perception. (1) Reaction time The time between
presentation of a stimulus and an observer's or listener's response to
the stimulus. Reaction time is often used in experiments as a measure of
speed of processing. (1) Real motion The physical movement of a
stimulus. Contrasts with apparent motion. (8) Receiver operating
characteristic (ROC) curve A graph in which the results of a signal
detection experiment are plotted as the proportion of hits versus the
proportion of false alarms for a number of different response criteria.
(Appendix C) Receptive field A neuron's receptive field is the area on
the receptor surface (the retina for vision; the skin for touch) that,
when stimulated, affects the firing of that neuron. (3) Receptor site
Small area on the postsynaptic neuron that is sensitive to specific
neurotransmitters. (2) Recognition The ability to place an object in a
category that gives it meaning---for example, recognizing a particular
red object as a tomato. (1) Recognition by components (RBC) theory
Theory that states that objects are comprised of individual geometric
components called geons, and we recognize objects based on the
arrangement of those geons. (5) Recognition profile The pattern of
olfactory activation for an odorant, indicating which ORNs (olfactory
receptor neurons) are activated by the odorant. (16) Reflectance The
percentage of light reflected from a surface. (9) Reflectance curve A
plot showing the percentage of light reflected from an object versus
wavelength. (9) Reflectance edge An edge between two areas where the
reflectance of two surfaces changes. (9) Refractive errors Errors that
can affect the ability of the cornea and/or lens to focus incoming light
onto the retina. (3) Refractive myopia Myopia (nearsightedness) in which
the cornea and/or the lens bends the light too much. See also Axial
myopia. (3) Refractory period The time period of about 1/1,000th of a
second that a nerve fiber needs to recover from conducting a nerve
impulse. No new nerve impulses can be generated in the fiber until the
refractory period is over. (2) Regularities in the environment
Characteristics of the environment that occur regularly and in many
different situations. (5) Reichardt detector A neural circuit proposed
by Werner Reichardt, in which signals caused by movement of a stimulus
across the receptors are processed by a delay unit and an output unit so
that signals are generated by movement in one direction but not in the
opposite direction. (8) Relative disparity The difference between two
objects' absolute disparities. (10) Relative height A monocular depth
cue. Objects that have bases below the horizon appear to be farther away
when they are higher in the field of view. Objects that have bases above
the horizon appear to be farther away when they are lower in the field
of view. (10)

Glossary

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

439

Relative size A cue for depth perception. When two objects are of equal
size, the one that is farther away will take up less of the field of
view. (10) Representation, principle of A principle of perception that
everything a person perceives is based not on direct contact with
stimuli but on representations of stimuli on the receptors and in the
person's nervous system. (1) Representational momentum Occurs when
motion depicted in a still picture continues in an observer's mind. (8)
Resolved harmonics Harmonics in a complex tone that create separated
peaks in basilar membrane vibration, and so can be distinguished from
one another. Usually lower harmonics of a complex tone. (11) Resonance A
mechanism that enhances the intensity of certain frequencies because of
the reflection of sound waves in a closed tube. Resonance in the
auditory canal enhances frequencies between about 2,000 and 5,000 Hz.
(11) Resonant frequency The frequency that is most strongly enhanced by
resonance. The resonance frequency of a closed tube is determined by the
length of the tube. (11) Response compression The result when doubling
the physical intensity of a stimulus less than doubles the subjective
magnitude of the stimulus. (Appendix B) Response criterion In a signal
detection experiment, the subjective magnitude of a stimulus above which
the participant will indicate that the stimulus is present. (Appendix C)
Response expansion The result when doubling the physical intensity of a
stimulus more than doubles the subjective magnitude of the stimulus.
(Appendix B) Resting potential The difference in charge between the
inside and the outside of the nerve fiber when the fiber is not
conducting electrical signals. Most nerve fibers have resting potentials
of about 270 mV, which means the inside of the fiber is negative
relative to the outside. (2) Resting-state fMRI The signal recorded
using functional magnetic resonance imaging when the brain is not
involved in a specific task. (2) Resting-state functional connectivity A
method in which restingstate fMRI is used to determine functional
connectivity. (2) Retina A complex network of cells that covers the
inside back of the eye. These cells include the receptors, which
generate an electrical signal in response to light, as well as the
horizontal, bipolar, amacrine, and ganglion cells. (3) Retinitis
pigmentosa A retinal disease that causes a gradual loss of vision,
beginning in the peripheral retina. (3) Retinotopic map A map on a
structure in the visual system, such as the lateral geniculate nucleus
or the cortex, that indicates locations on the structure that correspond
to locations on the retina. In retinotopic maps, locations adjacent to
each other on the retina are usually represented by locations that are
adjacent to each other on the structure. (4) Retronasal route The
opening from the oral cavity, through the nasal pharnyx, into the nasal
cavity. This route is the basis for the way smell combines with taste to
create flavor. (16) Return to the tonic Occurs when a song begins with
the tonic and ends with the tonic, where the tonic is the pitch
associated with a composition's key. (13) Reverberation time The time it
takes for a sound produced in an enclosed space to decrease to 1/1,000th
of its original pressure. (12) Reversible figure--ground A
figure--ground pattern that perceptually reverses as it is viewed, so
that the figure becomes the ground and the ground becomes the figure.
The best-known reversible figure--ground pattern is Rubin's vase--face
pattern. (5) 440

Rhythm In music, the series of changes across time (a mixture of shorter
and longer notes) in a temporal pattern. (13) Rising phase of the action
potential In the axon, or nerve fiber, the decrease in negativity from
270 mV to 140 mV (the peak action potential level) that occurs during
the action potential. This increase is caused by an inflow of Na1 ions
into the axon. (2) Rod A cylinder-shaped receptor in the retina that is
responsible for vision at low levels of illumination. (3) Rod--cone
break The point on the dark adaptation curve at which vision shifts from
cone vision to rod vision. (3) Rod monochromat A person who has a retina
in which the only functioning receptors are rods. (3) Rod spectral
sensitivity curve The curve plotting visual sensitivity versus
wavelength for rod vision. This function is typically measured when the
eye is dark adapted by a test light presented to the peripheral retina.
(3) Ruffini cylinder (SA2) A receptor structure in the skin associated
with slowly adapting fibers. It has been proposed that the Ruffini
cylinder is involved in perceiving "stretching." (15) SA1 fiber Fiber in
the skin associated with Merkel receptors that adapts slowly to
stimulation and so responds continuously as long as a tactile stimulus
is applied. (15) SA2 fiber A slowly adapting fiber in the cutaneous
system that is associated with the Ruffini cylinder and is located
deeper in the skin than the SA1 fiber. This fiber also responds
continuously to a tactile stimulus. (15) Saccadic eye movement Rapid eye
movement between fixations that occurs when scanning a scene. (6)
Saliency map A "map" of a visual display that takes into account
characteristics of the display such as color, contrast, and orientation
that are associated with capturing attention. (6) Same-object advantage
The faster responding that occurs when enhancement spreads within an
object. Faster reaction times occur when a target is located within the
object that is receiving the subject's attention, even if the subject is
looking at another place within the object. (6) Saturation (color) The
relative amount of whiteness in a chromatic color. The less whiteness a
color contains, the more saturated it is. (9) Scale illusion An illusion
that occurs when successive notes of a scale are presented alternately
to the left and right ears. Even though each ear receives notes that
jump up and down in frequency, smoothly ascending or descending scales
are heard in each ear. Also called melodic channeling. (12) Scene A view
of a real-world environment that contains (a) background elements and
(b) multiple objects that are organized in a meaningful way relative to
each other and the background. (5) Scene schema An observer's knowledge
about what is contained in typical scenes. An observer's attention is
affected by knowledge of what is usually found in the scene. (5)
Secondary olfactory area An area in the frontal lobe, near the eyes,
that receives signals originating in the olfactory receptors. Also known
as the orbitofrontal cortex. (16) Secondary somatosensory cortex (S2)
The area in the parietal lobe next to the primary somatosensory area
(S1) that processes neural signals related to touch, temperature, and
pain. (15) Seed location Location on the brain that is involved in
carrying out a specific task and which is used a reference point when
measuring resting-state functional connectivity. (2) Segregation The
process of separating one area or object from another. See also
Figure--ground segregation. (5) Selective adaptation A procedure in
which a person or animal is selectively exposed to one stimulus, and
then the effect of this

Glossary

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

exposure is assessed by testing with a wide range of stimuli. Typically,
sensitivity to the exposed stimulus is decreased. (4) Selective
attention Occurs when a person selectively focuses attention on a
specific location or stimulus property. (6) Selective rearing A
procedure in which animals are reared in special environments. An
example of selective rearing is the experiment in which kittens were
reared in an environment of vertical stripes to determine the effect on
orientation selectivity of cortical neurons. (4) Selective reflection
When an object reflects some wavelengths of the spectrum more than
others. (9) Selective transmission When some wavelengths pass through
visually transparent objects or substances and others do not. Selective
transmission is associated with the perception of chromatic color. See
also Selective reflection. (9) Semantic regularities Characteristics
associated with the functions associated with different types of scenes.
These characteristics are learned from experience. For example, most
people are aware of the kinds of activities and objects that are usually
associated with kitchens. (5) Semitone The smallest interval in Western
music---roughly the difference between two notes in a musical scale,
such as between C and C#. There are 12 semitones in an octave. (13)
Sensation Often identified with elementary processes that occur at the
beginning of a sensory system. See also Structuralism. (1) Sensorimotor
hearing loss Decrease in the ability to hear and perceive speech caused
by damage to the hair cells in the cochlea. (14) Sensory coding How
neurons represent various characteristics of the environment. See also
Population coding; Sparse coding; Specificity coding. (2) Sensory
component of pain Pain perception described with terms such as
throbbing, prickly, hot, or dull. See also Affective (emotional)
component of pain. (15) Sensory receptors Cells specialized to respond
to environmental energy, with each sensory system's receptors
specialized to respond to a specific type of energy. (1)
Sensory-specific satiety The effect on perception of the odor associated
with food eaten to satiety (the state of being satiated or "full"). For
example, after eating bananas until satiety, the pleasantness rating for
vanilla decreased slightly (but was still positive), but the rating for
banana odor decreased much more and became negative. (16) Sequential
grouping In auditory scene analysis, grouping that occurs as sounds
follow one another in time. (12) Shadowing Listeners' repetition aloud
of what they hear as they are hearing it. (6) Shortest path constraint
In the perception of apparent motion, the principle that apparent
movement tends to occur along the shortest path between two stimuli. (8)
Signal The stimulus presented to a participant. A concept in signal
detection theory. (Appendix C) Signal detection approach An approach to
detection of stimuli in which subjects' ability to detect stimuli is
measured and analyzed in terms of hits and false alarms. This approach
can take a subject's criterion into account in determining sensitivity
to a stimulus. See also Correct rejection; False alarm; Hit; Miss;
Noise; Payoffs; Receiver operating characteristic (ROC) curve; Response
criterion; Signal. (Appendix C) Similarity, principle of A Gestalt
principle stating that similar things appear to be grouped together. (5)
Simple cortical cell A neuron in the visual cortex that responds best to
bars of a particular orientation. (4) Simplicity, principle of See
Pragnanz, principle of. (5)

Simultaneous grouping The situation that occurs when sounds are
perceptually grouped together because they occur simultaneously in time.
(12) Size constancy Occurs when the size of an object is perceived to
remain the same even when it is viewed from different distances. (10)
Size--distance scaling A hypothesized mechanism that helps maintain size
constancy by taking an object's perceived distance into account.
According to this mechanism, an object's perceived size, S, is
determined by multiplying the size of the retinal image, R, by the
object's perceived distance, D. (10) Size-weight illusion Erroneously
predicting weight when observing two differently sized objects that have
the same weight. The error occurs when the perceiver predicts that
larger object will be heavier, and therefore uses more force to lift it,
causing it to be lifted higher and to feel lighter. (7) Slowly adapting
(SA) fiber See SA1 fiber; SA2 fiber. (15) Social pain Pain caused by
negative social situations, such as rejection. (15) Social touch One
person touching another person. See also interpersonal touching. (15)
Social touch hypothesis Hypothesis that CT afferents and their central
projections are responsible for social touch. (15) Somatosensory
receiving area (S1) An area in the parietal lobe that receives inputs
from the skin and the viscera associated with somatic senses such as
touch, temperature, and pain. See also Primary somatosensory cortex
(S1); Secondary somatosensory cortex (S2). (15) Somatosensory system The
system that includes the cutaneous senses (senses involving the skin),
proprioception (the sense of position of the limbs), and kinesthesis
(sense of movement of the limbs). (15) Sound (perceptual) The perceptual
experience of hearing. The statement "I hear a sound" is using sound in
this sense. (11) Sound (physical) The physical stimulus for hearing. The
statement "The sound's level was 10 dB" is using sound in this sense.
(11) Sound level The pressure of a sound stimulus, expressed in
decibels. See also Sound pressure level (SPL). (11) Sound pressure level
(SPL) A designation used to indicate that the reference pressure used
for calculating a tone's decibel rating is set at 20 micropascals, near
the threshold in the most sensitive frequency range for hearing. (11)
Sound spectrogram A plot showing the pattern of intensities and
frequencies of a speech stimulus. (14) Sound wave Pattern of pressure
changes in a medium. Most of the sounds we hear are due to pressure
changes in the air, although sound can be transmitted through water and
solids as well. (11) Sparse coding The idea that a particular object is
represented by the firing of a relatively small number of neurons. (2)
Spatial attention Attention to a specific location. (6) Spatial cue In
tactile perception, information about the texture of a surface that is
determined by the size, shape, and distribution of surface elements such
as bumps and grooves. (15) Spatial layout hypothesis Proposal that the
parahippocampal cortex responds to the surface geometry or geometric
layout of a scene. (5) Spatial neglect Neurological condition in which
patients with damage to one hemisphere of the brain do not attend to the
opposite side of their visual world. (6) Spatial updating Process by
which people and animals keep track of their position within a
surrounding environment when they move. (7) Specificity coding Type of
neural code in which different perceptions are signaled by activity in
specific neurons. See also Distributed coding. (2)

Glossary

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

441

Spectral colors Colors that appear in the visible spectrum. See also
Nonspectral colors. (9) Spectral cue In hearing, the distribution of
frequencies reaching the ear that are associated with specific locations
of a sound. The differences in frequencies are caused by interaction of
sound with the listener's head and pinnae. (12) Spectral sensitivity The
sensitivity of visual receptors to different parts of the visible
spectrum. See also Spectral sensitivity curve. (3) Spectral sensitivity
curve The function relating a subject's sensitivity to light to the
wavelength of the light. The spectral sensitivity curves for rod and
cone vision indicate that the rods and cones are maximally sensitive at
500 nm and 560 nm, respectively. See also Purkinje shift. (3) Speech
segmentation The process of perceiving individual words from the
continuous flow of the speech signal. (14) Speech spectrograph Machine
that records the time and frequency patterns of acoustic signals. Speech
spectrograph or speech spectrogram also refers to the records created by
this machine. (14) Speechreading Process by which deaf people determine
what people are saying by observing their lip and facial movements. (12)
Spinothalamic pathway One of the nerve pathways in the spinal cord that
conducts nerve impulses from the skin to the somatosensory area of the
thalamus. (15) Spontaneous activity Nerve firing that occurs in the
absence of environmental stimulation. (2) Stapes The last of the three
ossicles in the middle ear. It receives vibrations from the incus and
transmits these vibrations to the oval window of the inner ear. (11)
Statistical learning The process of learning about transitional
probabilities and other characteristics of the environment. Statistical
learning for properties of language has been demonstrated in young
infants. (14) Stereocilia Thin processes that protrude from the tops of
the hair cells in the cochlea that bend in response to pressure changes.
(11) Stereopsis The impression of depth that results from binocular
disparity---the difference in the position of images of the same object
on the retinas of the two eyes. (10) Stereoscope A device that presents
pictures to the left and the right eyes so that the binocular disparity
a person would experience when viewing an actual scene is duplicated.
The result is a convincing illusion of depth. (10) Stereoscopic depth
perception The perception of depth that is created by input from both
eyes. See also Binocular disparity. (10) Stereoscopic vision Two-eyed
depth perception involving mechanisms that take into account differences
in the images formed on the left and right eyes. (10) Stevens's power
law A law concerning the relationship between the physical intensity of
a stimulus and the perception of the subjective magnitude of the
stimulus. The law states that P 5 KSn, where P is perceived magnitude, K
is a constant, S is the stimulus intensity, and n is an exponent.
(Appendix B) Stimulus--behavior relationship The relationship between
stimuli and behavioral responses, where behavioral responses can be
perception, recognition, or action. (1) Stimulus--physiology
relationship The relationship between stimuli and physiological
responses. (1) Strabismus Misalignment of the eyes, such as crossed eyes
or walleyes (outward looking eyes), in which the visual system
suppresses vision in one of the eyes to avoid double vision, so the
person sees the world with only one eye at a time. (10) 442

Striate cortex The visual receiving area of the cortex, located in the
occipital lobe. (4) Structural connectivity The structural "road map" of
fibers connecting different areas of the brain. (2) Structuralism The
approach to psychology, prominent in the late 19th and early 20th
centuries, that postulated that perceptions result from the summation of
many elementary sensations. The Gestalt approach to perception was, in
part, a reaction to structuralism. (5) Subcortical structure Structure
below the cerebral cortex. For example, the superior colliculus is a
subcortical structure in the visual system. The cochlear nucleus and
superior olivary nucleus are among the subcortical structures in the
auditory system. (11) Subtractive color mixture. See Color mixture,
subtractive. (9) Superior colliculus An area in the brain that is
involved in controlling eye movements and other visual behaviors. This
area receives about 10 percent of the ganglion cell fibers that leave
the eye in the optic nerve. (4) Superior olivary nucleus A nucleus along
the auditory pathway from the cochlea to the auditory cortex. The
superior olivary nucleus receives inputs from the cochlear nucleus. (11)
Surface texture The visual and tactile quality of a physical surface
created by peaks and valleys. (15) Sustentacular cell Cells that provide
metabolic and structural support to the olfactory sensory neurons. (16)
Synapse A small space between the end of one neuron (the presynaptic
neuron) and the cell body of another neuron (the postsynaptic neuron).
(2) Syncopation In music, when notes begin "off the beat" on the "and"
count, which causes a "jumpiness" to the music. (13) Syntax In language,
grammatical rules that specify correct sentence construction. See also
Musical syntax. (13) Tactile acuity The smallest details that can be
detected on the skin. (15) Task-related fMRI fMRI measured as a person
is engaged in a specific task. (2) Taste The chemical sense that occurs
when molecules---often associated with food---enter the mouth in solid
or liquid form and stimulate receptors on the tongue. (16) Taste bud A
structure located within papillae on the tongue that contains the taste
cells. (16) Taste cell Cell located in taste buds that causes the
transduction of chemical to electrical energy when chemicals contact
receptor sites or channels located at the tip of this cell. (16) Taste
pore An opening in the taste bud through which the tips of taste cells
protrude. When chemicals enter a taste pore, they stimulate the taste
cells and result in transduction. (16) Tectorial membrane A membrane
that stretches the length of the cochlea and is located directly over
the hair cells. Vibrations of the cochlear partition cause the tectorial
membrane to bend the hair cells by rubbing against them. (11) Temporal
coding The connection between the frequency of a sound stimulus and the
timing of the auditory nerve fiber firing. (11) Temporal cue In tactile
perception, information about the texture of a surface that is provided
by the rate of vibrations that occur as we move our fingers across the
surface. (15) Temporal lobe A lobe on the side of the cortex that is the
site of the cortical receiving area for hearing and the termination
point for the ventral, or what, stream for visual processing. A number
of areas in the temporal lobe, such as the fusiform face area and the
extrastriate body area, serve functions related to perceiving and
recognizing objects. (1)

Glossary

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Temporal structure The time dimension of music, which consists of a
regular beat, organization of the beat into measures (meter), and the
time pattern created by the notes (rhythm). (13) Test location
Resting-state fMRI measured at a location other than the seed location.
(2) Texture gradient The visual pattern formed by a regularly textured
surface that extends away from the observer. This pattern provides
information for distance because the elements in a texture gradient
appear smaller as distance from the observer increases. (10) Threshold
The minimum stimulus energy necessary for an observer to detect a
stimulus. (1) Tiling The adjacent (and often overlapping) location
columns working together to cover the entire visual field (similar to
covering a floor with tiles). (4) Timbre The quality that distinguishes
between two tones that sound different even though they have the same
loudness, pitch, and duration. Differences in timbre are illustrated by
the sounds made by different musical instruments. (11) Tip links
Structures at the tops of the cilia of auditory hair cells, which
stretch or slacken as the cilia move, causing ion channels to open or
close. (11) Tonal hierarchy Ratings of how well notes fit in a scale.
Notes that sound "right" in a scale would be high in the tonal
hierarchy. Notes that don't sound like they fit in a scale are low in
the hierarchy. (13) Tonality Organizing pitches around the note
associated with a composition's key. (13) Tone chroma The perceptual
similarity of notes separated by one or more octaves. (11) Tone height
The increase in pitch that occurs as frequency is increased. (11) Tonic
The key of a musical composition. (13) Tonotopic map An ordered map of
frequencies created by the responding of neurons within structures in
the auditory system. There is a tonotopic map of neurons along the
length of the cochlea, with neurons at the apex responding best to low
frequencies and neurons at the base responding best to high frequencies.
(11) Top-down processing Processing that starts with the analysis of
high-level information, such as the knowledge a person brings to a
situation. Also called knowledge-based processing. Distinguished from
bottom-up, or data-based processing, which is based on incoming data.
(1) Transcranial magnetic stimulation (TMS) Presenting a strong magnetic
field to the head that temporarily disrupts the functioning of a
specific area of the brain. (8) Transduction In the senses, the
transformation of environmental energy into electrical energy. For
example, the retinal receptors transduce light energy into electrical
energy. (1) Transformation, principle of A principle of perception that
stimuli and responses created by stimuli are transformed, or changed,
between the environmental stimulus and perception. (1) Transitional
probabilities In language, the chances that one sound will follow
another sound. Every language has transitional probabilities for
different sounds. Part of learning a language involves learning about
the transitional probabilities in that language. (14) Transmission cell
(T-cell) According to gate control theory, the cell that receives 1 and
2 inputs from cells in the dorsal horn. T-cell activity determines the
perception of pain. (15) Transmission curves Plots of the percentage of
light transmitted through a liquid or object at each wavelength. (9)

Traveling wave In the auditory system, vibration of the basilar membrane
in which the peak of the vibration travels from the base of the membrane
to its apex. (11) Trichromacy of color vision The idea that our
perception of color is determined by the ratio of activity in three
receptor mechanisms with different spectral sensitivities. (9)
Trichromat A person with normal color vision. Trichromats can match any
wavelength in the spectrum by mixing three other wavelengths in various
proportions. (9) Triple meter In Western music, meter in which accents
are in groups of three, such as 123 123 123, as in a waltz. (13)
Tritanopia A form of dichromatism in which a person is missing the
short-wavelength pigment. A tritanope sees blue at short wavelengths,
red at long wavelengths. (9) Tuning curve, frequency See Frequency
tuning curve. (11) Tuning curve, orientation See Orientation tuning
curve. (4) Two-flash illusion An illusion that occurs when one flash of
light is presented, accompanied by two rapidly presented tones.
Presentation of the two tones causes the observer to perceive two
flashes of light. (12) Two-point threshold The smallest separation
between two points on the skin that is perceived as two points; a
measure of acuity on the skin. See also Grating acuity. (15) Tympanic
membrane A membrane at the end of the auditory canal that vibrates in
response to vibrations of the air and transmits these vibrations to the
ossicles in the middle ear. (11) Unconscious inference The idea proposed
by Helmholtz that some of our perceptions are the result of unconscious
assumptions that we make about the environment. See also Likelihood
principle. (5) Uncrossed disparity Disparity that occurs when one object
is being fixated, and is therefore on the horoptor, and another object
is located behind the horoptor, farther from the observer. (10) Uniform
connectedness, principle of A modern Gestalt principle that states that
connected regions of a visual stimulus are perceived as a single unit.
(5) Unilateral dichromat A person who has dichromatic vision in one eye
and trichromatic vision in the other eye. People with this condition
(which is extremely rare) have been tested to determine what colors
dichromats perceive by asking them to compare the perceptions they
experience with their dichromatic eye and their trichromatic eye. (9)
Unique hues Name given by Ewald Hering to what he proposed were the
primary colors: red, yellow, green, and blue. (9) Univariance, principle
of Once a photon of light is absorbed by a visual pigment molecule, the
identity of the light's wavelength is lost. This means that the receptor
does not know the wavelength of the light that is absorbed, only the
total amount of light it has absorbed. (9) Unresolved harmonics
Harmonics of a complex tone that can't be distinguished from one another
because they are not indicated by separate peaks in the basilar membrane
vibration. The higher harmonics of a tone are most likely to be
unresolved. (11) Value The light-to-dark dimension of color. (9)
Variability problem In speech perception, the fact that there is no
simple relationship between a particular phoneme and the acoustic
signal. (14) Ventral pathway Pathway that conducts signals from the
striate cortex to the temporal lobe. Also called the what pathway
because it is involved in recognizing objects. (4) Ventriloquism effect
See Visual capture. (12)

Glossary

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

443

Ventrolateral nucleus Nucleus in the thalamus that receives signals from
the cutaneous system. (15) Vestibular system The mechanism in the inner
ear that is responsible for balance and sensing the position of the
body. (13) Viewpoint invariance The condition in which object properties
don't change when viewed from different angles. Responsible for our
ability to recognize objects when viewed from different angles. (5)
Visible light The band of electromagnetic energy that activates the
visual system and that, therefore, can be perceived. For humans, visible
light has wavelengths between 400 and 700 nanometers. (3) Visual acuity
The ability to resolve small details. (3) Visual angle The angle of an
object relative to an observer's eyes. This angle can be determined by
extending two lines from the eye---one to one end of an object and the
other to the other end of the object. Because an object's visual angle
is always determined relative to an observer, its visual angle changes
as the distance between the object and the observer changes. (10) Visual
capture When sound is heard coming from a seen location, even though it
is actually originating somewhere else. Also called the ventriloquism
effect. (12) Visual direction strategy A strategy used by moving
observers to reach a destination by keeping their body oriented toward
the target. (7) Visual evoked potential An electrical response to visual
stimulation recorded by the placement of disk electrodes on the back of
the head. This potential reflects the activity of a large population of
neurons in the visual cortex. (3) Visual form agnosia The inability to
recognize objects. (1) Visual masking stimulus A visual pattern that,
when presented immediately after a visual stimulus, decreases a person's
ability to perceive the stimulus. This stops the persistence of vision
and therefore limits the effective duration of the stimulus. (5) Visual
pigment A light-sensitive molecule contained in the rod and cone outer
segments. The reaction of this molecule to light results in the
generation of an electrical response in the receptors. (3) Visual
pigment bleaching The change in the color of a visual pigment that
occurs when visual pigment molecules are isomerized by exposure to
light. (3) Visual pigment regeneration Occurs after the visual pigment's
two components---opsin and retinal---have become separated due to the
action of light. Regeneration, which occurs in the dark, involves a
rejoining of these two components to reform the visual pigment molecule.
This process depends on enzymes located in the pigment epithelium. (3)
Visual receiving area The area of the occipital lobe where signals from
the retina and LGN first reach the cortex. (4) Visual salience
Characteristics such as bright colors, high contrast, and highly visible
orientations that cause stimuli to stand out and therefore attract
attention. (6)

444

Visual search A procedure in which a person's task is to find a
particular element in a display that contains a number of elements. (6)
Visuomotor grip cell A neuron that initially responds when a specific
object is seen and then also responds as a hand grasps the same object.
(7) Voice cells Neurons in the temporal lobe that respond more strongly
to same-species voices than to calls of other animals or to "non-voice"
sounds. (14) Voice onset time (VOT) In speech production, the time delay
between the beginning of a sound and the beginning of the vibration of
the vocal chords. (14) Waterfall illusion An aftereffect of movement
that occurs after viewing a stimulus moving in one direction, such as a
waterfall. Viewing the waterfall makes other objects appear to move in
the opposite direction. See also Movement aftereffect. (8) Wavelength
For light energy, the distance between one peak of a light wave and the
next peak. (3) Wayfinding The process of navigating through the
environment. Wayfinding involves perceiving objects in the environment,
remembering objects and their relation to the overall scene, and knowing
when to turn and in what direction. (7) Weber fraction The ratio of the
difference threshold to the value of the standard stimulus in Weber's
law. (Appendix A) Weber's law A law stating that the ratio of the
difference threshold (DL) to the value of the stimulus (S) is constant.
According to this relationship, doubling the value of a stimulus will
cause a doubling of the difference threshold. The ratio DL/S is called
the Weber fraction. (Appendix A) Wernicke's aphasia An inability to
comprehend words or arrange sounds into coherent speech, caused by
damage to Wernicke's area. (14) Wernicke's area An area in the temporal
lobe involved in speech perception. Damage to this area causes
Wernicke's aphasia, which is characterized by difficulty in
understanding speech. (2) What pathway See Ventral pathway. (4) What
pathway, auditory Pathway that extends from the anterior belt to the
front of the temporal lobe and then to the frontal cortex. This pathway
is responsible for perceiving complex sounds and patterns of sounds.
(12) Where pathway See Dorsal pathway. (4) Where pathway, auditory
Pathway that extends from the posterior belt to the parietal lobe and
then to the frontal cortex. This pathway is responsible for localizing
sounds. (12) Word deafness Occurs in the most extreme form of Wernicke's
aphasia, when a person cannot recognize words, even though the ability
to hear pure tones remains intact. (14) Young-Helmholtz theory See
Trichromacy of color vision. (9)

Glossary

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

References Aartolahti, E., Hakkinen, A., & Lonnroos, E. (2013).
Relationship between functional vision and balance and mobility
performance in community-dwelling older adults. Aging Clinical and
Experimental Research, 25, 545--552. Abell, F., Happé, F., & Frith, U.
(2000). Do triangles play tricks? Attribution of mental states to
animated shapes in normal and abnormal development. Journal of Cognitive
Development, 15, 1--16. Abramov, I., Gordon, J., Hendrickson, A.,
Hainline, L., Dobson, V., & LaBossiere. (1982). The retina of the
newborn human infant. Science, 217, 265--267. Ackerman, D. (1990). A
natural history of the senses. New York: Vintage Books. Addams, R.
(1834). An account of a peculiar optical phenomenon seen after having
looked at a moving body. London and Edinburgh Philosophical Magazine and
Journal of Science, 5, 373--374. Adelson, E. H. (1999). Light perception
and lightness illusions. In M. Gazzaniga (Ed.), The new cognitive
neurosciences (pp. 339--351). Cambridge, MA: MIT Press. Adolph, K. E., &
Hoch, J. E. (2019). Motor development: Embodied, embedded, enculturated,
and enabling. Annual Review of Psychology, 70, 141--164. Adolph, K. E.,
& Robinson, S. R. (2015). Motor development. In Liben & Muller (Eds.),
Handbook of child psychology and developmental science (7th ed., Vol. 2
Cognitive Processes, pp. 114--157). New York: Wiley. Adolph, K. E., &
Tamis-LeMonda, C. S. (2014). The costs and benefits of development: The
transition from crawling to walking. Child Development Perspectives 8,
187--192. Aguirre, G. K., Zarahn, E., & D'Esposito, M. (1998). An area
within human ventral cortex sensitive to "building" stimuli: Evidence
and implications. Neuron, 21, 373--383. Alain, C., Arnott, S. R.,
Hevenor, S., Graham, S., & Grady, C. L. (2001). "What" and "where" in
the human auditory system. Proceedings of the National Academy of
Sciences, 98, 12301--12306. Alain, C., McDonald, K. L., Kovacevic, N., &
McIntosh, A. R. (2009). Spatiotemporal analysis of auditory "what" and
"where" working memory. Cerebral Cortex, 19, 305--314. Albouy, P.,
Benjamin, L., Morillon, B., & Zatorre, R. J. (2020). Distinct
sensitivity to spectrotemporal modulation supports brain asymmetry for
speech and melody. Science, 367, 1043--1047. Alpern, M., Kitahara, K., &
Krantz, D. H. (1983). Perception of color in unilateral tritanopia.
Journal of Physiology, 335, 683--697. Altenmüller, E., Siggel, S.,
Mohammadi, B., Samii, A., & Münte, T. F. (2014). Play it again Sam:
Brain correlates of emotional music recognition. Frontiers in
Psychology, 5, Article 114, 1--8. Aminoff, E. M., Kveraga, K., & Bar, M.
(2013). The role of the parahippocampal cortex in cognition. Trends in
Cognitive Sciences, 17, 379--390. Anton-Erxleben, K., Henrich, C., &
Treue, S. (2007). Attention changes perceived size of moving visual
patterns. Journal of Vision, 7(11), 1--9. Appelle, S. (1972). Perception
and discrimination as a function of

stimulus orientation: The "oblique effect" in man and animals.
Psychological Bulletin, 78, 266--278. Arshamian, A., Iannilli, E.,
Gerber, J. C., Willamder, J., Persson, J., Seo, H-S., Hummel, T., &
Larsson, M. (2013). The functional anatomy of odor evoked memories cued
by odors and words. Neuropsychologia, 51, 123--131. Arzi, A., & Sobel,
N. (2011). Olfactory perception as a compass for olfactory and neural
maps. Trends in Cognitive Sciences, 10, 537--545. Ashley, R. (2002).
Do\[n't\] change a hair for me: The art of jazz rubato. Music
Perception, 19(3), 311--332. Ashmore, J. (2008). Cochlear outer hair
cell motility. Physiological Review, 88, 173--210. Ashmore, J., Avan,
P., Brownell, W. E., Dallos, P., Dierkes, K., Fettiplace, R., et
al. (2010). The remarkable cochlear amplifier. Hearing Research, 266,
1--17. Aslin, R. N. (1977). Development of binocular fixation in human
infants. Journal of Experimental Child Psychology, 23, 133--150.
Attneave, F., & Olson, R. K. (1971). Pitch as a medium: A new approach
to psychophysical scaling. American Journal of Psychology, 84, 147--166.
Austin, J. H. (2009). How does meditation train attention? Insight
Journal, Summer, 16--22. Avanzini, P., Abdollahi, R. O., Satori, I., et
al. (2016). Four-dimensional maps of the human somatosensory system.
Proceedings of the National Academy of Sciences, 113(13), E1936--E1943.
Avenanti, A., Bueti, D., Galati, G., & Aglioti, S. M. (2005).
Transcranial magnetic stimulation highlights the sensorimotor side of
empathy for pain. Nature Neuroscience, 8, 955--960. Azzopardi, P., &
Cowey, A. (1993). Preferential representation of the fovea in the
primary visual cortex. Nature, 361, 719--721. Baars, B. J. (2001). The
conscious access hypothesis: Origins and recent evidence. Trends in
Cognitive Sciences, 6, 47--52. Bach, M., & Poloschek, C. M. (2006).
Optical illusions. Advances in Clinical Neuroscience and Rehabilitation,
6, 20--21. Baddeley, A. D., & Hitch, G. J. (1974). Working memory. In G.
A. Bower (Ed.), The psychology of learning and motivation (pp. 47--89).
New York: Academic Press. Baird, A., & Thompson, W. F. (2018). The
impact of music on the self in dementia. Journal of Alzheimer's Disease,
61, 827--841. Baird, A., & Thompson, W. F. (2019). When music
compensates language: A case study of severe aphasia in dementia and the
use of music by a spousal caregiver. Aphasiology, 33(4), 449--465.
Baird, J. C., Wagner, M., & Fuld, K. (1990). A simple but powerful
theory of the moon illusion. Journal of Experimental Psychology: Human
Perception and Performance, 16, 675--677. Baldassano, C., Esteva, A.,
Fei-Fei, L., & Beck, D. M. (2016). Two distinct scene-processing
networks connecting vision and memory. eNeuro, 3(5). Banks, M. S., &
Bennett, P. J. (1988). Optical and photoreceptor immaturities limit the
spatial and chromatic vision of human neonates. Journal of the Optical
Society of America, A5, 2059--2079. 445

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Banks, M. S., & Salapatek, P. (1978). Acuity and contrast sensitivity in
1-, 2-, and 3-month-old human infants. Investigative Ophthalmology and
Visual Science, 17, 361--365. Bara-Jimenez, Catlan, M. J., Hallett, M.,
& Gerloff, C. (1998). Abnormal somatosensory homunculus in dystonia of
the hand. Annals of Neurology, 44(5), 828--831. Bardy, B. G., & Laurent,
M. (1998). How is body orientation controlled during somersaulting?
Journal of Experimental Psychology: Human Perception and Performance,
24, 963--977. Barks, A., Searight, R., & Ratwik, S. (2011). Effect of
text messaging on academic performance. Signum Temporis, 4(1), 4--9.
Barlow, H. B. (1972). Single units and sensation: A neuron doctrine for
perceptual psychology? Perception, 1(4), 371--394. Barlow, H. B., &
Hill, R. M. (1963). Evidence for a physiological explanation of the
waterfall illusion. Nature, 200, 1345--1347. Barlow, H. B., & Mollon, J.
D. (Eds.). (1982). The senses. Cambridge, UK: Cambridge University
Press. Barlow, H. B., Blakemore, C., & Pettigrew, J. D. (1967). The
neural mechanism of binocular depth discrimination. Journal of
Physiology, 193, 327--342. Barlow, H. B., Fitzhigh, R., & Kuffler, S. W.
(1957). Change of organization in the receptive fields of the cat's
retina during dark adaptation. Journal of Physiology, 137, 338--354.
Barlow, H. B., Hill, R. M., & Levickm, W. R. (1964). Retinal ganglion
cells responding selectively to direction and speed of image motion in
the rabbit. Journal of Physiology, 173, 377--407. Barrett, H. C., Todd,
P. M., Miller, G. F., & Blythe, P. (2005). Accurate judgments of
intention from motion alone: A cross-cultural study. Evolution and Human
Behavior, 26, 313--331. Barry, S. R. (2011). Fixing my gaze. New York:
Basic Books. Bartoshuk, L. M. (1971). The chemical senses: I. Taste. In
J. W. Kling & L. A. Riggs (Eds.), Experimental psychology (3rd ed.). New
York: Holt, Rinehart and Winston. Bartoshuk, L. M. (1979). Bitter taste
of saccharin: Related to the genetic ability to taste the bitter
substance propylthioural (PROP). Science, 205, 934--935. Bartoshuk, L.
M. (1980, September). Separate worlds of taste. Psychology Today, 243,
48--56. Bartoshuk, L. M., & Beauchamp, G. K. (1994). Chemical senses.
Annual Review of Psychology, 45, 419--449. Bartrip, J., Morton, J., & de
Schonen, S. (2001). Responses to mother's face in 3-week- to 5-month-old
infants. British Journal of Developmental Psychology, 19, 219--232.
Basso, J. C., McHale, A., Ende, V., Oberlin, D. J., & Suzuki, W. A.
(2019). Brief, daily meditation enhances attention, memory, mood, and
emotional regulation in non-experienced meditators. Behavioural Brain
Research, 356, 208--220. Bathini, P., Brai, E., & Ajuber, L. A. (2019).
Olfactory dysfunction in the pathophysiological continuum of dementia.
Ageing Research Reviews, 55. 100956. Battelli, L., Cavanagh, P., &
Thornton, I. M. (2003). Perception of biological motion in parietal
patients. Neuropsychologia, 41, 1808--1816. Bay, E. (1950). Agnosie und
funktionswandel. Springer: Berlin. Baylor, D. (1992). Transduction in
retinal photoreceptor cells. In P. Corey & S. D. Roper (Eds.), Sensory
transduction (pp. 151--174). New York: Rockefeller University Press.
Beauchamp, G. K., & Mennella, J. A. (2009). Early flavor learning and
its impact on later feeding behavior. Journal of Pediatric
Gastroenterology and Nutrition, 48, S25--S30. Beauchamp, G. K., Cowart,
B. J., Mennella, J. A., & Marsh, R. R. (1994). Infant salt taste:
Developmental, methodological and contextual factors. Developmental
Psychobiology, 27, 353--365. Beauchamp, G. L., & Mennella, J. A. (2011).
Flavor perception in human infants: Development and functional
significance. Digestion, 83(suppl. 1), 1--6. Beck, C. J. (1993).
Attention means attention. Tricycle: The Buddhist Review, 3(1). 446

Beckers, G., & Homberg, V. (1992). Cerebral visual motion blindness:
Transitory akinetopsia induced by transcranial magnetic stimulation of
human area V5. Proceedings of the Royal Society of London B, Biological
Sciences, 249, 173--178. Beecher, H. K. (1959). Measurement of
subjective responses. New York: Oxford University Press. Beilock, S.
(2012). How humans learn: Lessons from the sea squirt. Psychology Today,
Posted July 11, 2012. Békésy, G. von. (1960). Experiments in hearing.
New York: McGraw-Hill. Belfi, A. M., Karlan, B., & Tranel, D. (2016).
Music evokes vivid autobiographical memories. Memory, 24(7), 979--989.
Belin, P., Zatorre, R. J., Lafaille, P., Ahad, P., & Pike, B. (2000).
Voiceselective areas in human auditory cortex. Nature, 403(6767),
309--312. Bendor, D., & Wang, X. (2005). The neuronal representation of
pitch in primate auditory cortex. Nature, 436, 1161--1165. Benedetti,
F., Arduino, C., & Amanzio, M. (1999). Somatotopic activation of opioid
systems by target-directed expectations of analgesia. Journal of
Neuroscience, 19, 3639--3648. Benjamin, L. T. (1997). A history of
psychology (2nd ed.). New York: Mc-Graw Hill. Bensmaia, S. J., Denchev,
P. V., Dammann, J. F. III, Craig, J. C., & Hsiao, S. S. (2008). The
representation of stimulus orientation in the early stages of
somatosensory processing. Journal of Neuroscience, 28, 776--786.
Beranek, L. L. (1996). Concert and opera halls: How they sound.
Woodbury, NY: Acoustical Society of America. Berger, K. W. (1964). Some
factors in the recognition of timbre. Journal of the Acoustical Society
of America, 36, 1881--1891. Berkowitz, A. (2018). You can observe a lot
by watching: Hughlings Jackson's underappreciated and prescient ideas
about brain control of movement. The Neuroscientist, 24(5), 448--455.
Bess, F. H., & Humes, L. E. (2008). Audiology: The fundamentals (4th
ed.). Philadelphia: Lippencott, Williams & Wilkins. Bharucha, J., &
Krumhansl, C. L. (1983). The representation of harmonic structure in
music: Hierarchies of stability as a function of content. Cognition, 13,
63--102. Biederman, I. (1987). Recognition-by-components: A theory of
human image understanding. Psychological Review, 94(2), 115. Bilalić,
M., Langner, R., Ulrich, R., & Grodd, W. (2011). Many faces of
expertise: Fusiform face area in chess experts and novices. Journal of
Neuroscience, 31, 10206--10214. Bilinska, K., Jakubowska, P., Von
Bartheld, C. S., & Butowt, R. (2020). Expression of the SARS-CoV-2 entry
proteins, ACE2 and TMPRSS2, in cells of the olfactory epithelium:
Identification of cell types and trends with age. ACS Chemical
Neuroscienec, 11, 1555--1562. Bingel, U., Wanigesekera, V., Wiech, K.,
Mhuircheartaigh, R. N., Lee, M. C., Ploner, M., et al. (2011). The
effect of treatment expectation on drug efficacy: Imaging the analgesic
benefit of the opioid Remifentanil. Science Translational Medicine, 3,
70ra14. Birnbaum, M. (2011). Season to taste. New York: Harper Collins.
Birnberg, J. R. (1988, March 21). My turn. Newsweek. Bisiach, E., &
Luzzatti, G. (1978). Unilateral neglect of representational space.
Cortex, 14, 129--133. Biswal, B., Zerrin Yetkin, F., Haughton, V. M., &
Hyde, J. S. (1995). Functional connectivity in the motor cortex of
resting human brain using echo-planar MRI. Magnetic Resonance in
Medicine, 34(4), 537--541. Blake, R., & Hirsch, H. V. B. (1975).
Deficits in binocular depth perception in cats after alternating
monocular deprivation. Science, 190, 1114--1116. Blakemore, C., &
Cooper, G. G. (1970). Development of the brain depends on the visual
environment. Nature, 228, 477--478. Blaser, E., & Sperling, G. (2008).
When is motion "motion"? Perception, 37, 624--627. Block, N. (2009).
Comparing the major theories of consciousness. In M. S. Gazzaniga (Ed.),
The cognitive neurosciences (4th ed.). Cambridge, MA: MIT Press.

References

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Blood, A. J., & Zatorre, R. J. (2001). Intensely pleasurable responses
to music correlate with activity in brain regions implicated in reward
and emotion. Proceedings of the National Academy of Sciences, 98(20),
11818--11823. Bolya, D., Zhou, C., Xiao, F., & Lee, Y. J. (2019).
YOLACT: Real-time instance segmentation. In Proceedings of the IEEE
International Conference on Computer Vision (pp. 9157--9166). Boring, E.
G. (1942). Sensation and perception in the history of experimental
psychology. New York: Appleton-Century-Crofts. Borji, A., & Itti, L.
(2014). Defending Yarbus: Eye movements reveal observers' task. Journal
of Vision, 14(3), 1--22. Borjon, J. I., Schroer, S. E., Bambach, S.,
Slone, L. K., Abney, D. H., Crandall, D. J., & Smith, L. B. (2018). A
view of their own: Capturing the egocentric view of infants and toddlers
with headmounted cameras. Journal of Visualized Experiments, (140),
e58445, doi:10.3791/58445 (2018). Bornstein, M. H., Kessen, W., &
Weiskopf, S. (1976). Color vision and hue categorization in young human
infants. Journal of Experimental Psychology: Human Perception and
Performance, 2, 115--119. Borst, A., & Egelhaaf, M. (1989). Principles
of visual motion detection. Trends in Neurosciences, 12, 297--306.
Bortfeld, H. (2018). Functional near-infrared spectroscopy as a tool for
assessing speech and spoken language processing in pediatric and adult
cochlear implant users. Developmental Psychobiology, 61, 430--433.
Bosker, B. (2016). Tristan Harris believes Silicon Valley is addicting
us to our phones. He's determined to make it stop. The Atlantic, 56--65.
Bosten, J. M., & Boehm, A. E. (2014). Empirical evidence for unique
hues? Journal of the Optical Society of America A, 31(4), A365--A393.
Bouvier, S. E., & Engel, S. A. (2006). Behavioral deficits and cortical
damage loci in cerebral achromatopsia. Cerebral Cortex, 16, 183--191.
Bowmaker, J. K., & Dartnall, H. J. A. (1980). Visual pigments of rods
and cones in a human retina. Journal of Physiology, 298, 501--511.
Boynton, R. M. (1979). Human color vision. New York: Holt, Rinehart and
Winston. Brainard, D. H., Longere, P., Delahunt, P. B., Freeman, W. T.,
Kraft, J. M., & Xiao, B. (2006). Bayesian model of human color
constancy, Journal of Vision, 6, 1267--1281. Brainard, D. H. (1998).
Color constancy in the nearly natural image. 2. Achromatic loci. Journal
of the Optical Society of America, 15(2), 307--325. Brainard, D. H., &
Hulbert, A. C. (2015). Colour vision: Understanding #TheDress. Current
Biology, 25, R549--R568. Bregman, A. S. (1990). Auditory scene analysis.
Cambridge: MIT Press. Bregman, A. S. (1993). Auditory scene analysis:
Hearing in complex environments. In S. McAdams & E. Bigand (Eds.),
Thinking in sound: The cognitive psychology of human audition
(pp. 10--36). Oxford, UK: Oxford University Press. Bregman, A. S., &
Campbell, J. (1971). Primary auditory stream segregation and perception
of order in rapid sequence of tones. Journal of Experimental Psychology,
89, 244--249. Bremmer, F. (2011). Multisensory space: From eye-movements
to selfmotion. Journal of Physiology, 589, 815--823. Bremner, A. J., &
Spence, D. (2017). The development of tactile perception. Advances in
Child Development and Behavior, 52, 227--268. Brendt, M. R., & Siskind,
J. M. (2001). The role of exposure to isolated words in early vocabulary
development. Cognition, 81, B33--B34. Breslin, P. A. S. (2001). Human
gustation and flavour. Flavour and Fragrance Journal, 16, 439--456.
Breveglieri, R., De Vitis, M., Bosco, A., Galletti, C., & Fattori, P.
(2018). Interplay between grip and vision in the monkey medial parietal
lobe. Cerebral Cortex, 28, 2028--2042. Britten, K. H., Shadlen, M. N.,
Newsome, W. T., & Movshon, J. A. (1992). The analysis of visual motion:
A comparison of neuronal and psychophysical performance. Journal of
Neuroscience, 12, 4745--4765. Broadbent, D. E. (1958). Perception and
communication. London: Pergamon Press.

Broca, P. (1861). Sur le volume et al forme du cerveau suivant les
individus et suivant les races. Bulletin Societé d'Anthropologie Paris,
2, 139--207, 301--321, 441--446. (See psychclassics.yorku.ca for
translations of portions of this paper.) Brockmole, J. R., Davoli, C.
C., Abrams, R. A., & Witt, J. K. (2013). The world within reach: Effects
of hand posture and tool-use on visual cognition. Current Directions in
Psychological Science, 22, 38--44. Brown, A. E., Stecker, G. C., &
Tollin, D. J. (2015). The precedence effect in sound localization.
Journal of the Association for Research in Otolaryngology, 16(1), 1--28.
Brown, P. K., & Wald, G. (1964). Visual pigments in single rods and
cones of the human retina. Science, 144, 45--52. Brunec, I. K., Robin,
J., Patai, E. Z., Ozubko, J. D., Javadi, A-H., Barense, M. D., Spiers,
H. J., Moscovitch, M. (2019). Cognitive mapping style relates to
posterior-anterior hippocampal volume ratio. Hippocampus, 29, 748--754.
Bruno, N., & Bertamini, M. (2015). Perceptual organization and the
aperture problem. In J. Wagemans (Ed.), Oxford handbook of perceptual
organization (pp. 504--520). Oxford, UK: Oxford University Press.
Buccino, G., Lui, G., Canessa, N., Patteri, I., Lagravinese, G.,
Benuzzi, F., et al. (2004). Neural circuits involved in the recognition
of actions performed by nonconspecifics: An fMRI study. Journal of
Cognitive Neuroscience, 16, 114--126. Buck, L. B. (2004). Olfactory
receptors and coding in mammals. Nutrition Reviews, 62, S184--S188.
Buck, L., & Axel, R. (1991). A novel multigene family may encode odorant
receptors: A molecular basis for odor recognition. Cell, 65, 175--187.
Buckingham, G. (2014). Getting a grip on heaviness perception: A review
of weight illusions and their possible causes. Experimental Brain
Research, 232, 1623--1629. Budd, K. (2017). Keep your mental focus. AARP
Bulletin. November 27, 2017. Bufe, B., Breslin, P. A. S., Kuhn, C.,
Reed, D. R., Tharp, C. D., Slack, J. P., et al. (2005). The molecular
basis of individual differences in phenylthiocarbamide and
propylthiouracil bitterness perception. Current Biology, 15, 322--327.
Bugelski, B. R., & Alampay, D. A. (1961). The role of frequency in
developing perceptual sets. Canadian Journal of Psychology, 15,
205--211. Buhle, J. T., Stebens, B. L., Friedman, J. J., & Wager, T. D.
(2012). Distraction and placebo: Two separate routes to pain control.
Psychological Science, 23, 246--253. Bukach, C. M., Gauthier, I., &
Tarr, M. J. (2006). Beyond faces and modularity: The power of an
expertise framework. Trends in Cognitive Sciences, 10, 159--166. Bunch,
C. C. (1929). Age variations in auditory acuity. Archives of
Otolaryngology, 9, 625--636. Burns, E. M., & Viemeister, N. F. (1976).
Nonspectral pitch. Journal of the Acoustical Society of America, 60,
863--869. Burton, A. M., Young, A. W., Bruce, V., Johnston, R. A., &
Ellis, A. W. (1991). Understanding covert recognition. Cognition, 39,
129--166. Bushdid, C., Magnasco, M. O., Vosshall, L. B., & Keller, A.
(2014). Humans can discriminate more than 1 trillion olfactory stimuli.
Science, 343, 1370--1372. Bushnell, C. M., Ceko, M., & Low, L. A.
(2013). Cognitive and emotional control of pain and its disruption in
chronic pain. Nature Reviews Neuroscience, 14, 502--511. Bushnell, I. W.
R. (2001). Mother's face recognition in newborn infants: Learning and
memory. Infant and Child Development, 10, 67--74. Bushnell, I. W. R.,
Sai, F., & Mullin, J. T. (1989). Neonatal recognition of the mother's
face. British Journal of Developmental Psychology, 7, 3--15. Busigny,
T., & Rossion, B. (2010). Acquired prosopognosia abolishes the face
inversion effect. Cortex, 46, 965--981. Byl, N., Merzenich, M., &
Jenkins, W. (1996). A primate genesis model of focal dystonia and
repetitive strain injury. Neurology, 47, 508--520. References

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

447

Caggiano, V., Fogassi, L., Rizzolatti, G., Thier, P., & Casile, A.
(2009). Mirror neurons differentially encode the peripersonal and
extrapersonal space of monkeys. Science, 324, 403--406. Cain, W. S.
(1979). To know with the nose: Keys to odor identification. Science,
203, 467--470. Cain, W. S. (1980). Sensory attributes of cigarette
smoking (Branbury Report: 3. A safe cigarette?, pp. 239--249). Cold
Spring Harbor, NY: Cold Spring Harbor Laboratory. Calder, A. J., Beaver,
J. D., Winston, J. S., Dolan, R. J., Jenkins, R., Eger, E., et
al. (2007). Separate coding of different gaze directions in the superior
temporal sulcus and inferior parietal lobule. Current Biology, 17,
20--25. Calvert, G. A., Bullmore, E. T., Brammer, M. J., Campbell, R.,
Williams, S. C. R., McGuire, P. K., et al. (1997). Activation of
auditory cortex during silent lipreading. Science, 276, 593--595.
Cameron, E. L. (2018). Olfactory perception in children. World Journal
of Othorhinolaryngology-Head Surgery, 4, 57--66. Campbell, F. W.,
Kulikowski, J. J., & Levinson, J. (1966). The effect of orientation on
the visual resolution of gratings. Journal of Physiology (London), 187,
427--436. Carello, C., & Turvey, M. T. (2004). Physics and psychology of
the muscle sense. Current Directions in Psychological Science, 13,
25--28. Carlson, N. R. (2010). Psychology: The science of behavior (7th
ed.). New York: Pearson. Carr, C. E., & Konishi, M. (1990). A circuit
for detection of interaural time differences in the brain stem of the
barn owl. Journal of Neuroscience, 10, 3227--3246. Carrasco, M. (2011).
Visual attention: The past 25 years. Vision Research, 51, 1484--1525.
Carrasco, M., & Barbot, A. (2019). Spatial attention alters visual
appearance. Current Opinion in Psychology, 29, 56--64. Carrasco, M.,
Ling, S., & Read, S. (2004). Attention alters appearance. Nature
Neuroscience, 7, 308--313. Cartwright-Finch, U., & Lavie, N. (2007). The
role of perceptual load in inattentional blindness. Cognition, 102,
321--340. Carvalho, F. R., Wang, Q. J., van E, R., Persoone, D., &
Spence, C. (2017). "Smooth operator": Music modulates the perceived
creaminess, sweetness, and bitterness of chocolate. Appetite, 108,
383--390. Casagrande, V. A., & Norton, T. T. (1991). Lateral geniculate
nucleus: A review of its physiology and function. In J. R.
Coonley-Dillon (Vol. Ed.) & A. G. Leventhal (Ed.), Vision and visual
dysfunction: The neural basis of visual function (Vol. 4, pp. 41--84).
London: Macmillan. Cascio, C. J., Moore, D., & McGlone, F. (2019).
Social touch and human development. Developmental Cognitive
Neuroscience, 35, 5--11. Caspers, S., Ziles, K., Laird, A. R., &
Eickoff, S. B. (2010). ALE metaanalysis of action observation and
imitation in the human brain. NeuroImage, 50, 1148--1167. Castelhano, M.
S., & Henderson, J. M. (2008). Stable individual differences across
images in human saccadic eye movements. Canadian Journal of Psychology,
62, 1--14. Castelhano, M. S., & Henderson, J. M. (2008). The influence
of color and structure on perception of scene gist. Journal of
Experimental Psychology: Human Perception and Performance, 34, 660--675.
Castelhano, M. S., & Henderson, J. M. (2008). The influence of color on
the perception of scene gist. Journal of Experimental Psychology: Human
Perception and Performance, 34, 660--675. Castelli, F., Happe, F.,
Frith, U., & Frith, C. (2000). Movement and mind: A functional imaging
study of perception and interpretation of complex intentional movement
patterns. Neuroimage, 12, 314--325. Castiello, U., Becchio, C., Zoia,
S., et al. (2010). Wired to be social: The ontogeny of human
interaction. PLoS One 5(10) e13199, 1--10. Cattaneo, L., & Rizzolatti,
G. (2009). The mirror neuron system. Archives of Neurology, 66,
557--560. Cavallo, A. K., Koul, A., Ansuini, C., Capozzi F., & Becchio,
C. (2016). Decoding intentions from movement kinematics, Scientific
Reports, 6, 37036. 448

Cavanagh, P. (2011). Visual cognition. Visual Research, 51, 1538--1551.
Centelles, L., Assainte, C., Etchegoyen, K., Bouvard, M., & Schmitz, C.
(2013). From action to inaction: Exploring the contribution of body
motion cues to social understanding in typical development and in autism
spectrum disorder. Journal of Autism Developmental Disorder, 43,
1140--1150. Cerf, M., Thiruvengadam, N., Mormann, F., Kraskov, A.,
Quiroga, R. Q., Koch, C., et al. (2010). On-line voluntary control of
human temporal lobe neurons. Nature, 467, 1104--1108. Chanda, M. L., &
Levitin, D. J. (2013). The neurochemistry of music. Trends in Cognitive
Sciences, 17(4), 179--193. Chandler, R. (1950). The simple act of
murder. Atlantic Monthly. Chandrashekar, J., Hoon, M. A., Ryba, N. J.
P., & Zuker, C. S. (2006). The receptors and cells for mammalian taste.
Nature, 444, 288--294. Chapman, C. R. (1995). The affective dimension of
pain: A model. In B. Bromm & J. Desmedt (Eds.), Pain and the brain: From
nociception to cognition: Advances in pain research and therapy (Vol.
22, pp. 283--301). New York: Raven. Charpentier, A. (1891). Analyse
expérimentale: De quelques éléments de la sensation de poids"
\[Experimental study of some aspects of weight perception\], Archives de
Physiologie Normales et Pathologiques, 3, 122--135. Chatterjee, S. H.,
Freyd, J., & Shiffrar, M. (1996). Configural processing in the
perception of apparent biological motion. Journal of Experimental
Psychology: Human Perception and Performance, 22, 916--929. Chen, J. L.,
Penhune, V. B., & Zatorre, R. J. (2008). Listening to musical rhythms
recruits motor regions of the brain. Cerebral Cortex, 18, 2844--2854.
Cheong, D., Zubieta, J-K., & Liu, J. (2012). Neural correlates of visual
motion perception. PLoS One, 7, Issue 6. Cherry, E. C. (1953). Some
experiments on the recognition of speech, with one and with two ears.
Journal of the Acoustical Society of America, 25, 975--979. Chiu, Y.-C.,
& Yantis, S. (2009). A domain-independent source of cognitive control
for task sets: Shifting spatial attention and switching categorization
rules. Journal of Neuroscience, 29, 3930--3938. Chobert, J., Marie, C.,
Francois, C., Schon, D., & Bresson, M. (2011). Enhanced passive and
active processing of syllables in musician children. Journal of
Cognitive Neuroscience, 23(12), 3874--3887. Choi, G. B., Stettler, D.
D., Kallman, B. R., Bhaskar, S. T., Fleischmann, A., & Axel, R. (2011).
Driving opposing behaviors with ensembles of piriform neurons. Cell,
146, 1004--1015. Cholewaik, R. W., & Collins, A. A. (2003). Vibrotactile
localization on the arm: Effects of place, space, and age. Perception &
Psychophysics, 65, 1058--1077. Chun, M. M., Golomb, J. D., &
Turk-Browne, N. B. (2011). A taxonomy of external and internal
attention. Annual Review of Psychology, 62, 73--101. Churchland, P. S.,
& Ramachandran, V. S. (1996). Filling in: Why Dennett is wrong. In K.
Akins (Ed.), Perception (pp. 132--157). Oxford, UK: Oxford University
Press. Cirelli, L. K., Jurewicz, Z. B., & Trehub, S. E. (2019). Effects
of maternal singing style on mother-infant arousal and behavior. Journal
of Cognitive Neuroscience, 32(7), 1213--1220. Cisek, P., & Kalaska, J.
F. (2010). Neural mechanisms for interacting with a world full of action
choices. Annual Review of Neuroroscience, 33, 269--298. Clarke, F. F., &
Krumhansl, C. L. (1990). Perceiving musical time. Music Perception, 7,
213--252. Clarke, T. C., Barnes, P. M., Lindsey, I. B., Stussman, B. J.,
& Nahin, R. L. (2018). Use of yoga, meditation, and chiropractors among
U.S. adults aged 18 and over. NCHS Data Brief, No. 325. U.S. Department
of Health and Human Services. Collett, T. S. (1978). Peering: A locust
behavior pattern for obtaining motion parallax information. Journal of
Experimental Biology, 76, 237--241.

References

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Colloca, L., & Benedetti, F. (2005). Placebos and painkillers: Is mind
as real as matter? Nature Reviews Neuroscience, 6, 545--552. Colombo,
M., Colombo, A., & Gross, C. G. (2002). Bartolomeo Panizza's
observations on the optic nerve (1855). Brain Research Bulletin, 58(6),
529--539. Coltheart, M. (1970). The effect of verbal size information
upon visual judgments of absolute distance. Perception and
Psychophysics, 9, 222--223. Comèl, M. (1953). Fisiologia normale e
patologica della cute umana. Milan, Italy: Fratelli Treves Editori.
Connolly, J. D., Andersen, R. A., & Goodale, M. A. (2003). fMRI evidence
for a "parietal reach region" in the human brain. Experimental Brain
Research, 153, 140--145. Conway, B. R. (2009). Color vision, cones, and
color-coding in the cortex. The Neuroscientist, 15(3), 274--290. Conway,
B. R., Chatterjee, S., Field, G. D., Horwitz, D., Johnson, E. N., Koida,
K., & Mancuso, K. (2010). Advances in color science: From retina to
behavior. Journal of Neuroscience, 30(45), 14955--14963. Cook, R., Bird,
G., Catmur, C., Press, C., & Heyes, C. (2014). Mirror neurons: From
origin to function. Behavioral and Brain Sciences, 37, 177--241.
Coppola, D. M., Purves, H. R., McCoy, A. N., & Purves, D. (1998). The
distribution of oriented contours in the real world. Proceedings of the
National Academy of Sciences, 95, 4002--4006. Coppola, D. M., White, L.
E., Fitzpatrick, D., & Purves, D. (1998). Unequal distribution of
cardinal and oblique contours in ferret visual cortex. Proceedings of
the National Academy of Sciences, 95, 2621--2623. Corbeil, M., Trehub,
S. E., & Peretz, I. (2016). Singing delays the onset of infant distress.
Infancy, 21(3), 373--391. Craig, J. C., & Lyle, K. B. (2001). A
comparison of tactile spatial sensitivity on the palm and fingerpad.
Perception & Psychophysics, 63, 337--347. Craig, J. C., & Lyle, K. B.
(2002). A correction and a comment on Craig and Lyle (2001). Perception
& Psychophysics, 64, 504--506. Crick, F. C., & Koch, C. (2003). A
framework for consciousness. Nature Neuroscience, 6, 119--127. Crisinel,
A-S., & Spence, C. (2010). As bitter as a trombone: Synesthetic
correspondences in nonsynesthetes between tastes/flavors and musical
notes. Attention, Perception & Psychophysics, 72(7), 1994--2002.
Crisinel, A-S., & Spence, C. (2012). A fruity note: Crossmodal
associations between odors and musical notes. Chemical Senses, 37,
151--158. Crisinel, A-S., Cosser, S., King, S., Jones, R., Petrie, J., &
Spence, C. (2012). A bittersweet symphony: Systematically modulating the
taste of food by changing the sonic properties of the soundtrack playing
in the background. Food Quality and Preference, 24, 201--204. Crouzet,
S. M., Kirchner, H., & Thorpe, S. J. (2010). Fast saccades toward faces:
Face detection in just 100 ms. Journal of Vision, 10(4), 1--17. Croy,
I., Bojanowski, V., & Hummel, T. (2013). Men without a sense of smell
exhibit a strongly reduced number of sexual relationships, women exhibit
reduced partnership security---a reanalysis of previously published
data. Biological Psychology, 92, 292--294. Csibra, G. (2008). Goal
attribution to inanimate agents by 6.5-monthold infants. Cognition, 107,
705--717. Çukur, T., Nishimoto, S., Huth, A. G., & Gallant, J. L.
(2013). Attention during natural vision warps semantic representation
across the human brain. Nature Neuroscience, 16, 763--770. Culler, E. A.
(1935). An experimental study of tonal localization in the cochlea of
the guinea pig. Annals of Otology, Rhinology & Laryngology, 44, 807.
Culler, E. A., Coakley, J. D., Lowy, K., & Gross, N. (1943). A revised
frequency-map of the guinea-pig cochlea. American Journal of Psychology,
56, 475--500. Cutting, J. E., & Rosner, B. S. (1974). Categories and
boundaries in speech and music. Perception & Psychophysics, 16,
564--570.

Cutting, J. E., & Vishton, P. M. (1995). Perceiving layout and knowing
distances: The integration, relative potency, and contextual use of
different information about depth. In W. Epstein & S. Rogers (Eds.),
Handbook of perception and cognition: Perception of space and motion
(pp. 69--117). New York: Academic Press. D'Ausilio, A., Pulvermuller,
F., Salmas, P., Bufalari, I., Begliomini, C., & Fadiga, L. (2009). The
motor somatotopy of speech perception. Current Biology, 19, 381--385. Da
Cruz, L., Coley, B. F., Dorn, J., Merlini, F., Filley, E., Christopher,
P., ... & Humayun, M. (2013). The Argus II epiretinal prosthesis system
allows letter and word reading and long-term function in patients with
profound vision loss. British Journal of Ophthalmology, 97(5), 632--636.
Dallos, P. (1996). Overview: Cochlear neurobiology. In P. Dallos, A. N.
Popper, & R. R. Fay (Eds.), The cochlea (pp. 1--43). New York: Springer.
Dalton, D. S., Cruickshanks, K. J., Wiley, T. L., Klein, B. E. K.,
Klein, R., & Tweed, T. S. (2001). Association of leisure-time noise
exposure and hearing loss. Audiology, 40, 1--9. Dannemiller, J. L.
(2009). Perceptual development: Color and contrast. In E. B. Goldstein
(Ed.), Sage encyclopedia of perception (pp. 738--742). Thousand Oaks,
CA: Sage. Dapretto, M., Davies, M. S., Pfeifer, J. H., Scott, A. A.,
Sigman, M., Bookheimer, S. Y., et al. (2006). Understanding emotions in
others: Mirror neuron dysfunction in children with autism spectrum
disorders. Nature Neuroscience, 9, 28--30. Dartnall, H. J. A., Bowmaker,
J. K., & Mollon, J. D. (1983). Human visual pigments:
Microspectrophotometric results from the eyes of seven persons.
Proceedings of the Royal Society of London B, 220, 115--130. Darwin, C.
(1871). The descent of man. London: John Murray. Darwin, C. J. (2010).
Auditory scene analysis. In E. B. Goldstein (Ed.), Sage encyclopedia of
perception. Thousand Oaks, CA: Sage. Datta, R., & DeYoe, E. A. (2009). I
know where you are secretly attending! The topography of human visual
attention revealed with fMRI. Vision Research, 49, 1037--1044.
Davatzikos, C., Ruparel, K., Fan, Y., Shen, D. G., Acharyya, M.,
Loughead, J. W., ... & Langleben, D. D. (2005). Classifying spatial
patterns of brain activity with machine learning methods: Application to
lie detection. Neuroimage, 28(3), 663--668. David, A. S., & Senior, C.
(2000). Implicit motion and the brain. Trends in Cognitive Sciences, 4,
293--295. Davidovic, M., Starck, G., & Olausson, H. (2019). Processing
of affective and emotionally neutral tactile stimuli in the insular
cortex. Developmental Cognitive Neuroscience, 35, 94--103. Davis, H.
(1983). An active process in cochlear mechanics. Hearing Research, 9,
79--90. Davis, M. H., Johnsrude, I. S., Hervais-Adelman, A., Taylor, K.,
& McGettigan, C. (2005). Lexical information drives perceptual learning
of distorted speech: Evidence from the comprehension of noise-vocoded
sentences. Journal of Experimental Psychology: General, 134, 222--241.
Day, R. H. (1989). Natural and artificial cues, perceptual compromise
and the basis of veridical and illusory perception. In D. Vickers & P.
L. Smith (Eds.), Human information processing: Measures and mechanisms
(pp. 107--129). North Holland, The Netherlands: Elsevier Science. Day,
R. H. (1990). The Bourdon illusion in haptic space. Perception and
Psychophysics, 47, 400--404. de Araujo, I. E., Geha, P., & Small, D.
(2012). Orosensory and homeostatic functions of the insular cortex.
Chemical Perception, 5, 64--79. de Araujo, I. E., Rolls, E. T., Velazco,
M. I., Margot, C., & Cayeux, I. (2005). Cognitive modulation of
olfactory processing. Neuron, 46, 671--679. de Haas, B., Kanai, R.,
Jalkanen, L., & Rees, G. (2012). Grey-matter volume in early human
visual cortex predicts proneness to the sound-induced flash illusion.
Proceedings of the Royal Society B, 279, 4955--4961.

References

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

449

De Santis, L., Clarke, S., & Murray, M. (2007). Automatic and intrinsic
auditory "what" and "where" processing in humans revealed by electrical
neuroimaging. Cerebral Cortex, 17, 9--17. DeAngelis, G. C., Cumming, B.
G., & Newsome, W. T. (1998). Cortical area MT and the perception of
stereoscopic depth. Nature, 394, 677--680. DeCasper, A. J., & Fifer, W.
P. (1980). Of human bonding: Newborns prefer their mother's voices.
Science, 208(4448), 1174--1176. DeCasper, A. J., & Spence, M. J. (1986).
Prenatal maternal speech influences newborns' perception of speech
sounds. Infant Behavior and Development, 9, 133--150 DeCasper, A. J.,
Lecanuet, J.-P., Busnel, M.-C., Deferre-Granier, C., & Maugeais, R.
(1994). Fetal reactions to recurrent maternal speech. Infant Behavior
and Development, 17, 159--164. Delahunt, P. B., & Brainard, D. H.
(2004). Does human color constancy incorporate the statistical
regularity of natural daylight? Journal of Vision, 4, 57--81. Delay, E.
R., Hernandez, N. P., Bromley, K., & Margolskee, R. F. (2006). Sucrose
and monosodium glutamate taste thresholds and discrimination ability of
T1R3 knockout mics. Chemical Senses, 31, 351--357. Deliege, I. (1987).
Grouping conditions in listening to music: An approach to Lerdhal &
Jackendoff's grouping preference rules. Music Perception, 4, 325--360.
DeLucia, P., & Hochberg, J. (1985). Illusions in the real world and in
the mind's eye \[Abstract\]. Proceedings of the Eastern Psychological
Association, 56, 38. DeLucia, P., & Hochberg, J. (1986). Real-world
geometrical illusions: Theoretical and practical implications
\[Abstract\]. Proceedings of the Eastern Psychological Association, 57,
62. DeLucia, P., & Hochberg, J. (1991). Geometrical illusions in solid
objects under ordinary viewing conditions. Perception and Psychophysics,
50, 547--554. Delwiche, J. F., Buletic, Z., & Breslin, P. A. S. (2001a).
Covariation in individuals' sensitivities to bitter compounds: Evidence
supporting multiple receptor/transduction mechanisms. Perception &
Psychophysics, 63, 761--776. Delwiche, J. F., Buletic, Z., & Breslin, P.
A. S. (2001b). Relationship of papillae number to bitter intensity of
quinine and PROP within and between individuals. Physiology and
Behavior, 74, 329--337. Dematte, M. L., Sanabria, D., Sugarman, R., &
Spence, C. (2006). Crossmodal interactions between olfaction and touch.
Chemical Senses, 31, 291--300. Denes, P. B., & Pinson, E. N. (1993). The
speech chain (2nd ed.). New York: Freeman. Derbyshire, S. W. G., Jones,
A. K. P., Gyulia, F., Clark, S., Townsend, D., & Firestone, L. L.
(1997). Pain processing during three levels of noxious stimulation
produces differential patterns of central activity. Pain, 73, 431--445.
Desor, J. A., & Beauchamp, G. K. (1974). The human capacity to transmit
olfactory information. Perception and Psychophysics, 13, 271--275.
Deutsch, D. (2013b). The processing of pitch combinations. In D. Deutsch
(Ed.), The psychology of music, 3e (pp. 249--325). New York: Elsevier.
Deutsch, D. (1975). Two-channel listening to musical scales. Journal of
the Acoustical Society of America, 57, 1156--1160. Deutsch, D. (1996).
The perception of auditory patterns. In W. Prinz & B. Bridgeman (Eds.),
Handbook of perception and action (Vol. 1, pp. 253--296). San Diego, CA:
Academic Press. Deutsch, D. (1999). The psychology of music (2nd ed.).
San Diego, CA: Academic Press. Deutsch, D. (2013a). Grouping mechanisms
in music. In D. Deutsch (Ed.), The psychology of music, 3e
(pp. 183--248). New York: Elsevier. DeValois, R. L. (1960). Color vision
mechanisms in monkey. Journal of General Physiology, 43, 115--128.
Devanand, D. P., Lee, S., Manly, J., et al. (2015). Olfactory deficits
predict cognitive decline and Alzheimer dementia in an urban community.
Neurology, 84, 182--189. 450

Devlin, J. T., & Aydelott, J. (2009). Speech perception: Motoric
contributions versus the motor theory. Current Biology, 19(5),
R198--R200. DeWall, C. N., MacDonald, G., Webster, G. D., Masten, C. L.,
Baumeister, R. F., Powell, C., et al. (2010). Tylenol reduces social
pain: Behavioral and neural evidence. Psychological Science, 21,
931--937. deWied, M., & Verbaten, M. N. (2001). Affective pictures
processing, attention, and pain tolerance. Pain, 90, 163--172. Dick, F.,
Bates, E., Wulfeck, B., Utman, J. A., Dronkers, N., & Gernsbacher, M. A.
(2001). Language deficits, localization, and grammar: Evidence for a
distributive model of language breakdown in aphasic patients and
neurologically intact individuals. Psychological Review, 108, 759--788.
Dingus, T. A., Klauer, S. G., Neale, V. L., Petersen, A., Lee, S. E.,
Sudweeks, J., et al. (2006). The 100-car naturalistic driving study:
Phase II. Results of the 100-car field experiment (Interim Project
Report for DTNH22-00-C-07007, Task Order 6; Report No. DOT HS 810 593).
Washington, DC: National Highway Traffic Safety Administration. Divenyi,
P. L., & Hirsh, I. J. (1978). Some figural properties of auditory
patterns. Journal of the Acoustical Society of America, 64(5),
1369--1385. Djourno, A., & Eyries, C. (1957). Prosthèse auditive par
excitation électrique à distance du nerf sensoriel à l'aide d'un
bobinage inclus à demeure. Presse médicale, 65(63). Dobson, V., &
Teller, D. (1978). Visual acuity in human infants: Review and comparison
of behavioral and electrophysiological studies. Vision Research, 18,
1469--1483. Dooling, R. J., Okanoya, K., & Brown, S. D. (1989). Speech
perception by budgerigars (Melopsittacus undulates): The
voiced-voiceless distinction. Perception & Psychophysics, 46, 65--71.
Dougherty, R. F., Koch, V. M., Brewer, A. A., Fischer, B., Modersitzki,
J., & Wandell, B. A. (2003). Visual field representations and locations
of visual areas V1/2/3 in human visual cortex. Journal of Vision, 3,
586--598. Dowling, J. E., & Boycott, B. B. (1966). Organization of the
primate retina. Proceedings of the Royal Society of London, 166B,
80--111. Dowling, W. J., & Harwood, D. L. (1986). Music cognition. New
York: Academic Press. Downing, P. E., Jiang, Y., Shuman, M., &
Kanwisher, N. (2001). Cortical area selective for visual processing of
the human body. Science, 293, 2470--2473. Driver, J., & Vuilleumier, P.
(2001). Perceptual awareness and its loss in unilateral neglect and
extinction. Cognition, 79, 39--88. Dube, L., & Le Bel, J. L. (2003). The
content and structure of laypeople's concept of pleasure. Cognition and
Emotion, 17(2), 263--295. DuBose, C. N., Cardello, A. V., & Maller, O.
(1980). Effects of colorants and flavorants on identification, perceived
flavor intensity, and hedonic quality of fruit-flavored beverages and
cake. Journal of Food Science, 45, 1393--1400. Duncan, R. O., & Boynton,
G. M. (2007). Tactile hyperacuity thresholds correlate with finger maps
in primary somatosensory cortex (S1). Cerebral Cortex, 17, 2878--2891.
Durgin, F. H., & Gigone, K. (2007). Enhanced optic flow speed
discrimination while walking: Multisensory tuning of visual coding.
Perception, 36, 1465--1475. Durgin, F. H., Baird, J. A., Greenburg, M.,
Russell, R., Shaughnessy, K., & Waymouth, S. (2009). Who is being
deceived? The experimental demands of wearing a backpack. Psychonomic
Bulletin & Review, 16, 964--969. Durgin, F. H., Klein, B., Spiegel, A.,
Strawser, C. J., & Williams, M. (2012). The social psychology of
perception experiments: Hills, backpacks, glucose and the problem of
generalizability. Journal of Experimental Psychology: Human Perception
and Performance, 38, 1582--1595. Durrani, M., & Rogers, P. (1999,
December). Physics: Past, present, future. Physics World, 12(12), 7--13.
Durrant, J., & Lovrinic, J. (1977). Bases of hearing science. Baltimore:
Williams & Wilkins.

References

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Eames, C. (1977). Powers of ten. Pyramid Films. Eerola, T., Firberg, A.,
& Bresin, R. (2013). Emotional expression in music: Contribution,
linearity, and additivity of primary musical cues. Frontiers in
Psychology, 4, Article 487. Egbert, L. D., Battit, G. E., Welch, C. E.,
& Bartlett, M. D. (1964). Reduction of postoperative pain by
encouragement and instruction of patients. New England Journal of
Medicine, 270, 825--827. Eggermont, J. (2014). Music and the brain. In
Eggermont, J. (Ed.), Noise and the brain (Chapter 9, pp. 240--265). New
York: Elsevier. Egly, R., Driver, J., & Rafal, R. D. (1994). Shifting
visual attention between objects and locations: Evidence from normal and
parietal lesion subjects. Journal of Experimental Psychology: General,
123, 161--177. Ehrenstein, W. (1930). Untersuchungen über Figur-Grund
Fragen \[Investigations of more figure--ground questions\]. Zeitschrift
für Psychologie, 117, 339--412. Eimas, P. D., & Corbit, J. D. (1973).
Selective adaptation of linguistic feature detectors. Cognitive
Psychology, 4, 99--109. Eimas, P. D., & Quinn, P. C. (1994). Studies on
the formation of perceptually based basic-level categories in young
infants. Child Development, 65, 903--917. Eimas, P. D., Miller, J. L., &
Jusczyk, P. W. (1987). On infant speech perception and the acquisition
of language. In S. Hamad (Ed.), Categorical perception. New York:
Cambridge University Press. Eimas, P. D., Siqueland, E. R., Jusczyk, P.,
& Vigorito, J. (1971). Speech perception in infants. Science, 171,
303--306. Eisenberger, N. I. (2012). The pain of social disconnection:
Examining the shared neural underpinnings of physical and social pain.
Nature Reviews Neuroscience, 13, 421--434. Eisenberger, N. I. (2015).
Social pain and the brain: Controversies, questions, and where to go
from here. Annual Review of Psychology, 66, 601--629. Eisenberger, N.
I., & Lieberman, M. D. (2004). Why rejection hurts: A common neural
alarm system for physical and social pain. Trends in Cognitive Sciences,
8, 294--300. Eisenberger, N. I., Inagaki, T. K., Muscatell, K. A.,
Haltom, K. E. B., & Leary, M. R. (2011). The neural sociometer: Brain
mechanisms underlying state self-esteem. Journal of Cognitive
Neuroscience, 23, 3448--3455. Eisenberger, N. I., Lieberman, M. D., &
Williams, K. D. (2003). Does rejection hurt? An fMRI study of social
exclusion. Science, 302, 290--292. Ekstrom, A. D., Kahana, M. J.,
Caplan, J. B., Fields, T. A., Isham, E. A., Newman, E. L., et
al. (2003). Cellular networks underlying human spatial navigation.
Nature, 425, 184--187. El Haj, Mohamad, Clement, S., Fasotti, L., &
Allain, P. (2013). Effects of music on autobiographical verbal narration
in Alzheimer's disease. Journal of Neurolinguistics, 26, 691--700.
Elbert, T., Pantev, C., Wienbruch, C., Rockstroh, B., & Taub, E. (1995).
Increased cortical representation of the fingers of the left hand in
string players, Science, 270, 305--307. Ellingsen, D-M., Leknes, S.,
Loseth, G., Wessberg, J., & Olausson, H. (2016). The neurobiology
shaping affective touch: Expectation, motivation, and meaning in the
multisensory context. Frontiers in Psychology, 6, Article 1986. Emmert,
E. (1881). Grossenverhaltnisse der Nachbilder. Klinische Monatsblätter
für Augenheilkunde, 19, 443--450. Engen, T., & Pfaffmann, C. (1960).
Absolute judgments of odor quality. Journal of Experimental Psychology,
59, 214--219. Epstein, R. A. (2005). The cortical basis of visual scene
processing. Visual Cognition, 12, 954--978. Epstein, R. A. (2008).
Parahippocampal and retrosplenial contributions to human spatial
navigation. Trends in Cognitive Sciences, 12, 388--396. Epstein, R. A.,
& Baker, C. I. (2019). Scene perception in the human brain. Annual
Review of Vision Science, 5, 373--397. Epstein, R. A., & Kanwisher, N.
(1998). A cortical representation of the local visual environment.
Nature, 392, 598--601.

Epstein, R., Harris, A., Stanley, D., & Kanwisher, N. (1999). The
parahippocampal place area: Recognition, navigation, or encoding?
Neuron, 23, 115--125. Epstein, W. (1965). Nonrelational judgments of
size and distance. American Journal of Psychology, 78, 120--123.
Erickson, R. (1975). Sound structure in music. Berkeley: University of
California Press. Erickson, R. P. (1963). Sensory neural patterns and
gustation. In Y. Zotterman (Ed.), Olfaction and taste (Vol. 1,
pp. 205--213). Oxford, UK: Pergamon Press. Erickson, R. P. (2000). The
evolution of neural coding ideas in the chemical senses. Physiology and
Behavior, 69, 3--13. Fairhurst, M. T., Loken, L., & Grossmann, T.
(2014). Physiological and behavioral responses reveal 9-month-old
infants' sensitivity to pleasant touch. Psychological Science, 25(5),
1124--1131. Fajen, B. R., & Warren, W. H. (2003). Behavioral dynamics of
steering, obstacle avoidance and route selection. Journal of
Experimental Psychology: Human Perception and Performance, 29, 343--362.
Fantz, R. L., Ordy, J. M., & Udelf, M. S. (1962). Maturation of pattern
vision in infants during the first six months. Journal of Comparative
and Physiological Psychology, 55, 907--917. Farah, M. J., Wilson, K. D.,
Drain, H. M., & Tanaka, J. R. (1998). What is "special" about face
perception? Psychological Review, 105, 482--498. Farroni, T., Chiarelli,
A. M., Lloyd-Fox, S., Massaccesi, S., Merla, A., Di Gangi, V., ... &
Johnson, M. H. (2013). Infant cortex responds to other humans from
shortly after birth. Scientific Reports, 3(1), 1--5. Fattori, P.,
Breveglieri, R., Raos, V., Boco, A., & Galletti, C. (2012). Vision for
action in the Macaque medial posterior parietal cortex. Journal of
Neuroscience, 32, 3221--3234. Fattori, P., Raos, V., Breveglieri, R.,
Bosco, A., Marzocchi, N., & Galleti, C. (2010). The dorsomedial pathway
is not just for reaching: Grasping neurons in the medial
parieto-occipital cortex of the macaque monkey. Journal of Neuroscience,
30, 342--349. Fechner, G. T. (1966). Elements of psychophysics. New
York: Holt, Rinehart and Winston. (Original work published 1860)
Fedorenko, E., McDermott, J. H., Norman-Haignere, S., & Kanwisher, N.
(2012). Sensitivity to musical structure in the human brain. Journal of
Neurophysiology, 108, 3289--3300. Fei-Fei, L., Iyer, A., Koch, C., &
Perona, P. (2007). What do we perceive in a glance of a real-world
scene? Journal of Vision, 7, 1--29. Fernald, A., & Kuhl, P. (1987).
Acoustic determinants of infant preference for motherese speech. Infant
Behavior and Development, 10, 279--293. Fernald, R. D. (2006). Casting a
genetic light on the evolution of eyes. Science, 313, 1914--1918.
Ferrari, P. F., Gallese, V., Rizzolatti, G., & Fogassi, L. (2003).
Mirror neurons responding to the observation of ingestive and
communicative mouth actions in the monkey ventral premotor cortex.
European Journal of Neuroscience, 15, 399--402. Ferreri, L.,
Mas-Herrero, E., Zatorre, R. J., et al. (2019). Dopamine modulates the
reward experiences elicited by music. Proceedings of the National
Academy of Sciences, 116(9), 3793--3798. Fettiplace, R., & Hackney, C.
M. (2006). The sensory and motor roles of auditory hair cells. Nature
Reviews Neuroscience, 7, 19--29. Field, T. (1995). Massage therapy for
infants and children. Journal of Behavioral and Developmental
Pediatrics, 16, 105--111. Fields, H. L., & Basbaum, A. I. (1999).
Central nervous system mechanisms of pain modulation. In P. D. Wall & R.
Melzak (Eds.), Textbook of pain (pp. 309--328). New York: Churchill
Livingstone. Filimon, F., Nelson, J. D., Huang, R.-S., & Sereno, M. I.
(2009). Multiple parietal reach regions in humans: Cortical
representations for visual and proprioceptive feedback during on-line
reaching. Journal of Neuroscience, 29, 2961--2971. References

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

451

Finger, T. E. (1987). Gustatory nuclei and pathways in the central
nervous system. In T. E. Finger & W. L. Silver (Eds.), Neurobiology of
taste and smell (pp. 331--353). New York: Wiley. Finniss, D. G., &
Benedetti, F. (2005). Mechanisms of the placebo response and their
impact on clinical trials and clinical practice. Pain, 114, 3--6. Fitch,
W. T. (2015). Four principles of bio-musicology. Philosophical
Transactions of the Royal Society B, 370: 20140091. Fitch, W. T., &
Martins, M. D. (2014). Hierarchical processing in music, language, and
action: Lashley revisited. Annals of the New York Academy of Sciences,
1316, 87--104. Fletcher, H., & Munson, W. A. (1933). Loudness: Its
definition, measurement, and calculation. Journal of the Acoustical
Society of America, 5, 82--108. Fogassi, L., Ferrari, P. F., Gesierich,
B., Rozzi, S., Chersi, F., & Rizzolatti, G. (2005). Parietal lobe: From
action organization to intention understanding. Science, 302, 662--667.
Fogel, A. R., Rosenberg, J. C., Lehman, F. M., Kuperberg, G. R., &
Patel, A. D. (2015). Studying musical and linguistic prediction in
comparable ways: The melodic cloze probability method. Frontiers in
Psychology, 6, Article 1718. Forestell, C. A. (2017). Flavor perception
and preference development in human infants. Annals of Nutrition and
Metabolism, 70(suppl. 3), 17--25. Formisano, E., De Martino, F., Bonte,
M., & Goebel, R. (2008). "Who" is saying "what"? Brain-based decoding of
human voice and speech. Science, 322(5903), 970--973. Fortenbaugh, F.
C., Hicks, J. C., Hao, L., & Turano, K. A. (2006). Highspeed navigators:
Using more than what meets the eye. Journal of Vision, 6, 565--579.
Foster, D. H. (2011). Color constancy. Vision Research, 51, 674--700.
Fox, C. R. (1990). Some visual influences on human postural equilibrium:
Binocular versus monocular fixation. Perception and Psychophysics, 47,
409--422. Fox, K. C. R., Dixon, M. L., Nijeboer, S., Girn, M., Floman,
J. L., Lifshitz, M., Ellamil, M., Sedlmeier, P., & Christoff, K. (2016).
Functional neuroanatomy of meditation: A review and meta-analysis of 78
functional neuroimaging investigations. Neuroscience and Biobehavioural
Reviews, 65, 208--228. Fox, R., Aslin, R. N., Shea, S. L., & Dumais, S.
T. (1980). Stereopsis in human infants. Science, 207, 323--324.
Franconeri, S. L., & Simons, D. J. (2003). Moving and looming stimuli
capture attention. Perception & Psychophysics, 65, 999--1010. Frank, M.
E., & Rabin, M. D. (1989). Chemosensory neuroanatomy and physiology.
Ear, Nose and Throat Journal, 68, 291--292, 295--296. Frank, M. E.,
Lundy, R. F., & Contreras, R. J. (2008). Cracking taste codes by tapping
into sensory neuron impulse traffic. Progress in Neurobiology, 86,
245--263. Frankland, B. W., & Cohen, A. J. (2004). Parsing of melody:
Quantification and testing of the local grouping rules of Lerdahl and
Jackendoff's A generative theory of tonal music. Music Perception, 21,
499--543. Franklin, A., & Davies, R. L. (2004). New evidence for infant
colour categories. British Journal of Developmental Psychology, 22,
349--377. Freire, A., Lee, K., & Symons, L. A. (2000). The
face-inversion effect as a deficit in the encoding of configural
information: Direct evidence. Perception, 29, 159--170. Freire, A.,
Lewis, T. L., Maurer, D., & Blake, R. (2006). The development of
sensitivity to biological motion in noise. Perception, 35, 647--657.
Freyd, J. (1983). The mental representation of movement when static
stimuli are viewed. Perception & Psychophysics, 33, 575--581. Friedman,
H. S., Zhou, H., & von der Heydt, R. (2003). The coding of uniform
colour figures in monkey visual cortex. Journal of Physiology, 548,
593--613. Friston, K. J., Buechel, C., Fink, G. R., Morris, J., Rolls,
E., & Dolan, R. J. (1997). Psychophysiological and modulatory
interactions in neuroimaging. Neuroimage, 6, 218--229. Fritz, T.,
Jentschke, S., Gosselin, N., Sammler, D., Peretz, I., Turner, R., et
al. (2009). Universal recognition of three basic emotions in music.
Current Biology, 19, 573--576. 452

Fujioka, T., Trainor, L. J., Large, E. W., & Ross, B. (2012).
Internalized timing of isochronous sounds is represented in
neuromagnetic beta oscillations. Journal of Neuroscience, 32,
1791--1802. Fuller, S., & Carrasco, M. (2006). Exogenous attention and
color perception: Performance and appearance of saturation and hue.
Vision Research, 46, 4032--4047. Furmanski, C. S., & Engel, S. A.
(2000). An oblique effect in human visual cortex. Nature Neuroscience,
3, 535--536. Fushan, A. A., Simons, C. T., Slack, J. P., Manichalkul,
A., & Drayna, D. (2009). Allelic polymorphism within the TAS1R3 promoter
is associated with human taste sensitive to sucrose. Current Biology,
19, 1288--1293. Fyhn, M., Hafting, T., Witter, M. P., Moser, E. I., &
Moser, M.-B. (2008). Grid cells in mice. Hippocampus, 18, 1230--1238.
Gallace, A., & Spence, C. (2010). The science of interpersonal touch: A
review. Neuroscience and Biobehavioral Reviews, 34, 246--259. Gallese,
V. (2007). Before and below "theory of mind": Embodied simulation and
the neural correlates of social cognition. Philosophical Transactions of
the Royal Society B, 362, 659--669. Gallese, V., Fadiga, L., Fogassi,
L., & Rizzolatti, G. (1996). Action recognition in the premotor cortex.
Brain, 119, 593--609. Ganel, T., Tanzer, M., & Goodale, M. A. (2008). A
double dissociation between action and perception in the context of
visual illusions. Psychological Science, 19, 221--225. Gao, T., Newman,
G. E., & Scholl, B. J. (2009). The psychophysics of chasing: A case
study in the perception of animacy. Cognitive Psychology, 59, 154--179.
Gardner, M. B., & Gardner, R. S. (1973). Problem of localization in the
median plane: Effect of pinnae cavity occlusion. Journal of the
Acoustical Society of America, 53, 400--408. Gauthier, I., Skudlarski,
P., Gore, J. C., & Anderson, A. W. (2000). Expertise for cars and birds
recruits brain areas involved in face recognition. Nature Neuroscience,
3, 191--197. Gauthier, I., Tarr, M. J., Anderson, A. W., Skudlarski, P.,
& Gore, J. C. (1999). Activation of the middle fusiform face area
increases with expertise in recognizing novel objects. Nature
Neuroscience, 2, 568--573. Geers, A. E., & Nicholas, J. G. (2013).
Enduring advantages of early cochlear implantation for spoken language
development. Journal of Speech, Language, and Hearing Research, 56,
643--653. Gegenfurtner, K. R., & Kiper, D. C. (2003). Color vision.
Annual Review of Neuroscience, 26, 181--206. Gegenfurtner, K. R., &
Rieger, J. (2000). Sensory and cognitive contributions of color to the
recognition of natural scenes. Current Biology, 10, 805--808. Geiger,
A., Bente, G., Lammers, S., Tepest, R., Roth, D., Bzdok, D., & Vogeley,
K. (2019). Distinct functional roles of the mirror neurons system and
the mentalizinfg system. Neuroimage, 202, 116102. 1--10. Geirhos, R.,
Temme, C. R., Rauber, J., Schütt, H. H., Bethge, M., & Wichmann, F. A.
(2018). Generalisation in humans and deep neural networks. Proceedings
of the 32nd conference on neural information processing systems,
pp. 7549--7561. Geisler, W. S. (2008). Visual perception and statistical
properties of natural scenes. Annual Review of Psychology, 59, 167--192.
Geisler, W. S. (2011). Contributions of ideal observer theory to vision
research. Vision Research, 51, 771--781. Gelbard-Sagiv, H., Mukamel, R.,
Harel, M., Malach, R., & Fried, I. (2008). Internally generated
reactivation of single neurons in human hippocampus during free recall.
Science, 322, 96--101. Gerkin, R. C., & Castro, J. B. (2015). The number
of olfactory stimuli that humans can discriminate is still unknown.
eLife, 4, e08127. Gibson, B. S., & Peterson, M. A. (1994). Does
orientation-independent object recognition precede orientation-dependent
recognition? Evidence from a cueing paradigm. Journal of Experimental
Psychology: Human Perception and Performance, 20, 299--316.

References

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Gibson, J. J. (1950). The perception of the visual world. Boston:
Houghton Mifflin. Gibson, J. J. (1962). Observations on active touch.
Psychological Review, 69, 477--491. Gibson, J. J. (1966). The senses as
perceptual systems. Boston: Houghton Mifflin. Gibson, J. J. (1979). The
ecological approach to visual perception. Boston: Houghton Mifflin.
Gilaie-Dotan, S., Saygin, A. P., Lorenzi, L., Egan, R., Rees, G., &
Behrmann, M. (2013). The role of human ventral visual cortex in motion
perception. Brain, 136, 2784--2798. Gilbert, C. D., & Li, W. (2013).
Top-down influences on visual processing. Nature Reviews Neuroscience,
14, 350--363. Gilchrist, A. (2012). Objective and subjective sides of
perception. In S. Allred & G. Hatfield (Eds.), Visual experience:
Sensation, cognition and constancy. New York: Oxford University Press.
Gilchrist, A. L. (Ed.). (1994). Lightness, brightness, and transparency.
Hillsdale, NJ: Erlbaum. Gilchrist, A., Kossyfidis, C., Bonato, F.,
Agostini, T., Cataliotti, J., Li, X., et al. (1999). An anchoring theory
of lightness perception. Psychological Review, 106, 795--834. Gill, S.
V., Adolph, K. E., & Vereijken, B. (2009). Change in action: How infants
learn to walk down slopes. Developmental Science, 12, 888--902. Glanz,
J. (2000, April 18). Art 1 physics 5 beautiful music. New York Times,
pp. D1--D4. Glasser, D. M., Tsui, J., Pack, C. C., & Tadin, D. (2011)
Perceptual and neural consequences of rapid motion adaptation. PNAS,
108, E1080--E1088. Glickstein, M., & Whitteridge, D. (1987). Tatsuji
Inouye and the mapping of the visual fields on the human cerebral
cortex. Trends in Neurosciences, 10(9), 350--353. Gobbini, M. I., &
Haxby, J. V. (2007). Neural systems for recognition of familiar faces.
Neuropsychologia, 45, 32--41. Goffaux, V., Jacques, C., Mauraux, A.,
Oliva, A., Schynsand, P. G., & Rossion, B. (2005). Diagnostic colours
contribute to the early stages of scene categorization: Behavioural and
neurophysiological evidence. Visual Cognition, 12, 878--892. Golarai,
G., Ghahremani, G., Whitfield-Gabrieli, S., Reiss, A., Eberhardt, J. L.,
Gabrieli, J. E. E., et al. (2007). Differential development of highlevel
cortex correlates with category-specific recognition memory. Nature
Neuroscience, 10, 512--522. Gold, J. E., Rauscher, K. J., & Hum, M.
(2015). A validity study of selfreported daily texting frequency, cell
phone characteristics, and texting styles among young adults. BMC
Research Notes, 8, 120. Gold, T. (1948). Hearing. II. The physical basis
of the action of the cochlea. Proceedings of the Royal Society London B,
135, 492--498. Gold, T. (1989). Historical background to the proposal,
40 years ago, of an active model for cochlear frequency analysis. In J.
P. Wilson & D. T. Kemp (Eds.), Cochlear mechanisms: Structure, function,
and models (pp. 299--305). New York: Plenum Press. Goldstein, A. (1980).
Thrills in response to music and other stimuli. Physiological
Psychology, 8(1), 126--129. Goldstein, E. B., (2001). Pictorial
perception and art. In E. B. Goldstein (Ed.), Blackwell handbook of
perception (pp. 344--378). Oxford, UK: Blackwell. Goldstein, E. B.
(2020). The mind: Consciousness, prediction, and the brain. Cambridge,
MA: MIT Press. Goldstein, E. B., & Brockmole, J. (2019). Sensation &
Perception, 10th ed. Boston: Cengage. Goldstein, E. B., & Fink, S. I.
(1981). Selective attention in vision: Recognition memory for
superimposed line drawings. Journal of Experimental Psychology: Human
Perception and Performance, 7, 954--967. Goldstein, P., Weissman-Fogel,
I., Dumas, G., & Shamay-Tsoory, S. G. (2018). Brain-to-brain coupling
during handholding is associated with pain reduction. Proceedings of the
National Academy of Sciences, 115(11), E2528--E2537.

Golinkoff, R. M., Can, D. D., Soderstrom, M., & Hirsh-Pasek, K. (2015).
(Baby)Talk to me: The social context of infant-directed speech and its
effects on early language acquisition. Current Directions in
Psychological Science, 24(5), 339--344. Goncalves, N. R., & Welchman, A.
E. (2017). "What not" detectors help the brain see in depth. Current
Biology, 27, 1403--1412. Goodale, M. A. (2011). Transforming vision into
action. Vision Research, 51, 1567--1587. Goodale, M. A. (2014). How (and
why) the visual control of action differs from visual perception.
Proceedings of the Royal Society B, 281, 20140337. Goodale, M. A., &
Humphrey, G. K. (1998). The objects of action and perception. Cognition,
67, 181--207. Goodale, M. A., & Humphrey, G. K. (2001). Separate visual
systems for action and perception. In E. B. Goldstein (Ed.), Blackwell
handbook of perception (pp. 311--343). Oxford, UK: Blackwell. Gosselin,
N., Peretz, I., Noulhiane, M., Hasboun, D., Beckett, C., Baulac, M., &
Samson, S. (2005). Impaired recognition of scary music following
unilateral temporal lobe excision. Brain, 128, 628--640. Gosselin, N.,
Samson, S., Adolphs, R., Noulhiane, M., Roy, M., Hasboun, D., Baulac,
M., & Peretz, I. (2006). Emotional responses to unpleasant music
correlates with damage to the parahippocampal cortex. Brain, 129,
2585--2592. Gottfried, J. A. (2010). Central mechanisms of odour object
perception. Nature Reviews Neuroscience, 11, 628--641. Goyal, M., Singh,
S., Sibinga, E. M., Gould, N. F., Royland-Seymour, A., Sharmam, R.,
Berger, Z., Sleicher, D., Maron, D. D., & Shihab, H. M. (2014).
Meditation programs for psychological stress and well-being: A
systematic review and meta-analysis. JAMA Internal Medicine, 174,
357--368. Graham, C. H., Sperling, H. G., Hsia, Y., & Coulson, A. H.
(1961). The determination of some visual functions of a unilaterally
color-blind subject: Methods and results. Journal of Psychology, 51,
3--32. Graham, D. M. (2017). A second shot at sight using a fully
organic retinal prosthesis. Lab Animal, 46(6), 223--224. Grahn, J. A.
(2009). The role of the basal ganglia in beat perception. Annals of the
New York Academy of Sciences, 1169, 35--45. Grahn, J. A., & Rowe, J. B.
(2009). Feeling the beat: Premotor and striatal interactions in
musicians and nonmusicians during beat perception. Journal of
Neuroscience, 29, 7540--7548. Granrud, C. E., Haake, R. J., & Yonas, A.
(1985). Infants' sensitivity to familiar size: The effect of memory on
spatial perception. Perception and Psychophysics, 37, 459--466. Gray,
L., Watt, L., & Blass, E. M. (2000). Skin-to-skin contact is analgesic
in healthy newborns. Pediatrics, 105(1), 1--6. Gregory, R. L. (1966).
Eye and brain. New York: McGraw- Hill. Griffin, D. R. (1944).
Echolocation by blind men and bats. Science, 100, 589--590. Griffiths,
T. D. (2012). Cortical mechanisms for pitch perception. Journal of
Neuroscience, 32, 13333--13334. Griffiths, T. D., & Hall, D. A. (2012).
Mapping pitch representation in neural ensembles with fMRI. Journal of
Neuroscience, 32, 13343--13347. Griffiths, T. D., Warren, J. D., Dean,
J. L., & Howard, D. (2004). "When the feeling's gone": A selective loss
of musical emotion. Journal of Neurology, Neurosurgery and Psychiatry,
75, 344--345. Grill-Spector, K. (2003). The neural basis of object
perception. Current Opinion in Neurobiology, 13(2), 159--166.
Grill-Spector, K. (2009). Object perception: Physiology. Encyclopedia of
Perception. Sage Publications. Grill-Spector, K., & Weiner, K. S.
(2014). The functional architecture of the ventral temporal cortex and
its role in categorization. Nature Reviews Neuroscience, 15, 536--548.
References

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

453

Grill-Spector, K., Golarai, G., & Gabrieli, J. (2008). Developmental
neuroimaging of the human ventral visual cortex. Trends in Cognitive
Sciences, 12, 152--162. Grill-Spector, K., Knouf, N., & Kanwisher, N.
(2004). The fusiform face area subserves face perception, not generic
within-category identification. Nature Neuroscience, 7, 555--562.
Grosbras, M. H., Beaton, S., & Eickhoff, S. B. (2012). Brain regions
involved in human movement perception: A quantitative voxel-based
meta-analysis. Human Brain Mapping, 33, 431--454. Gross, C. G. (1972).
Visual functions of inferotemporal cortex. In R. Jung (Ed.), Handbook of
sensory physiology (Vol. 7, Part 3, pp. 451--482). Berlin: Springer.
Gross, C. G. (2002). Genealogy of the "grandmother cell". The
Neuroscientist, 8(5), 512--518. Gross, C. G. (2008). Single neuron
studies of inferior temporal cortex. Neuropsychologia, 46, 841--852.
Gross, C. G., Bender, D. B., & Rocha-Miranda, C. E. (1969). Visual
receptive fields of neurons in inferotemporal cortex of the monkey.
Science, 166, 1303--1306. Gross, C. G., Rocha-Miranda, C. E., & Bender,
D. B. (1972). Visual properties of neurons in inferotemporal cortex of
the macaque. Journal of Neurophysiology, 5, 96--111. Grossman, E. D., &
Blake, R. (2001). Brain activity evoked by inverted and imagined
biological motion. Vision Research, 41, 1475--1482. Grossman, E. D., &
Blake, R. (2002). Brain areas active during visual perception of
biological motion. Neuron, 56, 1167--1175. Grossman, E. D., Batelli, L.,
& Pascual-Leone, A. (2005). Repetitive TMS over posterior STS disrupts
perception of biological motion. Vision Research, 45, 2847--2853.
Grossman, E. D., Donnelly, M., Price, R., Pickens, D., Morgan, V.,
Neighbor, G., et al. (2000). Brain areas involved in perception of
biological motion. Journal of Cognitive Neuroscience, 12, 711--720.
Grothe, R., Pecka, M., & McAlpine, D. (2010). Mechanisms of sound
localization in mammals. Physiological Review, 90, 983--1012. Gulick, W.
L., Gescheider, G. A., & Frisina, R. D. (1989). Hearing. New York:
Oxford University Press. Gupta, G., Gross, N., Pastilha, R., & Hurlbert,
A. (2020). The time course of colour constancy by achromatic adjustment
in immersive illumination: What looks white under coloured lights?
bioRiv preprint: https://doi.org/10.1101/2020.03.10.984567. Gurney, H.
(1831). Memoir of the life of Thomas Young, M.D., F.R.S. London: John &
Arthur Arch. Gwiazda, J., Brill, S., Mohindra, I., & Held, R. (1980).
Preferential looking acuity in infants from two to fifty-eight weeks of
age. American Journal of Optometry and Physiological Optics, 57,
428--432. Haber, R. N., & Levin, C. A. (2001). The independence of size
perception and distance perception. Perception & Psychophysics, 63,
1140--1152. Hadad, B.-S., Maurer, D., & Lewis, T. L. (2011). Long
trajectory for the development of sensitivity to global and biological
motion. Developmental Science, 14, 1330--1339. Hafting, T., Fyhn, M.,
Molden, S., Moser, M.-B., & Moser, E. I. (2005). Microstructure of a
spatial map in the entorhinal cortex. Nature, 436, 801--806. Haigney,
D., & Westerman, S. J. (2001). Mobile (cellular) phone use and driving:
A critical review of research methodology. Ergonomics, 44, 132--143.
Hall, D. A., Fussell, C., & Summerfield, A. Q. (2005). Reading fluent
speech from talking faces: Typical brain networks and individual
differences. Journal of Cognitive Neuroscience, 17, 939--953. Hall, M.
J., Bartoshuk, L. M., Cain, W. S., & Stevens, J. C. (1975). PTC taste
blindness and the taste of caffeine. Nature, 253, 442--443. Hallemans,
A., Ortibus, E., Meire, F., & Aerts, P. (2010). Low vision affects
dynamic stability of gait. Gait and Posture, 32, 547--551. Hamer, R. D.,
Nicholas, S. C., Tranchina, D., Lamb, T. D., & Jarvinen, J. L. P.
(2005). Toward a unified model of vertebrate rod phototransduction.
Visual Neuroscience, 22, 417--436. 454

Hamid, S. N., Stankiewicz, B., & Hayhoe, M. (2010). Gaze patterns in
navigation: Encoding information in large-scale environments. Journal of
Vision, 10(12), 1--11. Handford, M. (1997). Where's Waldo? Cambridge,
MA: Candlewick Press. Hansen, T., Olkkonen, M., Walter, S., &
Gegenfurtner, K. R. (2006). Memory modulates color appearance. Nature
Neuroscience, 9, 1367--1368. Harding-Forrester, S., & Feldman, D. E.
(2018). Somatosensory maps. In G. Vallar & H. B. Coslett (Eds.),
Handbook of clinical neurology, Vol. 151. New York: Elsevier. Harmelech,
T., & Malach, R. (2013). Neurocognitive biases and the patterns of
spontaneous correlations in the human cortex. Trends in Cognitive
Sciences, 17(12), 606--615. Harris, J. M., & Rogers, B. J. (1999). Going
against the flow. Trends in Cognitive Sciences, 3, 449--450. Harris, L.,
Atkinson, J., & Braddick, O. (1976). Visual contrast sensitivity of a
6-month-old infant measured by the evoked potential. Nature, 246,
570--571. Hart, B., & Risley, T. R. (1995). Meaningful differences in
the everyday experiences of young American children. Baltimore: Brookes.
Hartline, H. K. (1938). The response of single optic nerve fibers of the
vertebrate eye to illumination of the retina. American Journal of
Physiology, 121, 400--415. Hartline, H. K. (1940). The receptive fields
of optic nerve fibers. American Journal of Physiology, 130, 690--699.
Hartline, H. K., Wagner, H. G., & Ratliff, F. (1956). Inhibition in the
eye of Limulus. Journal of General Physiology, 39, 651--673. Harvey, M.,
& Rossit, S. (2012). Visuospatial neglect in action. Neuropsychologia,
50, 1018--1028. Hasenkamp, W., Wilson-Mendenhall, C. D., Duncan, E., &
Barsalou, L. W. (2012). Mind wandering and attention during focused
meditation: A fine-grained temporal analysis of fluctuating cognitive
states. Neuroimage, 59, 750--760. Haxby, J. V., Gobbini, M. I., Furey,
M. L., Ishai, A., Schouten, J. L., & Pietrini, P. (2001). Distributed
and overlapping representations of faces and objects in ventral temporal
cortex. Science, 293(5539), 2425--2430. Hayhoe, M., & Ballard, C.
(2005). Eye movements in natural behavior. Trends in Cognitive Sciences,
9, 188--194. Heaton, P. (2009). Music---Shelter for the frazzled mind?
The Psychologist, 22(12), 1018--1020. Hecaen, H., & Angelerques, R.
(1962). Agnosia for faces (prosopagnosia). Archives of Neurology, 7,
92--100. Heesen, R. (2015). The Young-(Helmholtz)-Maxwell theory of
color vision. Unpublished manuscript. Carnegie Mellon University,
Pittsburgh, PA. http://philsci-archive.pitt.edu/11279/ Heider, F., &
Simmel, M. (1944). An experimental study of apparent behavior. American
Journal of Psychology, 13, 243--259. Heise, G. A., & Miller, G. A.
(1951). An experimental study of auditory patterns. American Journal of
Psychology, 57, 243--249. Held, R., Birch, E., & Gwiazda, J. (1980).
Stereoacuity of human infants. Proceedings of the National Academy of
Sciences, 77, 5572--5574. Helmholtz, H. von. (1911). Treatise on
physiological optics (J. P. Southall, Ed. & Trans.; 3rd ed., Vols. 2 &
3). Rochester, NY: Optical Society of America. (Original work published
1866) Helmholtz, H. von. (1860). Handbuch der physiologischen Optik
(Vol. 2). Leipzig: Voss. Henderson, J. M. (2017). Gaze control as
prediction. Trends in Cognitive Sciences, 21(1), 15--23. Henderson, J.
M., & Hollingworth, A. (1999). High-level scene perception. Annual
Review of Psychology, 50, 243--271. Henderson, J. M., Shinkareva, S. V.,
Wang, J., Luke, S. G., & Olejarczyk, J. (2013). Predicting cognitive
state from eye movements. PLoS One, 8(5): e64937. Henriksen, S., Tanabe,
S., & Cumming, B. (2016). Disparity processing in primary visual cortex.
Philosophical Transactions of the Royal Society B, 371:20150255, 1--12.

References

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Hering, E. (1878). Zur Lehre vom Lichtsinn. Vienna: Gerold. Hering, E.
(1964). Outlines of a theory of the light sense (L. M. Hurvich & D.
Jameson, Trans.). Cambridge, MA: Harvard University Press. Hershenson,
M. (Ed.). (1989). The moon illusion. Hillsdale, NJ: Erlbaum. Herz, R.
S., & Schooler, J. W. (2002). A naturalistic study of autobiographical
memories evoked by olfactory and visual cues: Testing the Proustian
hypothesis. American Journal of Psychology, 115, 21--32. Herz, R. S.,
Eliassen, J. C., Beland, S. L., & Souza, T. (2004). Neuroimaging
evidence for the emotional potency of odor-evoked memory.
Neuropsychologia, 42, 371--378. Hettinger, T. P., Myers, W. E., & Frank,
M. E. (1990). Role of olfaction in perception of nontraditional "taste"
stimuli. Chemical Senses, 15, 755--760. Heywood, C. A., Cowey, A., &
Newcombe, F. (1991). Chromatic discrimination in a cortically colour
blind observer. European Journal of Neuroscience, 3, 802--812. Hickock,
G. (2009). Eight problems for the mirror neuron theory of action
understanding in monkeys and humans. Journal of Cognitive Neuroscience,
21, 1229--1243. Hickock, G., & Poeppel, D. (2007). The cortical
organization of speech processing. Nature Reviews Neuroscience, 8,
393--401. Hickok, G., & Poeppel, D. (2015). Neural basis of speech
perception. Handbook of Clinical Neurology, 129, 149--160. Hinton, G.,
E., McClelland, J. L., & Rumelhart, D. E. (1986). Distributed
representations. In D. E. Rumelhart & J. L. McClelland (Eds.), Parallel
distributed processing: Explorations in the microstructure of cognition,
volume 1. Cambridge, MA: MIT Press. Hochberg, J. E. (1987). Machines
should not see as people do, but must know how people see. Computer
Vision, Graphics and Image Processing, 39, 221--237. Hodgetts, W. E., &
Liu, R. (2006). Can hockey playoffs harm your hearing? CMAJ, 175,
1541--1542. Hofbauer, R. K., Rainville, P., Duncan, G. H., & Bushnell,
M. C. (2001). Cortical representation of the sensory dimension of pain.
Journal of Neurophysiology, 86, 402--411. Hoff, E. (2013). Interpreting
the early language trajectories of children from low-SES and language
minority homes: Implications for closing achievement gaps. Developmental
Psychology, 49(1), 4--14. Hoffman, H. G., Doctor, J. N., Patterson, D.
R., Carrougher, G. J., & Furness, T. A. III (2000). Virtual reality as
an adjunctive pain control during burn wound care in adolescent
patients. Pain, 85, 305--309. Hoffman, H. G., Patterson, D. R., Seibel,
E., Soltani, M., Jewett-Leahy, L., & Sharar, S. R. (2008). Virtual
reality pain control during burn wound debridement in the hydrotank.
Clinical Journal of Pain, 24, 299--304. Hoffman, T. (2012). The man
whose brain ignores one half of his world. The Guardian, November 23,
2012. Hofman, P. M., Van Riswick, J. G. A., & Van Opstal, A. J. (1998).
Relearning sound localization with new ears. Nature Neuroscience, 1,
417--421 Holland, R. W., Hendriks, M., & Aarts, H. (2005). Smells like
clean spirit. Psychological Science, 16(9), 689--693. Hollins, M., &
Risner, S. R. (2000). Evidence for the duplex theory of texture
perception. Perception & Psychophysics, 62, 695--705. Hollins, M.,
Bensmaia, S. J., & Roy, E. A. (2002). Vibrotaction and texture
perception. Behavioural Brain Research, 135, 51--56. Holway, A. H., &
Boring, E. G. (1941). Determinants of apparent visual size with distance
variant. American Journal of Psychology, 54, 21--37. Honig, H., &
Bouwer, F. L. (2019). Rhythm. In J. Rentfrow & D. Levitin (Eds.),
Foundations of music psychology: Theory and research (pp. 33--69).
Cambridge, Mass: MIT Press. Horn, D. L., Houston, D. M., & Miyamoto, R.
T. (2007). Speech discrimination skills in deaf infants before and after
cochlear implantation. Audiological Medicine, 5, 232--241. Howgate, S.,
& Plack, C. J. (2011). A behavioral measure of the cochlear changes
underlying temporary threshold shifts. Hearing Research, 277, 78--87.

Hsiao, S. S., Johnson, K. O., Twombly, A., & DiCarlo, J. (1996). Form
processing and attention effects in the somatosensory system. In O.
Franzen, R. Johannson, & L. Terenius (Eds.), Somesthesis and the
neurobiology of the somatosensory cortex (pp. 229--247). Basel:
Biorkhauser Verlag. Hsiao, S. S., O'Shaughnessy, D. M., & Johnson, K. O.
(1993). Effects of selective attention on spatial form processing in
monkey primary and secondary somatosensory cortex. Journal of
Neurophysiology, 70, 444--447. Huang, X., Baker, J., & Reddy, R. (2014).
A historical perspective of speech recognition. Communications of the
ACM, 57, 94--103. Hubel, D. H. (1982). Exploration of the primary visual
cortex, 1955--1978. Nature, 299, 515--524. Hubel, D. H., & Wiesel, T. N.
(1959). Receptive fields of single neurons in the cat's striate cortex.
Journal of Physiology, 148, 574--591. Hubel, D. H., & Wiesel, T. N.
(1961). Integrative action in the cat's lateral geniculate body. Journal
of Physiology, 155, 385--398. Hubel, D. H., & Wiesel, T. N. (1965).
Receptive fields and functional architecture in two non-striate visual
areas (18 and 19) of the cat. Journal of Neurophysiology, 28, 229--289.
Hubel, D. H., & Wiesel, T. N. (1965). Receptive fields and functional
architecture in two non-striate visual areas (18 and 19) of the cat.
Journal of Neurophysiology, 28, 229--289. Hubel, D. H., & Wiesel, T. N.
(1970). Cells sensitive to binocular depth in area 18 of the macaque
monkey cortex. Nature, 225, 41--42. Hubel, D. H., Wiesel, T. N., Yeagle,
E. M., Lafer-Sousa, R., & Conway, B. R. (2015). Binocular stereoscopy in
visual areas V-2, V-3, and V-3a of the macaque monkey. Cerebral Cortex,
25, 959--971. Hughes, M. (1977). A quantitative analysis. In M. Yeston
(Ed.), Readings in Schenker analysis and other approaches
(pp. 114--164). New Haven, CT: Yale University Press. Humayun, M. S., de
Juan Jr, E., & Dagnelie, G. (2016). The bionic eye: A quarter century of
retinal prosthesis research and development. Ophthalmology, 123(10),
S89--S97. Hummel, T., Delwihe, J. F., Schmidt, C., & Huttenbrink, K.-B.
(2003). Effects of the form of glasses on the perception of wine
flavors: A study in untrained subjects. Appetite, 41, 197--202.
Humphrey, A. L., & Saul, A. B. (1994). The temporal transformation of
retinal signals in the lateral geniculate nucleus of the cat:
Implications for cortical function. In D. Minciacchi, M. Molinari, G.
Macchi, & E. G. Jones (Eds.), Thalamic networks for relay and modulation
(pp. 81--89). New York: Pergamon Press. Humphreys, G. W., & Riddoch, M.
J. (2001). Detection by action: Neuropsychological evidence for
action-defined templates in search. Nature Neuroscience, 4, 84--88.
Huron, D. (2006). Sweet anticipation: Music and the psychology of
expectation. Cambridge, MA: MIT Press. Huron, D., & Margulis, E. H.
(2010). Musical expectancy and thrills. In P. N. Juslin & J. Sloboda
(Eds.), Handbook of music and emotion: Theory, research, applications
(pp. 575--604). New York: Oxford University Press. Hurvich, L. M., &
Jameson, D. (1957). An opponent-process theory of color vision.
Psychological Review, 64, 384--404. Huth, A. G., De Heer, W. A.,
Griffiths, T. L., Theunissen, F. E., & Gallant, J. L. (2016). Natural
speech reveals the semantic maps that tile human cerebral cortex.
Nature, 532(7600), 453--458. Huth, A. G., Nishimoto, S., Vo, A. T., &
Gallant, J. L. (2012). A continuous semantic space describes the
representation of thousands of objects and action categories across the
human brain. Neuron, 76, 1210--1224. Hyvärinin, J., & Poranen, A.
(1978). Movement-sensitive and direction and orientation-selective
cutaneous receptive fields in the hand area of the postcentral gyrus in
monkeys. Journal of Physiology, 283, 523--537.

References

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

455

Iacoboni, M., Molnar-Szakacs, I., Gallese, V., Buccino, G., Mazziotta,
J. C., & Rizzolatti, G. (2005). Grasping the intentions of others with
one's own mirror neuron system. PLoS Biology, 3, 529--535. Iannetti, G.
D., Salomons, T. V., Moayedi, M., Mouraux, A., & Davis, K. D. (2013).
Beyond metaphor: Contrasting mechanisms of social and physical pain.
Trends in Cognitive Sciences, 17, 371--378. Ilg, U. J. (2008). The role
of areas MT and MST in coding of visual motion underlying the execution
of smooth pursuit. Vision Research, 48, 2062--2069. Ishai, A., Pessoa,
L., Bikle, P. C., & Ungerleider, L. G. (2004). Repetition suppression of
faces is modulated by emotion. Proceedings of the National Academy of
Sciences USA, 101, 9827--9832. Ishai, A., Ungerleider, L. G., Martin,
A., & Haxby, J. V. (2000). The representation of objects in the human
occipital and temporal cortex. Journal of Cognitive Neuroscience, 12,
35--51. Ishai, A., Ungerleider, L. G., Martin, A., Schouten, J. L., &
Haxby, J. V. (1999). Distributed representation of objects in the human
ventral visual pathway. Proceedings of the National Academy of Sciences
USA, 96, 9379--9384. Ittelson, W. H. (1952). The Ames demonstrations in
perception. Princeton, NJ: Princeton University Press. Itti, L., & Koch,
C. (2000). A saliency-based search mechanism for overt and covert shifts
of visual attention. Vision Research, 40, 1489--1506. Iversen, J. R., &
Patel, A. D. (2008). Perception of rhythmic grouping depends on auditory
experience. Journal of the Acoustical Society of America, 124a,
2263--2271. Iversen, J. R., Repp, B. H., & Patel, A. (2009). Top-down
control of rhythm perception modulates early auditory responses. Annals
of the New York Academy of Sciences, 1169, 58--73. Iwamura, Y. (1998).
Representation of tactile functions in the somatosensory cortex. In J.
W. Morley (Ed.), Neural aspects of tactile sensation (pp. 195--238). New
York: Elsevier Science.

Jensen, T. S., & Nikolajsen, L. (1999). Phantom pain and other phenomena
after amputation. In P. D. Wall & R. Melzak (Eds.), Textbook of pain
(pp. 799--814). New York: Churchill Livingstone. Jiang, W., Liu, H.,
Zeng, L., Liao, J., Shen, H., Luo, A., ... & Wang, W. (2015). Decoding
the processing of lying using functional connectivity MRI. Behavioral
and Brain Functions, 11(1), 1. Joffily, L., Ungierowicz, A., David, A.
G., et al. (2020). The close relationship between sudden loss of smell
and COVID-19. Brizilian Journal of Otorhinolaryngology, 86(5), 632-638.
Johansson, G. (1973). Visual perception of biological motion and a model
for its analysis. Perception & Psychophysics, 14, 195--204. Johansson,
G. (1975). Visual motion perception. Scientific American, 232, 76--89.
Johnson, B. A., & Leon, M. (2007). Chemotopic odorant coding in a
mammalian olfactory system. Journal of Comparative Neurology, 503,
1--34. Johnson, B. A., Ong., J., & Michael, L. (2010). Glomerular
activity patterns evoked by natural odor objects in the rat olfactory
bulb and related to patterns evoked by major odorant components. Journal
of Comparative Neurology, 518, 1542--1555. Johnson, E. N., Hawken, M.
J., & Shapley, R. (2008). The orientation selectivity of
color-responsive neurons in macaque V1. Journal of Neuroscience, 28,
8096--8106. Johnson, K. O. (2002). Neural basis of haptic perception. In
H. Pashler & S. Yantis (Eds.), Steven's handbook of experimental
psychology (3rd ed.): Vol. 1. Sensation and perception (pp. 537--583).
New York: Wiley. Jouen, F., Lepecq, J-C., Gapenne, O., & Bertenthal, B.
(2000). Optic flow sensitivity in neonates. Infant Behavior &
Development, 23, 271--284. Julesz, B. (1971). Foundations of cyclopean
perception. Chicago: University of Chicago Press. Julian, J. B.,
Keinath, A. T., Marchette, S. A., & Epstein, R. A. (2018). The
neurocognitive basis of spatial reorientation. Current Biology, 28,
R1059--R1073.

Jackendoff, R. (2009). Parallels and nonparallels between language and
music. Music Perception, 26(3), 195--204. Jackson, J. H. (1870). A study
of convulsions. Transactions of St. Andrews Medical Graduate
Association, III, 8--36. Jacobs, J., Weidman, C. T., Miller, J. F.,
Solway, A., Burke, J. F., Wei, X.-X., et al. (2013). Direct recordings
of grid-like neuronal activity in human spatial navigation. Nature
Neuroscience, 9, 1188--1190. Jacobson, A., & Gilchrist, A. (1988). The
ratio principle holds over a million-to-one range of illumination.
Perception and Psychophysics, 43, 1--6. Jaeger, S. R., McRae, J. F.,
Bava, C. M., Beresford, M. K., Hunter, D., Jia, Y., et al. (2013). A
Mendelian trait for olfactory sensitivity affects odor experience and
food selection. Current Biology, 22, 1601--1605. James, W. (1981). The
principles of psychology (Rev. ed.). Cambridge, MA: Harvard University
Press. (Original work published 1890) Janata, P., Tomic, S. T., &
Haberman, J. M. (2011). Sensorimotor coupling in music and the
psychology of the groove. Journal of Experimental Psychology: General,
14, 54--75. Janata, P., Tomic, S. T., & Rakowski, S. K. (2007).
Characterization of music-evoked autobiographical memories. Memory,
15(8), 845--860. Janzen, G. (2006). Memory for object location and route
direction in virtual large scale space. Quarterly Journal of
Experimental Psychology, 59, 493--508. Janzen, G., & van Turennout, M.
(2004). Selective neural representation of objects relevant for
navigation. Nature Neuroscience, 7, 673--677. Janzen, G., Janzen, C., &
van Turennout, M. (2008). Memory consolidation of landmarks in good
navigators. Hippocampus, 18, 40--47. Jeffress, L. A. (1948). A place
theory of sound localization. Journal of Comparative and Physiological
Psychology, 41, 35--39. Jenkins, W. M., & Merzenich, M. M. (1987).
Reorganization of neocortical representations after brain injury: A
neurophysiological model of the bases of recovery from stroke. Progress
in Brain Research, 71, 249--266.

Kaiser, A., Schenck, W., & Moller, R. (2013). Solving the correspondence
problem in stereo vision by internal simulation. Adaptive Behavior, 21,
239--250. Kamitani, Y., & Tong, F. (2005). Decoding the visual and
subjective contents of the human brain. Nature Neuroscience, 8,
679--685. Kamps, F. S., Hendrix, C. L., Brennan, P. A., & Dilks, D. D.
(2020). Connectivity at the origins of domain specificity in the
cortical face and place networks. Proceedings of the National Academy of
Sciences. 10.1073/ pnas.1911359117. Kandel, E. R., & Jessell, T. M.
(1991). Touch. In E. R. Kandel, J. H. Schwartz, & T. M. Jessell (Eds.),
Principles of neural science (3rd ed., pp. 367--384). New York:
Elsevier. Kandel, F. I., Rotter, A., & Lappe, M. (2009). Driving is
smoother and more stable when using the tangent point. Journal of
Vision, 9(11), 1--11. Kanizsa, G., & Gerbino, W. (1976). Convexity and
symmetry in figureground organization. In M. Henle (Ed.), Vision and
artifact (pp. 25--32). New York: Springer. Kanwisher, N. (2003). The
ventral visual object pathway in humans: Evidence from fMRI. In L. M.
Chalupa & J. S. Werner (Eds.), The visual neurosciences
(pp. 1179--1190). Cambridge, MA: MIT Press. Kanwisher, N. (2010).
Functional specificity in the human brain: A window into the functional
architecture of the mind. Proceedings of the National Academy of
Sciences, 107(25), 11163--11170. Kanwisher, N., McDermott, J., & Chun,
M. M. (1997). The fusiform face area: A module in human extrastriate
cortex specialized for face perception. Journal of Neuroscience, 17,
4302--4311. Kapadia, M. K., Westheimer, G., & Gilbert, C. D. (2000).
Spatial distribution of contextual interactions in primary visual cortex
and in visual perception. Journal of Neurophsiology, 84, 2048--2062.
Kaplan, G. (1969). Kinetic disruption of optical texture: The perception
of depth at an edge. Perception and Psychophysics, 6, 193--198.

456

References

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Karpathy, A., & Fei-Fei, L. (2015). Deep visual-semantic alignments for
generating image descriptions. In Proceedings of the IEEE conference on
computer vision and pattern recognition (pp. 3128--3137). Katz, D.
(1989). The world of touch. Trans. L. Kruger. Hillsdale, NJ: Erlbaum.
(Original work published 1925) Katz, J., & Gagliese, L. (1999). Phantom
limb pain: A continuing puzzle. In R. J. Gatchel & D. C. Turk (Eds.),
Psychosocial factors in pain (pp. 284--300). New York: Guilford Press.
Kaufman, L., & Kaufman, J. H. (2000). Explaining the moon illusion.
Proceedings of the National Academy of Sciences, 97(1), 500--505.
Kaufman, L., & Rock, I. (1962a). The moon illusion. Science, 136,
953--961. Kaufman, L., & Rock, I. (1962b). The moon illusion. Scientific
American, 207, 120--132. Kavšek, M., Granrud, C. E., & Yonas, A. (2009).
Infants' responsiveness to pictorial depth cues in preferential-reaching
studies: A meta-analysis. Infant Behavior and Development, 32, 245--253.
Keller, A., Zhuang, H., Chi., Q., Vosshall, L. B., & Matsunami, H.
(2007). Genetic variation in a human odorant receptor alters odour
perception. Nature, 449, 468--472. Kersten, D., Mamassian, P., & Yuille,
A. (2004). Object perception as Bayesian inference. Annual Review of
Psychology, 55, 271--304. Keysers, C., Kaas, J., & Gazzola, V. (2010).
Somatosensation in social perception. Nature Reviews Neuroscience, 11,
417--428. Keysers, C., Wicker, B., Gazzola, V., Anton, J.-L., Fogassi,
L., & Gallese, V. (2004). A touching sight: SII/PV activation cueing the
observation and experience of touch. Neuron, 42, 335--346. Khanna, S.
M., & Leonard, D. G. B. (1982). Basilar membrane tuning in the cat
cochlea. Science, 215, 305--306. Killingsworth, M. A., & Gilbert, D. T.
(2010). A wandering mind is an unhappy mind. Science, 330, 932. Kim, A.,
& Osterhout, L. (2005). The independence of combinatory semantic
processing: Evidence from event-related potentials. Journal of Memory
and Language, 52, 205--255. Kim, U. K., Jorgenson, E., Coon, H.,
Leppert, M., Risch, N., & Drayna, D. (2003). Positional cloning of the
human quantitative trait locus underlying taste sensitivity to
phenylthiocarbamide. Science, 299, 1221--1225. King, A. J., Schnupp, J.
W. H., & Doubell, T. P. (2001). The shape of ears to come: Dynamic
coding of auditory space. Trends in Cognitive Sciences, 5, 261--270.
King, W. L., & Gruber, H. E. (1962). Moon illusion and Emmert's law.
Science, 135, 1125--1126. Kish, D. (2012, April 13). Sound vision: The
consciousness of seeing with sound. Presentation at Toward a Science of
Consciousness, Tucson, AZ. Kisilevsky, B. S., Hains, S. M. J., Brown, C.
A., Lee, C. T., Cowperthwaite, B., Stutzman, S. S., et al. (2009). Fetal
sensitivity to properties of maternal speech and language. Infant
Behavior and Development, 32, 59--71. Kisilevsky, B. S., Hains, S. M.
J., Lee, K., Xie, X., Huang, H., Ye, H. H., et al. (2003). Effects of
experience on fetal voice recognition. Psychological Science, 14,
220--224. Klatzky, R. L., Lederman, S. J., & Metzger, V. A. (1985).
Identifying objects by touch: An "expert system." Perception and
Psychophysics, 37, 299--302. Klatzky, R. L., Lederman, S. J., Hamilton,
C., Grindley, M., & Swendsen, R. H. (2003). Feeling textures through a
probe: Effects of probe and surface geometry and exploratory factors.
Perception & Psychophysics, 65, 613--631. Kleffner, D. A., &
Ramachandran, V. S. (1992). On the perception of shape from shading.
Perception and Psychophysics, 52, 18--36. Klimecki, O. M., Leiberg, S.,
Ricard, M., & Singer, T. (2014). Differential pattern of functional
brain plasticity after compassion and empathy training. SCAN, 9,
873--879. Knill, D. C., & Kersten, D. (1991). Apparent surface curvature
affects lightness perception. Nature, 351, 228--230. Knopoff, L., &
Hutchinson, W. (1983). Entropy as a measure of style: The influence of
sample length. Journal of Music Theory, 27, 75--97.

Koelsch, S. (2005). Neural substrates of processing syntax and semantics
in music. Current Opinion in Neurobiology, 15, 207--212. Koelsch, S.
(2011). Toward a neural basis of music perception: A review and updated
model. Frontiers of Psychology, 2, Article 110. Koelsch, S. (2014).
Brain correlates of music-evoked emotions. Nature Reviews Neuroscience,
15, 170--180. Koelsch, S. (2018). Investigating the neural encoding of
emotion with music. Neuron, 98, 1075--1079. Koelsch, S., Gunter, T.,
Friederici, A. D., & Schroger, E. (2000). Brain indices of music
processing: "Nonmusicians" are musical. Journal of Cognitive
Neuroscience, 12, 520--541. Koelsch, S., Vuust, P., & Friston, K.
(2019). Predictive processes and the peculiar case of music. Trends in
Cognitive Sciences, 23(1), 63--77. Koenecke, A., Nam, A., Lake, E., et
al. (2020). Racial disparities in automatic speech recognition.
Proceedings of the National Academy of Sciences, 117, 7684--7689.
Koffka, K. (1935). Principles of Gestalt psychology. New York: Harcourt
Brace. Kogutek, D. L., Holmes, J. D., Grahn, J. A., Lutz, S. G., & Read,
E. (2016). Active music therapy and physical improvements from
rehabilitation for neurological conditions. Advances in Mind, Body
Medicine, 30(4), 14--22. Kohler, E., Keysers, C., Umilta, M. A.,
Fogassi, L., Gallese, V., & Rizzolatti, G. (2002). Hearing sounds,
understanding actions: Action representation in mirror neurons. Science,
297, 846--848. Kolb, N., & Whishaw, I. Q. (2003). Fundamentals of
neuropsychology (5th ed.). New York: Worth. Konkle, T., & Caramazza, A.
(2013). Tripartite organization of the ventral stream by animacy and
object size. Journal of Neuroscience, 33(25), 10235--10242. Konorski, J.
(1967). Integrative activity of the brain. Chicago: University of
Chicago Press. Koppensteiner, M. (2013). Motion cues that make an
impression. Predicting perceived personality by minimal motion
information. Journal of Experimental Social Psychology, 49, 1137--1143.
Koul, A., Soriano, M., Tversky, B., Becchio, C., & Cavallo, A. (2019).
The kinematics that you do not expect: Integrating prior information and
kinematics to understand intentions. Cognition, 182, 213--219. Kourtzi,
Z., & Kanwisher, N. (2000). Activation of human MT/MST by static images
with implied motion. Journal of Cognitive Neuroscience, 12, 48--55.
Kozinn, A. (2020). Leon Fleisher, 92, dies; spellbinding pianist using
one hand or two. New York Times, August 2, 2020. Kraus, N., &
Chanderasekaran, B. (2010). Music training for the development of
auditory skills. Nature Reviews Neuroscience, 11, 599--605. Kretch, K.
S., & Adolp, K. E. (2013). Cliff or step? Posture-specific learning at
the edge of a drop-off. Child Development, 84, 226--240. Kretch, K. S.,
Franchak, J. M., & Adolph, K. E. (2014). Crawling and walking infants
see the world differently. Child Development, 85, 1503--1518. Krishnan,
A., Woo, C-W., Chang, L. J., Ruzic, L., et al. (2016). Somatic and
vicarious pain are represented by dissociable multivariate brain
patterns. eLife, 5:e15166. Kristjansson, A., & Egeth, H. (2019). How
feature integration theory integrated cognitive psychology,
neurophysiology, and psychophysics. Attention, Perception, &
Psychophysics. https://doi.org/10.3758/ s13414-019-01803-7 Kross, E.,
Berman, M. G., Mischel, W., Smith, E. E., & Wager, T. D. (2011). Social
rejection shares somatosensory representations with physical pain.
Proceedings for the National Academy of Sciences, 108, 6270--6275.
Kruger, L. E. (1970). David Katz: Der Aufbau der Tastwelt \[The world of
touch: A synopsis\]. Perception and Psychophysics, 7, 337--341.
Krumhansl, C. L. (1985). Perceiving tonal structure in music. American
Scientist, 73, 371--378. References

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

457

Krumhansl, C., & Kessler, E. J. (1982). Tracing the dynamic changes in
perceived tonal organization in a spatial representation of musical
keys. Psychological Review, 4, 334--368. Kuffler, S. W. (1953).
Discharge patterns and functional organization of mammalian retina.
Journal of Neurophysiology, 16, 37--68. Kuhl, P. K., & Miller, J. D.
(1978). Speech perception by the chinchilla: Identification functions
for synthetic VOT stimuli. Journal of the Acoustical Society of America,
29, 117--123. Kuhl, P. K., Andruski, J. E., Chistovich, I. A., et
al. (1997). Cross-language analysis of phonetic units in language
addressed to infants. Science, 277, 684--686. Kujawa, S. G., & Liberman,
M. C. (2009). Adding insult to injury: Cochlear nerve degeneration after
"temporary" noise-induced hearing loss. Journal of Neuroscience, 45,
14077--14085. Kunert, R., Willems, R. M., Cassanto, D., Patel, A. D., &
Hagoort, P. (2015). Music and language syntax interact in Broca's area:
An fMRI Study. PLoS One, 10(11), e0141069. Kuznekoff, J. H., Munz, S., &
Titsworth, S. (2015). Mobile phones in the classroom: Examining the
effects of texting, twitter, and message content on student learning.
Communication Education, 64(3), 344--365. Kuznekoff, J. H., & Titsworth,
S. (2013). The impact of mobile phone usage on student learning.
Communication Education, 62, 233--252. LaBarbera, J. D., Izard, C. E.,
Vietze, P., & Parisi, S. A. (1976). Four- and six-month-old infants'
visual responses to joy, anger, and neutral expressions. Child
Development, 47, 535--538. Lafer-Sousa, R., Conway, B. R., & Kanwisher,
N. G. (2016). Color-biased regions of the ventral visual pathway lie
between face- and placeselective regions in humans, as in macaques.
Journal of Neuorscience, 36(5), 1682--1697. Lafer-Sousa, R., Hermann, K.
L., & Conway, B. (2015). Striking individual differences in color
perception uncovered by "the dress" photograph. Current Biology, 25,
R523--R548. Lamble, D., Kauranen, T., Laakso, M., & Summala, H. (1999).
Cognitive load and detection thresholds in car following situations:
Safety implications for using mobile (cellular) telephones while
driving. Accident Analysis and Prevention, 31, 617--623. Lamm, C.,
Batson, C. D., & Decdety, J. (2007). The neural substrate of human
empathy: Effects of perspective-taking and cognitive appraisal. Journal
of Cognitive Neuroscience, 19, 42--58. Land, E. H. (1983). Recent
advances in retinex theory and some implications for cortical
computations: Color vision and the natural image. Proceedings of the
National Academy of Sciences, USA, 80, 5163--5169. Land, E. H. (1986).
Recent advances in retinex theory. Vision Research, 26, 7--21. Land, E.
H., & McCann, J. J. (1971). Lightness and retinex theory. Journal of the
Optical Society of America, 61, 1--11. Land, M. F., & Hayhoe, M. (2001).
In what ways do eye movements contribute to everyday activities? Vision
Research, 41, 3559--3565. Land, M. F., & Horwood, J. (1995). Which parts
of the road guide steering? Nature, 377, 339--340. Land, M. F., & Lee,
D. N. (1994). Where we look when we steer. Nature, 369, 742--744. Land,
M., Mennie, N., & Rusted, J. (1999). The roles of vision and eye
movements in the control of activities of daily living. Perception, 28,
1311--1328. Lane, H. (1965). The motor theory of speech perception: A
critical review. Psychological Review, 72(4), 275--309. Larsen, A.,
Madsen, K. H., Lund, T. E., & Bundesen, C. (2006). Images of illusory
motion in primary visual cortex. Journal of Cognitive Neuroscience, 18,
1174--1180. Larson, T. (2010). The saddest music ever written: The story
of Samuel Barber's Adagio for Strings. New York: Pegasus Books. Larsson,
M., & Willander, J. (2009). Autobiographical odor memory. Annals of the
New York Academy of Sciences, 1170, 318--323. 458

Laska, M. (2017). Human and animal olfactory capabilities compared. In
A. Buettner (Ed.), Springer handbook of odor (pp. 678--689). New York:
Springer. Lawless, H. (1980). A comparison of different methods for
assessing sensitivity to the taste of phenylthiocarbamide PTC. Chemical
Senses, 5, 247--256. Lawless, H. (2001). Taste. In E. B. Goldstein
(Ed.), Blackwell handbook of perception (pp. 601--635). Oxford, UK:
Blackwell. Lederman, S. J., & Klatzky, R. L. (1987). Hand movements: A
window into haptic object recognition. Cognitive Psychology, 19,
342--368. Lederman, S. J., & Klatzky, R. L. (1990). Haptic
classification of common objects: Knowledge-driven exploration.
Cognitive Psychology, 22, 421--459. Lee, D. N., & Aronson, E. (1974).
Visual proprioceptive control of standing in human infants. Perception
and Psychophysics, 15, 529--532. LeGrand, Y. (1957). Light, color and
vision. London: Chapman & Hall. Lemon, R. (2015). Is the mirror cracked?
Brain, 138, 2109--2111. Lerdahl, F., & Jackedoff, R. (1983). A
generative theory of tonal music. Cambridge, MA: MIT Press. Levitin, D.
J. (2013). Neural correlates of musical behaviors: A brief overview.
Music Therapy Perspectives, 31, 15--24. Levitin, D. J., & Tirovolas, A.
K. (2009). Current advances in the cognitive neuroscience of music.
Annals New York Academy of Sciences, 1156, 211--231. Levitin, D. J.,
Grahn, J. A., & London, J. (2018). The psychology of music: Rhythm and
movement. Annual Review of Psychology, 69, 51--75. Lewis, E. R., Zeevi,
Y. Y., & Werblin, F. S. (1969). Scanning electron microscopy of
vertebrate visual receptors. Brain Research, 15, 559--562. Li, L.,
Sweet, B. T., & Stone, L. S. (2006). Humans can perceive heading without
visual path information. Journal of Vision, 6, 874--881. Li, P., Prieto,
L., Mery, D., & Flynn, P. (2018). Face recognition in low quality
images: A survey. arXiv preprint arXiv:1805.11519. Li, X., Li, W., Wang,
H., Cao, J., Maehashi, K., Huang, L., et al. (2005). Pseudogenization of
a sweet-receptor gene accounts for cats' indifference toward sugar. PLoS
Genetics, 1(1), e3. Liang, C. E. (2016). Here's why "baby talk" is good
for your baby. theconversation.com. November 16, 2013. Liberman, A. M.,
Cooper, F. S., Harris, K. S., & MacNeilage, P. F. (1963). A motor theory
of speech perception. Proceedings of the Symposium on Speech
Communication Seminar, Royal Institute of Technology, Stockholm, Paper
D3, Volume II. Liberman, A. M., Cooper, F. S., Shankweiler, D. P., &
Studdert-Kennedy, M. (1967). Perception of the speech code.
Psychological Review, 74, 431--461. Liberman, A. M., & Mattingly, I. G.
(1989). A specialization for speech perception. Science, 243, 489--494.
Liberman, M. C., & Dodds, L. W. (1984). Single-neuron labeling and
chronic cochlear pathology: III. Stereocilia damage and alterations of
threshold tuning curves. Hearing Research, 16, 55--74. Lieber, J. D., &
Bensmaia, S. J. (2019). High-dimensional representation of texture in
somatosensory cortex of primates. Proceedings of the National Academy of
Sciences, 116(8), 3268--3277. Lindsay, P. H., & Norman, D. A. (1977).
Human information processing (2nd ed.). New York: Academic Press.
Linhares, J. M. M., Pinto, P. D., & Nascimento, S. M. C. (2008). The
number of discernible colors in natural scenes. Journal of the Optical
Society of America A, 2918--2924. Lister-Landman, K. M., Domoff, S. E.,
& Dubow, E. F. (2015). The role of compulsive texting in adolescents'
academic functioning. Psychology of Popular Media Culture, 6(4),
311--325. Litovsky, R. Y. (2012). Spatial release from masking.
Acoustics Today, 8(2), 18--25. Liu, L., Ouyang, W., Wang, X., Fieguth,
P., Chen, J., Liu, X., & Pietikäinen, M. (2019). Deep learning for
generic object detection: A survey. International Journal of Computer
Vision, 1--58.

References

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Liu, T., Abrams, J., & Carrasco, M. (2009). Voluntary attention enhances
contrast appearance. Psychological Science, 20, 354--362. Loken, L. S.,
Wessberg, J., Morrison, I., McGlone, F., & Olausson, H. (2009). Coding
of pleasant touch by unmyelinated afferents in humans. Nature
Neuroscience, 12(5), 547--548. Loomis, J. M., & Philbeck, J. W. (2008).
Measuring spatial perception with spatial updating and action. In R. L.
Klatzky, B. MacWhinney, & M. Behrmann (Eds.), Embodiment, ego-space, and
action (pp. 1--43). New York: Taylor and Francis. Loomis, J. M.,
DaSilva, J. A., Fujita, N., & Fulusima, S. S. (1992). Visual space
perception and visually directed action. Journal of Experimental
Psychology: Human Perception and Performance, 18, 906--921. Lopez-Sola,
M., Geuter, S., Koban, L., Coan, J. A., & Wager, T. D. (2019). Brain
mechanisms of social touch-induced analgesia in females. Pain, 160,
2072--2085. Lord, S. R., & Menz, H. B. (2000). Visual contributions to
postural stability in older adults. Gerontology, 46, 306--310. Lorteije,
J. A. M., Kenemans, J. L., Jellema, T., van der Lubbe, R. H. J., de
Heer, F., & van Wezel, R. J. A. (2006). Delayed response to animate
implied motion in human motion processing areas. Journal of Cognitive
Neuroscience, 18, 158--168. Lotto, A. J., Hickok, G. S., & Holt, L. L.
(2009). Reflections on mirror neurons and speech perception. Trends in
Cognitive Sciences, 13, 110--114. Lowenstein, W. R. (1960). Biological
transducers. Scientific American, 203, 98--108. Ludington-Hoe, S. M., &
Hosseini, R. B. (2005). Skin-to-skin contact analgesia for preterm
infant heel stick. AACN Clinical Issues, 16(3), 373--387. Lundy, R. F.,
Jr., & Contreras, R. J. (1999). Gustatory neuron types in rat geniculate
ganglion. Journal of Neurophysiology, 82, 2970--2988. Lyall, V., Heck,
G. L., Phan, T.-H. T., Mummalaneni, S., Malik, S. A., Vinnikova, A. K.,
et al. (2005). Ethanol modulates the VR-1 variant amiloride-insensitive
salt taste receptor: I. Effect on TRC volume and Na1 flux. Journal of
General Physiology, 125, 569--585. Lyall, V., Heck, G. L., Vinnikova, A.
K., Ghosh, S., Phan, T.-H. T., Alam, R. I., et al. (2004). The mammalian
amiloride-insensitive nonspecific salt taste receptor is a vanilloid
receptor-1 variant. Journal of Physiology, 558, 147--159. Macherey, O.,
& Carlyon, R. P. (2014). Cochlear implants. Current Biology, 24(18),
R878--R884. Mack, A., & Rock, I. (1998). Inattentional blindness.
Cambridge, MA: MIT Press. Macuga, K. L., Beall, A. C., Smith, R. S., &
Loomis, J. M. (2019). Visual control of steering in curve driving.
Journal of Vision, 19(5), 1--12. Madzharov, A., Ye, N., Morrin, M., &
Block, L. (2018). The impact of coffee-like scent on expectations and
performance. Journal of Environmental Psychology, 57, 83--86. Maess, B.,
Koelsch, S., Gunter, T. C., & Friederici, A. D. (2001). Musical syntax
is processed in Broca's area: An MEG study. Nature Neuroscience, 4(5),
540--545. Maguire, E. A., Wollett, K., & Spiers, H. J. (2006). London
taxi drivers and bus drivers: A structural MRI and neuropsychological
analysis. Hippocampus, 16, 1091--1101. Malach, R., Reppas, J. B.,
Benson, R. R., Kwong, K. K., Jiang, H., Kennedy, W. A., ... & Tootell,
R. B. (1995). Object-related activity revealed by functional magnetic
resonance imaging in human occipital cortex. Proceedings of the National
Academy of Sciences, 92(18), 8135--8139. Malhotra, S., & Lomber, S. G.
(2007). Sound localization during homotopic and hererotopic bilateral
cooling deactivation of primary and nonprimary auditory cortical areas
in the cat. Journal of Neurophysiology, 97, 26--43. Malhotra, S.,
Stecker, G. C., Middlebrooks, J. C., & Lomber, S. G. (2008). Sound
localization deficits during reversible deactivation of primary auditory
cortex and/or the dorsal zone. Journal of Neurophysiology, 99,
1628--1642.

Mallik, A., Chanda, M. L., & Levitin, D. J. (2017). Anhedonia to music
and mu-opioids: Evidence from the administration of naltrexone.
Scientific Reports, 7, 41952. Malnic, B., Hirono, J., Sata, T., & Buck,
L. B. (1999). Combinatorial receptor codes for odors. Cell, 96,
713--723. Mamassian, P. (2004). Impossible shadows and the shadow
correspondence problem. Perception, 33, 1279--1290. Mamassian, P.,
Knill, D., & Kersten, D. (1998). The perception of cast shadows. Trends
in Cognitive Sciences, 2, 288--295. Mangione, S., & Nieman, L. Z.
(1997). Cardiac auscultatory skills of internal medicine and family
practice trainees: A comparison of diagnostic proficiency. Journal of
the American Medical Association, 278, 717--722. Margulis, E. H. (2014).
On repeat: How music plays the mind. New York: Oxford University Press.
Maric, Y., & Jacquot, M. (2013). Contribution to understanding
odourcolour associations. Food Quality and Preference, 27, 191--195.
Marino, A. C., & Scholl, B. J. (2005). The role of closure in defining
the "objects" of object-based attention. Perception and Psychophysics,
67, 1140--1149. Marks, W. B., Dobelle, W. H., and Macnichol, E. F.
Jr. (1964). Visual pigments of single primate cones. Science, 143,
1181--1182. Marr, D., & Poggio, T. (1979). A computation theory of human
stereo vision. Proceedings of the Royal Society of London B: Biological
Sciences, 204, 301--328. Martin, A. (2007). The representation of object
concepts in the brain. Annual Review of Psychology, 58, 25--45. Martin,
A., Wiggs, C. L., Ungerleider, L. G., & Haxby, J. V. (1996). Neural
correlates of category-specific knowledge. Nature, 379(6566), 649--652.
Martorell, R., Onis, M., Martines, J., Black, M., Onyango, A., & Dewey,
K. G. (2006). WHO motor development study: Windows of achievement for
six gross motor development milestones. Acta Paediatrica, 95(S450),
86--95. Marx, V., & Nagy, E. (2017). Fetal behavioral responses to the
touch of the mother's abdomen: A frame-by-frame analysis. Infant
Behavior and Development, 14, 83--91. Mather, G., Verstraten, F., &
Anstis, S. (1998). The motion aftereffect: A modern perspective.
Cambridge, MA: MIT Press. Maule, J., & Franklin, A. (2019). Color
categorization in infants, Current Opinion in Behavioral Sciences, 30,
163--168. Maxwell, J. C. (1855). Experiments on colour, as perceived by
the eye, with remarks on colour-blindness. Transactions of the Royal
Society of Edinburgh, 21, 275--278. Mayer, D. L., Beiser, A. S., Warner,
A. F., Pratt, E. M., Raye, K. N., & Lang, J. M. (1995). Monocular acuity
norms for the Teller Acuity Cards between ages one month and four years.
Investigative Ophthalmology and Visual Science, 36, 671--685. McAlpine,
D. (2005). Creating a sense of auditory space. Journal of Physiology,
566, 21--22. McAlpine, D., & Grothe, B. (2003). Sound localization and
delay lines: Do mammals fit the model? Trends in Neurosciences, 26,
347--350. McBurney, D. H. (1969). Effects of adaptation on human taste
function. In C. Pfaffmann (Ed.), Olfaction and taste (pp. 407--419). New
York: Rockefeller University Press. McCarthy, G., Puce, A., Gore, J. C.,
& Allison, T. (1997). Face-specific processing in the human fusiform
gyrus. Journal of Cognitive Neuroscience, 9, 605--610. McCartney, P.
(1970). The long and winding road. Apple Records. McFadden, S. A.
(1987). The binocular depth stereoacuity of the pigeon and its relation
to the anatomical resolving power of the eye. Vision Research, 27,
1967--1980. McFadden, S. A., & Wild, J. M. (1986). Binocular depth
perception in the pigeon. Journal of Experimental Analysis of Behavior,
45, 149--160. McGann, J. P. (2017). Poor human olfaction is a
19th-century myth. Science, 356, 598--602. References

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

459

McGettigan, C., Fulkner, A., Altarelli, I., Obleser, J., Baverstock, H.,
& Scott, S. K. (2012). Speech comprehension aided by multiple
modalities: Behavioural and neural interactions. Neuropsychologia, 50,
762--776. McGurk, H., & MacDonald, T. (1976). Hearing lips and seeing
voices. Nature, 264, 746--748. McIntosh, R. D., & Lashley, G. (2008).
Matching boxes: Familiar size influences action programming.
Neuropsychologica, 46, 2441--2444. McRae, J. F., Jaeger, S. R., Bava, C.
M., Beresford, M. K., Hunter, D., Jia, Y., et al. (2013). Identification
of region associated with variation in sensitivity to food-related odors
in the human genome. Current Biology, 23, 1596--1600. McRoberts, G. W.
(2020). Speech perception. Encyclopedia of Infant and Early Childhood
Development, 2e, Vol 3, 267--277. McRoberts, G. W., McDonough, C., &
Lakusta, L. (2009). The role of verbal repetition in the development of
infant speech preferences from 4 to 14 months of age. Infancy 14(2),
162--194. Mehler, J. (1981). The role of syllables in speech processing:
Infant and adult data. Transactions of the Royal Society of London,
B295, 333--352. Mehr, S. A., Singh, M., Knox, D., et al. (2019).
Universality and diversity in human song. Science, 366, 970--987.
Meister, M. (2015). On the dimensionality of odor space, 4, e07865.
Melcher, D. (2007). Predictive remapping of visual features precedes
saccadic eye movements. Nature Neuroscience, 10, 903--907. Melzack, R.
(1992). Phantom limbs. Scientific American, 266, 121--126. Melzack, R.
(1999). From the gate to the neuromatrix. Pain, Suppl. 6, S121--S126.
Melzack, R., & Wall, P. D. (1965). Pain mechanisms: A new theory.
Science, 150, 971--979. Melzack, R., & Wall, P. D. (1983). The challenge
of pain. New York: Basic Books. Melzack, R., & Wall, P. D. (1988). The
challenge of pain (Rev. ed.). New York: Penguin Books. Melzer, A.,
Shafir, T., & Tsachor, R. P. (2019). How do we recognize emotion from
movement? Specific motor components contribute to recognition of each
emotion. Frontiers of Psychology, 10, 1389. Mennella, J. A., Jagnow, C.
P., & Beauchamp, G. K. (2001). Prenatal and postnatal flavor learning by
human infants. Pediatrics, 107(6), 1--6. Menzel, R., & Backhaus, W.
(1989). Color vision in honey bees: Phenomena and physiological
mechanisms. In D. G. Stavenga & R. C. Hardie (Eds.), Facets of vision
(pp. 281--297). Berlin: Springer-Verlag. Menzel, R., Ventura, D. F.,
Hertel, H., deSouza, J., & Greggers, U. (1986). Spectral sensitivity of
photoreceptors in insect compound eyes: Comparison of species and
methods. Journal of Comparative Physiology, 158A, 165--177. Merchant,
H., Grahn, J., Trainor, L., Rohmeier, M., & Fitch, W. T. (2015). Finding
the beat: A neural perspective across humans and nonhuman primates.
Philosophical Transactions of the Royal Society B, 370, 20140093.
Merigan, W. H., & Maunsell, J. H. R. (1993). How parallel are the
primate visual pathways? Annual Review of Neuroscience, 16, 369--402.
Merskey, H. (1991). The definition of pain. European Journal of
Psychiatry, 6, 153--159. Merzenich, M. M., Recanzone, G., Jenkins, W.
M., Allard, T. T., & Nudom, R. J. (1988). Cortical representational
plasticity. In P. Rakic and W. Singer (Eds.), Neurobiology of
neurocortex (pp. 42--67). New York: John Wiley. Mesgarani, N., Cheung,
C., Johnson, K., & Chang, E. F. (2014). Phonetic feature encoding in
human superior temporal gyrus. Science, 343, 1006--1010. Meyer, L. B.
(1956). Emotion and meaning in music. Chicago: University of Chicago
Press. Miall, R. C., Christensen, L. O. D., Owen, C., & Stanley, J.
(2007). Disruption of state estimation in the human lateral cerebellum,
PLoS Biology, 5, e316. 460

Micheyl, C., & Oxenham, A. J. (2010). Objective and subjective
psychophysical measures of auditory stream integration and segregation.
Journal of the Association for Research in Otolaryngology, 11, 709--724.
Miller, G. (2000). Evolution of music through sexual selection. In N.
Wallin, B. Merker, & S. Brown (Eds.), The origins of music
(pp. 329--360). Boston: MIT Press. Miller, G. A., & Heise, G. A. (1950).
The trill threshold. Journal of the Acoustical Society of America, 22,
637--683. Miller, G. A., & Isard, S. (1963). Some perceptual
consequences of linguistic rules. Journal of Verbal Learning and Verbal
Behavior, 2, 212--228. Miller, J. D. (1974). Effects of noise on people.
Journal of the Acoustical Society of America, 56, 729--764. Miller, J.,
& Carlson, L. (2011). Selecting landmarks in novel environments.
Psychonomic Bulletin & Review, 18, 184--191. Miller, R. L., Schilling,
J. R., Franck, K. R., & Young, E. D. (1997). Effects of acoustic trauma
on the representation of the vowel /e/ in cat auditory nerve fibers.
Journal of the Acoustical Society of America, 101, 3602--3616. Milner,
A. D., & Goodale, M. A. (1995). The visual brain in action. New York:
Oxford University Press. Milner, A. D., & Goodale, M. A. (2006). The
visual brain in action. New York: Oxford University Press. Miner, A. S.,
Haque, A., Fries, J. A., et al. (2020). Assessing the accuracy of
automatic speech recognition for psychotherapy. Digital Medicine, 3(82),
1--8. Minini, L., Parker, A. J., & Bridge, H. (2010). Neural modulation
by binocular disparity greatest in human dorsal visual stream. Journal
of Neurophysiology, 104, 169--178. Mishkin, M., Ungerleider, L. G., &
Macko, K. A. (1983). Object vision and spatial vision: Two central
pathways. Trends in Neuroscience, 6, 414--417. Mitchell, M. (2019).
Artificial intelligence: A guide for thinking humans. London: Penguin
UK. Mizokami, U. (2019). Three-dimensions. Stimuli and environment for
studies of color constancy. Current Opinion in Behavioral Sciences, 30,
217--222. Mizokami, Y., & Yaguchi, H. (2014). Color constancy influenced
by unnatural spatial structure. Journal of the Optical Society of
America, 31(4), A179--A185. Molenberghs, P., Hayward, L., Mattingley, J.
B., & Cunnington, R. (2012). Activation patterns during action
observation are modulated by context in mirror system areas. NeuroImage,
59, 608--615. Moller, A. R. (2006). Hearing: Anatomy, physiology, and
disorders of the auditory system (2nd ed.). San Diego: Academic Press.
Mollon, J. D. (1989). "Tho' she kneel'd in that place where they
grew..." Journal of Experimental Biology, 146, 21--38. Mollon, J. D.
(1997). "Tho she kneel'd in that place where they grew..." The uses and
origins of primate colour visual information. In A. Byrne & D. R.
Hilbert (Eds.), Readings on color: Vol. 2. The science of color
(pp. 379--396). Cambridge, MA: MIT Press. Mollon, J. D. (2003).
Introduction: Thomas Young and the trichromatic theory of colour vision.
In J. D. Mollon, J. Pokorny, & K. Knoblauch (Eds.), Normal and defective
color vision. Oxford, UK: Oxford University Press. Mon-Williams, M., &
Tresilian, J. R. (1999). Some recent studies on the extraretinal
contribution to distance perception. Perception, 28, 167--181. Mondloch,
C. J., Dobson, K. S., Parsons, J., & Maurer, D. (2004). Why 8-year-olds
cannot tell the difference between Steve Martin and Paul Newman: Factors
contributing to the slow development of sensitivity to the spacing of
facial features. Journal of Experimental Child Psychology, 89, 159--181.
Mondloch, C. J., Geldart, S., Maurer, D., & LeGrand, R. (2003).
Developmental changes in face processing skills. Journal of Experimental
Child Psychology, 86, 67--84.

References

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Montag, J. L., Jones, M. N., & Smith, L. B. (2018). Quantity and
diversity: Simulating early word learning environments. Cognitive
Science, 42, 375--412. Montagna, B., Pestilli, F., & Carrasco, M.
(2009). Attention trades off spatial acuity. Vision Research, 49,
735--745. Montagna, W., & Parakkal, P. F. (1974). The structure and
function of skin (3rd ed.). New York: Academic Press. Monzée, J.,
Lamarre, Y., & Smith, A. M. (2003). The effects of digital anesthesia on
force control using a precision grip. Journal of Neurophysiology, 89,
672--683. Moon, R. J., Cooper, R. P., & Fifer, W. P. (1993).
Two-day-olds prefer their native language. Infant Behavior and
Development, 16, 495--500. Moore, A., & Malinowski, P. (2009).
Meditation, mindfulness, and cognitive flexibility. Consciousness and
Cognition, 18, 176--186. Moore, B. C. J. (1995). Perceptual consequences
of cochlear damage. Oxford, UK: Oxford University Press. Moray, N.
(1959). Attention in dichotic listening: Affective cues and the
influence of instructions. Quarterly Journal of Experimental Psychology,
11(1), 56--60. Mori, K., & Iwanaga, M. (2017). Two types of peak
emotional responses to music: The psychophysiology of chills and tears.
Scientific Reports, 7.46063. Morton, J., & Johnson, M. H. (1991).
CONSPEC and CONLEARN: A two-process theory of infant face recognition.
Psychological Review, 98, 164--181. Moser, E. I., Moser, M.-B., & Roudi,
Y. (2014). Network mechanisms of grid cells. Philosophical Transactions
of the Royal Society B, 369, 20120511. Moser, E. I., Roudi, Y., Witter,
M. P., Kentros, C., Bonhoeffer, T., & Moser, M.-B. (2014). Grid cells
and cortical representation. Nature Reviews Neuroscience, 15, 466--481.
Mountcastle, V. B., & Powell, T. P. S. (1959). Neural mechanisms
subserving cutaneous sensibility, with special reference to the role of
afferent inhibition in sensory perception and discrimination. Bulletin
of the Johns Hopkins Hospital, 105, 201--232. Movshon, J. A., & Newsome,
W. T. (1992). Neural foundations of visual motion perception. Current
Directions in Psychological Science, 1, 35--39. Mozell, M. M., Smith, B.
P., Smith, P. E., Sullivan, R. L., & Swender, P. (1969). Nasal
chemoreception in flavor identification. Archives of Otolaryngology, 90,
131--137. Mueller, K. L., Hoon, M. A., Erlenbach, I., Chandrashekar, J.,
Zuker, C. S., & Ryba, N. J. P. (2005). The receptors and coding logic
for bitter taste. Nature, 434, 225--229. Mukamel, R., Ekstrom, A. D.,
Kaplan, J., Iacoboni, M., & Fried, I. (2010). Single neuron responses in
humans during execution and observation of actions. Current Biology, 20,
750--756. Mullally, S. L., & Maguire, E. A. (2011). A new role for the
parahippocapal cortex in representing space. Journal of Neuroscience,
31, 7441--7449. Murphy, C., & Cain, W. S. (1980). Taste and olfaction:
Independence vs. interaction. Physiology and Behavior, 24, 601--606.
Murphy, K. J., Racicot, C. I., & Goodale, M. A. (1996). The use of
visuomotor cues as a strategy for making perceptual judgements in a
patient with visual form agnosia. Neuropsychology, 10, 396--401. Murphy,
P. K., Rowe, M. L., Ramani, G., & Silverman, R. (2014). Promoting
critical-analytic thinking in children and adolescents at home and in
school. Educational Psychology Review, 26(4), 561--578. Murray, M. M., &
Spierer, L. (2011). Multisensory integration: What you see is where you
hear. Current Biology, 21, R229--R231. Murthy, V. N. (2011). Olfactory
maps in the brain. Annual Review of Neuroscience, 34, 233--258. Myers,
D. G. (2004). Psychology. New York: Worth. Mythbusters. (2007). Episode
71: Pirate special. Program first aired on the Discovery Channel,
January 17, 2007.

Nanez, J. E. Sr. (1988). Perception of impending collision in 3- to
6-week-old human infants. Infant Behavior & Development, 11, 447--463.
Nardini, M., Bedford, R., & Mareschal, D. (2010). Fusion of visual cues
is not mandatory in children. Proceedings of the National Academy of
Sciences, 107, 17041--17046. Nassi, J. J., & Callaway, E. M. (2009).
Parallel processing strategies of the primate visual system. Nature
Reviews Neuroscience, 10, 360--372. Nathans, J., Thomas, D., & Hogness,
D. S. (1986). Molecular genetics of human color vision: The genes
encoding blue, green, and red pigments. Science, 232, 193--202. Natu,
V., & O'Toole, A. J. (2011). The neural processing of familiar and
unfamiliar faces: A review and synopsis. British Journal of Psychology,
102, 726--747. Neff, W. D., Fisher, J. F., Diamond, I. T., & Yela, M.
(1956). Role of the auditory cortex in discrimination requiring
localization of sound in space. Journal of Neurophysiology, 19,
500--512. Neisser, U., & Becklen, R. (1975). Selective looking:
Attending to visually specified events. Cognitive Psychology, 7,
480--494. Newsome, W. T., & Paré, E. B. (1988). A selective impairment
of motion perception following lesions of the middle temporal visual
area (MT). Journal of Neuroscience, 8, 2201--2211. Newsome, W. T.,
Shadlen, M. N., Zohary, E., Britten, K. H., & Movshon, J. A. (1995).
Visual motion: Linking neuronal activity to psychophysical performance.
In M. S. Gazzaniga (Ed.), The cognitive neurosciences (pp. 401--414).
Cambridge, MA: MIT Press. Newton, I. (1704). Optiks. London: Smith and
Walford. Newtson, D., & Engquist, G. (1976). The perceptual organization
of ongoing behavior. Journal of Experimental Psychology: General, 130,
29--58. Nikonov, A. A., Finger, T. E., & Caprio, J. (2005). Beyond the
olfactory bulb: An odotopic map in the forebrain. Proceedings of the
National Academy of Sciences, 102, 18688--18693. Nishimoto, S., Vu, A.
T., Naselaris, T., Benjamini, Y., Yu, B., & Gallant, J. L. (2011).
Reconstructing visual experiences from brain activity evoked by natural
movies. Current Biology, 21(19), 1641--1646. Nityananda, V., Tarawneh,
G., Henriksen, S., Umeton, D., Simmons, A., & Read, J. (2018). A novel
form of stereo vision in the praying mantis. Current Biology, 28,
588--593. Nityananda, V., Tarawneh, G., Rosner, R., Nicolas, J.,
Crichton, S., & Read, J. (2016). Insect stereopsis demonstrated using a
3D insect cinema. Scientific Reports, 6(18718), DOI: 10.1038. Nodal, F.
R., Kacelnik, O., Bajo, V. M., Bizley, J. K., Moore, D. R., & King, A.
J. (2010). Lesions of the auditory cortex impair azimuthal sound
localization and its recalibration in ferrets. Journal of
Neurophysiology, 103, 1209--1225. Norcia, A. M., & Tyler, C. W. (1985).
Spatial frequency sweep VEP: Visual acuity during the first year of
life. Vision Research, 25, 1399--1408. Nordby, K. (1990). Vision in a
complete achromat: A personal account. In R. F. Hess, L. T. Sharpe, & K.
Nordby (Eds.), Night vision (pp. 290--315). Cambridge, UK: Cambridge
University Press. Norman-Haignere, S., Kanwisher, N., & McDermott, J. H.
(2013). Cortical pitch regions in humans respond primarily to resolved
harmonics and are located in specific tonotopic regions of anterior
auditory cortex. Journal of Neuroscience, 33, 19451--19469. Norman, L.
J., & Thaler, L. (2019). Retinotopic-like maps of spatial sound in
primary "visual" cortex of blind human echolocators. Proceedings of the
Royal Society B., 286, 20191910. Noton, D., & Stark, L. W. (1971).
Scanpaths in eye movements during pattern perception. Science, 171,
308--311. Novick, J. M., Trueswell, J. C., & Thomson-Schill, S. L.
(2005). Cognitive control and parsing: Reexamining the role of Broca's
area in sentence comprehension. Cognitive, Affective and Behavioral
Neuroscience, 5, 263--281. References

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

461

Nozaradan, S., Peretz, I., Missal, M., & Mouraux, A. (2011). Tagging the
neuronal entrainment to beat and meter. Journal of Neuroscience, 31(28),
10234--10240. Nunez, V., Shapley, R. M., & Gordon, J. (2018). Cortical
doubleopponent cells in color perception: Perceptual scaling and
chromatic visual evoked potentials. I-Perception, January--February,
1--16. O'Craven, K. M., Downing, P. E., & Kanwisher, N. (1999). fMRI
evidence for objects as the units of attentional selection. Nature, 401,
584--587. O'Doherty, J., Rolls, E. T., Francis, S., Bowtell, R.,
McGlone, F., Kobal, G., et al. (2000). Sensory-specific satiety-related
olfactory activation of the human orbitofrontal cortex. Neuroreport, 11,
893--897. O'Keefe, J., & Dostrovsky, J. (1971). The hippocampus as a
spatial map. Preliminary evidence from unit activity in the
freely-moving rat. Brain Research, 34, 171--175. O'Keefe, J., & Nadel,
L. (1978). The hippocampus as a cognitive map. Oxford, UK: Clarendon
Press. Oatley, K., & Johnson-Laird, P. N. (2014). Cognitive approaches
to emotions. Trends in Cognitive Sciences, 18(3), 134--140. Oberman, L.
M., Hubbard, E. M., McCleery, J. P., Altschuler, E. L., Ramachandran, V.
S., & Pineda, J. (2005). EEG evidence for mirror neuron dysfunction in
autism spectrum disorders. Cognitive Brain Research, 24, 190--198.
Oberman, L. M., Ramachandran, V. S., & Pineda, J. A. (2008). Modulation
of mu suppression in children with autism spectrum disorders in response
to familiar or unfamiliar stimuli: The mirror neuron hypothesis.
Neuropsychologia, 46, 1558--1565. Ocelak, R. (2015). The myth of unique
hues. Topoi, 34, 513--522. Ockelford, A. (2008). Review of D. Huron,
Sweet anticipation: Music and the psychology of expectation. Psychology
of Music, 36(3), 367--382. Olausson, H., Lamarre, Y., Backlund, H., et
al. (2002). Unmyelinated tactile afferents signal touch and project to
insular cortex. Nature Neuroscience, 5(9), 900--904. Oliva, A., &
Schyns, P. G. (2000). Diagnostic colors mediate scene recognition.
Cognitive Psychology, 41, 176--210. Oliva, A., & Torralba, A. (2001).
Modeling the shape of the scene: A holistic representation of the
spatial envelope. International Journal of Computer Vision, 42,
145--175. Oliva, A., & Torralba, A. (2006). Building the gist of a
scene: The role of global image features in recognition. Progress in
Brain Research, 155, 23--36. Oliva, A., & Torralba, A. (2007). The role
of context in object recognition. Trends in Cognitive Sciences, 11,
521--527. Olkkonen, M., Witzel, C., Hansen, T., & Gegenfurtner, K. R.
(2010). Categorical color constancy for real surfaces. Journal of
Vision, 10(9), 1--22. Olshausen, B. A., & Field, D. J. (2004). Sparse
coding of sensory inputs. Current Opinion in Neurobiology, 14, 481--487.
Olsho, L. W., Koch, E. G., Carter, E. A., Halpin, C. F., & Spetner, N.
B. (1988). Pure-tone sensitivity of human infants. Journal of the
Acoustical Society of America, 84, 1316--1324. Olsho, L. W., Koch, E.
G., Halpin, C. F., & Carter, E. A. (1987). An observer-based
psychoacoustic procedure for use with young infants. Developmental
Psychology, 23, 627--640. Olson, C. R., & Freeman, R. D. (1980). Profile
of the sensitive period for monocular deprivation in kittens.
Experimental Brain Research, 39, 17--21. Olson, H. (1967). Music,
physics, and engineering (2nd ed.). New York: Dover. Olson, R. L.,
Hanowski, R. J., Hickman, J. S., & Bocanegra, J. (2009). Driver
distraction in commercial vehicle operations. U.S. Department of
Transportation Report No. FMCSA-RRR-09-042. Orban, G. A., Vandenbussche,
E., & Vogels, R. (1984). Human orientation discrimination tested with
long stimuli. Vision Research, 24, 121--128. Osmanski, B. F., Martin,
C., Montaldo, G., Laniece, P., Pain, F., Tanter, M., & Gurden, H.
(2014). Functional ultrasound imaging reveals different odor-evoked
patterns of vascular activity in the main olfactory bulb and the
anterior piriform cortex. Neuroimage, 95, 176--184. 462

Osterhout, L., McLaughlin, J., & Bersick, M. (1997). Event-related brain
potentials and human language. Trends in Cognitive Sciences, 1,
203--209. Oxenham, A. J. (2013). The perception of musical tones. In D.
Deutsch (Ed.), The psychology of music (3rd ed., pp. 1--33). New York:
Elsevier. Oxenham, A. J., Micheyl, C., Keebler, M. V., Loper, A., &
Santurette, S. (2011). Pitch perception beyond the traditional existence
region of pitch. Proceedings of the National Academy of Sciences, 108,
7629--7634. Pack, C. C., & Born, R. T. (2001). Temporal dynamics of a
neural solution to the aperture problem in visual area MT of macaque
brain. Nature, 409, 1040--1042. Pack, C. C., Livingston, M. S., Duffy,
K. R., & Born, R. T. (2003). Endstopping and the aperture problem:
Two-dimensional motion signals in macaque V1. Neuron, 59, 671--680.
Palmer, C. (1997). Music performance. Annual Review of Psychology, 48,
115--138. Palmer, S. E. (1975). The effects of contextual scenes on the
identification of objects. Memory and Cognition, 3, 519--526. Palmer, S.
E. (1992). Common region: A new principle of perceptual grouping.
Cognitive Psychology, 24, 436--447. Palmer, S. E., & Rock, I. (1994).
Rethinking perceptual organization: The role of uniform connectedness.
Psychonomic Bulletin and Review, 1, 29--55. Panichello, M. F., Cheung,
O. S., & Bar, M. (2013). Predictive feedback and conscious visual
experience. Frontiers in Psychology, 3, 620. Paré, M., Smith, A. M., &
Rice, F. L. (2002). Distribution and terminal arborizations of cutaneous
mechanoreceptors in the glabrous finger pads of the monkey. Journal of
Comparative Neurology, 445, 347--359. Park, W. J., & Tadin, D. (2018).
Motion perception. In J. Wixted (Ed.), Stevens' handbook of experimental
psychology and cognitive neuroscience, 4th ed. New York: Wiley. Parker,
A. J., Smith, J. E. T., & Krug, K. (2016). Neural architectures for
stereo vision. Philosophical Transactions of the Royal Society B, 371,
2015026, 1--14. Parkhurst, D., Law, K., & Niebur, E. (2002). Modeling
the role of salience in the allocation of overt visual attention. Vision
Research, 42, 107--123. Parkin, A. J. (1996). Explorations in cognitive
neuropsychology. Oxford, UK: Blackwell. Parkinson, A. J., Arcaroli, J.,
Staller, S. J., Arndt, P. L., Cosgriff, A., & Ebinger, K. (2002). The
Nucleus 24 Contour cochlear implant system: Adult clinical trial
results. Ear & Hearing, 23(1S), 41S--48S. Parma, V., Ohla, K.,
Veldhuizen, M. G., et al. (2020). More than smell--- COVID-19 is
associated with severe impairment of smell, taste, and chemesthesis.
Chemical Senses, 20, 1--14. Pascalis, O., de Schonen, S., Morton, J.,
Deruelle, C., & Fabre-Grenet, M. (1995). Mother's face recognition by
neonates: A replication and an extension. Infant Behavior and
Development, 18, 79--85. Pasternak, T., & Merigan, E. H. (1994). Motion
perception following lesions of the superior temporal sulcus in the
monkey. Cerebral Cortex, 4, 247--259. Patel, A. D. (2008). Music,
language, and the brain. New York: Oxford University Press. Patel, A.
D., Gibson, E., Ratner, J., Besson, M., & Holcomb, P. J. (1998).
Processing syntactic relations in language and music: An eventrelated
potential study. Journal of Cognitive Neuroscience, 10, 717--733. Patel,
A. D., Iversen, J. R., Wassenaar, M., & Hagoort, P. (2008). Musical
syntax processing in agrammatic Broca's aphasia. Aphasiology, 22(7--8),
776--789. Pawling, R., Cannon, P. R., McGlone, F. P., & Walker, S. C.
(2017). C-tactile afferent stimulating touch carries a positive
affective value. PLoS One, 10.1371. Peacock, G. (1855). Life of Thomas
Young MD, FRS. London: John Murray. Pecka, M., Bran, A., Behrend, O., &
Grothe, B. (2008). Interaural time difference processing in the
mammalian medial superior olive: The role of glycinergic inhibition.
Journal of Neuroscience, 28, 6914--6925.

References

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Pei, Y.-C., Hsiao, S. S., Craig, J. C., & Bensmaia, S. J. (2011). Neural
mechanisms of tactile motion integration in somatosensory cortex.
Neuron, 69, 536--547. Pelchat, M. L., Bykowski, C., Duke, F. F., & Reed,
D. R. (2011). Excretion and perception of a characteristic odor in urine
after asparagus ingestion: A psychophysical and genetic study. Chemical
Senses, 36, 9--17. Pelphrey, K. A., Mitchell, T, V., McKeown, M, J.,
Goldstein, J., Allison, T., & McCarthy, G. (2003). Brain activity evoked
by the perception of human walking: Controlling for meaningful coherent
motion. Journal of Neuroscience, 23, 6819--6825. Pelphrey, K., Morris,
J., Michelich, C., Allison, T., & McCarthy, G. (2005). Functional
anatomy of biological motion perception in posterior temporal cortex: An
fMRI study of eye, mouth and hand movements. Cerebral Cortex, 15,
1866--1876. Penfield, W., & Rasmussen, T. (1950). The cerebral cortex of
man. New York: Macmillan. Peng, J.-H., Tao, Z.-A., & Huang, Z.-W.
(2007). Risk of damage to hearing from personal listening devices in
young adults. Journal of Otolaryngology, 36, 181--185. Peretz, I.
(2006). The nature of music from a biological perspective. Cognition,
100, 1--32. Peretz, I., & Zatorre, R. J. (2005). Brain organization for
music processing. Annual Review of Psychology, 56, 89--114. Peretz, I.,
Vivan, D., Lagrois, M-E., & Armony, J. L. (2015). Neural overlap in
processing music and speech. Philosophical Transactions of the Royal
Society, B370, 20140090. Perez, J. A., Deligianni, F., Ravi, D., & Yang,
G-Z. (2017). Artificial intelligence and robotics. UKRAS.ORG. Perl, E.
R. (2007). Ideas about pain, a historical view. Nature Reviews
Neuroscience, 8, 71--80. Perl, E. R., & Kruger, L. (1996). Nociception
and pain: Evolution of concepts and observations. In L. Kruger (Ed.),
Pain and touch (pp. 180--211). San Diego, CA: Academic Press. Perrett,
D. I., Rolls, E. T., & Caan, W. (1982). Visual neurons responsive to
faces in the monkey temporal cortex. Experimental Brain Research, 7,
329--342. Perrodin, C., Kayser, C., Logothetis, N. K., & Petkov, C. I.
(2011). Voice cells in the primate temporal lobe. Current Biology, 21,
1408--1415. Pessoa, L. (2014). Understanding brain networks and brain
organization. Physics of Life Reviews, 11(3), 400--435. Peterson, M. A.
(1994). Object recognition processes can and do operate before
figure-ground organization. Current Directions in Psychological Science,
3, 105--111. Peterson, M. A. (2001). Object perception. In E. B.
Goldstein (Ed.), Blackwell handbook of perception (pp. 168--203).
Oxford, UK: Blackwell. Peterson, M. A. (2019). Past experience and
meaning affect object detection: A hierarchical Bayesian approach. In
Psychology of Learning and Motivation 70, 223--257. Peterson, M. A.
(2019). Past experience and meaning affect object detection: A
hierarchical Bayesian approach. Knowledge and Vision, 70, 223. Peterson,
M. A., & Kimchi, R. (2013). Perceptual organization. In D. Reisberg
(Ed.) Handbook of cognitive psychology (pp. 9--31). New York: Oxford
University Press. Peterson, M. A., & Salvagio, E. (2008). Inhibitory
competition in figureground perception: Context and convexity. Journal
of Vision, 8(16), 1--13. Pew Research Center. (2019). Mobile fact sheet.
June 12, 2019. Pew Research Center, Washington, DC. Pewinternet.org.
Pfaffmann, C. (1974). Specificity of the sweet receptors of the squirrel
monkey. Chemical Senses, 1, 61--67. Philbeck, J. W., Loomis, J. M., &
Beall, A. C. (1997). Visually perceived location is an invariant in the
control of action. Perception & Psychophysics, 59, 601--612.

Phillips-Silver, J., & Trainor, L. J. (2005). Feeling the beat: Movement
influences infant rhythm perception. Science, 208, 1430.
Phillips-Silver, J., & Trainor, L. J. (2007). Hearing what the body
feels: Auditory encoding of rhythmic movement. Cognition, 105, 533--546.
Phillips, J. R., & Johnson, K. O. (1981). Tactile spatial resolution:
II: Neural representation of bars, edges, and gratings in monkey primary
afferent. Journal of Neurophysiology, 46, 1177--1191. Pinker, S. (1997).
How the mind works. New York: W.W. Norton. Pinker, S. (2010). Mind over
mass media. New York Times, June 10, 2010. Pinna, F. deR, Deusdedit, B.
N., Fornazieri, M. A., & Voegels, R. L. (2020). Olfaction and COVID: The
little we know and what else we need to know. International Journal of
Otorhinolaryngology, 24(3), 386--387. Pinto, J. M., Wroblewski, K. E.,
Kern, D. W., Schumm, L. P., & McClintock, M. K. (2014). Olfactory
dysfunction predicts 5-year mortality in older adults. PL0S One, 9,
Issue 10, e107541. Piqueras-Fiszman, G., Alcaide, J., Roura, E., &
Spence, C. (2012). Is it the plate or is it the food? Assessing the
influence of the color (black or white) and shape of the plate on the
perception of the food placed on it. Food Quality and Preference, 24,
205--208. Pitcher, D., Dilks, D. D., Saxe, R. R., Triantafyllou, C., &
Kanwisher, N. (2011). Differential selectivity for dynamic versus static
information in face-selective cortical regions. Neuroimage, 56(4),
2356--2363. Plack, C. (2014). The Sense of Hearing, 2nd ed. New York:
Psychology Press. Plack, C. J. (2005). The sense of hearing. New York:
Psychology Press. Plack, C. J. (2014). The sense of hearing (2nd ed.).
New York: Psychology Press. Plack, C. J., Barker, D., & Hall, D. A.
(2014). Pitch coding and pitch processing in the human brain. Hearing
Research, 307, 53--64. Plack, C. J., Barker, D., & Prendergast, G.
(2014). Perceptual consequences of "hidden" hearing loss. Trends in
Hearing, 18, 1--11. Plack, C. J., Drga, V., & Lopez-Poveda, E. (2004).
Inferred basilar-membrane response functions for listeners with mild to
moderate sensorineural hearing loss. Journal of the Acoustical Society
of America, 115, 1684--1695. Plassmann, H., O'Doherty, J., Shiv, B., &
Rangel, A. (2008). Marketing actions can modulate neural representations
of experienced pleasantness. Proceedings of the National Academy of
Sciences, 105, 1050--1054. Ploner, M., Lee, M. C., Wiech, K., Bingel,
U., & Tracey, I. (2010). Prestimulus functional connectivity determines
pain perception in humans. Proceedings of the National Academy of
Sciences, 107(1), 355--360. Plug, C., & Ross, H. E. (1994). The natural
moon illusion: A multifactor account. Perception, 23, 321--333. Porter,
J., Craven, B., Khan, R. M., et al. (2007). Mechanisms of scenttracking
in humans. Nature Neuroscience, 10(1), 27--29. Posner, M. I., Nissen, M.
J., & Ogden, W. C. (1978). Attended and unattended processing modes: The
role of set for spatial location. In H. L. Pick & I. J. Saltzman (Eds.),
Modes of perceiving and processing information. Hillsdale, NJ: Erlbaum.
Potter, M. C. (1976). Short-term conceptual memory for pictures. Journal
of Experimental Psychology (Human Learning), 2, 509--522. Pressnitzer,
D., Graves, J., Chambers, C., de Gardelle, V., & Egré, P. (2018).
Auditory perception: Laurel and Yanny together at last. Current Biology,
28, R739--R741. Price, D. D. (2000). Psychological and neural mechanisms
of the affective dimension of pain. Science, 288, 1769--1772.
Prinzmetal, W., Shimamura, A. P., & Mikolinski, M. (2001). The Ponzo
illusion and the perception of orientation. Perception & Psychophysics,
63, 99--114. Proust, M. (1913). Remembrance of Things Past. Vol. 1.
Swann's Way. Paris: Grasset and Gallimard. Proverbio, A. M., Adorni, R.,
& D'Aniello, G. E. (2011). 250 ms to code for action affordance during
observation of manipulable objects. Neuropsychologia, 49, 2711--2717.
Puce, A., Allison, T., Bentin, S., Gore, J. C., & McCarthy, G. (1998).
Temporal cortex activation in humans viewing eye and mouth movements.
Journal of Neuroscience, 18, 2188--2199. References

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

463

Quiroga, R. Q., Reddy, L., Kreiman, G., Koch, C., & Fried, I. (2005).
Invariant visual representation by single neurons in the human brain.
Nature, 435, 1102--1107. Quiroga, R. Q., Reddy, L., Kreiman, G., Koch,
C., & Fried, I. (2008). Sparse but not "grandmother-cell" coding in the
medial temporal lobe. Trends in Cognitive Sciences, 12, 87--91. Rabin,
J., Houser, B., Talbert, C., & Patel, R. (2016). Blue-black or
whitegold? Early stage processing and the color of "The Dress." PLoS
One, DOI:10.1371/journal.pone.0161090. Rabin, R. C. (2021). Some COVID
survivors haunted by loss of smell and taste. New York Times, January 2,
2021. Radwanick, S. (2012). Five years later: A look back at the rise of
the iPhone. comScore, June 29, 2012. Rafel, R. D. (1994). Neglect.
Current Opinion in Neurobiology, 4, 231--236. Rainville, P. (2002).
Brain mechanisms of pain affect and pain modulation. Current Opinion in
Neurobiology, 12, 195--204. Rainville, P., Hofbauer, R. K., Paus, T.,
Duncan, G. H., Bushnell, M. C., & Price, D. D. (1999). Cerebral
mechanisms of hypnotic induction and suggestion. Journal of Cognitive
Neuroscience, 11, 110--125. Ramachandran, V. S. (1987). Interaction
between colour and motion in human vision. Nature, 328, 645--647.
Ramachandran, V. S. (1992, May). Blind spots. Scientific American,
86--91. Ramachandran, V. S., & Hirstein, W. (1998). The perception of
phantom limbs. Brain, 121, 1603--1630. Rao, H. M., Mayo, J. P., &
Sommer, M. A. (2016). Circuits for presaccadic visual remapping. Journal
of Neurophysiology, 116, 2624--2636. Rao, R. P., & Ballard, D. H.
(1999). Predictive coding in the visual cortex: A functional
interpretation of some extra-classical receptive-field effects. Nature
Neuroscience, 2(1), 79--87. Ratliff, F. (1965). Mach bands: Quantitative
studies on neural networks in the retina. San Francisco: Holden-Day.
Ratner, C., & McCarthy, J. (1990). Ecologically relevant stimuli and
color memory. Journal of General Psychology, 117, 369--377. Rauschecker,
J. P. (2011). An expanded role for the dorsal auditory pathway in
sensorimotor control and integration. Hearing Research, 271, 16--25.
Rauschecker, J. P., & Scott, S. K. (2009). Maps and streams in the
auditory cortex: Nonhuman primates illuminate human speech processing.
Nature Neuroscience, 12, 718--724. Rauschecker, J. P., & Tian, B.
(2000). Mechanisms and streams for processing of "what" and "where" in
auditory cortex. Proceedings of the National Academy of Sciences, USA,
97, 11800--11806. Recanzone, G. H. (2000). Spatial processing in the
auditory cortex of the macaque monkey. Proceedings of the National
Academy of Sciences, 97, 11829--11835. Redmon, J., & Farhadi, A. (2018).
Yolov3: An incremental improvement. arXiv preprint arXiv:1804.02767.
Redmon, J., Divvala, S., Girshick, R., & Farhadi, A. (2016). You only
look once: Unified, real-time object detection. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition
(pp. 779--788). Regev, M., Honey, C. J., Simony, E., & Hasson, U.
(2013). Selective and invariant neural responses to spoken and written
narratives. Journal of Neuroscience, 33, 15978--15988. Reichardt, W.
(1961). Autocorrelation, a principle for the evaluation of sensory
information by the central nervous system. In W. A. Rosenblith (Ed.),
Sensory communication (pp. 303--317). New York: MIT Press; Wiley.
Reichardt, W. (1987). Evaluation of optical motion information by
movement detectors. Journal of Comparative Physiology A, 161, 533--547.
Rémy, F., Vayssière, N., Pins, D., Boucart, M., & Fabre-Thorpe, M.
(2014). Incongruent object/context relationships in visual scenes: Where
are they processed in the brain? Brain and Cognition, 84(1), 34--43.
Rennaker, R. L., Chen, C.-F. F., Ruyle, A. M., Sloan, A. M., & Wilson,
D. A. (2007). Spatial and temporal distribution of odorant-evoked
activity in the piriform cortex. Journal of Neuroscience, 27,
1534--1542. 464

Rensink, R. A. (2002). Change detection. Annual Review of Psychology,
53, 245--277. Rensink, R. A., O'Regan, J. K., & Clark, J. J. (1997). To
see or not to see: The need for attention to perceive changes in scenes.
Psychological Science, 8, 368--373. Rentfro, P. J., & Greenberg, D. M.
(2019). The social psychology of music. In P. J. Rentfro and D. J.
Levitin (Eds.), Foundations in Music Psychology. Cambridge, MA: MIT
Press. p. 827--855. Restrepo, D., Doucette, W., Whitesell, J. D.,
McTavish, T. S., & Salcedo, E. (2009). From the top down: Flexible
reading of a fragmented odor map. Trends in Neurosciences, 32, 525--531.
Reybrouck, M., Vuust, P., & Brattio, E. (2018). Music and brain
plasticity: How sounds trigger neurogenerative adaptations. In V. V.
Chaban (Ed.), Neuroplasticity: Insights of neural reorganization
(pp. 85--103). London: Intech Open Publishers. Rhode, W. S. (1971).
Observations of the vibration of the basilar membrane in squirrel
monkeys using the Mössbauer technique. Journal of the Acoustical Society
of America, 49(suppl.), 1218--1231. Rhode, W. S. (1974). Measurement of
vibration of the basilar membrane in the squirrel monkey. Annals of
Otology, Rhinology & Laryngology, 83, 619--625. Rhudy, J. L., Williams,
A. E., McCabe, K. M., Thu, M. A., Nguyen, V., & Rambo, P. (2005).
Affective modulation of nociception at spinal and supraspinal levels.
Psychophysiology, 42, 579--587. Risset, J. C., & Mathews, M. W. (1969).
Analysis of musical instrument tones. Physics Today, 22, 23--30.
Rizzolatti, G., & Sinigaglia, C. (2010). The functional role of the
parietofrontal mirror circuit: Interpretations and misinterpretations.
Nature Reviews Neuroscience, 11, 264--274. Rizzolatti, G., & Sinigaglia,
C. (2016). The mirror mechanism: A basic principle of brain function.
Nature Reviews Neuroscience, 17, 757--765. Rizzolatti, G., Fogassi, L.,
& Gallese, V. (2006, November). Mirrors in the mind. Scientific
American, 295, 54--61. Rizzolatti, G., Forgassi, L., & Gallese, V.
(2000). Cortical mechanisms subserving object grasping and action
recognition: A new view on the cortical motor functions. In M. Gazzaniga
(Ed.), The new cognitive neurosciences (pp. 539--552). Cambridge, MA:
MIT Press. Robbins, J. (2000, July 4). Virtual reality finds a real
place. New York Times. Rocha-Miranda, C. (2011). Personal communication.
Rolfs, M., Jonikatis, D., Deubel, H., & Cavanagh, P. (2011). Predictive
remapping of attention across eye movements. Nature Neuroscience, 14(2),
252--258. Rollman, G. B. (1991). Pain responsiveness. In M. A. Heller &
W. Schiff (Eds.), The psychology of touch (pp. 91--114). Hillsdale, NJ:
Erlbaum. Rolls, E. T. (1981). Responses of amygdaloid neurons in the
primate. In Y. Ben-Ari (Ed.), The amygdaloid complex (pp. 383--393).
Amsterdam: Elsevier. Rolls, E. T., & Baylis, L. L. (1994). Gustatory,
olfactory, and visual convergence within the primate orbitofrontal
cortex. Journal of Neuroscience, 14, 5437--5452. Rolls, E. T., & Tovee,
M. J. (1995). Sparseness of the neuronal representation of stimuli in
the primate temporal visual cortex. Journal of Neurophysiology, 73,
713--726. Rolls, E. T., Critchley, H. D., Verhagen, J. V., & Kadohisa,
M. (2010). The representation of information about taste and odor in the
orbitofrontal cortex. Chemical Perception, 3, 16--33. Roorda, A., &
Williams, D. R. (1999). The arrangement of the three cone classes in the
living human eye. Nature, 397, 520--522. Rosen, L. D., Carrier, L. M., &
Cheever, N. A. (2013). Facebook and texting made me do it: Media-induced
task-switching while studying. Computers in Human Behavior, 29,
948--958. Rosenblatt, F. (1957). The perceptron, a perceiving and
recognizing automaton. Project Para. Cornell Aeronautical Laboratory.
Rosenblatt, F. (1958). The perceptron: A probabilistic model for
information storage and organization in the brain. Psychological Review,
65(6), 386.

References

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Rosenstein, D., & Oster, H. (1988). Differential facial responses to
four basic tastes in newborns. Child Development, 59, 1555--1568. Ross,
M. G., & Nijland, M. J. (1997). Fetal swallowing: Relation to amniotic
fluid regulation. Clinical Obstetrics and Gynecology, 40, 352--365.
Ross, V. (2011). How did researchers manage to read movie clips from the
brain? Discover Newsletter, September 28. https://www
.discovermagazine.com/mind/how-did-researchers-manage-to-read
-movie-clips-from-the-brain Rossato-Bennet, M. (2014). Alive Inside.
Documentary film produced by Music and Memory. Rossel, S. (1983).
Binocular stereopsis in an insect. Nature, 302, 821--822. Rowe, M.
(2012). A longitudinal investigation of the role of quantity and quality
of child-directed speech in vocabulary development. Child Development,
83(5), 1762--1774. Rowe, M. J., Turman, A. A., Murray, G. M., & Zhang,
H. Q. (1996). Parallel processing in somatosensory areas I and II of the
cerebral cortex. In O. Franzen, R. Johansson, & L. Terenius (Eds.),
Somesthesis and the neurobiology of the somatosensory cortex
(pp. 197--212). Basel: Birkhauser Verlag. Roy, M., Peretz, I., &
Rainville, P. (2008). Emotional valence contribute to music-induced
analgesia. Pain, 134, 140--147. Rubin, E. (1958). Figure and ground. In
D. C. Beardslee & M. Wertheimer (Eds.), Readings in perception
(pp. 194--203). Princeton, NJ: Van Nostrand. (Original work published
1915) Rubin, P., Turvey, M. T., & Van Gelder, P. (1976). Initial
phonemes are detected faster in spoken words than in spoken nonwords.
Perception & Psychophysics, 19, 394--398. Rullman, M., Preusser, S., &
Pleger, B. (2019). Prefrontal and posterior parietal contributions to
the perceptual awareness of touch. Scientific Reports, 9:16981. Rushton,
S. K., & Salvucci, D. D. (2001). An egocentric account of the visual
guidance of locomotion. Trends in Cognitive Sciences, 5, 6--7. Rushton,
S. K., Harris, J. M., Lloyd, M. R., & Wann, J. P. (1998). Guidance of
locomotion on foot uses perceived target location rather than optic
flow. Current Biology, 8, 1191--1194. Rushton, W. A. H. (1961).
Rhodopsin measurement and dark adaptation in a subject deficient in cone
vision. Journal of Physiology, 156, 193--205. Rust, N. C., Mante, V.,
Simoncelli, E. P., & Movshon, J. A. (2006). How MT cells analyze the
motion of visual patterns. Nature Neuroscience, 9, 1421--1431. Sachs, O.
(2007). Musicophilia: Tales of music and the brain. New York: Vintage
Books. Sacks, O. (1985). The man who mistook his wife for a hat. London:
Duckworth. Sacks, O. (1995). An anthropologist on Mars. New York:
Vintage. Sacks, O. (2006, June 19). Stereo Sue. The New Yorker, p. 64.
Sacks, O. (2010). The mind's eye. New York: Knopf. Sadaghiani, S.,
Poline, J. B., Kleinschmidt, A., & D'Esposito, M. (2015). Ongoing
dynamics in large-scale functional connectivity predict perception.
Proceedings of the National Academy of Sciences, 112(27), 8463--8468.
Saenz, M., & Langers, D. R. M. (2014). Tonotopic mapping of human
auditory cortex. Hearing Research, 307, 42--52. Saffran, J. R.., Aslin,
R. N., & Newport, E. L. (1996). Statistical learning by 8-month-old
infants. Science, 274, 1926--1928. Sakata, H., & Iwamura, Y. (1978).
Cortical processing of tactile information in the first somatosensory
and parietal association areas in the monkey. In G. Gordon (Ed.), Active
touch (pp. 55--72). Elmsford, NY: Pergamon Press. Sakata, H., Taira, M.,
Mine, S., & Murata, A. (1992). Hand-movementrelated neurons of the
posterior parietal cortex of the monkey: Their role in visual guidance
of hand movements. In R. Caminiti, P. B. Johnson, & Y. Burnod (Eds.),
Control of arm movement in space: Neurophysiological and computational
approaches (pp. 185--198). Berlin: Springer-Verlag.

Salapatek, P., Bechtold, A. G., & Bushnell, E. W. (1976). Infant visual
acuity as a function of viewing distance. Child Development, 47,
860--863. Salasoo, A., & Pisoni, D. B. (1985). Interaction of knowledge
sources in spoken word identification. Journal of Memory and Language,
24, 210--231. Salimpoor, V. N., Benovoy, M., Larcher, K., Dagher, A., &
Zatorre, R. J. (2011). Anatomically distinct dopamine release during
anticipation and experience of peak emotion to music. Nature
Neuroscience, 14(2), 257--264. Samuel, A. G. (1990). Using
perceptual-restoration effects to explore the architecture of
perception. In G. T. M. Altmann (Ed.), Cognitive models of speech
processing (pp. 295--314). Cambridge, MA: MIT Press. Samuel, A. G.
(1997). Lexical activation produces potent phonemic percepts. Cognitive
Psychology, 32, 97--127. Samuel, A. G. (2001). Knowing a word affects
the fundamental perception of the sounds within it. Psychological
Science, 12, 348--351. Santos, D. V., Reiter, E. R., DiNardo, L. J., &
Costanzo, R. M. (2004). Hazardous events associated with impaired
olfactory function. Archives of Otolaryngology Head and Neck Surgery,
130, 317--319. Sato, M., Ogawa, H., & Yamashita, S. (1994). Gustatory
responsiveness of chorda tympani fibers the cynomolgus monkey. Chemical
Senses, 19, 381--400. Saygin, A. P. (2007). Superior temporal and
premotor brain areas necessary for biological motion perception. Brain,
130, 2452--2461. Saygin, A. P. (2012). Biological motion perception and
the brain: Neuropsychological and neuroimaging studies. In K. Johnson &
M. Shiffrar (Eds.), People watching: Social, perceptual, and
neurophysiological studies of body perception. Oxford Series in Visual
Cognition, Oxford University Press. Saygin, A. P., Wilson, S. M.,
Hagler, D. J., Jr., Bates, E., & Sereno, M. I. (2004). Point-light
biological motion perception activates human premotor cortex. Journal of
Neuroscience, 24, 6181--6188. Schaefer, R. S., Morcom, A. M., Roberts,
N., & Overy, K. (2014). Moving to music: Effects of heard and imagined
musical cues on movementrelated brain activity. Frontiers in Human
Neuroscience, 8, Article 774. Schaette, R., & McAlpine, D. (2011).
Tinnitus with a normal audiogram: Physiological evidence for hidden
hearing loss and computational model. Journal of Neuroscience, 31,
13452--13457. Scherf, K. S., Behrmann, M., Humphreys, K., & Luna, B.
(2007). Visual category-selectivity for faces, places and objects
emerges along different developmental trajectories. Developmental
Science, 10, F15--F30. Schiffman, H. R. (1967). Size-estimation of
familiar objects under informative and reduced conditions of viewing.
American Journal of Psychology, 80, 229--235. Schiffman, S. S., &
Erickson, R. P. (1971). A psychophysical model for gustatory quality.
Physiology and Behavior, 7, 617--633. Schiller, P. H., Logohetis, N. K.,
& Charles, E. R. (1990). Functions of the colour-opponent and broad-band
channels of the visual system. Nature, 343, 68--70. Schinazi, V. R., &
Epstein, R. A. (2010). Neural correlates of real-world route learning.
NeuroImage, 53, 725--735. Schlack, A., Sterbing-D'Angelo, J., Hartung,
K., Hoffmann, K.-P., & Bremmer, F. (2005). Multisensory space
representations in the macaque ventral intraparietal area. Journal of
Neuroscience, 25, 4616--4625. Schmuziger, N., Patscheke, J., & Probst,
R. (2006). Hearing in nonprofessional pop/rock musicians. Ear & Hearing,
27, 321--330. Scholz, J., & Woolf, C. J. (2002). Can we conquer pain?
Nature Neuroscience, 5, 1062--1067. Schomers, M. R., & Pulvermüller, F.
(2016). Is the sensorimotor cortex relevant for speech perception and
understanding? An integrative review. Frontiers in Human Neuroscience,
10, 435. Schubert, E. D. (1980). Hearing: Its function and dysfunction.
Wien: SpringerVerlag. Scott, T. R., & Giza, B. K. (1990). Coding
channels in the taste system of the rat. Science, 249, 1585--1587.
References

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

465

Scott, T. R., & Plata-Salaman, C. R. (1991). Coding of taste quality. In
T. V. Getchell, R. L. Doty, L. M. Bartoshuk, & J. B. Snow (Eds.), Smell
and taste in health and disease (pp. 345--368). New York: Raven Press.
Scoville, W. B., & Milner, B. (1957). Loss of recent memory after
bilateral hippocampus lesions. Journal of Neurosurgery and Psychiatry,
20, 11--21. Sedgwick, H. (2001). Visual space perception. In E. B.
Goldstein (Ed.), Blackwell handbook of perception (pp. 128--167).
Oxford, UK: Blackwell. Segui, J. (1984). The syllable: A basic
perceptual unit in speech processing? In H. Bouma & D. G. Gouwhuis
(Eds.), Attention and performance X (pp. 165--181). Hillsdale, NJ:
Erlbaum. Seiler, S. J. (2015). Hand on the wheel, mind on the mobile: An
analysis of social factors contributing to texting while driving.
Cyberpsychology, Behavior, and Social Networking, 18, 72--78. Sekuler,
A. B., & Bennett, P. J. (2001). Generalized common fate: Grouping by
common luminance changes. Psychological Science, 12(6), 437--444.
Semple, R. J. (2010). Does mindfulness meditation enhance attention? A
randomized controlled trial. Mindfulness, 1, 121--130. Senior, C.,
Barnes, J., Giampietro, V., Simmons, A., Bullmore, E. T., Brammer, M.,
et al. (2000). The functional neuoroanatomy of implicit-motion
perception or "representational momentum." Current Biology, 10, 16--22.
Shadmehr, R., Smith, M. A., & Krakauer, J. W. (2010). Error correction,
sensory prediction, and adaptation in motor control. Annual Review of
Neuroscience, 33, 89--108. Shahbake, M. (2008). Anatomical and
psychophysical aspects of the development of the sense of taste in
humans (Unpublished doctoral dissertation). University of Western
Sydney, New South Wales, Australia. Shamma, S. A., & Micheyl, C. (2010).
Behind the scenes of auditory perception. Current Opinion in
Neurobiology, 20, 361--366. Shamma, S. A., Elhilali, M., & Micheyl, C.
(2011). Temporal coherence and attention in auditory scene analysis.
Trends in Neurosciences, 34, 114--123. Shannon, R. V., Zeng, F.-G.,
Kamath, V., Wygonski, J., & Ekelid, M. (1995). Speech recognition with
primarily temporal cues. Science, 270, 303--304. Sharma, S. D., Cushing,
S. L., Papsin, B. C., & Gordon, K. A. (2020). Hearing and speech
benefits of cochlear implantation in children: A review of the
literature. International Journal of Pediatric Otorhinolarylgology, 133,
1--5. Shea, S. L., Fox, R., Aslin, R. N., & Dumas, S. T. (1980).
Assessment of stereopsis in human infants. Investigative Ophthalmology
and Visual Science, 19(11), 1400--1404. Shek, D., Shek, L. Y., & Sun, R.
C. F. (2016). Internet addiction. In D. W. Pfaff & N. D. Volkow (Eds.),
Neuroscience in the 21st century. New York: Springer. Shepherd, G. M.
(2012). Neurogastronomy. New York: Columbia University Press. Sherman,
P. D. (1981). Colour vision in the nineteenth century: The
YoungHelmholtz-Maxwell Theory. Bristol: Adam Hilger. Sherman, S. M., &
Koch, C. (1986). The control of retinogeniculate transmission in the
mammalian lateral geniculate nucleus. Experimental Brain Research, 63,
1--20. Shiffrar, M., & Freyd, J. (1990). Apparent motion of the human
body. Psychological Science, 1, 257--264. Shiffrar, M., & Freyd, J.
(1993). Timing and apparent motion path choice with human body
photographs. Psychological Science, 4, 379--384. Shimamura, A. P., &
Prinzmetal, W. (1999). The mystery spot illusion and its relation to
other visual illusions. Psychological Science, 10, 501--507. Shimojo,
S., Bauer, J., O'Connell, K. M., & Held, R. (1986). Pre-stereoptic
binocular vision in infants. Vision Research, 26, 501--510. Shinoda, H.,
Hayhoe, M. M., & Shrivastava, A. (2001). What controls attention in
natural environments? Vision Research, 41, 3535--3545. Shneidman, L. A.,
Arroyo, M. E., Levince, S. C., & Goldin-Meadow, S. (2013). What counts
as effective input for word learning? Journal of Child Language, 40,
672--686. 466

Shuwairi, S. M., & Johnson, S. P. (2013). Oculomotor exploration of
impossible figures in early infancy. Infancy, 18, 221--232. Sifre, R.,
Olson, L., Gillespie, S., Klin, A., Jones, W., & Shultz, S. (2018). A
longitudinal investigation of preferential attention to biological
motion in 2- to 24-month-old infants. Scientific Reports, 8, 2527.
Silbert, L. J., Honey, C. J., Simony, E., Poeppel, D., & Hasson, U.
(2014). Coupled neural systems underlie the production and comprehension
of naturalistic narrative speech. Proceedings of the National Academy of
Sciences, 111, E4687--E4696. Silver, M. A., & Kastner, S. (2009).
Topographic maps in human frontal and parietal cortex. Trends in
Cognitive Sciences, 13, 488--495. Simion, F., Regolin, L., & Bulf, H.
(2008). A predisposition for biological motion in the newborn baby.
Proceedings of the National Academy of Sciences, 105, 809--813. Simons,
D. J., & Chabris, C. F. (1999). Gorillas in our midst: Sustained
inattentional blindness for dynamic events. Perception, 28, 1059--1074.
Singer, T., & Klimecki, O. M. (2014). Empathy and compassion. Current
Biology, 24, R875--R878. Singer, T., Seymour, B., O'Doherty, J., Kaube,
H., Dolan, R. J., & Frith, C. D. (2004). Empathy for pain involves the
affective but not sensory components of pain. Science, 303, 1157--1162.
Sinha, P. (2002). Recognizing complex patterns. Nature Neuroscience, 5,
1093--1097. Siveke, I., Pecka, M., Seidl, A. H., Baudoux, S., & Grothe,
B. (2006). Binaural response properties of low-frequency neurons in the
gerbil dorsal nucleus of the lateral lemniscus. Journal of
Neurophysiology, 96, 1425--1440. Skelton, A. E., Catchpole, G., Abbott,
J. T., Bosten, J. M., & Franklin, A. (2017). Biological origins of color
categorization. Proceedings of the National Academy of Sciences,
114(21), 5545--5550. Skinner, B. F. (1938). The behavior of organisms.
New York: Appleton Century. Skipper, J. I., Devlin, J. T., & Lametti, D.
R. (2017). The hearing ear is always found close to the speaking tongue:
Review of the role of the motor system in speech perception. Brain &
Language, 164, 77--105. Slater, A. M., & Findlay, J. M. (1975).
Binocular fixation in the newborn baby. Journal of Experimental Child
Psychology, 20, 248--273. Slevc, L. R. (2012). Language and music:
Sound, structure, and meaning. WIREs Cognitive Science, 3, 483--492.
Sloan, L. L., & Wollach, L. (1948). A case of unilateral deuteranopia.
Journal of the Optical Society of America, 38, 502--509. Sloboda, J. A.
(1991). Music structure and emotional response: Some empirical findings.
Psychology of Music, 19, 110--120. Sloboda, J. A. (2000). Individual
differences in music performance. Trends in Cognitive Sciences, 4(10),
397--403. Sloboda, J. A., & Gregory, A. H. (1980). The psychological
reality of musical segments. Canadian Journal of Psychology, 34(3),
274--280. Small, D. M. (2008). Flavor and the formation of
category-specific processing in olfaction. Chemical Perception, 1,
136--146. Small, D. M. (2012). Flavor is in the brain. Physiology and
Behavior, 107, 540--552. Smith, A. T., Singh, K. D., Williams, A. L., &
Greenlee, M. W. (2001). Estimating receptive field size from fMRI data
in human striate and extrastriate visual cortex. Cerebral Cortex,
11(12), 1182--1190. Smith, D. V., & Scott, T. R. (2003). Gustatory
neural coding. In R. L. Doty (Ed.), Handbook of olfaction and gustation
(2nd ed.). New York: Marcel Dekker. Smith, D. V., St. John, S. J., &
Boughter, J. D., Jr. (2000). Neuronal cell types and taste quality
coding. Physiology and Behavior, 69, 77--85. Smith, M. A., Majaj, N. J.,
& Movshon, J. A. (2005). Dynamics of motion signaling by neurons in
macaque area MT. Nature Neuroscience, 8, 220--228. Smithson, H. E.
(2005). Sensory, computational and cognitive components of human colour
constancy. Philosophical Transactions of the Royal Society of London B,
Biological Sciences, 360, 1329--1346. Smithson, H. E. (2015). Perceptual
organization of colour. In J. Wagemans (Ed.), Oxford handbook of
perceptual organization. Oxford, UK: Oxford University Press.

References

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Sobel, E. C. (1990). The locust's use of motion parallax to measure
distance. Journal of Comparative Physiology A, 167, 579--588.
Soderstrom, M. (2007). Beyond babytalk: Re-evaluating the nature and
content of speech input to preverbal infants. Developmental Review, 27,
501--532. Sommer, M. A., & Wurtz, R. H. (2008). Brain circuits for the
internal monitoring of movements. Annual Review of Neuroscience, 31,
317--338. Sosulski, D. L., Bloom, M. L., Cutforth, T., Axel, R., &
Sandeep, R. D. (2011). Distinct representations of olfactory information
in different cortical centres. Nature, 472, 213--219. Soto-Faraco, S.,
Lyons, J., Gazzaniga, M., Spence, C., & Kingstone, A. (2002). The
ventriloquist in motion: Illusory capture of dynamic information across
sensory modalities. Cognitive Brain Research, 14, 139--146. Soto-Faraco,
S., Spence, C., Lloyd, D., & Kingstone, A. (2004). Moving multisensory
research along: Motion perception across sensory modalities. Current
Directions in Psychological Science, 13, 29--32. Soucy, E. R., Albenau,
D. F., Fantana, A. L., Murthy, V. N., & Meister, M. (2009). Precision
and diversity in an odor map on the olfactory bulb. Nature Neuroscience,
12, 210--220. Spector, A. C., & Travers, S. P. (2005). The
representation of taste quality in the mammalian nervous system.
Behavioral and Cognitive Neuroscience Reviews, 4, 143--191. Spector, F.,
& Maurer, D. (2012). Making sense of scents: The color and texture of
odors. Seeing and Perceiving 25, 655--677. Spence, C. (2015).
Multisensory flavor perception. Cell, 161, 24--35. Spence, C. (2020).
Wine psychology: Basic and applied. Cognitive Research: Principles and
Implications, 5(22). Spence, C., & Read, L. (2003). Speech shadowing
while driving: On the difficulty of splitting attention between eye and
ear. Psychological Science, 14, 251--256. Spence, C., Levitan, C. A.,
Shankar, M. U., & Zampini, M. (2010). Does food color influence taste
and flavor perception in humans? Chemical Perception, 3, 68--84. Spille,
C., Kollmeier, B., & Meyer, B. T. (2018). Comparing human and automatic
speech recognition in simple and complex acoustic scenes. Computer
Speech & Language, 52, 128--140. Sporns, O. (2014). Contributions and
challenges for network models in cognitive neuroscience. Nature
Neuroscience, 17(5), 652. Srinivasan, M. V., & Venkatesh, S. (Eds.).
(1997). From living eyes to seeing machines. New York: Oxford University
Press. Stasenko, A., Garcea, F. E., & Mahon, B. Z. (2013). What happens
to the motor theory of perception when the motor system is damaged?
Language and Cognition, 5(2--3), 225--238. Steiner, J. E. (1974).
Innate, discriminative human facial expressions to taste and smell
stimulation. Annals of the New York Academy of Sciences, 237, 229--233.
Steiner, J. E. (1979). Human facial expressions in response to taste and
smell stimulation. Advances in Child Development and Behavior, 13,
257--295. Stevens, J. A., Fonlupt, P., Shiffrar, M., & Decety, J.
(2000). New aspects of motion perception: Selective neural encoding of
apparent human movements. NeuroReport, 111, 109--115. Stevens, S. S.
(1957). On the psychophysical law. Psychological Review, 64, 153--181.
Stevens, S. S. (1961). To honor Fechner and repeal his law. Science,
133, 80--86. Stevens, S. S. (1962). The surprising simplicity of sensory
metrics. American Psychologist, 17, 29--39. Stiles, W. S. (1953).
Further studies of visual mechanisms by the two-color threshold method.
Coloquio sobre problemas opticos de la vision (Vol. 1, pp. 65--103).
Madrid: Union Internationale de Physique Pure et Appliquée. Stoffregen,
T. A., Smart, J. L., Bardy, B. G., & Pagulayan, R. J. (1999). Postural
stabilization of looking. Journal of Experimental Psychology: Human
Perception and Performance, 25, 1641--1658. Stokes, R. C., Venezia, J.
H., & Hickock, G. (2019). The motor system's \[modest\] contribution to
speech perception. Psychonomic Bulletin & Review, 26, 1354--1366.

Strayer, D. L., & Johnston, W. A. (2001). Driven to distraction:
Dual-task studies of simulated driving and conversing on a cellular
telephone. Psychological Science, 12, 462--466. Strayer, D. L., Cooper,
J. M., Turrill, J., Coleman, J., Medeiros-Ward, N., & Biondi, F. (2013).
Measuring driver distraction in the automobile. Washington, DC: AAA
Foundation for Traffic Safety. Stupacher, J., Wood, G., & Witte, M.
(2017). Synchrony and sympathy: Social entrainment with music compared
to a metronome. Psychomusicology: Music, Mind and Brain, 27(3),
158--166. Suarez-Rivera, C., Smith, L. B., & Chen, Y. (2019). Multimodal
parent behaviors within joint attention support sustained attention in
infants. Developmental Psychology, 55(1), 96--109. Subramanian, D.,
Alers, A., & Sommer, M. (2019). Corollary discharge for action and
cognition. Biological Psychiatry: Cognitive Neuroscience and
Neuroimaging, 4, 782--790. Sufka, K. J., & Price, D. D. (2002). Gate
control theory reconsidered. Brain and Mind, 3, 277--290. Suga, N.
(1990, June). Biosonar and neural computation in bats. Scientific
American, 60--68. Sugovic, M., & Witt, J. K. (2013). An older view on
distance perception: Older adults perceive walkable extents and farther.
Experimental Brain Research, 226, 383--391. Sumby, W. H., & Pollack, J.
(1954). Visual contributions to speech intelligibility in noise. Journal
of the Acoustical Society of America, 26, 212--215. Sumner, P., &
Mollon, J. D. (2000). Catarrhine photopigments are optimized for
detecting targets against a foliage background. Journal of Experimental
Biology, 23, 1963--1986. Sun, H.-J., Campos, J., Young, M., Chan, G. S.
W., & Ellard, C. G. (2004). The contributions of static visual cues,
nonvisual cues, and optic flow in distance estimation. Perception, 33,
49--65. Sun, L. D., & Goldberg, M. E. (2016). Corollary discharge and
oculomotor proprioception: Cortical mechanisms for spatially accurate
vision. Annual Review of Visual Science, 2, 61--84. Sutherland, S.
(2020). Mysteries of COVID smell loss finally yield some answers.
Scientific American.com, Nov. 18, 2020. Svaetichin, G. (1956). Spectral
response curves from single cones. Acta Physiologica Scandinavica
Supplementum, 134, 17--46. Svirsky, M. (2017). Cochlear implants and
electronic hearing. Physics Today, 70, 52--58. Svirsky, M. (2017).
Cochlear implants and electronic hearing. Physics Today, 70, 52--58.
Taira, M., Mine, S., Georgopoulis, A. P., Murata, A., & Sakata, H.
(1990). Parietal cortex neurons of the monkey related to the visual
guidance of hand movement. Experimental Brain Research, 83, 29--36. Tan,
S-L., Pfordresher, P., & Harré, R. (2010). Psychology of music. New
York: Psychology Press. Tan, S-L., Pfordresher, P., & Harre', R. (2013).
Psychology of music: From sound to significance. New York: Psychology
Press. Tanaka, J. W., & Curran, T. (2001). A neural basis for expert
object recognition. Psychological Science, 12(1), 43--47. Tanaka, J. W.,
& Presnell, L. M. (1999). Color diagnosticity in object recognition.
Perception & Psychophysics, 61, 1140--1153. Tanaka, J., Weiskopf, D., &
Williams, P. (2001). The role of color in highlevel vision. Trends in
Cognitive Sciences, 5, 211--215. Tang, Y-Y., Tang, Y., Tang, R., &
Lewis-Peacock, J. A. (2017). Brief mental training reorganizes
large-scale brain networks. Frontiers in System Neuroscience, 11,
Article 6. Tarr, B., Launay, J., & Dunbar, R. I. M. (2014). Music and
social bonding: "self-other" and neurohormonal mechanisms. Frontiers in
Psychology, 5, Article 1096. Tarr, B., Luunay, J., & Dunmbar, R. I. M.
(2016). Silent disco: Dancing in synchrony leads to elevated pain
thresholds and social closeness. Evolution and Human Behavior, 37,
343--349. References

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

467

Tatler, B. W., Hayhoe, M. M., Land, M. F., & Ballard, D. H. (2011). Eye
guidance in natural vision: Reinterpreting salience. Journal of Vision,
11(5): 1--23. Teller, D. Y. (1997). First glances: The vision of
infants. Investigative Ophthalmology and Visual Science, 38, 2183--2199.
Tenenbaum, J. B., Kemp, C., Griffiths, T. L., & Goodman, N. D. (2011).
How to grow a mind: Statistics, structure, and abstraction. Science,
331, 1279--1285. Terwogt, M. M., & Hoeksma, J. B. (1994). Colors and
emotions: Preferences and combinations. Journal of General Psychology,
122, 5--17. Thaler, L., Arnott, S. R., & Goodale, M. A. (2011). Neural
correlates of natural human echolocation in early and late blind
echolocation experts. PLoS One, 6(5), e20162
doi:10.1371.journal.pone.0020162. Theeuwes, J. (1992). Perceptual
selectivity for color and form. Perception & Psychophysics, 51,
599--606. Thompson, W. F. (2015). Music, thought, and feeling:
Understanding the psychology of music, 2nd ed. New York: Oxford
University Press. Thompson, W. F., & Quinto, L. (2011). Music and
emotion: Psychological consideration. In E. Schellekens & P. Gold
(Eds.), The aesthetic mind: Philosophy and psychology (pp. 357--375).
New York: Oxford. Thompson, W. F., Sun, Y., & Fritz, T. (2019). Music
across cultures. In P. J. Rentfrow & D. J. Levitin (Eds.), Foundations
in music psychology: Theory and research (pp. 503--541). Cambridge, MA:
MIT Press. Thorstenson, C. A., Puzda, A. D., Young, S. G., & Elliot, A.
J. (2019). Face color facilitates the disambiguation of confusing
emotion expressions: Toward a social functional account of face color in
emotional communication. Emotion, 9(5), 799--807. Timney, B., & Keil, K.
(1999). Local and global stereopsis in the horse. Vision Research, 39,
1861--1867. Tindell, D. R., & Bohlander, R. W. (2012). The use and abuse
of cell phones and text messaging in the classroom: A survey of college
students. College Teaching, 60, 1--9. Todrank, J., & Bartoshuk, L. M.
(1991). A taste illusion: Taste sensation localized by touch. Physiology
and Behavior, 50, 1027--1031. Tolman, E. C. (1938). The determinants of
behavior at a choice point. Psychological Review, 45, 1--41. Tolman, E.
C. (1948). Cognitive maps in rats and men. Psychological Review, 55,
189--208. Tong, F., Nakayama, K., Vaughn, J. T., & Kanwisher, N. (1998).
Binocular rivalry and visual awareness in human extrastriate cortex.
Neuron, 21, 753--759. Tonndorf, J. (1960). Shearing motion in scalia
media of cochlear models. Journal of the Acoustical Society of America,
32, 238--244. Torralba, A., Oliva, A., Castelhano, M. S., & Henderson,
J. M. (2006). Contextual guidance of eye movements and attention in
real-world scenes: The role of global features in object search.
Psychological Review, 113, 766--786. Tracey, I. (2010). Getting the pain
you expect: Mechanisms of placebo, nocebo and reappraisal effects in
humans. Nature Medicine, 16, 1277--1283. Trainor, L. J., Gao, X., Lei,
J-J., Lehtovaara, K., & Harris, L. R. (2009). The primal role of the
vestibular system in determining musical rhythm. Corex, 45, 35--43.
Trehub, S. E. (1973). Infants' sensitivity to vowel and tonal contrasts.
Developmental Psychology 9(1), 91--96. Trehub, S. E., Ghazban, N., &
Corbeil, M. (2015). Musical affect regulation in infancy. Annals of the
New York Academy of Sciences, 1337, 186--192. Treisman, A. (1985).
Preattentive processing in vision. Computer Vision, Graphics, and Image
Processing, 31, 156--177. Treisman, A., & Gelade, G. (1980). A
feature-integration theory of attention. Cognitive Psychology, 12,
97--113. Treisman, A., & Schmidt, H. (1982). Illusory conjunctions in
the perception of objects. Cognitive Psychology, 14, 107--141.
Tresilian, J., R., Mon-Williams, M., & Kelly, B. (1999). Increasing
confidence in vergence as a cue to distance. Proceedings of the Royal
Society of London, 266B, 39--44. 468

Troiani, V., Stigliani, A., Smith, M. E., & Epstein, R. A. (2014).
Multiple object properties drive scene-selective regions. Cerebral
Cortex, 24, 883--897. Truax, B. (1984). Acoustic communication. Ablex.
Tsao, D. Y., Freiwald, W. A., Tootell, R. B., & Livingstone, M. S.
(2006). A cortical region consisting entirely of face-selective cells.
Science, 311, 670--674. Turano, K. A., Yu, D., Hao, L., & Hicks, J. C.
(2005). Optic-flow and egocentric-directions strategies in walking:
Central vs peripheral visual field. Vision Research, 45, 3117--3132.
Turatto, M., Vescovi, M., & Valsecchi, M. (2007). Attention makes moving
objects be perceived to move faster. Vision Research, 47, 166--178.
Turk, D. C., & Flor, H. (1999). Chronic pain: A biobehavioral
perspective. In R. J. Gatchel & D. C. Turk (Eds.), Psychosocial factors
in pain (pp. 18--34). New York: Guilford Press. Turman, A. B., Morley,
J. W., & Rowe, M. J. (1998). Functional organization of the
somatosensory cortex in the primate. In J. W. Morley (Ed.), Neural
aspects of tactile sensation (pp. 167--193). New York: Elsevier Science.
Tuthill, J. C., & Azim, E. (2018). Proprioception. Current Biology, 28,
R187--R207. Tuulari, J. J., Scheinin, N. M., Lehtola, S., et al. (2019).
Neural correlates of gentle skin stroking in early infancy.
Developmental Cognitive Neuroscience, 35, 36--41. Tyler, C. W. (1997a).
Analysis of human receptor density. In V. Lakshminarayanan (Ed.), Basic
and clinical applications of vision science (pp. 63--71). Norwell, MA:
Kluwer Academic. Tyler, C. W. (1997b). Human cone densities: Do you know
where all your cones are? Unpublished manuscript. Uchida, N., Takahashi,
Y. K., Tanifuji, M., & Mori, K. (2000). Odor maps in the mammalian
olfactory bulb: Domain organization and odorant structural features.
Nature Neuroscience, 3, 1035--1043. Uchikawa, K., Uchikawa, H., &
Boynton, R. M. (1989). Partial color constancy of isolated surface
colors examined by a color-naming method. Perception, 18, 83--91. Uddin,
L. Q., Iacoboni, M., Lange, C., & Keenan, J. P. (2007). The self and
social cognition: The role of cortical midline structures and mirror
neurons. Trends in Cognitive Sciences, 11, 153--157. Uka, T., &
DeAngelis, G. C. (2003). Contribution of middle temporal area to coarse
depth discrimination: Comparison of neuronal and psychophysical
sensitivity. Journal of Neuroscience, 23, 3515--3530. Ungerleider, L.
G., & Haxby, J. V. (1994). "What" and "where" in the human brain.
Current Opinion in Neurobiology, 4, 157--165. Ungerleider, L. G., &
Mishkin, M. (1982). Two cortical visual systems. In D. J. Ingle, M. A.
Goodale, & R. J. Mansfield (Eds.), Analysis of visual behavior
(pp. 549--580). Cambridge, MA: MIT Press. Valdez, P., & Mehribian, A.
(1994). Effect of color on emotions. Journal of Experimental Psychology:
General, 123, 394--409. Vallbo, A. B., & Hagbarth, K-E. (1968). Activity
from skin mechanoreceptors recorded percutaneously in awake human
subjects. Experimental Neurology, 21, 270--289. Vallbo, A. B., &
Johansson, R. S. (1978). The tactile sensory innervation of the glabrous
skin of the human hand. In G. Gordon (Ed.), Active touch (pp. 29--54).
New York: Oxford University Press. Vallbo, A. B., Olausson, H.,
Wessberg, J., & Norrsell, U. (1993). A system of unmyelinated afferents
for innocuous mechanoreception in the human skin. Brain Research, 628,
301--304. Vallortigara, G., Regolin, L., & Marconato, F. (2005).
Visually inexperienced chicks exhibit spontaneous preference for
biological motion patterns. PLoS Biology, 3, e208. Van Den Heuvel, M.
P., & Pol, H. E. H. (2010). Exploring the brain network: A review on
resting-state fMRI functional connectivity. European
Neuropsychopharmacology, 20(8), 519--534.

References

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Van Doorn, G. H., Wuilemin, D., & Spence, C. (2014). Does the colour of
the mug influence the taste of the coffee? Flavour, 3, 1--7. Van Essen,
D. C., & Anderson, C. H. (1995). Information processing strategies and
pathways in the primate visual system. In S. F. Zornetzer, J. L. Davis,
& C. Lau (Eds.), An introduction to neural and electronic networks (2nd
ed., pp. 45--75). San Diego: Academic Press. Van Kemenade, B. M.,
Muggleton, N., Walsh, V., & Saygin, A. P. (2012). Effects of TMS over
premotor and superior temporal cortices on biological motion perception.
Journal of Cognitive Neuroscience, 24, 896--904. Van Wanrooij, M. M., &
Van Opstal, A. J. (2005). Relearning sound localization with a new ear.
Journal of Neuroscience, 25, 5413--5424. van Wassenhove, V., Grant, K.
W., & Poeppel, D. (2005). Visual speech speeds up the neural processing
of auditory speech. Proceedings of the National Academy of Sciences,
102, 1181--1186. Vecera, S. P., Vogel, E. K., & Woodman, G. F. (2002).
Lower region: A new cue for figure--ground assignment. Journal of
Experimental Psychology: General, 131, 194--205. Veldhuizen, M. G.,
Nachtigal, D., Teulings, L., Gitelman, D. R., & Small, D. M. (2010). The
insular taste cortex contributes to odor quality coding. Frontiers in
Human Neuroscience, 4(Article 58), 1--11. Verhagen, J. V., Kadohisa, M.,
& Rolls, E. T. (2004). Primate insular/ opercular taste cortex: Neuronal
representations of viscosity, fat texture, grittiness, temperature, and
taste of foods. Journal of Neurophysiology, 92, 1685--1699. Vermeij, G.
(1997). Privileged hands: A scientific life. New York: Freeman.
Vingerhoets, G. (2014). Contribution of the posterior parietal cortex in
reaching, grasping, and using objects and tools. Frontiers in
Psychology, 5, 151. Violanti, J. M. (1998). Cellular phones and fatal
traffic collisions. Accident Analysis and Prevention, 28, 265--270. Võ,
M. L. H., & Henderson, J. M. (2009). Does gravity matter? Effects of
semantic and syntactic inconsistencies on the allocation of attention
during scene perception. Journal of Vision, 9(3), 1--15. von der Emde,
G., Schwarz, S., Gomez, L., Budelli, R., & Grant, K. (1998). Electric
fish measure distance in the dark. Nature, 395, 890--894. Von Hipple, P.
V., & Huron, D. (2000). Why do skips precede reversals? The effect of
tessitura on melodic structure. Music Perception, 18(1), 59--85. von
Holst, E., & Mittelstaedt, H. (1950). Das Reafferenzprinzip.
Wechselwirkungen zwischen zentralnervensystem und peripherie,
Naturwissenschaften, 37, 464--476. von Kriegstein, K., Kleinschmidt, A.,
Sterzer, P., & Giraud, A. L. (2005). Interaction of face and voice areas
during speaker recognition. Journal of Cognitive Neuroscience, 17,
367--376. Vonderschen, K., & Wagner, H. (2014). Detecting interaural
time differences and remodeling their representation. Trends in
Neurosciences, 37, 289--300. Vos, P. G., & Troost, J. M. (1989).
Ascending and descending melodic intervals: Statistical findings and
their perceptual relevance. Music Perception, 6(4), 383--396.
Vuilleumier, P., & Schwartz, S. (2001a). Emotional facial expressions
capture attention. Neurology, 56, 153--158. Vuilleumier, P., & Schwartz,
S. (2001b). Beware and be aware: Capture of spatial attention by
fear-related stimuli in neglect. NeuroReport, 12(6), 1119--1122. Vuust,
P., Ostergaard, L., Pallesen, K. J., Bailey, C., & Roepstorff, A.
(2009). Predictive coding of music---Brain responses to rhythmic
incongruity. Cortex, 45, 80--92. Wager, T., Atlas, L. Y., Botvinick, M.
M., et al. (2016). Pain in the ACC? Proceedings of the National Academy
of Sciences, 113(18), E2474--E2475. Wald, G. (1964). The receptors of
human color vision. Science, 145, 1007--1017. Wald, G. (1968). Molecular
basis of visual excitation \[Nobel lecture\]. Science, 162, 230--239.

Wald, G., & Brown, P. K. (1958). Human rhodopsin. Science, 127,
222--226. Waldrop, M. M. (1988). A landmark in speech recognition.
Science, 240, 1615. Wall, P. D., & Melzack, R. (Eds.). (1994). Textbook
of pain (3rd ed.). Edinburgh: Chruchill Livingstone. Wallace, G. K.
(1959). Visual scanning in the desert locust Schistocerca Gregaria
Forskal. Journal of Experimental Biology, 36, 512--525. Wallace, M. N.,
Rutowski, R. G., Shackleton, T. M., & Palmer, A. R. (2000). Phase-locked
responses to pure tones in guinea pig auditory cortex. Neuroreport, 11,
3989--3993. Wallach, H. (1963). The perception of neutral colors.
Scientific American, 208, 107--116. Wallach, H., Newman, E. B., &
Rosenzweig, M. R. (1949). The precedence effect in sound localization.
American Journal of Psychology, 62, 315--336. Wallisch, P. (2017).
Illumination assumptions account for individual differences in the
perceptual interpretation of a profoundly ambiguous stimulus in the
color domain: "The dress." Journal of Vision, 17(4):5, 1--14. Walls, G.
L. (1942). The vertebrate eye. New York: Hafner. (Reprinted in 1967)
Wandell, B. A. (2011). Imaging retinotopic maps in the human brain.
Vision Research, 51, 718--737. Wandell, B. A., Dumoulin, S. O., &
Brewer, A. A. (2009). Visual areas in humans. In L. Squire (Ed.),
Encyclopedia of neuroscience. New York: Academic Press. Wang, L., He, J.
L., & Zhang, X. H. (2013). The efficacy of massage on preterm infants: A
meta-analysis. American Journal of Perinatology, 30(9), 731--738. Wang,
Q. J., & Spence, C. (2018). Assessing the influence of music on wine
perception among wine professionals. Food Science & Nutrition, 6,
285--301. Wang, Q. J., & Spence, C. (2019). Drinking through
rosé-colored glasses: Influence of wine color on the perception of aroma
and flavor in wine experts and novices. Food Research International,
126, 108678. Wang, R. F. (2003). Spatial representations and spatial
updating. In D. E. Irwin & B. H. Ross (Eds.), The psychology of learning
and motivation: Advances in research and theory (Vol. 42, pp. 109--156).
San Diego, CA: Elsevier. Wang, Y., Bergeson, T. R., & Houston, D. M.
(2017). Infant-directed speech enhances attention to speech in deaf
infants with cochlear implants. Journal of Speech, Language, and Hearing
Research, 60(11), 1--13. Ward, A. F., Duke, K., Gneezy, A., & Bos, M. W.
(2017). Brain drain: The mere presence of one's own smartphone reduces
available cognitive capacity. Journal of the Association for Consumer
Research, 2(2), 140--154. Warren, R. M. (1970). Perceptual restoration
of missing speech sounds. Science, 167, 392--393. Warren, R. M.,
Obuseck, C. J., & Acroff, J. M. (1972). Auditory induction of absent
sounds. Science, 176, 1149. Warren, W. H. (1995). Self-motion: Visual
perception and visual control. In W. Epstein & S. Rogers (Eds.),
Handbook of perception and cognition: Perception of space and motion
(pp. 263--323). New York: Academic Press. Warren, W. H. (2004). Optic
flow. In L. M. Chalupa & J. S. Werner (Eds.), The visual neurosciences
(pp. 1247--1259). Cambridge, MA: MIT Press. Warren, W. H., Kay, B. A., &
Yilmaz, E. H. (1996). Visual control of posture during walking:
Functional specificity. Journal of Experimental Psychology: Human
Perception and Performance, 22, 818--838. Warren, W. H., Kay, B. A.,
Zosh, W. D., Duchon, A. P., & Sahuc, S. (2001). Optic flow is used to
control human walking. Nature Neuroscience, 4, 213--216. Watkins, L. R.,
& Maier, S. F. (2003). Glia: A novel drug discovery target for clinical
pain. Nature Reviews Drug Discovery, 2, 973--985. Weber, A. I., Hannes,
P. S., Lieber, J. D., Cheng, J.-W., Manfredi, L. R., Dammann, J. F., &
Bensmaia, S. J. (2013). Spatial and temporal codes mediate the tactile
perception of natural textures. Proceedings of the National Academy of
Sciences, 110, 17107--17112. References

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

469

Webster, M. (2018). Color vision. In J. Serences (Ed.), Stevens'
handbook of experimental psychology and cognitive neuroscience
(pp. 1--23). New York: Wiley. Webster, M. A. (2011). Adaptation and
visual coding. Journal of Vision, 11, 1--23. Weinstein, S. (1968).
Intensive and extensive aspects of tactile sensitivity as a function of
body part, sex, and laterality. In D. R. Kenshalo (Ed.), The skin senses
(pp. 195--218). Springfield, IL: Thomas. Weisenberg, M. (1977). Pain and
pain control. Psychological Bulletin, 84, 1008--1044. Weiser, B. (2020).
Concert for one: I.C.U. doctor brings classical music to coronavirus
patients. New York Times, May 4, 2020, Section A, Page 14. Weisleder,
A., & Fernald, A. (2013). Talking to children matters. Early language
experience strengthens processing and builds vocabulary. Psychological
Science, 24, 2143--2152. Weissberg, M. (1999). Cognitive aspects of
pain. In P. D. Wall & R. Melzak (Eds.), Textbook of pain (4th ed.,
pp. 345--358). New York: Churchill Livingstone. Werner, L. A., &
Bargones, J. Y. (1992). Psychoacoustic development of human infants. In
C. Rovee-Collier & L. Lipsett (Eds.), Advances in infancy research (Vol.
7, pp. 103--145). Norwood, NJ: Ablex. Wernicke, C. (1874). Der
aphasische Symptomenkomplex. Breslau: Cohn. Wertheimer, M. (1912).
Experimentelle Studien über das Sehen von Beuegung. Zeitchrift für
Psychologie, 61, 161--265. Wever, E. G. (1949). Theory of hearing. New
York: Wiley. Wexler, M., Panerai, I. L., & Droulez, J. (2001).
Self-motion and the perception of stationary objects. Nature, 409,
85--88. Whalen, D. H. (2019). The motor theory of speech perception.
Oxford Research Encyclopedia. Linguistics. DOI:
10.1093/acrefore/978019938 4655.013.404 Wiech, K., Ploner, M., & Tracey,
I. (2008). Neurocognitive aspects of pain perception. Trends in
Cognitive Sciences, 12, 306--313. Wiederhold, B. K. (2016). Why do
people still text while driving? Cyberpsychology, Behavior, and Social
Networking, 19(8), 473--474. Wightman, F. L., & Kistler, D. J. (1992).
The dominant role of lowfrequency interaural time differences in sound
localization. Journal of the Acoustical Society of American, 91,
1648--1661. Wightman, F. L., & Kistler, D. J. (1998). Of Vulcan ears,
human ears and "earprints." Nature Neuroscience, 1, 337--339. Wilkie, R.
M., & Wann, J. P. (2003). Eye-movements aid the control of locomotion.
Journal of Vision, 3, 677--684. Willander, J., & Larsson, M. (2007).
Olfaction and emotion: The case of autobiographical memory. Memory and
Cognition, 35, 1659--1663. Williams, J. H. G., Whiten, A., Suddendorf,
T., & Perrett, D. I. (2001). Imitation, mirror neurons and autism.
Neuroscience and Biobehavioral Reviews, 25, 287--295. Wilson, D. A.
(2003). Rapid, experience-induced enhancement in odorant discrimination
by anterior piriform cortex neurons. Journal of Neurophysiology, 90,
65--72. Wilson, D. A., & Stevenson, R. J. (2006). Learning to smell.
Baltimore: Johns Hopkins University Press. Wilson, D. A., & Sullivan, R.
M. (2011). Cortical processing of odor objects. Neuron, 72, 506--519.
Wilson, D. A., Best, A. R., & Sullivan, R. M. (2004). Plasticity in the
olfactory system: Lessons for the neurobiology of memory.
Neuroscientist, 10, 513--524. Wilson, D. A., Xu, W., Sadrian, B.,
Courtiol, E., Cohen, Y., & Barnes, D. (2014). Cortical odor processing
in health and disease. Progress in Brain Research, 208, 275--305.
Wilson, J. R., Friedlander, M. J., & Sherman, M. S. (1984).
Ultrastructural morphology of identified X- and Y-cells in the cat's
lateral geniculate nucleus. Proceedings of the Royal Society, 211B,
411--436. Wilson, S. M. (2009). Speech perception when the motor system
is compromised. Trends in Cognitive Sciences, 13(8), 329--330. 470

Winawer, J., Huk, A. C., & Boroditsky, L. (2008). A motion aftereffect
from still photographs depicting motion. Psychological Science, 19,
276--283. Winkler, I., Haden, G. P., Landinig, O., Sziller, I., &
Honing, H. (2009). Newborn infants detect the beat in music. Proceedings
of the National Academy of Sciences, 106(7), 2468--2471. Winston, J. S.,
O'Doherty, J., Kilner, J. M., Perrett, D. I., & Dolan, R. J. (2007).
Brain systems for assessing facial attractiveness. Neuropsychologia, 45,
195--206. Wissinger, C. M., VanMeter, J., Tian, B., Van Lare, J., Pekar,
J., & Rauschecker, J. P. (2001). Hierarchical organization of the human
auditory cortex revealed by functional magnetic resonance imaging.
Journal of Cognitive Neuroscience, 13, 1--7. Witt, J. K. (2011a).
Action's effect on perception. Current Directions in Psychological
Science, 20, 201--206. Witt, J. K. (2011b). Tool use influences
perceived shape and parallelism: Indirect measures of perceived
distance. Journal of Experimental Psychology: Human Perception and
Performance, 37, 1148--1156. Witt, J. K., & Dorsch, T. (2009). Kicking
to bigger uprights: Field goal kicking performance influences perceived
size. Perception, 38, 1328--1340. Witt, J. K., & Proffitt, D. R. (2005).
See the ball, hit the ball: Apparent ball size is correlated with
batting average. Psychological Science, 16, 937--938. Witt, J. K., &
Sugovic, M. (2010). Performance and ease influence perceived speed.
Perception, 39, 1341--1353. Witt, J. K., Linkenauger, S. A., Bakdash, J.
Z., Augustyn, J. A., Cook, A. S., & Proffitt, D. R. (2009). The long
road of pain: Chronic pain increases perceived distance. Experimental
Brain Research, 192, 145--148. Witt, J. K., Proffitt, D. R., & Epstein,
W. (2010). When and how are spatial perceptions scaled? Journal of
Experimental Psychology: Human Perception and Performance, 36,
1153--1160. Witzel, C., Maule, J., & Franklin, A. (2019). Red, yellow,
green, and blue are not particularly colorful. Journal of Vision,
19(14):27, 1--26. Wolpert, D. M., & Flanagan, J. R. (2001). Motor
prediction. Current Biology, 11(18), R729--R732. Wolpert, D. M., &
Ghahramani, Z. (2005). Bayes rule in perception, action and cognition.
The Oxford Companion to the Mind. Oxford University Press. Womelsdorf,
T., Anton-Erxleben, K., Pieper, F., & Treue, S. (2006). Dynamic shifts
of visual receptive fields in cortical area MT by spatial attention.
Nature Neuroscience, 9, 1156--1160. Woo, C.-W., Koban, L., Kross, E.,
Lindquist, M. A., Banich, M. T., Ruzic, L., et al. (2014). Separate
neural representations for physical pain and social rejection. Nature
Communications, 5, Article 5380. doi:10.138/ncomms6380. Woods, A. J.,
Philbeck, J. W., & Danoff, J. V. (2009). The various perception of
distance: An alternative view of how effort affects distance judgments.
Journal of Experimental Psychology: Human Perception and Performance,
35, 1104--1117. Wozniak, R. H. (1999). Classics in psychology,
1855--1914: Historical essays. Bristol, UK: Thoemmes Press. Wurtz, R. H.
(2013). Corollary discharge in primate vision. Scholarpedia, 8(10),
12335. Wurtz, R. H. (2018). Corollary discharge contributions to
perceptual continuity across saccades. Annual Review of Visual Science,
4, 215--237. Yang, J. N., & Shevell, S. K. (2002). Stereo disparity
improves color constancy. Vision Research, 47, 1979--1989. Yarbus, A. L.
(1967). Eye movements and vision. New York: Plenum Press. Yau, J. M.,
Pesupathy, A., Fitzgerald, P. J., Hsiao, S. S., & Connon, C. E. (2009).
Analogous intermediate shape coding in vision and touch. Proceedings of
the National Academy of Sciences, 106, 16457--16462. Yonas, A., &
Granrud, C. E. (2006). Infants' perception of depth from cast shadows.
Perception and Psychophysics, 68, 154--160. Yonas, A., & Hartman, B.
(1993). Perceiving the affordance of contact in four- and five-month old
infants. Child Development, 64, 298--308.

References

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Yonas, A., Pettersen, L., & Granrud, C. E. (1982). Infant's sensitivity
to familiar size as information for distance. Child Development, 53,
1285--1290. Yoshida, K. A., Iverson, J. R., Patel, A. D., Mazuka, R.,
Nito, H., Gervain, J., & Werker, J. F. (2010). The development of
perceptual grouping biases in infancy: A Japanese-English
cross-linguistic study. Cognition, 115, 356--361. Yoshida, K., Saito,
N., Iriki, A., & Isoda, M. (2011). Representation of others' action by
neurons in monkey medial frontal cortex. Current Biology, 21, 249--253.
Yost, W. A. (1997). The cocktail party problem: Forty years later. In R.
H. Kilkey & T. R. Anderson (Eds.), Binaural and spatial hearing in real
and virtual environments (pp. 329--347). Hillsdale, NJ: Erlbaum. Yost,
W. A. (2001). Auditory localization and scene perception. In E. B.
Goldstein (Ed.), Blackwell handbook of perception (pp. 437--468).
Oxford, UK: Blackwell. Yost, W. A. (2009). Pitch perception. Attention,
Perception and Psychophysics, 71, 1701--1705. Yost, W. A., & Zhong, X.
(2014). Sound source localization identification accuracy: Bandwidth
dependencies. Journal of the Acoustical Society of America, 136(5),
2737--2746. Young-Browne, G., Rosenfield, H. M., & Horowitz, F. D.
(1977). Infant discrimination of facial expression. Child Development,
48, 555--562. Young, R. S. L., Fishman, G. A., & Chen, F. (1980).
Traumatically acquired color vision defect. Investigative Ophthalmology
and Visual Science, 19, 545--549. Young, T. (1802). The Bakerian
Lecture: On the theory of light and colours. Philosophical Transactions
of the Royal Society of London, 92, 12--48. Youngblood, J. E. (1958).
Style as information. Journal of Music Therapy, 2, 24--35. Yu, C., &
Smith, L. B. (2016). The social origins of sustained attention in
one-year-old human infants. Current Biology, 26(9), 1235--1240. Yu, C.,
Suanda, S. H., & Smith, L. B. (2018). Infant sustained attention but not
joint attention to objects at 9 months predicts vocabulary at 12 and 15
months. Developmental Science, 22(1), 22:e12735, 1--12. Yuille, A., &
Kersten, D. (2006). Vision as Bayesian inference: Analysis by synthesis?
Trends in Cognitive Sciences, 10, 301--308. Yuodelis, C., & Hendrickson,
A. (1986). A qualitative and quantitative analysis of the human fovea
during development. Vision Research, 26, 847--855. Zacks, J. M., &
Swallow, K. M. (2007). Event segmentation. Current Directions in
Psychological Science, 16, 80--84.

Zacks, J. M., & Tversky, B. (2001). Event structure in perception and
conception. Psychological Bulletin, 127(1), 3--27. Zacks, J. M., Braver,
T. S., Sheridan, M. A., Donaldson, D. I., Snyder, A. Z., Ollinger, J.
M., et al. (2001). Human brain activity time-locked to perceptual event
boundaries. Nature Neuroscience, 4, 651--655. Zacks, J. M., Kumar, S.,
Abrams, R. A., & Mehta, R. (2009). Using movement and intentions to
understand human activity. Cognition, 112, 201--206. Zampini, M., &
Spence, C. (2010). Assessing the role of sound in the perception of food
and drink. Chemical Perception, 3, 57--67. Zatorre, R. J. (2013).
Predispositions and plasticity in music and speech learning: Neural
correlates and implications. Science, 342, 585--589. Zatorre, R. J.
(2018). From perception to pleasure: Musical processing in the brain.
Presentation at The Amazing Brain Symposium, Lunds University. Zatorre,
R. J., Chen, J. L., & Penhune, V. B. (2007). When the brain plays music:
Auditory-motor interactions in music perception and production. Nature
Reviews Neuroscience, 8, 547--558. Zeidan, F., & Vago, D. (2016).
Mindfulness meditation-based pain relief: A mechanistic account. Annals
of the New York Academy of Sciences, 1373(1), 114--127. Zeidman, P.,
Mulally, S. L., Schwarzkopf, S., & Maguire, E. A. (2012). Exploring the
parahippocampal cortex response to high and low spatial frequency
spaces. Neuroreport, 23, 503--507. Zeki, S. (1983a). Color coding in the
cerebral cortex: The reaction of cells in monkey visual cortex to
wavelengths and colours. Neuroscience, 9, 741--765. Zeki, S. (1983b).
Color coding in the cerebral cortex: The responses of
wavelength-selective and color coded cells in monkey visual cortex to
changes in wavelength composition. Neuroscience, 9, 767--781. Zeki, S.
(1990). A century of cerebral achromatopsia. Brain, 113, 1721--1777.
Zellner, D. A., Bartoli, A. M., & Eckard, R. (1991). Influence of color
on odor identification and liking ratings. American Journal of
Psychology, 104, 547--561. Zhang, T., & Britten, K. H. (2006). The
virtue of simplicity. Nature Neuroscience, 9, 1356--1357. Zhao, G. Q.,
Zhang, Y., Hoon, M., Chandrashekar, J., Erienbach, I., Ryba, N. J. P.,
et al. (2003). The receptors for mammalian sweet and umami taste. Cell,
115, 255--266. Zihl, J., von Cramon, D., & Mai, N. (1983). Selective
disturbance of movement vision after bilateral brain damage. Brain, 106,
313--340. Zihl, J., von Cramon, D., Mai, N., & Schmid, C. (1991).
Disturbance of movement vision after bilateral brain damage. Brain, 114,
2235--2252.

References

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

471

Name Index Aartolahti, E., 152 Aarts, H., 413 Abbott, J. T., 213, 226
Abdollahi, R. O., 361 Abell, F., 177 Abramov, I., 62 Abrams, J., 4, 134
Abrams, R. A., 167, 177 Acharyya, M., 120 Acroff, J. M., 305 Addams, R.,
179 Adelson, E. H., 221 Adolph, K. E., 169, 170, 171 Adorni, R., 154
Aerts, P., 152 Aglioti, S. M., 358, 380 Agostini, T., 221 Aguirre, G.
K., 113 Ahad, P., 32 Ajuber, L. A., 399, 400 Alain, C., 299 Alam, R. I.,
395 Albenau, D. F., 403 Alcaide, J., 410 Alers, A., 128 Allain, P., 313
Allison, T., 84, 189 Alpern, M., 209 Altenmüller, E., 376 Altschuler, E.
L., 167 Amanzio, M., 378 Aminoff, E. M., 113 Andersen, R. A., 161
Anderson, A. W., 117, 118 Anderson, C. H., 75 Andruski, J. E., 354
Angelerques, R., 111 Anstis, S., 192 Ansuini, C., 177 Anton, J.-L., 379,
380 Anton-Erxleben, K., 73, 134, 136 Appelle, S., 105 Arcaroli, J., 352
Arduino, C., 378 Armony, J. L., 327 Arndt, P. L., 352 Arnott, S. R.,
299, 307 Arroyo, M. E., 354 Arshamian, A., 407 Arzi, A., 404 Ashley, R.,
316 Ashmore, J., 279 Aslin, R., 257 Aslin, R. N., 143, 257 Assainte, C.,
178

Atkinson, J., 62 Atlas, L. Y., 382 Augustyn, J. A., 168 Austin, J. H.,
143 Avan, P., 279 Avanzini, P., 361 Avenanti, A., 358, 380 Axel, R.,
401, 407 Aydelott, J., 342 Azim, E., 162 Azzopardi, P., 76 Baars, B.J.,
36 Bach, M., 252 Backhaus, W., 224 Backlund, H., 371 Baddeley, A. D.,
140 Bailey, C., 317, 324 Baird, A., 314 Baird, J. A., 168 Baird, J. C.,
256 Bajo, V. M., 299 Bakdash, J. Z., 168 Baker, C. I., 113 Baker, J.,
336 Baldassano, C., 113 Ballard, C., 133 Ballard, D. H., 109, 133
Banich, M. T., 382 Banks, M. S., 62 Bar, M., 109, 113 Bara-Jimenez, 383
Barber, S., 333 Barbot, A., 134 Bardy, B. G., 152 Bargones, J. Y., 286
Barker, D., 281, 285, 286 Barks, A., 140 Barlow, H. B., 27, 56, 183,
192, 243 Barnes, D., 407 Barnes, J., 191 Barnes, P. M., 143 Barrett, H.
C., 177, 178 Barry, S. R., 236, 237 Barsalou, L.W., 143 Bartlett, M. D.,
375 Bartoli, A. M., 413 Bartoshuk, L. M., 391, 396, 408 Bartrip, J., 118
Basbaum, A. I., 374 Basso, J. C., 143 Batelli, L., 190 Bates, E., 189,
349 Bathini, P., 399, 400 Batson, C. D., 380

Battelli, L., 190 Battit, G. E., 375 Baudoux, S., 297 Bauer, J., 257
Baulac, M., 326 Baumeister, R. F., 381 Bava, C. M., 398 Bayes, T., 108,
109 Baylis, L. L., 409 Baylor, D., 46 Beall, A. C., 154, 155 Beaton, S.,
189 Beauchamp, G. K., 390, 396, 398, 414 Becchio, C., 177, 384 Bechtold,
A. G., 62 Beck, C. J., 143 Beck, D. M., 113 Beckers, G., 185 Beckett,
C., 326 Becklen, R., 137 Bedford, R., 259 Beecher, H. K., 373
Begliomini, C., 342 Behrend, O., 297 Behrmann, M., 119, 183 Beilock, S.,
149 Beiser, A. S., 61 Békésy, G. von. 276, 277, 280 Beland, S. L., 407
Belfi, A. M., 313 Belin, P., 32 Bender, D. B., 83, 84 Bendor, D., 270,
282, 283 Benedetti, F., 375, 378 Benjamin, L., 19 Benjamini, Y., 115
Bennett, P. J., 62, 98 Benovoy, M., 326 Bensmaia, S. J., 366, 367
Benson, R. R., 110 Bente, G., 167 Benuzzi, F., 167 Beranek, L., 301
Beresford, M. K., 398 Berger, K. W., 271 Berger, Z., 143 Bergeson, T.
R., 354 Berkowitz, A., 362 Berman, M. G., 381 Bersick, M., 323
Bertamini, M., 187 Bess, F. H., 273 Besson, M., 324 Best, A. R., 407
Bethge, M., 91

472

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Bharucha, J., 319 Bhaskar, S. T., 407 Biederman, I., 102 Bilalić, M.,
118 Bilinska, K., 399 Bingel, U., 375 Biondi, F., 139 Birch, E., 257
Bird, G., 167 Birnbaum, M., 390 Bizley, J. K., 299 Black, M., 169 Blake,
R., 189, 192, 243 Blakemore, C., 74, 243 Blaser, E., 180 Blass, E. M.,
385 Block, L., 413 Block, N., 36 Blood, A. J., 313, 325 Bloom, M. L.,
407 Blythe, P., 177, 178 Bocanegra, J., 139 Boco, A., 161, 162 Boell,
E., 368 Bohlander, R. W., 140 Bojanowski, V., 389 Bonato, F., 221
Bookheimer, S. Y., 167 Boring, E., 247--249 Boring, E. G., 95 Borji, A.,
131 Born, R. T., 187 Bornstein, M. H., 225 Boroditsky, L., 191 Borst,
A., 183 Bos, M. W., 139, 140 Bosco, A., 162 Bosker, B., 140 Bosten, J.
M., 213, 226 Botvinick, M. M., 382 Boucart, M., 113 Boughter, J. D.,
Jr., 395 Bouvard, M., 178 Bouvier, S. E., 214 Bouwer, F. L., 317
Bowmaker, J. K., 51, 205 Bowtell, R., 410 Boynton, G. M., 362, 364
Boynton, R. M., 209, 216 Braddick, O., 62 Brai, E., 399, 400 Brainard,
D., 217, 218 Brammer, M., 191 Bran, A., 297 Brattio, E., 313 Braver, T.
S., 177 Bregman, A. S., 302, 303 Bremmer, F., 307 Bremner, A. J., 384
Brendt, M. R., 354 Brennan, P. A., 119 Bresin, R., 332 Breslin, P. A.
S., 391, 396, 397 Bresson, M., 313 Breveglieri, R., 161, 162 Brewer, A.
A., 76 Bridge, H., 243 Britten, K. H., 184, 185, 188

Broadbent, D., 125 Broca, P. P., 31, 348 Brockmole, J., 140 Brockmole,
J. R., 167 Bromley, K., 395 Brown, A. E., 300 Brown, C. A., 287 Brown,
P. K., 50, 205 Brown, S. D., 341 Brownell, W. E., 279 Bruce, V., 111
Bruno, N., 187 Buccino, G., 166, 167 Buck, L., 401 Budd, K., 138
Buechel, C., 315 Bueti, D., 358, 380 Bufalari, I., 342 Bufe, B., 396
Buhle, J. T., 376 Bukach, C. M., 118 Buletic, Z., 396, 397 Bulf, H., 193
Bullmore, E. T., 191 Bunch, C. C., 284 Bundesen, C., 180 Burke, J. F.,
158 Burns, V., 281 Burton, A. M., 111 Bushdid, C., 397 Bushnell, C. M.,
361, 376 Bushnell, E. W., 62 Bushnell, I. W. R., 118, 119 Bushnell, M.
C., 374, 377 Busigny, T., 116--117 Butowt, R., 399 Bykowski, C., 399
Byl, N., 383 Bzdok, D., 167 Caan, W., 84 Caggiano, V., 167 Cain, W. S.,
396, 398, 408 Callaway, E. M., 79 Campbell, F. W., 105 Campbell, J., 303
Campos, J., 155 Can, D. D., 354 Canessa, N., 167 Cannon, P. R., 372 Cao,
J., 396 Caplan, J. B., 158 Capozzi F., 177 Caprio, J., 403 Caramazza,
A., 111 Cardello, A. V., 413 Carlson, L., 156 Carlson, N. R., 6 Carlyon,
R. P., 352, 353 Carrasco, M., 4, 134 Carrier, L. M., 140 Carrougher, G.
J., 376 Carter, E. A., 286 Cartwright-Finch, U., 137 Carvalho, F. R.,
412 Casagrande, V. A., 68 Cascio, C. J., 384, 385 Casile, A., 167

Caspers, S., 165 Cassanto, D., 327 Castelhano, M. S., 104, 130, 131, 198
Castelli, F., 177 Castiello, U., 384 Castro, J. B., 397 Cataliotti, J.,
221 Catchpole, G., 213, 226 Catlan, M. J., 383 Catmur, C., 167 Cattaneo,
L., 165 Cavallo, A., 177 Cavallo, A. K., 177 Cavanagh, P., 129, 180, 190
Ceko, M., 361, 376 Centelles, L., 178 Cerf, M., 85 Chabris, C. F., 137
Chambers, C., 220 Chan, G. S. W., 155 Chanda, M. L., 313, 326 Chandler,
R., 345 Chandrashekar, J., 394, 395 Chang, L. J., 382 Chapman, C. R.,
377 Charles, E. R., 81 Charpientier, A., 163 Chatterjee, S., 212
Chatterjee, S. H., 188 Cheever, N. A., 140 Chen, C.-F. F., 405 Chen, F.,
198 Chen, J., 91 Chen, J. L., 315, 323 Cheng, J.-W., 367 Cheong, D., 183
Cherry, E. C., 124 Chersi, F., 166 Cheung, O. S., 109 Chevreul, E., 58
Chi, Q., 399 Chiarelli, A. M., 119 Chistovich, I. A., 354 Chiu, Y.-C.,
135 Chobert, J., 313 Choi, G. B., 407 Cholewaik, R. W., 363 Christensen,
L. O. D., 162 Christopher, P., 39 Chun, M. M., 84, 110, 111, 117
Churchland, P. S., 43 Cirelli, L. K., 329 Cisek, P., 149 Clark, J. J.,
138 Clark, S., 377 Clarke, F. F., 320 Clarke, S., 299 Clarke, T. C., 143
Clement, S., 313 Coakley, J. D., 278 Coan, J. A., 379 Cohen, A. J., 319,
320 Cohen, Y., 407 Coleman, J., 139 Coley, B. F., 39 Collett, T. S., 246
Collins, A. A., 363 Colloca, L., 375 Name Index

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

473

Colombo, A., 67 Colombo, M., 67 Coltheart, M., 232 Comèl, M., 358
Connolly, J. D., 161 Contreras, R. J., 393, 395 Conway, B., 218 Conway,
B. R., 212, 214, 243, 244 Cook, A. S., 168 Cook, R., 167 Coon, H., 396
Cooper, F. S., 338, 342 Cooper, G. G., 74 Cooper, J. M., 139 Coppola, D.
M., 12, 75, 105 Corbeil, M., 312, 329 Cosgriff, A., 352 Cosser, S., 412
Coulson, A. H., 209 Courtiol, E., 407 Cowart, B. J., 414 Cowey, A., 76,
198 Cowperthwaite, B., 287 Craig, J. C., 363, 364 Craven, B., 397 Crick,
F. C., 36 Crisinel, A-S., 412 Critchley, H. D., 409, 411 Crouzet, S. M.,
116 Croy, I., 389 Cruickshanks, K. J., 285 Csibra, G., 177 Culler, E.
A., 278 Cumming, B., 242 Cumming, B. G., 243 Curran, T., 118 Cutforth,
T., 407 Cutting, J. E., 231, 235, 341 Da Cruz, L., 39 Dagher, A., 326
Dagnelie, G., 39 Dallos, P., 275, 279 Dalton, D. S., 285 Dammann, J. F.,
367 D'Aniello, G. E., 154 Dannemiller, J. L., 226 Danoff, J. V., 168
Dapretto, M., 167 Dartnall, H. J. A., 51, 205 Darwin, C., 312, 331
Darwin, C. J., 302 DaSilva, J. A., 154 Datta, R., 135, 136 D'Ausilio,
A., 342 Davatzikos, C., 120 David, A. S., 191 Davidovic, M., 372 Davies,
M. S., 167 Davies, R. L., 225 Davis, H., 278--279 Davis, M. H., 347, 348
Davoli, C. C., 167 Day, R.H., 254 de Araujo, I. E., 410 de Gardelle, V.,
220 de Haas, B., 306 de Heer, F., 191 de Juan Jr, E., 39 474

De Santis, L., 299 de Schonen, S., 118 De Vitis, M., 162 Dean, J. L.,
326 DeAngelis, G. C., 243 DeCasper, A. J., 287, 353 Decdety, J., 380
Delay, E. R., 395 Deliege, I., 319 Deligianni, F., 3 DeLucia, P., 253
Delwiche, J. F., 396, 397 Delwihe, J. F., 410 Denes, P. B., 10, 274
Derbyshire, S. W. G., 377 Deruelle, C., 118 Desor, J. A., 398 deSouza,
J., 224 D'Esposito, M., 35, 113 Deubel, H., 129 Deusdedit, B. N., 389
Deutsch, D., 304, 305, 319 DeValois, R. L., 211 Devanand, D. P., 399,
400 Devlin, J. T., 342 DeWall, C. N., 381 Dewey, K. G., 169 deWied, M.,
376 DeYoe, E. A., 135, 136 Di Gangi, V., 119 Diamond, I. T., 299
DiCarlo, J., 370 Dick, F., 349 Dierkes, K., 279 Dilks, D. D., 119
Dingus, T. A., 138 Divenyi, P. L., 319 Divvala, S., 90, 91 Djourno, A.,
351 Dobelle, W. H., 205 Dobson, K. S., 119 Dobson, V., 62 Doctor, J. N.,
376 Dodds, L. W., 279, 285 Dolan, R. J., 315, 380 Domoff, S. E., 140
Donaldson, D. I., 177 Dooling, R. J., 341 Dorn, J., 39 Dorsch, T., 168
Doubell, T. P., 296 Doucette, W., 403 Dougherty, R. F., 76 Dowling, W.
J., 305 Downing, P. E., 135 Drain, H. M., 116 Drayna, D., 396 Drga, V.,
284 Driver, J., 17, 141 Dronkers, N., 349 Droulez, J., 176 Dube, L., 313
DuBose, C. N., 413 Dubow, E. F., 140 Duchon, A. P., 155 Duke, F. F., 399
Duke, K., 139, 140 Dumais, S. T., 143, 257 Dumas, G., 379

Dumoulin, S. O., 76 Dunbar, R. I. M., 312 Duncan, E., 143 Duncan, G. H.,
374, 377 Duncan, R. O., 362, 364 Durgin, F. H., 155, 168 Durrani, M.,
204 Durrant, J., 273 Eames, C., 266 Eberhardt, J. L., 119 Ebinger, K.,
352 Eckard, R., 413 Eerola, T., 332 Egan, R., 183 Egbert, L. D., 375
Egelhaaf, M., 183 Egeth, H., 127 Egly, R., 17 Egré, P., 220 Ehrenstein,
W., 100 Eickhoff, S. B., 189 Eickoff, S. B., 165 Eimas, P. D., 118, 340,
341 Eisenberger, N. I., 377, 381 Ekelid, M., 348 Ekstrom, A. D., 158,
165 El Haj, Mohamad, 313 Elbert, T., 383 Elhilali, M., 303 Eliassen, J.
C., 407 Ellard, C. G., 155 Ellingsen, D-M., 372 Elliot, A. J., 198
Ellis, A. W., 111 Emmert, E., 251 Ende, V., 143 Engel, S. A., 13, 214
Engen, T., 398 Epstein, R., 113 Epstein, R. A., 103, 113, 156 Epstein,
W., 231 Erickson, R., 271 Erickson, R. P., 391, 393, 394 Erienbach, I.,
394 Erlenbach, I., 394 Esteva, A., 113 Etchegoyen, K., 178 Eyries, C.,
351 Fabre-Grenet, M., 118 Fabre-Thorpe, M., 113 Fadiga, L., 164, 342
Fairhurst, M. T., 384 Fan, Y., 120 Fantana, A. L., 403 Farah, M. J., 116
Farhadi, A., 90, 91 Farroni, T., 119 Fasotti, L., 313 Fattori, P., 161,
162 Fechner, G., 14 Fedorenko, E., 327 Fei-Fei, L., 4, 90, 103, 104, 113
Feldman, D. E., 362 Fernald, A., 353, 354 Fernald, R. D., 40 Ferrari, P.
F., 166, 167 Ferreri, L., 326

Name Index

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Fettiplace, R., 279 Fieguth, P., 91 Field, G. D., 212 Field, T., 385
Fields, H. L., 374 Fields, T. A., 158 Fifer, W. P., 287, 353 Filley, E.,
39 Finger, T. E., 393, 403 Fink, G. R., 315 Fink, S. I., 137 Finniss, D.
G., 375 Firberg, A., 332 Firestone, L. L., 377 Fischer, B., 76 Fisher,
J. F., 299 Fishman, G. A., 198 Fitch, W. T., 312, 327, 328 Fitzhigh, R.,
56 Flanagan, J. R., 162 Fleischmann, A., 407 Fleisher, L., 383 Fletcher,
H., 269 Flor, H., 374 Flynn, P., 94 Fogassi, L., 164, 165, 166, 167,
379, 380 Fogel, A. R., 321, 323 Forestell, C. A., 414 Fornazieri, M. A.,
389 Fortenbaugh, F. C., 151 Foster, D. H., 217 Fox, C. R., 152 Fox, R.,
143, 257 Franchak, J. M., 170 Francis, S., 410 Franck, K. R., 278
Francois, C., 313 Frank, M. E., 393, 408, 409 Frankland, B. W., 319, 320
Franklin, A., 213, 225, 226 Freeman, R. D., 243 Freire, A., 117, 192
Freiwald, W. A., 84 Freyd, J., 188, 190, 191 Fried, I., 28, 29, 85, 165
Friederici, A. D., 324 Friedlander, M. J., 68 Friedman, H. S., 213
Friedman, J. J., 376 Frisina, R. D., 269 Friston, K., 323, 324, 325
Friston, K. J., 315 Frith, C., 177 Frith, C. D., 380 Frith, U., 177
Fritz, T., 312, 376 Fujita, N., 154 Fukuda, S., 92 Fuld, K., 256 Fuller,
S., 134 Fulusima, S. S., 154 Furey, M. L., 117 Furmanski, C. S., 13
Furness, T. A. III, 376 Fyhn, M., 157 Gabrieli, J., 119 Gabrieli, J. E.
E., 119 Gagliese, L., 373

Galati, G., 358, 380 Gall, F.J., 30 Gallace, A., 371 Gallant, J. L.,
111, 112, 115 Gallese, V., 164, 165, 166, 167, 379, 380 Galletti, C.,
161, 162 Ganel, T., 83 Gao, T., 177 Gao, X., 318 Garcea, F. E., 340
Gardner, M. B., 296 Gardner, R. S., 295 Garland, J., 138 Gauthier, I.,
117, 118 Gazzola, V., 379, 380 Gegenfurtner, K. R., 198, 212, 217 Geha,
P., 410 Geiger, A., 167 Geirhos, R., 91 Geisler, W. S., 108 Gelade, G.,
142 Gelbard-Sagiv, H., 85 Georgopoulis, A. P., 81 Gerber, J. C., 407
Gerbino, W., 100 Gerkin, R. C., 397 Gerloff, C., 383 Gernsbacher, M. A.,
349 Gervain, J., 318 Gescheider, G. A., 269 Gesierich, B., 166 Geuter,
S., 379 Ghahramani, Z., 108 Ghahremani, G., 119 Ghazban, N., 312, 329
Ghosh, S., 395 Giampietro, V., 191 Gibson, B. S., 6, 101 Gibson, E., 324
Gibson, J. J., 150, 152, 153, 169, 180, 181, 369 Gigone, K., 155
Gilaie-Dotan, S., 183 Gilbert, C. D., 69, 81, 86 Gilbert, D. T., 143
Gilchrist, A., 6, 220, 221 Gill, S. V., 171 Girshick, R., 90, 91
Gitelman, D. R., 410 Giza, B. K., 395 Glanz, J., 301 Glasser, D. M., 179
Gneezy, A., 139, 140 Gobbini, M. I., 117 Goffaux, V., 104 Golarai, G.,
119 Gold, J. E., 140 Gold, T., 279 Goldberg, M. E., 128 Goldin-Meadow,
S., 354 Goldstein, A., 322 Goldstein, E. B., 137, 140 Goldstein, J., 189
Goldstein, P., 379 Golinkoff, R. M., 354 Goncalves, N. R., 242 Goodale,
M. A., 81, 82, 83, 160, 161, 167, 168, 307 Gordon, J., 62, 212 Gore, J.
C., 84, 117, 118

Gosselin, N., 326, 376 Gottfried, J. A., 407 Gould, N. F., 143 Goyal,
M., 143 Grady, C. L., 299 Graham, C. H., 209 Graham, D.N., 39 Graham,
S., 299 Grahn, J. A., 314, 315, 316, 317 Granrud, C. E., 257, 258
Graves, J., 220 Gray, L., 385 Greenberg, D. M., 313 Greenburg, M., 168
Greenlee, M. W., 80 Greggers, U., 224 Gregory, A. H., 319 Gregory, R.
L., 250, 252, 253, 254 Griffin, D. R., 246 Griffiths, T. D., 284, 326
Grill-Spector, K., 110, 119 Grindley, M., 367 Grodd, W., 118 Grosbras,
M. H., 189 Gross, C. G., 27, 67, 83, 84 Gross, N., 217, 278 Grossman, E.
D., 189, 190 Grossmann, T., 384 Grothe, B., 297 Grothe, R., 298 Gruber,
H. E., 256 Gulick, W. L., 269 Gunter, T., 324 Gunter, T. C., 324 Gupta,
G., 217 Gurden, H., 405 Gurney, H., 204 Gwiazda, J., 257 Gyulia, F., 377
Haake, R. J., 257, 258 Haber, R. N., 250 Haberman, J. M., 317 Hackney,
C. M., 279 Hadad, B.-S., 192 Haden, G. P., 329 Hafting, T., 157
Hagbarth, K-E., 371 Hagler, D. J., Jr., 189 Hagoort, P., 327 Haigney,
D., 138 Hainline, L., 62 Hains, S. M. J., 287 Hakkinen, A., 152 Hall, D.
A., 281, 284 Hall, M. J., 396 Hallemans, A., 152 Hallett, M., 383
Halpin, C. F., 286 Haltom, K. E. B., 381 Hamer, R. D., 46 Hamid, S. N.,
156 Hamilton, C., 367 Handford, M., 126 Hannes, P. S., 367 Hanowski, R.
J., 139 Hansen, T., 217 Hao, L., 151, 155 Happé, F., 177 Name Index

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

475

Harding-Forrester, S., 362 Harel, M., 85 Harré, R., 315, 316, 319
Harris, A., 113 Harris, J. M., 154 Harris, L., 62 Harris, L. R., 318
Hart, B., 354 Hartline, H. K., 55, 56 Hartman, B., 258 Hartung, K., 307
Harvey, M., 141 Harwood, D. L., 305 Hasboun, D., 326 Hasenkamp, W., 143
Hasson, U., 308 Hawken, M. J., 213 Haxby, J. V., 33, 81, 111, 117
Hayhoe, M., 133, 156 Hayhoe, M. M., 131, 133 He, J. L., 385 Heaton, P.,
314 Hecaen, H., 111 Heck, G. L., 395 Heesen, R., 204 Heider, F., 177
Heise, G. A., 304 Held, R., 257 Helmholtz, H. von., 107, 128, 168, 204
Henderson, J. M., 103, 104, 130, 131, 132, 133, 198 Hendrickson, A., 62
Hendriks, M., 413 Hendrix, C. L., 119 Henrich, C., 134 Henriksen, S.,
242, 245 Hering, E., 210, 211 Hermann, K. L., 218 Hernandez, N. P., 395
Hershenson, M., 256 Hertel, H., 224 Hervais-Adelman, A., 347 Herz, R.
S., 407 Hester, L., 39 Hettinger, T. P., 408, 409 Hevenor, S., 299
Heyes, C., 167 Heywood, C. A., 198 Hickman, J. S., 139 Hickock, G., 167,
343 Hickok, G. S., 340 Hicks, J. C., 151, 155 Hill, R. M., 183, 192
Hinton, G. E.,, 33 Hirsch, H. V. B., 243 Hirsh, I. J., 319 Hirsh-Pasek,
K., 354 Hirstein, W., 373 Hitch, G. J., 140 Hitchcock, A., 322 Hoch, J.
E., 170 Hochberg, J., 253 Hodgetts, W. E., 285 Hoeksma, J. B., 197 Hof
bauer, R. K., 374, 377 Hoff, E., 144 Hoffman, H. G., 376 Hoffman, T.,
141 Hoffmann, K.-P., 307 476

Hofman, P. M., 295 Holcomb, P. J., 324 Holland, R. W., 413 Hollingworth,
A., 103 Hollins, M., 366 Holmes, J. D., 314 Holt, L. L., 340 Holway,
A.H., 247--249 Homberg, V., 185 Honey, C. J., 308 Honig, H., 317 Honing,
H., 329 Hoon, M., 394 Hoon, M. A., 394, 395 Horn, D. L., 354 Horowitz,
F. D., 118 Horwitz, D., 212 Horwood, J., 155 Hosseini, R. B., 385
Houser, B., 218 Houston, D. M., 354 Howard, D., 326 Howgate, S., 285
Hsia, Y., 209 Hsiao, S. S., 370 Huang, L., 396 Huang, X., 336 Huang,
Z.-W., 285 Hubbard, E. M., 167 Hubel, D. H., 56, 69, 70, 71, 77, 78,
183, 243, 244 Hughes, M., 320 Huk, A. C., 191 Hum, M., 140 Humayun, M.,
39 Humayun, M. S., 39 Humes, L. E., 273 Hummel, T., 389, 407, 410
Humphrey, A. L., 68 Humphrey, G. K., 81 Humphreys, G. W., 153 Humphreys,
K., 119 Hunter, D., 398 Hurlbert, A., 217 Huron, D., 319, 320, 323
Hurvich, L. M., 211 Hutchinson, W., 320 Huth, A. G., 111, 112
Huttenbrink, K.-B., 410 Hyvärinin, J., 370 Iacoboni, M., 165, 166, 167
Iannilli, E., 407 Ilg, U. J., 185 Inagaki, T. K., 381 Isard, S., 345
Ishai, A., 33, 117 Isham, E. A., 158 Ittelson, W. H., 254 Itti, L., 130,
131 Iversen, J. R., 318 Iwamura, Y., 369, 370 Iwanaga, M., 325 Iyer, A.,
103 Izard, C. E., 118 Jackendoff, R., 312, 315 Jackson, J. H., 361
Jacobs, J., 158

Jacobson, A., 220 Jacques, C., 104 Jacquot, M., 412 Jaeger, S. R., 398
Jagnow, C. P., 414 Jakubowska, P., 399 Jalkanen, L., 306 James, W., 134
Jameson, D., 211 Janata, P., 313, 317 Janzen, G., 156, 157 Jarvinen, J.
L. P., 46 Jeffress, L. A., 296 Jellema, T., 191 Jenkins, W., 383
Jenkins, W. M., 382 Jensen, T. S., 373 Jentschke, S., 376 Jessell, T.
M., 365 Jewett-Leahy, L., 376 Jia, Y., 398 Jiang, H., 110 Jiang, W., 120
Johansson, G., 178 Johansson, R. S., 364 Johnson, B. A., 403 Johnson, E.
N., 212, 213 Johnson, K. O., 363, 364, 370 Johnson, M. H., 118, 119
Johnson, S. P., 257 Johnson-Laird, P., 332 Johnsrude, I. S., 347
Johnston, R. A., 111 Johnston, W. A., 138, 139 Jones, A. K. P., 377
Jones, R., 412 Jonikatis, D., 129 Jorgenson, E., 396 Julesz, B., 240,
242 Jurewicz, Z. B., 329 Jusczyk, P., 341 Jusczyk, P. W., 340 Kacelnik,
O., 299 Kadohisa, M., 409, 411 Kahana, M. J., 158 Kaiser, A., 242
Kalaska, J. F., 149 Kallman, B. R., 407 Kamath, V., 348 Kamitani, Y.,
115 Kamps, F. S., 119 Kanai, R., 306 Kandel, E. R., 365 Kandel, F. I.,
155 Kanizsa, G., 100 Kanwisher, N., 33, 84, 110, 111, 112, 113, 114,
117, 118, 135, 191, 283, 327 Kanwisher, N. G., 214 Kapadia, M. K., 86
Kaplan, J., 165 Karlan, B., 313 Karpathy, A., 90 Kastner, S., 75 Katz,
D., 366 Katz, J., 373 Kaube, H., 380 Kaufman, J. H., 256 Kaufman, L.,
256

Name Index

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Kauranen, T., 138 Kavšek, M., 257 Kay, B. A., 152, 155 Kayser, C., 350
Keebler, M. V., 288 Keenan, J. P., 167 Keil, K., 244 Keller, A., 397,
399 Kelly, B., 231 Kenemans, J. L., 191 Kennedy, W. A., 110 Kersten, D.,
108, 227, 233 Kessen, W., 225 Kessler, E., 320 Keysers, C., 165, 379,
380 Khan, R. M., 397 Khanna, S. M., 278 Killingsworth, M. A., 143 Kim,
A., 324 Kim, U. K., 396 Kimchi, R., 94 King, A. J., 296, 299 King, S.,
412 King, W. L., 256 Kiper, D. C., 212 Kirchner, H., 116 Kish, D., 308
Kisilevsky, B. S., 287 Kistler, D. J., 293, 296 Kitahara, K., 209
Klatzky, R. L., 367, 369 Klauer, S. G., 138 Kleffner, D. A., 105 Klein,
B., 168 Klein, B. E. K., 285 Klein, R., 285 Kleinschmidt, A., 35
Klimecki, O. M., 380 Knill, D., 233 Knill, D. C., 227 Knopoff, L., 320
Knox, D., 312 Kobal, G., 410 Koban, L., 379, 382 Koch, C., 28, 29, 36,
68, 85, 103, 130 Koch, E. G., 286 Koch, V. M., 76 Koelsch, S., 312, 323,
324, 325, 327, 376 Koenecke, A., 335 Koffka, K., 100 Kogutek, D. L., 314
Kohler, E., 165 Koida, K., 212 Kolb, N., 350 Kollmeier, B., 335 Konkle,
T., 111 Konorski, J., 27 Koppensteiner, M., 178 Kossyfidis, C., 221
Koul, A., 177 Kourtzi, Z., 191 Kozinn, A., 383 Krantz, D. H., 209
Kraskov, A., 85 Kreiman, G., 28, 29, 85 Kretch, K. S., 170 Krishnan, A.,
382 Kristjansson, A., 127 Kross, E., 381, 382

Krug, K., 243 Kruger, L., 374 Kruger, L. E., 369 Krumhansl, C. L., 319,
320 Kuffler, S., 55 Kuffler, S. W., 56, 68 Kuhl, P., 353 Kuhl, P. K.,
341, 354 Kuhn, C., 396 Kujawa, S. G., 285 Kulikowski, J. J., 105 Kumar,
S., 177 Kunert, R., 327 Kuperberg, G. R., 321, 323 Kuznekoff, J. H., 140
Kveraga, K., 113 Kwong, K. K., 110 Laakso, M., 138 LaBarbera, J. D., 118
LaBossiere, 62 Lafaille, P., 32 Lafer-Sousa, R., 214, 218, 243, 244
Lagravinese, G., 167 Lagrois, M-E., 327 Laird, A. R., 165 Lake, E., 335
Lakusta, L., 354 Lamarre, Y., 358, 371 Lamb, T. D., 46 Lamble, D., 138
Lametti, D. R., 342 Lamm, C., 380 Lammers, S., 167 Land, E. H., 217
Land, M. F., 132, 133, 155 Landinig, O., 329 Lane, H., 340 Lang, J. M.,
61 Lange, C., 167 Langers, D. R. M., 284 Langleben, D. D., 120 Langner,
R., 118 Laniece, P., 405 Lappe, M., 155 Larcher, K., 326 Larsen, A., 180
Larson, T., 323 Larsson, M., 407 Laska, M., 397, 398 Launay, J., 312
Laurent, M., 152 Lavie, N., 137 Law, K., 130 Lawless, H., 396, 408 Le
Bel, J. L., 313 Leary, M. R., 381 Lederman, S. J., 367, 369 Lee, C. T.,
287 Lee, D. N., 155 Lee, K., 117 Lee, M. C., 375 Lee, S., 399, 400 Lee,
S. E., 138 LeGrand, Y., 207 Lehman, F. M., 321, 323 Lehtovaara, K., 318
Lei, J-J., 318 Leiberg, S., 380

Leigh, J., 332 Leknes, S., 372 Lemon, R., 167 Leon, M., 403 Leonard, D.
G. B., 278 Leppert, M., 396 Lerdahl, F., 315 Levickm, W. R., 183 Levin,
C. A., 250 Levince, S. C., 354 Levinson, J., 105 Levitan, C. A., 409
Levitin, D. J., 313, 314, 317, 326 Lewis, E. R., 41 Lewis, T. L., 192
Lewis-Peacock, J. A., 143 Li, L., 151 Li, P., 94 Li, W., 69, 81, 396 Li,
X., 221, 396 Liang, C. E., 354 Liao, J., 120 Liberman, A. M., 338, 340,
342 Liberman, M. C., 279, 285 Lieber, J. D., 367 Lieberman, M. D., 381
Lindquist, M. A., 382 Lindsay, P. H., 42, 272 Lindsey, I. B., 143 Ling,
S., 134 Linhares, J. M. M., 203 Linkenauger, S. A., 168 Lister-Landman,
K. M., 140 Litovsky, R. Y., 303 Liu, H., 120 Liu, J., 183 Liu, L., 91
Liu, R., 285 Liu, T., 4, 134 Liu, X., 91 Livingstone, M. S., 84
Lloyd-Fox, S., 119 Logohetis, N. K., 81 Logothetis, N. K., 350 Loken,
L., 384 Loken, L. S., 372 Lomber, S. G., 299 London, J., 317 Lonnroos,
E., 152 Loomis, J. M., 154, 155, 168 Loper, A., 288 Lopez-Poveda, E.,
284 Lopez-Sola, M., 379 Lord, S. R., 152 Lorenzi, L., 183 Lorteije, J.
A. M., 191 Loseth, G., 372 Lotto, A. J., 340 Loughead, J. W., 120
Lovrinic, J., 273 Low, L. A., 361, 376 Lowenstein, W., 365, 366 Lowy,
K., 278 Ludington-Hoe, S. M., 385 Lui, G., 167 Luke, S. G., 131 Luna,
B., 119 Lund, T. E., 180 Lundy, R. F., 393 Name Index

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

477

Lundy, R. F., Jr., 395 Luo, A., 120 Lutz, S. G., 314 Lyall, V., 395
Lyle, K. B., 363, 364 MacDonald, G., 381 Mach, E., 58 Macherey, O., 352,
353 Mack, A., 136 Macnichol, E. F. Jr., 205 Macuga, K. L., 155 Madsen,
K. H., 180 Madzharov, A., 413 Maehashi, K., 396 Maess, B., 324 Magnasco,
M. O., 397 Maguire, E. A., 113, 158, 159 Mahon, B. Z., 340 Mai, N., 175
Maier, S. F., 373 Majaj, N. J., 188 Malach, R., 85, 110 Malhotra, S.,
299 Malik, S. A., 395 Malinowski, P., 143 Maller, O., 413 Mallik, A.,
326 Mamassian, P., 108, 233 Mancuso, K., 212 Manfredi, L. R., 367
Mangione, S., 313 Manly, J., 399, 400 Mante, V., 188 Marconato, F., 193
Mareschal, D., 259 Margolskee, R. F., 395 Maric, Y., 412 Marie, C., 313
Marino, A. C., 126, 134 Marks, W. B., 205 Maron, D. D., 143 Marr, D.,
242 Marsh, R. R., 414 Martin, A., 33, 111 Martin, C., 405 Martines, J.,
169 Martins, M. D., 327, 328 Martorell, R., 169 Marx, V., 384
Mas-Herrero, E., 326 Massaccesi, S., 119 Masten, C. L., 381 Mather, G.,
192 Mathews, M. W., 271 Matsunami, H., 399 Maule, J., 213, 226 Maunsell,
J. H. R., 81 Mauraux, A., 104 Maurer, D., 119, 192, 412 Maxwell, J.C.,
204, 205 Mayer, D. L., 61 Mayo, J. P., 130 Mazuka, R., 318 Mazziotta, J.
C., 166 McAlpine, D., 297, 298 McBurney, D., 391 McCabe, K. M., 376
McCann, J. J., 217 478

McCarthy, G., 84, 189 McCarthy, J., 217 McCartney, P., 6 McCleery, J.
P., 167 McClelland, J. L., 33 McCoy, A. N., 12, 75, 105 McDermott, J.,
84, 110, 111, 117 McDermott, J. H., 283, 327 McDonough, C., 354
McFadden, S. A., 246 McGann, J. P., 397 McGettigan, C., 347 McGlone, F.,
372, 384, 385, 410 McGlone, F. P., 372 McHale, A., 143 McKeown, M, J.,
189 McLaughlin, J., 323 McRae, J. F., 398 McRoberts, G. W., 339, 354
McTavish, T. S., 403 Medeiros-Ward, N., 139 Mehler, J., 337 Mehr, S. A.,
312 Mehribian, A., 197 Mehta, R., 177 Meire, F., 152 Meister, M., 397,
403 Melcher, D., 130 Melzack, R., 357, 373, 374, 375, 377 Melzer, A.,
177 Mennella, J. A., 390, 414 Menz, H. B., 152 Menzel, R., 224 Merigan,
E. H., 185 Merigan, W. H., 81 Merla, A., 119 Merlini, F., 39 Merskey,
H., 373 Mery, D., 94 Merzenich, M., 383 Merzenich, M. M., 382 Metzger,
V. A., 369 Meyer, B. T., 335 Meyer, L. B., 319, 323 Mhuircheartaigh, R.
N., 375 Miall, R. C., 162 Michael, L., 403 Micheyl, C., 288, 303
Mikolinski, M., 254 Miller, G., 312 Miller, G. A., 304, 345 Miller, G.
F., 177, 178 Miller, J., 156 Miller, J. D., 285, 341 Miller, J. F., 158
Miller, J. L., 340 Miller, R. L., 278 Milner, A. D., 81, 82, 168 Milner,
B., 84 Mine, S., 81 Minini, L., 243 Mischel, W., 381 Mishkin, M., 80
Missal, M., 317 Mitchell, M., 3 Mitchell, T, V., 189 Mittelstaedt, H.,
128 Miyamoto, R. T., 354 Mizokami, Y., 217, 218

Modersitzki, J., 76 Mohammadi, B., 376 Molden, S., 157 Moller, R., 242
Mollon, J. D., 198, 204, 205 Molnar-Szakacs, I., 166 Mon-Williams, M.,
231 Mondloch, C. J., 119 Montagna, B., 134 Montagna, W., 358 Montaldo,
G., 405 Monzée, J., 358 Moore, A., 143 Moore, B. C. J., 284 Moore, D.,
384, 385 Moore, D. R., 299 Moray, N., 124 Morcom, A. M., 315 Mori, K.,
325, 403 Morley, J. W., 361 Mormann, F., 85 Morrin, M., 413 Morris, J.,
315 Morrison, I., 372 Morton, J., 118, 119 Moser, E. I., 157, 158 Moser,
M.-B., 157, 158 Mountcastle, V. B., 369 Mouraux, A., 317 Movshon, J. A.,
184, 185, 188 Movshon, J.A., 185 Mozell, M. M., 408 Mueller, K. L., 394
Mukamel, R., 85, 165 Mullally, S. L., 113 Mullin, J. T., 118
Mummalaneni, S., 395 Munson, W. A., 269 Münte, T. F., 376 Munz, S., 140
Murata, A., 81 Murphy, C., 408 Murphy, K. J., 82 Murphy, P. K., 144
Murray, G. M., 361 Murray, M., 299 Murthy, V. N., 403 Muscatell, K. A.,
381 Myers, W. E., 408, 409 Nachtigal, D., 410 Nadel, L., 157 Nagy, E.,
384 Nahin, R. L., 143 Nakayama, K., 114 Nam, A., 335 Nardini, M., 259
Nascimento, S. M. C., 203 Naselaris, T., 115 Nassi, J. J., 79 Neale, V.
L., 138 Neff, W. D., 299 Neisser, U., 137 Newcombe, F., 198 Newman, E.
B., 300 Newman, E. L., 158 Newman, G. E., 177 Newsome, W. T., 184, 185,
243 Newton, I., 199, 200, 204, 223, 224

Name Index

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Nguyen, V., 376 Nicholas, S. C., 46 Niebur, E., 130 Nieman, L. Z., 313
Nijland, M. J., 414 Nikolajsen, L., 373 Nikonov, A. A., 403 Nishimoto,
S., 111, 112, 115 Nissen, M. J., 125 Nito, H., 318 Nityananda, V., 245
Nodal, F. R., 299 Norcia, A. M., 61 Nordby, K., 198 Norman, D. A., 42,
272 Norman, L. J., 308 Norman-Haignere, S., 283, 327 Norrsell, U., 371
Norton, T. T., 68 Noton, D., 131 Noulhiane, M., 326 Nozaradan, S., 317
Nunez, V., 212 Oatley, K., 332 Oberlin, D. J., 143 Oberman, L. M., 167
Obuseck, C. J., 305 Ocelak, R., 213 Ockelford, A., 323 O'Connell, K. M.,
257 O'Craven, K. M., 135 O'Doherty, J., 380, 410 Ogawa, H., 395 Ogden,
W. C., 125 Ohla, K., 389 Okanoya, K., 341 O'Keefe, J., 157 Olausson, H.,
371, 372 Olejarczyk, J., 131 Oliva, A., 103, 104, 106, 130, 198
Olkkonen, M., 217 Ollinger, J. M., 177 Olsho, L. W., 286 Olson, C. R.,
243 Olson, H., 271 Olson, R. L., 139 Ong, J., 403 Onis, M., 169 Onyango,
A., 169 Orban, G. A., 105 O'Regan, J. K., 138 Ortibus, E., 152
O'Shaughnessy, D. M., 370 Osmanski, B. F., 405 Oster, H., 413
Ostergaard, L., 317, 324 Osterhout, L., 323, 324 Ouyang, W., 91 Overy,
K., 315 Owen, C., 162 Oxenham, A. J., 270, 280, 281, 282, 288, 303 Pack,
C. C., 179, 187 Pagulayan, R. J., 152 Pain, F., 405 Pallesen, K. J.,
317, 324 Palmer, A. R., 282 Palmer, C., 316

Palmer, S., 106 Palmer, S. E., 98 Panerai, I. L., 176 Panichello, M. F.,
109 Pantev, C., 383 Parakkal, P. F., 358 Paré, E. B., 185 Paré, M., 359
Parisi, S. A., 118 Park, W. J., 186 Parker, A. J., 243 Parkhurst, D.,
130 Parkin, A. J., 111 Parkinson, A. J., 352 Parma, V., 389 Parsons, J.,
119 Pascalis, O., 118 Pascual-Leone, A., 190 Pasternak, T., 185
Pastilha, R., 217 Patel, A. D., 315, 318, 321, 323, 324, 325, 327 Patel,
R., 218 Patscheke, J., 285 Patteri, I., 167 Patterson, D. R., 376 Paus,
T., 374 Pawling, R., 372 Peacock, G., 204 Pecka, M., 297, 298 Pekar, J.,
299 Pelchat, M. L., 399 Pelphrey, K. A., 189 Penfield, W., 362 Peng,
J.-H., 285 Penhune, V. B., 315, 323 Peretz, I., 312, 317, 325, 326, 327,
328, 329, 376 Perez, J. A., 3 Perkins, A., 332 Perl, E. R., 373, 374
Perona, P., 103 Perrett, D. I., 84, 167 Perrodin, C., 350 Persoone, D.,
412 Persson, J., 407 Pestilli, F., 134 Petersen, A., 138 Peterson, M.
A., 6, 94, 100, 101, 102 Petkov, C. I., 350 Petrie, J., 412 Pettersen,
L., 257 Pettigrew, J. D., 243 Pfaffmann, C., 396, 398 Pfeifer, J. H.,
167 Pfordresher, P., 315, 316, 319 Phan, T.-H. T., 395 Philbeck, J. W.,
154, 168 Phillips, J. R., 363, 364 Phillips-Silver, J., 317, 329, 330
Pieper, F., 73, 136 Pietikäinen, M., 91 Pietrini, P., 117 Pike, B., 32
Pineda, J., 167 Pinker, S., 140, 312 Pinna, F. deR, 389 Pins, D., 113
Pinson, E. N., 10, 274 Pinto, P. D., 203 Piqueras-Fiszman, G., 410

Pisoni, D. B., 345 Plack, C., 315, 319 Plack, C. J., 267, 268, 275, 281,
284, 285, 286, 297 Plassmann, H., 410 Plata-Salaman, C. R., 396 Pleger,
B., 361 Ploner, M., 374, 375 Plug, C., 256 Poggio, T., 242 Pol, H. E.
H., 35 Poline, J. B., 35 Poloschek, C. M., 252 Poranen, A., 370 Porter,
J., 397 Posner, M. I., 125 Potter, M. C., 103 Powell, C., 381 Powell, T.
P. S., 369 Pratt, E. M., 61 Prendergast, G., 285, 286 Presnell, L. M.,
198 Press, C., 167 Pressnitzer, D., 220 Preusser, S., 361 Price, D. D.,
374, 377 Prieto, L., 94 Prinzmetal, W., 254 Probst, R., 285 Proffitt, D.
R., 168 Proust, M., 407 Proverbio, A. M., 154 Puce, A., 84 Pulvermüller,
F., 342, 343 Purves, D., 12, 75, 105 Purves, H. R., 12, 75, 105 Puzda,
A. D., 198 Quinn, P. C., 118 Quiroga, R. Q., 28, 29, 85 Rabin, J., 218
Rabin, M. D., 393 Rabin, R. C., 389, 390 Rabinovitz, B., 36 Racicot, C.
I., 82 Radcliffe, D., 138 Radwanick, S., 140 Rafal, R. D., 17 Rafel, R.
D., 142 Rainville, P., 374, 376, 377 Rakowski, S. K., 313 Ramachandran,
V. S., 43, 105, 167, 214, 373 Ramani, G., 144 Rambo, P., 376 Rangel, A.,
410 Rao, H. M., 130 Rao, R. P., 109 Raos, V., 161, 162 Rasmussen, T.,
362 Ratliff, F., 56, 57 Ratner, C., 217 Ratner, J., 324 Ratwik, S., 140
Rauber, J., 91 Rauschecker, J. P., 299, 350 Rauscher, K. J., 140 Ravi,
D., 3 Raye, K. N., 61 Name Index

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

479

Read, E., 314 Read, J., 245 Read, L., 138 Read, S., 134 Reddy, L., 28,
29, 85 Reddy, R., 336 Redmon, J., 90, 91 Reed, D. R., 396, 399 Rees, G.,
183, 306 Regev, M., 308 Regolin, L., 193 Reichardt, W., 182 Reiss, A.,
119 Rémy, F., 113 Rennaker, R. L., 405 Rensink, R. A., 138 Rentfro, P.
J., 313 Reppas, J. B., 110 Restrepo, D., 403 Reybrouck, M., 313 Rhode,
W. S., 278 Rhudy, J. L., 376 Ricard, M., 380 Rice, F. L., 359 Riddoch,
M. J., 153 Rieger, J., 198 Risch, N., 396 Risley, T. R., 354 Risner, S.
R., 366 Risset, J. C., 271 Rizzolatti, G., 164, 165, 166, 167 Robbins,
J., 375 Roberts, J., 138 Roberts, N., 315 Rocha-Miranda, C. E., 83, 84
Rock, I., 98, 136 Rockstroh, B., 383 Roepstorff, A., 317, 324 Rogers, B.
J., 154 Rogers, P., 204 Rolfs, M., 129 Rollman, G. B., 357 Rolls, E.,
315 Rolls, E. T., 84, 409, 410, 411 Rosen, L. D., 140 Rosenberg, J. C.,
321, 323 Rosenblatt, F., 3 Rosenfield, H. M., 118 Rosenstein, D., 413
Rosenzweig, M. R., 300 Rosner, B. S., 341 Ross, H. E., 256 Ross, M. G.,
414 Ross, V., 115 Rossato-Bennet, M., 313 Rossel, S., 244, 245 Rossion,
B., 104, 116, 117 Rossit, S., 141 Roth, D., 167 Rotter, A., 155 Roudi,
Y., 158 Roura, E., 410 Rowe, J. B., 315 Rowe, M. J., 361 Rowe, M. L.,
144 Roy, E. A., 366 Roy, M., 376 Royland-Seymour, A., 143 Rozzi, S., 166
480

Rullman, M., 361 Rumelhart, D. E., 33 Ruparel, K., 120 Rushton, S. K.,
155 Rushton, W., 49 Russell, R., 168 Rust, N. C., 188 Rutowski, R. G.,
282 Ruyle, A. M., 405 Ruzic, L., 382 Ryba, N. J. P., 394, 395 Sacks, O.,
197, 236, 328 Sadaghiani, S., 35 Sadrian, B., 407 Saenz, M., 284 Sahuc,
S., 155 Sai, F., 118 Sakata, H., 81, 369, 370 Salapatek, P., 62 Salasoo,
A., 345 Salcedo, E., 403 Salimpoor, V. N., 326 Salmas, P., 342 Salvagio,
E., 100, 101 Salvucci, D. D., 155 Samii, A., 376 Sammler, D., 376
Samson, S., 326 Sandeep, R. D., 407 Santurette, S., 288 Sato, M., 395
Satori, I., 361 Saul, A. B., 68 Saygin, A. P., 183, 189 Schaefer, R. S.,
315 Schenck, W., 242 Scherf, K. S., 119 Schiffman, H. R., 232 Schiller,
P. H., 81 Schilling, J. R., 278 Schinazi, V. R., 156 Schlack, A., 307
Schmid, C., 175 Schmidt, C., 410 Schmidt, H., 126 Schmitz, C., 178
Schmuziger, N., 285 Schnupp, J. W. H., 296 Scholl, B. J., 126, 134, 177
Scholz, J., 373 Schomers, M. R., 343 Schon, D., 313 Schooler, J. W., 407
Schouten, J. L., 33, 117 Schroger, E., 324 Schubert, E. D., 273 Schütt,
H. H., 91 Schwartz, S., 142 Schwarzkopf, S., 113 Schyns, P. G., 198
Schynsand, P. G., 104 Scott, A. A., 167 Scott, S. K., 299 Scott, T. R.,
395, 396 Scoville, W. B., 84 Searight, R., 140 Sedgwick, H., 255 Segui,
J., 337

Seibel, E., 376 Seidl, A. H., 297 Seiler, S. J., 139 Sekuler, A. B., 98
Semple, R. J., 143 Senior, C., 191 Seo, H-S., 407 Sereno, M. I., 189
Seymour, B., 380 Shackleton, T. M., 282 Shadlen, M. N., 184, 185 Shafir,
T., 177 Shahbake, M., 393 Shamay-Tsoory, S. G., 379 Shamma, S. A., 303
Shankar, M. U., 409 Shankweiler, D. P., 338, 342 Shannon, R. V., 348
Shapley, R., 213 Shapley, R. M., 212 Sharar, S. R., 376 Sharmam, R., 143
Shaughnessy, K., 168 Shea, S. L., 143, 257 Shek, D., 140 Shek, L. Y.,
140 Shen, D. G., 120 Shen, H., 120 Shepherd, G. M., 408 Sheridan, M. A.,
177 Sherman, M. S., 68 Sherman, P. D., 204 Sherman, S. M., 68 Shevell,
S. K., 217 Shiffrar, M., 188 Shihab, H. M., 143 Shimamura, A. P., 254
Shimojo, S., 257 Shinkareva, S. V., 131 Shinoda, H., 131 Shiv, B., 410
Shneidman, L. A., 354 Shrivastava, A., 131 Shuwairi, S. M., 257 Sibinga,
E. M., 143 Siggel, S., 376 Sigman, M., 167 Silver, M. A., 75 Silverman,
R., 144 Simion, F., 193 Simmel, M., 177 Simmons, A., 191, 245
Simoncelli, E. P., 188 Simons, D. J., 137 Simony, E., 308 Singer, T.,
380 Singh, K. D., 80 Singh, M., 312 Singh, S., 143 Sinha, P., 336
Siqueland, E. R., 341 Siskind, J. M., 354 Siveke, I., 297 Skelton, A.
E., 213, 226 Skipper, J. I., 342 Skudlarski, P., 117, 118 Slack, J. P.,
396 Sleicher, D., 143 Sloan, A. M., 405

Name Index

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Sloan, L. L., 209 Sloboda, J., 332 Sloboda, J. A., 316, 319 Small, D.,
410 Small, D. M., 408, 409, 410 Smart, J. L., 152 Smith, A. M., 358, 359
Smith, A. T., 80 Smith, B. P., 408 Smith, D. V., 395 Smith, E. E., 381
Smith, J. E. T., 243 Smith, L. B., 144, 145 Smith, M. A., 188 Smith, M.
E., 113 Smith, P. E., 408 Smith, R. S., 155 Smithson, H. E., 198, 218
Snyder, A. Z., 177 Sobel, E. C., 234, 246 Sobel, N., 404 Soderstrom, M.,
354 Soltani, M., 376 Solway, A., 158 Sommer, M., 128 Sommer, M. A., 129,
130 Soriano, M., 177 Sosulski, D. L., 407 Soucy, E. R., 403 Souza, T.,
407 Spector, A. C., 395 Spector, F., 412 Spence, C., 138, 371, 409, 410,
412, 413 Spence, D., 384 Spence, M. J., 287 Sperling, G., 180 Sperling,
H. G., 209 Spetner, N. B., 286 Spiegel, A., 168 Spiers, H. J., 158, 159
Spille, C., 335 Spurzheim, J., 30 Srinivasan, M. V., 234, 246 St. John,
S. J., 395 Staller, S. J., 352 Stankiewicz, B., 156 Stanley, D., 113
Stanley, J., 162 Starck, G., 372 Stark, L. W., 131 Stasenko, A., 340
Stebens, B. L., 376 Stecker, G. C., 300 Steiner, J. E., 414
Sterbing-D'Angelo, J., 307 Stettler, D. D., 407 Stevens, J. C., 396
Stevens, S. S., 16, 18 Stevenson, R. J., 405 Stigliani, A., 113 Stiles,
W. S., 51 Stoffregen, T. A., 152 Stokes, R. C., 343 Stone, L. S., 151
Strawser, C. J., 168 Strayer, D. L., 138, 139 Studdert-Kennedy, M., 338,
342 Stupacher, J., 312 Stussman, B. J., 143

Stutzman, S. S., 287 Suanda, S. H., 144, 145 Subramanian, D., 128
Suddendorf, T., 167 Sudweeks, J., 138 Sufka, K. J., 374 Suga, N., 246
Sugovic, M., 168 Sullivan, R. L., 408 Sullivan, R. M., 407 Summala, H.,
138 Sumner, P., 198 Sun, H.-J., 155 Sun, L. D., 128 Sun, R. C. F., 140
Sun, Y., 312 Suzuki, W. A., 143 Svaetichin, G., 211, 212 Svirsky, M.,
284, 351, 352 Sweet, B. T., 151 Swender, P., 408 Swendsen, R. H., 367
Symons, L. A., 117 Sziller, I., 329 Tadin, D., 179, 186 Taira, M., 81
Takahashi, Y. K., 403 Talbert, C., 218 Tamis-LeMonda, C. S., 169 Tan,
S-L., 315, 316, 319 Tanabe, S., 242 Tanaka, J., 198 Tanaka, J. R., 116
Tanaka, J. W., 118, 198 Tang, R., 143 Tang, Y., 143 Tang, Y-Y., 143
Tanifuji, M., 403 Tanter, M., 405 Tanzer, M., 83 Tao, Z.-A., 285
Tarawneh, G., 245 Tarr, B., 312 Tarr, M. J., 117, 118 Tatler, B. W., 133
Taub, E., 383 Taylor, K., 347 Teller, D., 62 Teller, D. Y., 226 Temme,
C. R., 91 Tepest, R., 167 Terwogt, M. M., 197 Teulings, L., 410 Thaler,
L., 307, 308 Tharp, C. D., 396 Thier, P., 167 Thiruvengadam, N., 85
Thompson, W. F., 312, 314, 321, 323 Thornton, I. M., 190 Thorpe, S. J.,
116 Thorstenson, C. A., 198 Thu, M. A., 376 Tian, B., 299 Timney, B.,
244 Tindell, D. R., 140 Tirovolas, A. K., 314 Titsworth, S., 140 Todd,
P. M., 177, 178

Todrank, J., 408 Tollin, D. J., 300 Tolman, E. C., 157 Tomic, S. T.,
313, 317 Tong, F., 114, 115 Tonndorf, J., 277 Tootell, R. B., 84, 110
Torralba, A., 103, 104, 106, 130 Tovee, M. J., 84 Townsend, D., 377
Tracey, I., 374, 375, 377 Trainor, L. J., 317, 318, 329, 330 Tranchina,
D., 46 Tranel, D., 313 Travers, S. P., 395 Trehub, S. E., 312, 329, 353
Treisman, A., 126, 142 Tresilian, J. R., 231 Treue, S., 73, 134, 136
Troiani, V., 113 Troost, J. M., 319 Truax, B., 270 Tsachor, R. P., 177
Tsao, D. Y., 84 Tsui, J., 179 Turano, K. A., 151, 155 Turatto, M., 134
Turk, D. C., 374 Turman, A. A., 361 Turman, A. B., 361 Turner, R., 376
Turrill, J., 139 Tuthill, J. C., 162 Tversky, B., 177 Tweed, T. S., 285
Twombly, A., 370 Tyler, C. W., 41, 61 Uchida, N., 403 Uchikawa, H., 216
Uchikawa, K., 216 Uddin, L. Q., 167 Uka, T., 243 Ulrich, R., 118 Umeton,
D., 245 Umilta, M. A., 165 Ungerleider, L. G., 33, 80, 81, 111 Utman, J.
A., 349 Vago, D., 143 Valdez, P., 197 Vallbo, A. B., 364, 371
Vallortigara, G., 193 Valsecchi, M., 134 Van Den Heuvel, M. P., 35 van
der Lubbe, R. H. J., 191 Van Doorn, G. H., 410 van E, R., 412 Van Essen,
D. C., 75 Van Lare, J., 299 Van Opstal, A. J., 295, 296 Van Riswick, J.
G. A., 295 van Turennout, M., 156, 157 Van Wanrooij, M. M., 296 van
Wezel, R. J. A., 191 Vandenbussche, E., 105 VanMeter, J., 299 Vaughn, J.
T., 114 Vayssière, N., 113 Name Index

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

481

Veldhuizen, M. G., 389, 410 Venezia, J. H., 343 Venkatesh, S., 234, 246
Ventura, D. F., 224 Verbaten, M. N., 376 Vereijken, B., 171 Verhagen, J.
V., 409, 411 Vermeij, G., 368 Verstraten, F., 192 Vescovi, M., 134
Viemeister, N., 281 Vietze, P., 118 Vigorito, J., 341 Vingerhoets, G.,
161 Vinnikova, A. K., 395 Violanti, J. M., 138 Vishton, P. M., 231, 235
Vivan, D., 327 Vo, A. T., 111, 112 Võ, M. L. H., 131, 132 Voegels, R.
L., 389 Vogeley, K., 167 Vogels, R., 105 Von Bartheld, C. S., 399 von
Cramon, D., 175 von der Heydt, R., 213 Von Hipple, P. V., 319 von Holst,
E., 128 Vonderschen, K., 296 Vos, P. G., 319 Vosshall, L. B., 397, 399
Vu, A. T., 115 Vuilleumier, P., 141, 142 Vuust, P., 313, 317, 323, 324,
325 Wager, T., 382 Wager, T. D., 376, 379, 381 Wagner, H., 296 Wagner,
H. G., 56 Wagner, M., 256 Wald, G., 50, 205 Waldrop, M. M., 339 Walker,
S. C., 372 Wall, P. D., 357, 373, 374, 375 Wallace, G. K., 234 Wallace,
M. N., 282 Wallach, H., 220, 300 Wallisch, P., 218, 219 Walls, G. L.,
198 Walter, S., 217 Wandell, B. A., 76 Wang, H., 396 Wang, J., 131 Wang,
L., 385 Wang, Q. J., 412, 413 Wang, R. F., 155 Wang, W., 120 Wang, X.,
91, 270, 282, 283 Wang, Y., 354 Wanigesekera, V., 375 Wann, J. P., 155
Ward, A. F., 139, 140 Warner, A. F., 61 Warren, J. D., 326 Warren, R.
M., 305 Warren, W. H., 151, 152, 155 Waterman. I., 359

482

Watkins, L. R., 373 Watt, L., 385 Waymouth, S., 168 Weber, A. I., 367
Weber, E., 15 Webster, G. D., 381 Webster, M., 216, 219 Wei, X.-X., 158
Weidman, C. T., 158 Weisenberg, M., 375 Weiser, B., 313 Weiskopf, D.,
198 Weiskopf, S., 225 Weisleder, A., 354 Weissberg, M., 374
Weissman-Fogel, I., 379 Welch, C. E., 375 Welchman, A. E., 242 Werblin,
F. S., 41 Werker, J. F., 318 Werner, L. A., 286 Wernicke, C., 31, 349
Wertheimer, M., 95 Wessberg, J., 371, 372 Westerman, S. J., 138
Westheimer, G., 86 Wexler, M., 176 Whalen, D. H., 340 Whishaw, I. Q.,
350 Whiten, A., 167 Whitesell, J. D., 403 Whitfield-Gabrieli, S., 119
Wichmann, F. A., 91 Wicker, B., 379, 380 Wiech, K., 374, 375 Wiederhold,
B. K., 139 Wienbruch, C., 383 Wiesel, T. N., 56, 69, 70, 71, 77, 78,
183, 243, 244 Wiggs, C. L., 111 Wightman, F. L., 293, 296 Wild, J. M.,
246 Wiley, T. L., 285 Wilkie, R. M., 155 Willamder, J., 407 Willander,
J., 407 Willems, R. M., 327 Williams, A. E., 376 Williams, A. L., 80
Williams, J. H. G., 167 Williams, M., 168 Williams, P., 198 Wilson, D.
A., 405, 407 Wilson, J. R., 68 Wilson, K. D., 116 Wilson, S. M., 189,
343 Wilson-Mendenhall, C. D., 143 Winawer, J., 191 Winkler, I., 329
Wissinger, C. M., 299 Witt, J. K., 167, 168 Witte, M., 312 Witter, M.
P., 157 Witzel, C., 213 Wollach, L., 209 Wollett, K., 158, 159 Wolpert,
D. M., 108, 162

Womelsdorf, T., 73, 136 Woo, C.-W., 382 Wood, G., 312 Woods, A. J., 168
Woolf, C. J., 373 Wuilemin, D., 410 Wulfeck, B., 349 Wundt, W., 94
Wurtz, R. H., 129 Wygonski, J., 348 Xu, W., 407 Yaguchi, H., 217
Yamashita, S., 395 Yang, G-Z., 3 Yang, J. N., 217 Yantis, S., 135
Yarbus, A. L., 132 Ye, N., 413 Yeagle, E. M., 243, 244 Yela, M., 299
Yilmaz, E. H., 152 Yonas, A., 257, 258 Yoshida, K. A., 318 Yost, W. A.,
281, 293, 302, 303 Young, A. W., 111 Young, E. D., 278 Young, M., 155
Young, R. S. L., 198 Young, S. G., 198 Young, T., 204 Young-Browne, G.,
118 Youngblood, J. E., 320 Yu, B., 115 Yu, C., 144, 145 Yu, D., 155
Yuille, A., 108 Yuodelis, C., 62 Zacks, J. M., 177 Zampini, M., 409
Zarahn, E., 113 Zatorre, R. J., 32, 313, 315, 323, 325, 326, 328, 331
Zeevi, Y. Y., 41 Zeidan, F., 143 Zeidman, P., 113 Zeki, S., 198, 213
Zellner, D. A., 413 Zeng, F.-G., 348 Zeng, L., 120 Zhang, H. Q., 361
Zhang, T., 188 Zhang, X. H., 385 Zhang, Y., 394 Zhao, G. Q., 394 Zhong,
X., 293 Zhou, H., 213 Zhuang, H., 399 Zihl, J., 175 Ziles, K., 165
Zohary, E., 185 Zoia, S., 384 Zosh, W. D., 155 Zubieta, J-K., 183 Zuker,
C. S., 394, 395

Name Index

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Subject Index Aberrations, 205 Ablation, 80 Absolute disparity,
239--240, 243 Absolute threshold, 13 Absorption spectrum, 50--51
Accommodation, 43--44 monocular cues and, 231 oculomotor cues and, 231
Accretion, 234--235 ACE2, 399 Achromatic colors, 200 Acoustic shadow,
291 Acoustic signals, 302, 336--340 Acoustic stimulus, 336 Acoustics,
301 Across-fiber patterns, 393 Action, 9, 149--172 balance and, 152
demonstrations of, 152 driving, 155 invariant information, 151 moving
observer and, 150--151 observing other people's actions, 164--167
perception and, 167--168 predicting intentions, 165--167 review
questions on, 149, 159, 172 walking, 154--155 wayfinding, 155--159
Action potential chemical basis of, 24--25 definition of, 22 falling
phase of, 25 potassium flow across, 25 as propagated response, 23
properties of, 23--24 rising phase of, 24--25 size of, 23--24 sodium
flow across, 25 transmitting information, 25--27 Action-specific
perception hypothesis, 168 Active touch, 368 Acuity. See Tactile acuity;
Visual acuity Adaptation dark, 46--49, 53--54 evolutionary, 312
selective, 72--73 Adaptive optical imaging, 205 Additive color mixture,
202 Adjustment, 14--15 Adult-directed speech (ADS), 353--354 Advanced
precision grip, 161 Affective component of pain, 377 Affective function
of touch, 372 Affordances, 152--154 and infants 169--171 Aging
presbycusis and, 284 presbyopia and, 45 Alive Inside, 313--314

Alzheimer's disease, 313, 399--400 Amacrine cells, 52 Ambiguity, retinal
image and, 92--93, 107 Ames room, 254--255 Amiloride, 395 Amplitude,
265--266, 268--269, 279 Amplitude-modulated noise, 281 Amusia,
congenital, 328 Amygdala, 85, 111, 405, 407, 409 Angle of disparity,
239--240 Angular size contrast theory, 256 Animals camouflaged, 176
depth perception and, 244--246 echolocation, 246 electrolocation, 246
frontal eyes, 244--245 lateral eyes, 245--246 motion parallax, 246
olfaction and, 397--398 sound localization and, 298 Anomalous
trichromatism, 210 Anosmia, 398 Anterior auditory cortex, 283 Anterior
cingulate cortex, 380--381 Aperiodic sounds, 271 Aperture problem,
186--187 Apex of the cochlea, 277 Aphasias, 349 Apparent distance
theory, 256 Apparent motion/movement, 95--96, 179--180, 188 Appearance
of objects, 134--135 Arch trajectory, 320 Architectural acoustics,
301--302 Articulators, 336 Atmospheric perspective, 233 Attack, tone,
271 Attention, 123--146 appearance and, 134--135 benefits of, 133--135
brain activity and, 135--136 change detection and, 137--138 corollary
discharge theory, 128--130 covert, 124--125, 133 definition of, 124
demonstrations of, 130, 136--138 disorders of, 141--145 distraction and,
138--141 extinction, 141--142 eye movements and, 131 feature integration
theory, 126--127 filter model of, 125 focusing of, by meditating,
142--143 inattentional blindness, 136--138 infant, 143--145 observer's
interests and goals, 131 odor effects on, 413 overt, 124 pain perception
and, 375--376

physiology of, 135--136 precueing, 125--126 predictive remapping of, 130
receptive fields and, 136 research on, 124--127 response speeds and,
133--134 scanning process and, 127--128 scene schemas, 131--132
selective, 124--125 selective listening experiments, 124--125 spatial,
125--126 spatial neglect, 141--142 tactile object perception and,
370--371 task demands, 132--133 visual salience, 130--131 Attentional
capture, 130 Audibility curve, 269, 286--287 Audiogram, 285--286
Audiovisual mirror neurons, 165 Audiovisual speech perception, 344
Auditory canal, 272--273 Auditory cortex anterior, 283 damage to,
effects of, 269, 273 pitch perception and, 282--284 Auditory
localization, 291--299, 302--303 azimuth information for, 293, 295--296
binaural cues for, 293--294, 296 Jeffress model, 296--297 location cues
and, 292--293 physiology of, 296 review questions on, 302 spectral cues
for, 294--296 what pathway, 299 where pathway, 299 Auditory nerve, 282
Auditory pathways, 299 Auditory response area, 269 Auditory scene, 302
Auditory scene analysis, 302--306 Auditory space, 292 Auditory stream
analysis, 292 Auditory stream segregation, 303--304 Auditory system. See
also Hearing brain diagram of, 282 cortical processing and, 298--299
damage to, 286--288 frequency represented in, 276--277 infant
development and, 286--287 phase locking in, 276, 281 place theory of,
280--282 review questions on, 271--272, 279, 288 sound separation and,
302--306 sound stimulus and, 272 structure of, 272--279 Automatic speech
recognition (ASR), 335 Autonomous vehicles, 91 Axial myopia, 45

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

483

Axon, 21--24 Azimuth coordinate, 295 "Baby talk," 353--354 Balance
senses and, 152 visual information and, 152 Basal ganglia, 315 Base of
the cochlea, 277 Basilar membrane, 274--275, 279 Bass ratio, 301 Bats,
246 Bayesian inference, 108--109 Beat definition of, 315 infant's
response to, 329--330 Behavioral responses, 9--10 Bimodal neurons, 409
Binaural cues, 293--294, 296 Binocular cues, 236--242 binocular
disparity and, 238--240 corresponding retinal points and, 238--239
noncorresponding points and, 239--240 random-dot stereograms, 241--242,
257 stereopsis and, 240--242 3-D images and, 237 Binocular depth cells,
243--244 Binocular disparity, 238--240, 244--245 Binocular rivalry, 114
Binocularly fixate, 257 Biological motion, 188--190, 192--193 Bipolar
cells, 51 Birds, 298. See also Animals Bitter tastes, 390--391, 394--395
Blind spots demonstrations of, 43 description of, 42--43 Blind walking
experiment, 154--155 Blood flow, 31 Border ownership, 100 Borders, 100
Bottom-up processing, 10 Braille, 362--363 Brain attention to locations,
135--136 Broca's area, 349 comparator, 181 connections between areas,
33--35 distributed representation, 33 extrastriate body area (EBA), 189
face perception and, 116--119 functional connectivity of, 33--34
fusiform face area (FFA). See Fusiform face area (FFA) grid cells, 157
mapping function to structure, 30--32 middle temporal (MT) area,
183--185, 189--190 mirror neurons, 164--165 music effects on, 314
navigation and, 156--159 olfaction and, 408--410 opioid receptors,
377--378 pain perception and, 33, 376--378 parahippocampal place area
(PPA), 113--114 place cells/fields, 157 plasticity of, 382--383 primary
receiving areas, 8 scene perception and, 114--116 social touch and, 372
speech perception and, 349--351 484

structural connectivity of, 33--34 taste perception and, 393, 408--410
Brain areas and pathways (figures) anterior cingulate cortex, 380
anterior insula, 380 auditory pathways, 282, 299 Broca's area, 349
fusiform face area (FFA), 110--111, 119 parahippocampal place area
(PPA), 113, 119 somatosensory, 360 taste pathway, 393 Wernicke's area,
349 Brain damage affordances and, 153 behavior of people without, 82--83
double dissociations, 81--82 music emotion and, 326 object perception
and, 153 speech perception and, 349 Brain imaging definition of, 31
magnetic resonance imaging, 31 speech perception and, 349 wayfinding
and, 159 Brightness, 14 Broadly tuned neurons, 298 Broca's aphasia, 327,
349 Broca's area, 349 Calcium imaging, 401--402 Camouflaged animals, 176
Capsaicin, 378 Categorical perception, 340 Categorizing, 10 Cats,
243--244, 298--299, 396 Cell(s) ganglion, 51--53 hair, 274--275 Cell
body, 21 Center-surround antagonism, 56--58 Center-surround receptive
fields, 57--59, 69 Central control fibers, 374 Cerebral achromatopsia,
197 Cerebral cortex, 8 Change blindness, 137--138 Change detection,
137--138 Characteristic frequency, 278 Chemical senses. See Senses
Chevreul illusion, 58--59, 252 Children. See also Infant(s) fusiform
face area (FFA), 119 Chromatic adaptation, 215--216 Chromatic colors,
200, 203 Cilia, 272, 274--275 Ciliary muscles, 43 Circumvallate
papillae, 391 Classical psychophysical methods, 14 Cloze probability
task, 320 Coarticulation, 339 Cochlea, 273--274 apex of, 277 base of,
277 frequency and, 277--279 place theory and, 279 tonotopic maps of, 278
Cochlear amplifier, 278--279 Cochlear implants (CI), 351--353 Cochlear
nucleus, 282 Cochlear partition, 273--274 Cocktail party effect, 124

Code, sensory. See neural code Coding population, 29--30 sparse, 29
specificity, 27--29, 394--396 Cognition flavor perception and, 410
haptic exploration and, 369 Cognitive maps, 157--158 Cognitivist
approach, 321 Coherence, 184 Coincidence detectors, 297 Color
achromatic, 200 chromatic, 200, 203 flavor and, 413 mixing, 201--203
nonspectral, 203 odors and, 412 properties of, 199--200 saturation of,
203 spectral, 203 transmission and, 200--201 wavelengths and, 223--224
Color blindness, 197--198, 207 Color circle, 210 Color constancy,
215--220 chromatic adaptation and, 215--217 demonstrations of, 216
effect of surroundings on, 216--217 illumination and, 215--218 partial,
216 #TheDress, 218--220 Color deficiency anomalous trichromatism, 213
color blindness, 197--198 cortical damage and, 197--198 dichromats, 208
monochromatism, 207 receptor-based, 207 trichromats, 208 Color-matching
experiments, 205 Color perception, 197--227 color constancy and,
215--220 deficiency of, 207 demonstrations of, 216, 222 effect of
surroundings on, 216--217 functions of, 198 infants and, 225--226
lightness constancy and, 220--222 loss or blindness, 197--198 mixed
colors and, 201--203 nervous system and, 224 opponent-process theory,
210--213 reflectance and, 200--201 review questions on, 204, 214, 226
short wavelength sensitivity, 50 transmission and, 200--201 trichromatic
theory of, 204--210 wavelengths and, 199--203, 207, 223--224
Young-Helmholtz theory, 204 Columnar organization hypercolumns, 78
location columns, 77--78 ocular dominance columns, 78n orientation
columns, 77--78 Common fate, 98 Common logarithms, 267 Common region, 98
Comparator, 128, 181

Subject Index

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Complex cells, 70--71 Complex tones, 267--268 Compression, 265 Computer
errors, 90 Computer vision, 91 Concert hall acoustics, 301--302 Cone(s),
40--41. See also Rod and cone receptors convergence, 51--54 dark
adaptation and, 46--48 trichromatic color matching and, 206--207 visual
acuity, 53--55 Cone of confusion, 293--294 Cone pigments, 205 Cone
receptors, 205--206 Cone spectral sensitivity, 49 Cone vision, 53--55
Conflicting cues theory, 254 Congenital amusia, 328 "Conscious
awareness," 142 Consonance, 312 Consonants, 337--338, 412 Constancy
color, 215--220 lightness, 220--222 size, 250--252 speech perception
and, 339 Constant stimuli, 14--15 Context, speech perception and,
338--339 Contextual modulation, 86 Continuity errors, 138 Contrast,
perceived, 134--135 Contrast threshold, 72--73 Convergence, 51--55
demonstrations of, 54 monocular cues and, 232 oculomotor cues and, 231
perspective, 232--233 review questions on, 62--63 Coordinated receptive
fields, 307 Cornea, 40, 43 Corollary discharge signal (CDS), 128--130,
181--182 Corollary discharge theory, 128--130, 181--182 Correspondence
problem, 242 Corresponding retinal points, 238--239 Cortex. See also
Visual cortex auditory areas in, 282--284, 298--299 frontal operculum,
393 middle temporal area, 183--185, 189--190 odor perception and,
405--408 orbitofrontal, 405, 409--411 piriform, 405--407 primary
receiving areas, 8 somatosensory, 361--362, 370 striate, 79--83, 189
touch perception and, 369--370 Cortical cells complex, 70--71 simple,
70--71 Cortical magnification, 75--76 Cortical magnification factor,
75--76 Cortical organization hypercolumns, 78 location columns, 77--78
orientation columns, 77--78 review questions on, 79 Covert attention,
124--125, 133 COVID-19, 313, 389--390 Cross-talk, 307 Crossed disparity,
240 CT afferents, 371, 378 Cue approach to depth perception, 230

Cutaneous receptive field, 358 Cutaneous senses, 357--385. See also
Touch perception demonstrations of, 364, 367 detail perception and,
362--365 nerve pathways and, 359--361 object perception and, 368--371
pain perception and, 373--382 skin receptors and, 358--360 texture
perception and, 365--368 vibration perception and, 365--367 "Cyberball"
experiment, 381 Dark adaptation, 46--49, 53--54 Dark adaptation curve,
15, 46--49 Dark-adapted sensitivity, 46 Data-based processing, 10
Deactivating, 185 Decay, tone, 271 Decibel (dB), 266--267 Decision-point
landmarks, 156 Delay units, 182--183 Deletion, 234--235 Dendrites, 22
Depolarization, 26 Depth cues binocular, 236--242 monocular, 231--236
oculomotor, 231 pictorial, 231--234 Depth perception, 229--260 animals
and, 244--246 binocular cues and, 236--242 binocular disparity and,
244--245 cast shadows and, 258--259 cue approach to, 230 demonstrations
of, 231, 234 disparity information and, 239--240 illusions of size and,
252--255 infants and, 257--258 monocular cues and, 231--236 oculomotor
cues and, 231 physiology of, 243--244 pictorial cues and, 257--258
review questions on, 246, 259 size perception and, 247--255
stereoscopic, 236--237 3-D images and, 233, 237 Dermis, 358 Desaturated
hues, 203 Descent of Man, The, 331 Detached retina, 49 Detail
perception, touch and, 362--365 Detection change, 137--138 odor
perception and, 398 Detection threshold, 398 Deuteranopia, 209
Development feature Adapting to red, 216 Attentional capture, 130
Balance, keeping your, 152 Blind spot awareness, 43 Blind spot, filling
in, 43 Change detection, 137 Cortical magnification, 76 Degraded
sentences, perceiving, 345 Deletion and accretion, 234 Feelings in your
eyes, 231 Focus awareness, 44 Foveal vs. peripheral acuity, 54

Lightness perception at a corner, 222 Movement of a bar across an
aperture, 187 Muller-Lyer illusion with books, 253 Object
identification, 368 Odor identification, 398 Organizing strings of
sounds, 346 Penumbra and lightness perception, 222 Perceptual puzzles in
a scene, 89 Picture perception, 10 Scenes and objects, visualizing, 106
Size perception at a distance, 250 Size-Distance scaling and Emmert's
law, 250 Tasting and the nose, 408 Two eyes: two viewpoints, 236
Two-point thresholds, 364 Visual search, 126 Development of perception
affordances and, 169--170 attention, 143--144 biological motion
perception, 192--193 chemical sensitivity, 413--415 color vision and,
225--226 depth perception, 257--258 face perception, 118--119 hearing,
286--287 response to musical beat, 329--330 social touch, 383 visual
acuity, 60--62 Dichromacy, 208--210 Dichromatism, 209 Dichromats, 208
Difference threshold, 15 Direct pathway model of pain, 373--374 Direct
sound, 299--300 Discriminative functions of touch, 372 Dishabituation,
225 Disparity absolute, 239--240 angle of, 239--240 crossed, 240
uncrossed, 240 Disparity-selective cells, 243 Disparity tuning curve,
243 Dissonance, 312, 323 Distal stimulus, 7, 264 Distraction, 138--141
Distributed representation, 33 Dopamine, 326 Dorsal anterior cingulate
cortex (dACC), 381 Dorsal root, 359 Double dissociations, 81--82, 327
Driving environmental information and, 155 smartphone distractions
while, 138--141 Dual-stream model of speech perception, 350 Duple meter,
316--317 Duplex theory of texture perception, 366 Dystonia, 383 Ear. See
also Auditory system; Hearing inner, 273 middle, 272--273 outer, 272
structure of, 272--279 Eardrum, 272 Early right anterior negativity
(ERAN), 325 Echolocation, 246, 307--308 Ecological approach to motion
perception, 181 Ecological approach to perception description of, 150
environmental information and, 150--155 Subject Index

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

485

Edge enhancement, 58--59 Effect of the missing fundamental, 270
Electrical energy, 45--46 Electrical signals, 21--22 Electrolocation,
246 Electromagnetic spectrum, 18 Elements of Psychophysics (Fechner),
14--15 Emmert's Law, 251 Emotion(s) music and, 321--327 olfaction and,
407--408 pain perception and, 376 Emotional component of pain, 377
Emotivist approach, 321--322 Empathy, 379--380 End-stopped cells, 71
Endorphins, 377 Environment indoor, 299--302 interactions with,
150--151, 167 knowledge of, 94 regularities in the, 105--107
representing, 167 spatial updating in, 155 wayfinding, 155--157
Environmental information balance and, 152 driving and, 155 ecological
approach to perception and, 150--155 optic flow and, 150--151 sound
localization and, 292, 296, 298 walking and, 154--155 Epidermis, 358
Equal loudness curves, 269--270 Event boundary, 177 Event-related
potential (ERP), 323--324 Events, 176--177 Evolutionary adaptation, 312
Excitatory responses, 26--27 Expectancy in music, 323--325 Expectation
flavor perception and, 410 pain perception and, 375 Experience face
perception and, 119 motion perception and, 191--192 odor perception and,
406, 410 perceptual organization and, 101--102 Experience-dependent
plasticity definition of, 74 description of, 382--383 wayfinding and,
158 Experience sampling, 143 Expertise hypothesis, 117 Exploratory
procedures (EPs), 369 Extinction, 141--142 Extrastriate body area (EBA),
111--112, 189, 213 Eye(s) accommodation, 43--44 focusing, 43--45
frontal, 244--245 lateral, 245--246 misalignment of, 236 parts of,
40--43 receptors of, 7, 22, 40--45 refractive errors, 44--45 Eye
movements attention and, 131 corollary discharge theory and, 181--182
medial superior temporal area, 185 scanning a scene, 127--128 486

Face perception brain activity and, 116--118 experience and, 119
fusiform face area in, 110--111, 117 infants and, 118--119 neural
correlates of, 110--111 Falling phase of the action potential, 25
Familiar size, 231--232 Familiarization period, 258 Feature detectors
definition of, 71 in perception, 72--75 selective adaptation and, 72--73
selective rearing and, 74--75 Feature integration theory (FIT), 126--127
Feedback, 68, 81 FFA. See Fusiform face area (FFA) Figure, 99--102
Figure-ground segregation, 99--102 Filiform papillae, 391--392 First
harmonics, 268 Flavor color and, 413 definition of, 390 music effects
on, 412 Flavor perception, 408--411 brain and, 408--410 cognition and,
410 demonstrations of, 408 expectation and, 410 infants and, 413--415
multimodal nature of, 379--380 olfaction and, 408 review questions on,
415 sensory-specific satiety and, 410--411 taste and, 408--410
Fleisher's dystonia, 383 Focus of expansion (FOE), 151, 155 Focused
attention meditation, 143 Focusing, visual demonstrations of, 44
problems related to, 44 process of, 43--45 Foliate papillae, 391
Forced-choice method, 398 Formant transitions, 337 Formants, 336 Fovea,
41, 53, 62, 75--76 Foveal acuity, 54 Frequency auditory representation
of, 276--277 characteristic, 278 interaural level difference, 293--296
interaural time difference, 293, 295--296 resonant, 272 sound, 266 tone
and, 265 Frequency-matched noise, 283 Frequency spectra, 267--268, 271
Frontal eyes, 244--245 Frontal lobe, 8 Frontal operculum, 393 Functional
connectivity, 33--35 Functional magnetic resonance imaging (fMRI), 13,
31--34, 114--115, 379--380. See also Brain imaging Functional ultrasound
imagery, 405 Fundamental frequency, 268, 270 Fundamental tone, 268
Fungiform papillae, 391--393 Fusiform face area (FFA), 213

attention and, 135 face perception and, 110--111, 117 motion perception
and, 189 object perception and, 110--111, 116--118 speech perception
and, 344 Ganglion cells, 51--53, 69 Gap fill, 319 Gate control model of
pain, 373--374 Geons, 102--103 Gestalt psychology common fate, 98 common
region, 98 good continuation, 96--97 perceptual grouping, 94--99
perceptual segregation, 99--102 Pragnanz, 97 proximity (nearness), 98
similarity, 97--98 uniform connectedness, 98--99 Gist of a scene,
103--105 Global image features, 104 Good continuation, 96--97 Gradient
of flow, 150--151 Grandmother cells, 27, 29 Grasping, 160--162 Grasping
task, 82 Grating acuity, 13, 363 Greebles, 117--118 Grid cells, 157
Grip, 163--164 Ground, 99--102 Grouping perceptual, 94--99 sequential,
303--306 simultaneous, 303 Habituation procedure, 225 Hair cells,
274--275 Hand dystonia, 383 Haptic perception, 368--369 Haptics, 362
Harmonicity, 303 Harmonics, 268 Harmony, 312 Harry Potter and the
Sorcerer's Stone (film), 138 Head-mounted eye tracking, 143--144
Head-turning preference procedure, 330 Hearing. See also Auditory
system; Sound development of, 286--287 frequency and, 277--279
importance of, 263--264 indoor environments and, 299--302 infants and,
286--287 loss of, 286--288 loudness and, 268--270 pitch and, 270, 279
place theory of, 280--282 range of, 266--268, 272 review questions on,
271--272, 279, 288 sound localization and, 293--299 sound separation
and, 302--306 timbre and, 271 transduction for, 275 vision and, 306--308
Hearing impairments age-related, 284 hearing loss, 286--288 hidden
hearing loss, 285--286 noise-induced, 284--285

Subject Index

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Hearing loss cochlear implants for, 351--353 hidden, 285--286
sensorimotor, 351 Hering's primary colors, 210 Hidden hearing loss,
285--286 Hidden objects, 93--94 Higher harmonics, 268 Hippocampus, 85
History of Psychology, A (Benjamin), 19 Holway and Boring experiment,
247--250 Homunculus, 362 Honeybees, 59--60 Horizontal cells, 52
Horopter, 239 How pathway, 81--82 Hubble Telescope, 59--60 Hue(s), 203
cancellation, 211 scaling, 211 Hypercolumn, 78 Hyperopia
(farsightedness), 45 Hyperpolarization, 25 Hypnotic suggestion, 377
Hypothalamus, 407 Identity, 80 Illumination, 215--218 Illumination
edges, 221 Illusions. See also Visual illusions apparent movement,
95--96 of depth, 252--255 of motion, 179--180 of size, 252--255
waterfall, 179--180 Illusory contours, 96 Image displacement signal
(IDS), 128, 181 Implied motion, 190--192 Inattentional blindness,
136--138 Inattentional blindness (Mack and Rock), 136--137 Incus, 272
Indirect sound, 299--301 Infant(s) affordances and, 169--171 attention,
143--145 beat and, 329--330 binocularly fixate, 257 biological motion
perception of, 192--193 chemical sensitivity, 413--415 color vision and,
225--226 depth perception and, 257--258 face perception and, 118--119
familiarization period, 258 flavor perception and, 413--415 hearing and,
286--287 learning object names, 143--145 pictorial cues and, 257--259
preferential reaching, 258 social touch in, 384--385 taste and, 413--415
visual acuity in, 60--62 Infant-directed speech (IDS), 353--354, 385
Inference, 108--109 Bayesian, 108--109 Inference, unconscious, 107, 168,
342 Inferior colliculus, 282 Inferotemporal (IT) cortex, 83--85
Inflammatory pain, 373 Inhibitory responses, 26--27 Inner ear, 273--276
Inner hair cells, 274

Insula, 372, 393 Intensity, 15 Intentions, predicting, 165--167
Inter-onset interval, 316 Interaural level difference, 293--296
Interaural time difference, 293--296 Interpersonal touching, 371
Intimacy time, 301 Invariance invariant information, 151 viewpoint, 94
Invariant information, 151 Inverse projection problem, 92 Ions, 24
Ishihara plates, 208 Isomerization, 46, 49, 207 ITD detectors, 297 ITD
tuning curves, 297--298 Jeffress model, 296--297 Kinesthesis, 358
Knowledge. See also Cognition; Top-down processing action and, 155
categorizing, 10 perceptual process and, 10 scene perception and,
105--106 wayfinding, 158 Knowledge-based processing, 10, 372 Landmark
discrimination, 80 Landmarks, 156--157 Language aphasias, 349 knowledge
of, 344 music and, comparisons between, 327--329 syntax in, 323--324
word meaning in sentences, 345--346 Lateral eyes, 245--246 Lateral
geniculate nucleus, 68--69, 81 Lateral inhibition, 56--58 Lateral
occipital complex (LOC), 110 Lateral plexus, 57 Leisure noise, 285
Length estimation task, 82 Lens, 40, 43 Lesioning, 80, 185 Light, 40, 43
mixing colored, 202--203 properties of, 199--200 reflectance curves, 200
selective reflection and, 200 transduction of, 45--46 transmission
curves, 200 Light-adapted sensitivity, 46 Light-from-above assumption,
105 Lightness, 203, 220 Lightness constancy, 220--222 demonstrations of,
222 illumination and, 220--222 ratio principle, 220 shadows and,
221--222 surface orientation, 222 Lightness perception, 221--222
Likelihood principle, 108 Limulus (horseshoe crab), 56--57 Local
disturbance in the optic array, 181 Localizing sound. See Auditory
localization Location columns, 77--78 Loudness, 268--270

Mach bands, 58, 252 Macrosmatic, 397 Macular degeneration, 41--42
Magnetic resonance imaging, 31 Magnitude estimation, 16 Malleus, 272 Man
Who Mistook His Wife for a Hat, The (Sacks), 9 Manner of articulation,
337 Maps cognitive, 157--158 odotopic, 404--405 saliency, 130 tonotopic,
278 Masking, 104 McGurk effect, 343--344, 412 Meaning, scene perception
and, 103--104 Measuring perception, 13--18 adjustment, 14--15 classical
psychophysical methods, 14 constant stimuli, 14--15 method of limits, 14
thresholds, 14--15 Mechanisms, 15 Mechanoreceptors, 358--359, 374 Medial
geniculate nucleus, 282 Medial lemniscal pathway, 359 Medial superior
temporal (MST) area, 185, 189, 191 Medial temporal lobe (MTL), 85
Meditating, 142--143 Meissner corpuscle, 358--359 Melodic channeling,
304 Melodies characteristics of, 319--321 definition of, 319 description
of, 305--306, 312 intervals of notes in, 319--320 organization of notes
for, 319 tonality of notes in, 320--321 trajectory of notes in, 320
Melody schema, 305 Memories music and, 313--314 odor-evoked
autobiographical, 407 Memory olfaction and, 406--408 taste and, 407--408
Memory color, 217 Merkel receptor, 358--359, 364 Metamerism, 206
Metamers, 206 Meter description of, 315--316 language stress patterns
and, 318 movement and, 317--318 perception of, 318 Method of limits, 14
Methods feature brain ablation, 80 calcium imaging, 402 color matching,
205 cortical magnification, 76 dark adaptation curve measurement, 46
decibels and large ranges of pressures, 266 double dissociations, 81,
327 functional connectivity measurements, 34 head-mounted eye tracking,
144 hue cancellation, 211 magnitude estimation, 16 masking stimulus, 104
method of limits, 14 microstimulation, 185 Subject Index

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

487

Methods feature (Continued) neural frequency tuning curves, 278 neural
mind reading, 114 neuron recording, 22 precueing, 125 preferential
looking, 60--61 preferential reaching, 258 receptive fields, 69 spectral
sensitivity curve measurement, 49 tactile acuity measurement, 363
transcranial magnetic stimulation, 185 visual search, 126 Metrical
structure, 316 Metronome, 317 Microneurography, 371 Microsmatic, 397
Microspectrophotometry, 205 Microstimulation, 185, 244 Middle ear,
272--273 Middle-ear muscles, 273 Middle temporal (MT) area, 183--185,
189--190 Mild cognitive impairment (MCI), 399 Mind--body problem, 35--37
Mirror neurons, 164--165 Misapplied size constancy scaling, 252--254
Modularity, 31 Module, 31 Monkeys brain ablation, 80 depth perception
and, 244 double dissociations, 81 hand grip experiments, 161--162 mirror
neurons, 164 motion perception experiments, 184--186 pitch perception
experiment, 283--284 receptive fields and, 136 sound perception
experiments, 298--299 specificity coding, 395 tactile object perception
in, 365, 369--370 Monochromatism, 207 Monochromats, 207 Monocular cues,
231--236 integration of, 235--236 motion-produced cues, 234--236
pictorial cues and, 231--234 Monosodium glutamate (MSG), 412 Moon
illusion, 255--256 Motherese, 353--354 Motion. See also Movement
aftereffects of, 179--180, 191--192 apparent, 179--180, 188 biological,
188--190, 192--193 depth cues and, 234--236 implied, 190--192 induced,
179 no-implied, 191 real, 179--180 single-neuron responses to, 183--188
Motion aftereffect, 191--192 Motion parallax, 234, 246 Motion
perception, 175--194 aftereffects of, 179--180, 191--192 aperture
problem and, 186--187 apparent motion and, 179--180, 188 biological
motion and, 188--190, 192--193 brain activity and, 185, 189--190
corollary discharge theory and, 181--182 demonstrations of, 187
ecological approach to, 181 environmental information and, 181 functions
of, 176--178 488

implied motion and, 190--192 mechanisms for, 180 moving dot displays,
184--185 neural firing and, 183 optic array and, 181 point-light walkers
and, 188--190 real, 179--180 Reichardt detector and, 182--183
representational momentum and, 191 review questions on, 182, 194
shortest path constraint and, 188 Motion-produced cues, 234--236 Motor
signal (MS), 128--129, 181 Motor system, 369 Motor theory of speech
perception, 340--342 Movement. See also Motion beat and, 315 driving,
155 flow and, 151 focus of expansion, 151 gradient of flow, 150--151
invariant information, 151 optic flow, 150--152 perception and,
150--152, 176 walking, 154--155 wayfinding, 155--160 Movement-based
cues, 231 Moving dot displays, 184--185 Moving observer, 150--152
Mozart, 324--325 MRI. See Magnetic resonance imaging Müller-Lyer
illusion, 252--254 Multimodal interactions, 411 Multimodal nature of
flavor perception, 409 of pain, 377 of speech perception, 343--344
Multivoxel pattern analysis (MVPA), 114, 382 Munsell color system,
203--204 Music acoustics and, 301 adaptive function of, 312--313 brain
areas activated by, 314 cultural analysis of, 312 definition of,
311--312 emotions and, 321--327 as evolutionary adaptation, 312
expectancy in, 323--325 feelings and, 313 flavor affected by, 412
language and, comparisons between, 327--329 melody, 305--306 memories
and, 313--314 outcomes of, 313--314 pain perception and, 376 perception
and, 301 pitch perception and, 303 positive feelings elicited by, 313
review questions about, 321 rhythm of, 316 speech and, difference
between, 329 syncopation of, 316--317 vision and, 330--331 Music-evoked
autobiographical memory (MEAM), 313 Musical grouping, 321 Musical notes
intervals of, 319--320 organization of, 319 tonality of, 320--321
trajectory of, 320

Musical scale, 312 Musical syntax, 323--324 Musical timing, 314--318
beat, 315 meter, 315--316 rhythm, 316 syncopation, 316--317
Musicophilia: Tales of Music and the Brain, 328 Myopia
(nearsightedness), 45 Mythbusters (television program), 47 Naloxone,
377--378 Nasal pharynx, 408 Navigation brain areas for, 157--159 driving
and, 155 individual differences in, 158--159 walking and, 154--155
wayfinding, 155--157 Nerve fiber, 21--24 Nervous system color perception
and, 224 olfaction and, 408--410 taste perception and, 408--410 Neural
circuits, 51, 54 Neural code, 391 Neural convergence, 52--53. See also
Convergence Neural frequency tuning curve, 278 Neural maps, 75 Neural
mind reading, 114--116 Neural plasticity, 74 Neural processing, 8--9, 21
convergence and, 51--55 lateral inhibition and, 56--58 orientation
columns, 78 Neurons action potentials and, 23--24 audiovisual mirror,
165 bimodal, 409 binocular, 243 chemical basis of, 24--25 components of,
21--22 cortical, 70 delay units, 182--183 electrical signals in, 21--22,
27 higher-level, 83--85 ITD tuning curves, 297--298 mirror, 164--165
opponent, 211--213 orientation tuning curve of, 70 output units,
182--183 pitch, 282--283 properties of, 71--72 receptive fields of,
69--72 receptor sites, 26 recording electrical signals, 22--23 speech
and, 350--351 spontaneous activity of, 24 synaptic transmission, 25--26
transmission between, 25--27 V1, 78--79 Neuropathic pain, 373
Neuropsychology, 31, 81 Neurotransmitters, 26 Newborns. See Infant(s)
No-implied motion, 191 Nocebo effect, 375 Nociceptive pain, 373
Nociceptors, 373 Noise, 279

Subject Index

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

amplitude-modulated, 281 frequency-matched, 283 leisure, 285
Noise-induced hearing loss, 284--285 Non-decision-point landmarks, 156
Noncorresponding points, 239--240 Nonspectral colors, 203 Nontasters,
396 Nucleus accumbens (NAcc), 326 Nucleus of the solitary tract,
392--393 Object discrimination problem, 80 Object perception, 88--121
affordances and, 152--154 blurred objects, 93--94 brain activity and,
118 demonstrations of, 89, 106 face perception and, 110--111, 116--119
hidden objects, 93--94 inverse projection problem and, 91--92 movement
and, 176 perceptual organization and, 94--102 review questions on, 102,
109, 120 scenes and, 103--109 touch perception and, 368--371 viewpoint
invariance, 94 Oblique effect, 13 Observing action in others, 164--165
pain in others, 379--380 Occipital cortex, 8, 111 Occipital lobe, 69
Occlusion, 231--232 Octave, 270 Ocular dominance columns, 78n Oculomotor
cues, 231 Odor(s) attention affected by, 412 colors and, 412 detecting,
398 identification of, 398 identifying, 398 recognition profile of, 403
recognizing, 398, 406 representing, 405--406 textures and, 412
Odor-evoked autobiographical memories (OEAMs), 407 Odor objects, 401
Odotopic map, 404--405 Olfaction, 389--390, 397--408 brain and, 405--410
COVID-19 effects on, 389--390, 399 demonstrations of, 398 detecting
odors, 398 flavor perception and, 408--411 functions of, 398 genetic
differences in, 398--399 identifying odors, 398 importance of, 397--398
infant perception of, 413--415 memory and, 406--408 molecular features
and, 400--404 odor quality and, 400--401 Olfactory bulb, 401--402
Olfactory mucosa, 401, 408, 410 Olfactory pathway, 408 Olfactory
receptor neurons, 401--403, 410 Olfactory system, 400--401 brain and,
405--407 odor object perception, 401 receptor neurons and, 401--403

Ommatidia, 56 Onset synchrony, 303 Opioids, 377--378 Opponent neurons,
211--213 Opponent-process theory, 210--213 Opsin, 45, 48 Optic array,
181 Optic chiasm, 68 Optic flow, 150--152 Optic nerve, 42 Optical brain
imaging, 12 physiological, 13 Oral capture, 408 Orbitofrontal cortex,
405, 409--411 Organ of Corti, 274--275, 285 Orientation columns, 77--78
Orientation tuning curve, 70 Ossicles, 272--273 Outer ear, 272 Outer
hair cells, 274 Outer segments, 41 Output units, 182--183 Oval window,
272 Overt attention, 124 Pacinian corpuscle, 359, 364--365 Pain
definition of, 373 observing of, in others, 379--380 physical-social
pain overlap hypothesis, 381 review questions about, 385 social aspects
of, 378--385 of social rejection, 381--382 social touch effects on, 379
Pain perception, 373--382. See also Touch perception affective
(emotional) component of, 377 attention and, 375--376 brain and, 33,
376--378 direct pathway model of, 373--374 emotional components of, 376
empathy and, 379 endorphins and, 377 expectation and, 375 gate control
model of, 373--374 hypnotic suggestion and, 377 multimodal nature of,
377 music and, 376 observing in others, 379--380 opioids and, 377--378
phantom limbs and, 373--374 placebo effect and, 375, 378 sensory
component of, 377 types of, 373 Paint, mixing, 201--202 Papillae,
391--393 Parahippocampal cortex (PHC), 113--114 Parahippocampal gyrus,
156 Parahippocampal place area (PPA), 113--114, 213 Parentese, 353--354
Parietal lobe, 8 Parietal reach region, 160 Partial color constancy, 216
Passive touch, 368 Pauses, 319 PC fiber, 359 Peering amplitude, 246
Penumbra, 222 Perceived brightness, 18 Perceived contrast, 134--135
Perceived magnitude, 16 Perception, See also Face perception; Flavor
perception; Motion perception; Object

perception; Speech perception; Touch perception; Visual perception
bottom-up processing, 10 brain activity and, 113--114 convergence and,
51--55 demonstrations of, 10 difference between physical and, 18--19
ecological approach to, 150--154 environmental information and, 117
feature detectors in, 72--75 inference in, 108--109 introduction to,
3--5 lateral inhibition and, 56--58 of lightness, 58 measurement of,
13--18 of meter, 318 movement and, 150--152 performance and, 167--168
process of, 6--13 psychophysical approach to, 13--15 responses and,
167--168 reverberation time and, 301 review questions on, 13, 18
sensation versus, 6 top-down processing, 10 Perceptron, 3--4 Perceptual
constancy, 339 Perceptual functions mapping of, 30--32 modularity of, 31
Perceptual grouping, 94--99, 292 Perceptual organization, 86, 94 color
perception and, 198 defined, 94 experience and, 101--102 Gestalt
principles of, 94--101 grouping, 94--99 motion perception and, 188--190
segregation, 94, 99--102 Perceptual process dark adaptation and, 46--49
demonstrations of, 10 depth perception and, 243 description of, 6--10
diagrams of, 10 knowledge and, 10 light and, 40 spectral sensitivity,
49--51 study of, 11--13 transduction and, 45--46 visual receptors and,
40--45 Perceptual process (cycle), 184, 243 Perceptual segregation,
99--102 Performance, perception and, 167--168 Periodic sounds, 271
Periodic waveform, 268 Peripheral acuity, 54 Peripheral retina, 41
Peripheral tasks, 138 Permeability, 24 Persistence of vision, 104
Perspective convergence, 232--233 Phantom limbs, 373--374 Phase locking,
276, 281 Phenomenological reports, 17 Phenylthiocarbamide (PTC), 396
Phonemes, 338, 342 perception of, 344 phonetic features, 351 variability
problem and, 338--339 Phonemic restoration effect, 344 Subject Index

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

489

Phonetic boundary, 341 Phonetic features, 351 Phonetic symbols, 338
Photons, 207 Photoreceptors, 40 Phrenology, 30--31 Physical
regularities, 105--106 Physical-social pain overlap hypothesis, 381
Physical tasks and judgments, 17--18 Physiology-perception relationship
depth perception and, 243--244 differences, 18--19 physical activity
and, 149 sound and, 264--268 touch and pain, 373--382 Pictorial cues,
231--234, 257--259 Pigment epithelium, 49 Pinnae, 272, 295 Piriform
cortex, 406--407 Pitch brain mechanisms determining, 283--284 defined,
270, 312 perception of, 280--281 place and, 280 similarity of, 304
temporal information and, 281 Pitch neurons, 282--283 Place cells, 157
Place codes, 298 Place fields, 157 Place of articulation, 337 Place
theory of hearing, 281 defined, 280 physiological evidence for, 280
Placebo, 375 Placebo effect, 375, 378 Plasticity of brain, 382--383
experience-dependent, 74, 158, 382--383 Platoon, 323 Point-light
walkers, 178, 188--190 Ponzo illusion, 254 Population coding, 29--30,
298, 393--394 Pragnanz, 97 Preattentive processing, 142 Precedence
effect, 291, 300 Precueing, 125--126 Predictive coding, 109, 123
Predictive remapping of attention, 130 Preferential looking (PL)
technique, 60--61 Preferential reaching, 258 Presbycusis, 284, 286--288
Presbyopia, 45 Pretty Woman (film), 138 Primary auditory cortex, 282
Primary olfactory area, 405 Primary receiving area, 8 Primary
somatosensory cortex (S1), 361, 380 Principles of common fate, 98 of
common region, 98 of good continuation, 96--97 of good figure, 97 of
Pragnanz, 97 of proximity, 98 of representation, 7 of simplicity, 97 of
transformation, 7 of uniform connectedness, 98--99 of uninvariance, 207
Principles of Psychology (James), 124 490

Pronunciation, 339--340 Propagated responses, 23 Proprioception, 358
Prosopagnosia, 111 Protanopia, 209 Proust effect, 407 Proximal stimulus,
7 Proximity (nearness), 98 Psycho, 322 Psychophysics, 14--15 Pupil, 40
Pure tones, 265--266 Purkinje shift, 50 RA1 fibers, 359 Random-dot
stereograms, 241--242, 257 Rapidly adapting (RA) fibers, 359
Rarefaction, 265 Rat-man demonstration, 10--12, 14 Ratio principle, 220
Reaching, 160--162 Reaction time, 16--17 Real motion, 179--180 Receptive
fields attention and, 136 center-surround, 57--59 coordinated, 307 of
cortical cell, 70 flexible, 86 location columns, 77 on receptor surface,
69 stimuli for determining, 69 Receptor(s) olfactory, 390 review
questions on, 51 rod and cone, 40--42, 46--48 sensory, 5, 7, 22 skin,
358--360, 369--370 visual, 40--42 Receptor processes bottom-up
processing, 10 description of, 8 diagrams of, 8 top-down processing, 10
Receptor sites, 391--393 Recognition, 9, 16 odors and, 398, 403
Recognition by components, 102--103 Recognition profile, 403 Recognition
testing, 16 Recording electrodes, 22--23 Reference electrodes, 22
Reflectance, 220 curves, 200 edges, 221 Reflection, selective, 200
Refractive errors, 44--45 Refractive myopia, 45 Refractory period, 24
Regularities in the environment light-from-above assumption, 105
physical, 105--106 semantic, 105--107 Reichardt detector, 182--183
Relative height, 231 Relative size, 231--232, 256 Representational
momentum, 191 Resolved harmonics, 281 Resonance, 272 Resonant frequency,
272 Resting potential, 22

Resting-state functional connectivity, 35 Resting-state functional
magnetic resonance imaging, 34--35 Retina, 41 ambiguous stimuli, 92--93
binocular cues and, 238--239 focusing light onto, 43 pathway of, to
brain, 68--69 Retinal, 45 Retinitis pigmentosa, 42 Retinotopic map, 75,
77 Retronasal route, 408 Return to the tonic, 320 Reverberation time,
301 Reversible figure-ground, 99 Review questions action, 149, 172
auditory localization, 302 auditory system, 271--272, 279, 288 chemical
senses, 397 cochlear implants, 354 color perception, 204, 214, 226 depth
perception, 246, 259 flavor perception, 415 hearing, 271--272, 279, 288
motion perception, 182, 194 music, 321 object perception, 102, 109, 120
perception, 13, 18 taste, 397, 415 vision, 62--63 Reward value of food,
410 Rhythm, 316 Rising phase of the action potential, 24--25 Rod(s),
40--41 convergence and, 51--53 dark adaptation and, 46--48 spectral
sensitivity and, 49--50 Rod and cone receptors, 40--42, 46--48, 50
Rod-cone break, 48 Rod monochromats, 48 Rod spectral sensitivity, 49 Rod
vision, 53--55 Ruffini cylinder, 359 SA1 fibers, 358 Saccadic eye
movements, 128 Salience, 130--131 Saliency maps, 130 Salty tastes,
390--391, 395 Same-object advantage, 134 Saturation, 200 Scala tympani,
273 Scala vestibuli, 273 Scene, 103--104 Scene perception, 103--109 gist
of a scene, 103--105 global image features, 104 meaning and, 103--104
regularities in the environment, 105--107 Scene schema, 106, 131--132
Secondary olfactory area, 405 Secondary somatosensory cortex (S2),
z361-- 362, 380 "Seeing," 142 Segregation, perceptual, 94, 99--102
Selective adaptation effects of, 72--73 measurement of, 72 Selective
attention, 124--125 Selective listening experiments, 124--125

Subject Index

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Selective rearing, 74--75 Selective reflection, 200 Selective
transmission, 200 Semantic regularities, 105--107 Semitones, 319--320
Sensations, 5--6, 95 Senses, 389--416 balance and, 152 flavor perception
and, 408--411 multimodal interactions, 411 olfaction and, 397--408
overview of, 389--390 primary receiving areas, 8 review questions on,
397 taste perception and, 390--397 Sensorimotor hearing loss, 351
Sensory coding definition of, 27 population coding, 29--30 sparse
coding, 29 specificity coding, 27--29, 394--396 Sensory component of
pain, 377 Sensory receptors, 5, 7, 22 Sensory-specific satiety, 410--411
Sensory system, 368 Sentences, word meaning in, 345--346 Sequential
grouping, 303--306 Shadow(s) depth cues and, 233--234, 258--259
lightness constancy and, 221--222 penumbra, 222 three-dimensionality
and, 233 Shadow-casting technique, 59 Shadowing, 124, 345 Sharply tuned
neurons, 298 Shortest path constraint, 188 Similarity, 97--98 Simple
cortical cells, 70--71 Simultaneous grouping, 303 Sine wave, 265
Single-neuron responses to motion, 183--188 6-n-propylthiouracil (PROP),
396 Size constancy, 250--252 Size-distance scaling, 250 Size perception,
247--255 demonstrations of, 250, 253 depth perception and, 247--255
Holway and Boring experiment on, 247--250 illusions of depth and,
252--255 misapplied size constancy scaling and, 252--254 size constancy
and, 250--252 size-distance scaling and, 250 visual angles and, 247--248
Size-weight illusion, 163 Skin layers of, 358--359 mechanoreceptors in,
358--359, 369, 374 nerve pathways from, 359--361 vibration of, 365 Slide
projector, 70 Slowly adapting (SA) fibers, 358 Smartphone distractions
while driving, 138--141 Social pain, 378--382 Social perception,
177--178 Social rejection, 381--382 Social touch description of,
371--372 in infants, 384--385 pain reduction by, 379 Sodium channels, 24
Sodium-potassium pump, 25 Somatosensory cortex, 361--362, 370

Somatosensory system, 358 Sonar, 246 Sound amplitude of, 265--266,
268--269 aperiodic, 271 definitions of, 264 direct, 299--300 frequency
of, 265--266 indirect, 299--301 localizing, 293--299, 303 loudness and,
268--270 loudspeakers and, 300 perceptual aspects of, 268 periodic, 271
physical aspects of, 264--268 pressure changes and, 264--265 pure tones
and, 265--266 review questions on, 288 separating sources of, 302--306
Sound level, 267 Sound pressure level, 267 Sound spectrograms, 336--337,
339--340 Sound waves, 265 Sour tastes, 390, 395 Spaciousness factor, 301
Sparse coding, 29 Spatial attention, 125--126 Spatial cues, 366 Spatial
layout hypothesis, 113 Spatial neglect, 141--142 Spatial updating, 155
Specificity coding, 27--29, 394--396 Spectral colors, 203 Spectral cues,
294--296 Spectral sensitivity, 49--51 Spectral sensitivity curves,
49--50 Spectrograms, 340 Spectrograph, 340 Spectrometer, 49 Speech
perception, 335--355 acoustic signals and, 336--340 adult-directed
speech, 353--354 audiovisual, 344 brain activity and, 349--351
dual-stream model of, 350 face movements, 343--344 fusiform face area
and, 344 infant-directed speech, 353--354, 385 information for, 342--347
lip movements, 343--344 motor processes in, 342--343 motor theory of,
340--342 multimodality of, 343--344 perceptual constancy in, 339
phonemes and, 338 production and, 340, 343 pronunciation variability,
339--340 sound spectograms and, 336--337, 339--340 stimulus dimensions
of, 336--338 units of speech and, 337--338 variability problem and,
338--340 vision and, 344 voice onset time, 340--342 Speech segmentation,
345 Speech spectrograph, 340 Spinothalamic pathway, 359--360 SPL. See
Sound pressure level Spontaneous activity, 24 Spontaneous looking
preferences, 61 Stapes, 272 "Star-Spangled Banner, The," 316 Stars Wars,
Episode III: Revenge of the Sith, 345

Static orientation-matching task, 82 Statistical learning, 346
Stereopsis, 240--242 Stereoscope, 242 Stereoscopic depth perception,
236--237 Still pictures, motion responses to, 190--192 Stimulus constant
stimuli, 14--15 description of, 17 diagrams of, 12 distal, 7 identity
of, 16 perception and, 134--135 perceptual magnitude of, 16 proximal, 7
reaction time, 16--17 speech, 335--336 Stimulus-perception relationship,
184, 243 Stimulus-physiology relationship, 12--13 Strabismus, 236
Streams, information, 80--83 Striate cortex definition of, 69 motion
perception and, 189 neural map in, 75--77 what pathway, 80--82 where
pathway, 81 Stroboscope, 95 Structural connectivity, 33--34
Structuralism, 94--95 Subcortical structures, 282 Subtractive color
mixture, 202 Superior colliculus, 68 Superior olivary nucleus, 282
Superior temporal sulcus (STS), 32, 111, 189, 344 Supertasters, 397
Surface texture, 366--368 Sweet blindness, 396 Sweet tastes, 390--391,
395, 397 Swinging room experiment, 152--153 Synapse, 25--26 Synaptic
vesicles, 26 Syncopation, 316--317 Syntax event-related potential for
studying, 323--324 musical, 323--324 Tactile acuity, 363--364 attention
and, 370--371 cortical mechanisms for, 364--365, 369--370 methods of
measuring, 363 receptor mechanisms for, 363--364 Task-related functional
magnetic resonance imaging, 34 Taste, 390--397 basic qualities of, 390
flavor perception and, 408--410 genetic differences in, 396--397
individual differences in, 396--397 infants and, 413--415 memory and,
407--408 neural code for, 391--396 olfaction and, 408 physiology of,
391--396 population coding, 393--394, 396 preference and, 397 review
questions on, 397, 415 specialized receptors and, 397 specificity
coding, 394--396 Taste buds, 391--393, 396--397 Taste cells, 392--393
Taste pores, 392 Subject Index 491

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.

Taste quality, 390--391 Tasters, 396--397 Tectorial membrane, 274
Temporal coding, 281 Temporal cues, 366--367 Temporal lobe, 8 Temporal
structure, 312 Texture gradient, 233, 252 Texture perception, 365--368
#TheDress, 218--220 Theory of unconscious inference, 107--108
Thresholds, 13 absolute, 13 audibility curve and, 286--287 difference,
15 frequency, 269--270 measurement of, 15--16 wavelengths and, 49--50
Thrills, 322 Tiling, 79 Timbre, 271, 303, 312 Tip links, 275 Tonality of
notes, 320--321 Tone chroma, 270 Tone height, 270 Tongue, 391--393
Tonotopic map, 278 Top-down processing, 10 pain perception and, 375--376
perception and, 10 receptor processes, 10 social touch and, 372 speech
perception and, 344 Touch perception, 357--385. See also Cutaneous
senses active touch and, 368 cortical mechanisms for, 364--365
demonstrations of, 364, 367 detail perception and, 362--365 haptic
exploration and, 368--369 importance of, 357--358 measuring acuity of,
363 nerve pathways for, 359--361 object perception and, 368--371
observing in others, 379--380 pain perception and, 373--382 passive
touch and, 368 skin receptors and, 358--360 social touch, 371--372
texture perception and, 365--368 vibration perception and, 365--367
Trajectory of musical notes, 320 Transcranial magnetic stimulation
(TMS), 185, 190, 342 Transduction, 8, 45 auditory, 275 of light, 45--46
visual, 45 Transitional probabilities, 346 Transmission cells, 374
Transmission curves, 200 Traveling wave, 276--277 Trichromatic theory of
color vision, 204--210 Trichromats, 208 Triple meter, 316 Tritanopia,
209 Tuning curves disparity, 243 ITD, 297--298 neural frequency, 278
Two-flash illusion, 306 492

Two-point thresholds, 363--364 Tympanic membrane, 272--273 Ultraviolet
light, 60 Umami tastes, 390 Unconscious inference, 107--108 Uncrossed
disparity, 240 Unexpected Visitor, An (Repin), 131 Uniform
connectedness, 98--99 Unilateral dichromat, 209 Unique hues, 211, 213
Unresolved harmonics, 281 U.S. Occupational Safety and Health Agency
(OSHA), 285 Value, 203 Ventral pathway, 81--82 Ventriloquism effect,
306, 411 Ventrolateral nucleus, 360 Vestibular system, 318 Vibration
perception, 365--367 Video microscopy, 397 Viewpoint invariance, 94
Visible light, 40 Visible spectrum, 49, 198--200, 202, 223 Vision. See
also Visual system attention and, 130--131 balance and, 152 color
perception and, 197--227 dark adaptation and, 46--49 depth perception
and, 229--260 hearing and, 306--308 motion perception and, 181
persistence of, 104 size perception and, 247--255 speech perception and,
344 Visual acuity of cones, 53--55 development of, 60--62 Visual angle,
247--248 Visual attention. See Attention Visual cortex. See also Cortex
brain pathways, 68--69 column organization of, 77--78 cortical
magnification in, 75--77 fovea in, 75--76 hypercolumns of, 78 location
columns of, 77--78 neurons in, receptive fields of, 69--72 orientation
columns of, 77--78 signal processing in, 67--68 spatial orientation in,
74--79 Visual direction strategy, 154 Visual evoked potential (VEP),
61--62 Visual field, 68 Visual form agnosia, 9 Visual illusions,
252--255 Ames room, 254--255 apparent movement, 95 moon illusion,
255--256 Müller-Lyer illusion, 252--254 Ponzo illusion, 254 waterfall
illusion, 179--180 Visual impairments blind spots, 42--43 color
blindness, 197--198 detached retina, 49 macular degeneration, 41--42
retinitis pigmentosa, 42 strabismus, 236 Visual masking stimulus, 104

Visual perception dark adaptation, 46--49 demonstrations of, 43, 54
infants and, 60--62 review questions on, 62--63 spectral sensitivity,
49--51 Visual pigments, 41, 51 absorption spectrum of, 207 bleaching of,
48--49 color perception and, 207--210 molecules of, 45--46, 49
regeneration of, 48--49 Visual receiving area, 69 Visual receptors, 40
Visual salience, 130--131 Visual scanning, 127--128 covert attention,
124 overt attention, 124 saccadic eye movements, 128 Visual search, 126
Visual signal brain pathways for, 68--69 lateral geniculate nucleus, 68
striate cortex processing of, 79 Visual system. See also Eye(s); Vision
balance and, 152 color perception and, 215, 217--222, 224 cortical
columns and, 77--78 diagram of, 68 focusing, 43--45 impairments of,
41--43, 45, 49, 197--198, 236 receptors of, 7--8, 40--41 Visual
transduction, 45 Vocal tract, 336 Voice onset time (VOT), 340--342
"Vowel triangle," 354 Vowels, 336, 338 Voxels, 114 Walking blind
experiment in, 154--155 environmental information and, 154--155
navigation and, 154--155 Walleye, 236 Waterfall illusion, 179--180
Wavelengths, 40, 49--50, 200--203, 207, 223--224 Waves (Hurskainen), 98
Wayfinding brain and, 156--159 environmental information and, 155--157
individual differences in, 158--159 landmarks, 156--157 Wernicke's
aphasia, 349 Wernicke's area, 349--350 What auditory pathway, 299 What
pathway, 80--81 Where auditory pathway, 299 Where pathway, 81 Whiteout
conditions, 247 Whole-hand prehension, 161 Wizard of Oz (film), 138 Word
deafness, 350 Words learning about, 346--347 meaning of, in sentences,
345--346 meaningfulness of, 345 Young-Helmholtz theory, 204

Subject Index

Copyright 2022 Cengage Learning. All Rights Reserved. May not be copied,
scanned, or duplicated, in whole or in part. Due to electronic rights,
some third party content may be suppressed from the eBook and/or
eChapter(s). Editorial review has deemed that any suppressed content
does not materially affect the overall learning experience. Cengage
Learning reserves the right to remove additional content at any time if
subsequent rights restrictions require it.


