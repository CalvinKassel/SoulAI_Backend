Contents Title Page . . . . . . . . . . . . . . . . . . . . . . . . . .
. . i Publisher Information . . . . . . . . . . . . . . . . . . . . ii
Quote . . . . . . . . . . . . . . . . . . . . . . . . . . . . . iii
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . iv
Chapter 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . .1
Chapter 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
Chapter 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
Chapter 4 . . . . . . . . . . . . . . . . . . . . . . . . . . . 59
Chapter 5 . . . . . . . . . . . . . . . . . . . . . . . . . . . 77
Chapter 6 . . . . . . . . . . . . . . . . . . . . . . . . . . 101
Chapter 7 . . . . . . . . . . . . . . . . . . . . . . . . . . 117
Chapter 8 . . . . . . . . . . . . . . . . . . . . . . . . . . 137
Chapter 9 . . . . . . . . . . . . . . . . . . . . . . . . . . 166
Appendix 1 . . . . . . . . . . . . . . . . . . . . . . . . . 176 Notes
and Sources . . . . . . . . . . . . . . . . . . . . . 178 Also Available
. . . . . . . . . . . . . . . . . . . . . . . 196

Rethinking Thinking Problem Solving from Sun Tzu to Google

Martin Cohen imprint-academic.com

Published in 2022 by Imprint Academic imprintacademic.co.uk PO Box 200
Exeter EX5 5YX United Kingdom Digital edition converted and distributed
by Andrews UK Limited andrewsuk.com Copyright © 2022 Martin Cohen The
right of Martin Cohen to be identified as the author of this work has
been asserted in accordance with the Copyright, Designs and Patents Act
1988. All rights reserved. No reproduction, copy or transmission of this
publication may be made without express prior written permission. No
paragraph of this publication may be reproduced, copied or transmitted
except with express prior written permission or in accordance with the
provisions of the Copyright Act 1956 (as amended). Any person who
commits any unauthorised act in relation to this publication may be
liable to criminal prosecution and civil claims for damage. The views
and opinions expressed herein belong to the author and do not
necessarily reflect those of Imprint Academic or Andrews UK Limited.

Too often we don't think, we just respond. Yet that's not what makes us
human and that's not where the big prizes in life are to be found.

Introduction Everyone, as the French philosopher René Descartes pointed
out, thinks. That's the easy bit. The second part, which is what this
book is really about, is how to make your thinking effective. Because
too often we don't really engage the gears of our brain, don't really
look at issues in an original or active way, we just respond. Like
computers---or is it cats and dogs seeking treats?---inputs are
processed according to established rules and outputs are thus largely
predetermined. Yet that's not what makes us human and that's not where
the big prizes in life are to be found. The key thing about thinking is
that it is really a tool: a mechanism that countless millions of years
of evolution has sculpted and shaped to enable us to both make sense of
and flourish within our environment. And yet, too often, both in
education and practical life, thinking is really the poor relation to
action. Schools privilege rote learning, repetition and recall---but not
original thinking. Likewise, businesses expect their employees to
perform preordained rituals and routines. The end result is that Homo
sapiens, literally the 'thinking human', rarely justifies this special
biological category. Yet the ability to think, meaning the ability to
imagine new possibilities and find new solutions to old problems, is our
great gift as a species and where the big prizes in life are found. In
society today, though, that's not where the emphasis lies. Instead, in
an increasingly totalitarian age, governments erode our sphere of
control and whole populations are corralled into evermore circumscribed
roles and routines. The mass media serves up pre-formatted opinions,
which we consume passively. At the same time, when we look around us, it
seems obvious that social change

and technological progress makes it more important that we rethink
thinking. In the Third Millennium, when we look around us, two things
seem clear. First, that we need to think a bit more---not less! And the
second is that (with a nod to Apple computers) we need to think
differently. For both these reasons, the focus in this book will be not
on airy commentary but instead on practical suggestions about ways to
think differently... on thinking strategies that each have their own
style, applications and benefits. That said, any investigation of how we
think, and why we think the ways we do, is at root a profoundly
philosophical investigation. It is both a theoretical matter relating to
the process of discovery or gaining knowledge---and a way of seeking
powerful insights for everyday life. After all, it was the practical
value of his sudden insight (into how to calculate the volume of
irregularly shaped objects) that made Archimedes suddenly exclaim
'Eureka!' in his bath. So let's not hesitate to find time to ask
questions like: Why do we think the way we do? What drives our
decisions? And how do we process new ideas and information? Thinkers,
ranging from philosophers like Plato to scientists like Albert
Einstein---and on to social scientists like Amos Tversky (or even
management gurus, like Peter Drucker)---all emphasise the importance of
putting preconceptions aside and thinking things through afresh. Of
course, such talk can easily become mere platitudes, easy to say but
without application to real life. Yet here a favourite story of mine,
about a garden equipment firm, is revealing. It's a very short story,
actually: and just records what happened when, one year, the firm's
bosses asked its engineers to come up with a new kind of lawn mower.
What do you think happened next? The answer is 'not much'. After much
head-scratching, all that the engineers managed to come up with were
refinements to the existing machines. But then someone suggested that
they go back one step---and rather than trying to improve the existing
technology, which meant everyone's thoughts were channelled down the old
paths, instead to think about completely new machines to help maintain
lawns.

Out of this change of approach came the concept (and eventually the
product) of the 'strimmer'. Okay, it's a rather specific example, but
the point is as broad as they come: the answers you find depend on the
questions you ask. Or, as the old saying goes, 'If all you have is a
hammer, everything looks like a nail.' And that's why what I propose to
do in the chapters that follow is to offer a whole range of new 'tools
for thinking', drawn from all kinds of disciplines and activities, not
just airy commentary about great thinkers, let alone about the latest
science of the brain, but rather very practical suggestions about
different kinds of thinking strategies. Strategies that each have their
own style, applications and benefits. The chapters are modular---feel
free to dip in and out---and within them there are also distinct,
modular elements such as the different kinds of fact boxes, some marking
out 'interesting asides' and others working more as 'detail boxes',
essentially 'a closer look' at something in the main text. In this
structural sense, it is a book using the kind of devices made popular by
the 'For Dummies' books, for which I have written two titles, one on
thinking seen in the broadest sense, which is as mainstream philosophy,
and one on a particular branch of thinking skills: critical thinking.
These also use a modular system, with books avoiding the presumption
that people pick them up and work their way progressively and linearly
through them. Today's busy (distracted!) reader rarely does that. So
this is a 'dip in-and-out book', with a central thread---in this case,
the roots of inspiration. Throughout, my writing strategy has been to
distil large amounts of information into a short space. In the Internet
Age people want, indeed demand, that information be presented in an
easily digested form that is yet thoughtful and scholarly. They want
books that provide a good return on their time! My first chapter,
'Thinking like a Chinese General', illustrates the breadth of possible
sources for insights into thinking. Who would have thought that an
ancient study of 'military tactics' could have so many other practical
applications today? Yet, assuredly, it

has. Don't be confused by the surface details---this is a book with
insights for everyone. Indeed, I wouldn't say the Chinese invented the
art of thinking, but it certainly seems that they produced one of the
first books about it when Sun Tzu complied The Art of War, 2500 years
ago. The book is rightly considered the classic work on thinking
directed towards achieving precise ends. Military strategies, but also
business strategies and life strategies. Since its 'rediscovery' in the
West, in the last century or so, hundreds of books examining its
insights have been published in many different languages, and its ideas
have been applied to fiefs as diverse as business management and
training in sports. In a way, this is very odd, not to say whimsical.
After all, The Art of War is literally a guide to fighting wars using
medieval techniques---and today, technology has advanced somewhat with a
terrifying range of weapons seeming to make the tactics involved in
warfare less about human psychology, let alone human needs and values,
than who has the best missiles and computers. But that's looking at
warfare at a very superficial level. When we dig deeper, we find more
important truths that transcend the years. Not least because, as Sun Tzu
reminds us, technology may be clever stuff but the 'greatest victory is
that which requires no battle'. And so, what this Chinese classic really
offers today is timeless insights into human psychology and social life.
My second chapter, 'Thinking like a Designer', attempts to reunite two
very different kinds of thinking: the rational and the irrational. Now
'irrational' is a word that immediately sounds warning bells to most
people, because we are taught to imagine thinking as a kind of precise,
evidence-based conversation between 'higher and lower' level parts of
our brains. However, there are plenty of other philosophical and
cultural traditions that treat thinking as a much broader process,
encompassing thought not only as a kind of inner dialogue, but also
allowing a place for sensations, emotions, feelings---and aesthetic
judgments generally. Rational thinking attempts (shall we say
'pretends'?) to be universal and

transcend the individual, but irrational thinking allows a vital role
for individualism and self-awareness too. In this way, 'design skills'
and 'design thinking' are part of a very old tradition that seeks to put
the human factor at the heart of its solutions to problems. And so, this
chapter explores a rich seam of ideas hacked from the bedrock of ideas
and experience not only from design and engineering, but from social
science, business studies and computer science as well. In chapter
three, entitled 'Thinking like a Biologist', I explore some of the
latest 'thinking about thinking', in the sense of how the brain 'works'.
I look at thinking here as the kind of thing that neuroscientists and
'cognitive scientists' study because understanding the way our minds
work is a vital first step towards making better use of them. Actually,
this is a key thread running through the whole book. But in this
chapter, I start by noting, for example, that although for many years it
was assumed that, after age one, humans never made another brain cell,
and instead that all the ones they would ever need were already in place
in a tiny infant, nowadays, though, research indicates that human brains
continue to grow---to create new neurons---even in old age. Maybe the
old science idea fitted not only religious conceptions of babies as tiny
people, but social prejudices favouring inherited privilege, whereas the
new theories, to some extent, match economic and social forces
determined to promote 'lifelong learning' and flexible workforces.
Either way, what really seems to distinguish infant thinking from that
of adults is the new pathways and links created in the brain as a result
of experience. These connections represent ways of making sense of the
world. But before the links are created in physical networks of neurons,
they exist as playful patterns in the mind, just as daydreams or doodles
can lead to puzzles solved or problems identified. Chapter four,
'Thinking like a Scientist Ought to Think', investigates a thinking tool
that we use every time we imagine the world as it 'might be'. By
'thought experiments' I mean the distinctive kind of imaginary story
that actually seems to provide

a way to test out our ideas, explore relationships and even discover new
information. At their grandest, they are the kinds of things that
Galileo and Einstein used to explore the fundamental structure of the
universe. However, despite this splendid aspect and pedigree, many
philosophers don't like thought experiments too much, saying that they
overly rely on intuition rather than logic---and scientists often
downplay the insights too. Nonetheless, Einstein makes a good case for
why they should be part of every thinker's toolbox. If chapter four is
about imaginary experiments, chapter five is all about gritty reality.
'Thinking like Mission Control', looks at a very rigorous and methodical
approach to problem solving. This chapter might actually have been
called 'Thinking like a Software Engineer', although the problem with
that is that it makes it sound a bit dull, whereas, actually, a good
piece of software often includes its own element of insight and
inspiration. We should maybe recall that computer science is, in the
phrase of Carl Sagan, 'a way of thinking much more than it is a body of
knowledge'. Put short, the space programme offers crucial insights into
new ways to tackle complex, multi-dimensional problems. Chapter six is
quite a change of pace again. 'Thinking like an Artist---or at least, a
Doodler', offers stories and insights into some very different but
equally important thinking skills. However, here, the paradigm case is
not Michelangelo painting the ceiling of the Sistine Chapel, but rather
JFK scribbling pictures of yachts on the White House notepaper! In this
chapter, I pick up a notion briefly alluded to in chapter three, that
doodling is an example of the benefits of allowing experimentation. As I
say, too often we lock ourselves into preconceived schemes: our thinking
gets stuck in ruts. Take, for example, the success of the Google Doodle.
It is one of Google's most popular innovations---but it came about
really by accident, simply as a variation on the 'out of office'
message. Or listen to the advice of an 'expert doodler', Oki Sato, the
chief designer and founder of the design firm Nendo, who says that new
ideas often start small and then spread. He compares them to the

idea of the 'butterfly effect' in weather, where it is said that even
the tiny movement of a butterfly's wing in China can, by a process of
feedback and amplification, eventually create a tornado halfway across
the world in Chicago. Sato is such an innovative and original thinker
that we can only benefit from finding out a bit more of his personal
story. Chapter seven, 'Thinking like a Cyberneticist---or is it a
Weather-Forecaster?', is in some ways the flip side of artful thinking
and creative design. And, certainly, it too has its sphere of
operations. The philosophy is that '\[t\]here are no separate systems.
The world is a continuum. Where to draw a boundary around a system
depends on the purpose of the discussion---the questions we want to
ask.' These are the words of Donella Meadows, an American environmental
scientist, teacher and writer. Klaus Mainzer offers another way to put
it in his book, Thinking in Complexity: Computational Dynamics of
Matter, Mind, and Mankind, when he says that, today, many if not most of
our social, ecological, economical and political problems are
essentially of a global, complex and nonlinear nature. One way or
another, and like it or not, the science of complexity is likely to be
among the most salient features of the twenty-first century. And so,
nonlinear, complex system approaches have increasingly become the key to
approaching and tackling problems in the natural sciences. In chapter
eight, 'Thinking like a Social Scientist (and not a Gambler)', I look
more closely at the difference between good and bad thinking skills and
why it matters that we get the 'heuristics right', to use a term
popularised---in as much as such an odd word can be---by the social
scientists Amos Tversky and Daniel Kahneman. The latter notes that
thoughts come to mind in one of two ways: either by 'orderly
computation', which involves a series of stages of remembering rules and
then applying them, or by perception, an evolutionary function that
allows us to predict outcomes based on what we're perceiving. Kahneman
says that perception is 'extended almost directly to intuitive
thinking'.

This leads unfortunately to some very common thinking errors. Ambiguity
is suppressed with the result that we see the world as much more
coherent than it is. Put short, our brain, Kahneman says, is a 'machine
for jumping to conclusions'! In this chapter, I look at the kinds of
thinking errors that people make when evaluating their actions in terms
of their expected outcomes and contribution to future happiness, whether
buying a new car, dreaming of purchasing 'a place in the sun'---or
deciding to vote for political extremists. Consideration of situations
where we do not have the necessary information to reach 'logical'
conclusions leads finally to the topic of the final chapter, 'Thinking
like a Search Engine', which is all about emergent thinking. But what's
that all about then? The explanation brings us back to Google. Google is
the world's most-used search engine. It has achieved that status by
creating a vast network of computers that can not only keep up with all
the webpages being created for the internet, but also respond in
fractions of a second to human enquiries. Yet that's only the easy
bit---the technological bit. Google is also about something much more
subtle, which is weighing and evaluating information. And in some ways
here, its methods are revealing. Google uses an algorithm (and using
algorithms is itself a powerful thinking tool) that privileges websites
that lots of other, especially reputable, websites have links
to---because those links represent votes in terms of quality. It also
assumes that if the words in your query are in the page, in headings in
the page and even in the filename for the page, then that page is likely
relevant. Google's method is not actually particularly clever when taken
apart---but it sure is complex when all put together. Likewise, in
living organisms, complexity emerges as the result of simple chemical
reactions following certain rules. It is these more complex molecules
that later become cells, and these cells which in turn interact to
become specialised organs. Organs interact to form organisms, which
interact, communicate and reproduce on ever higher scales to eventually
form the universe. Within Google's

search engine there are virtual molecules, again guided by rules, that
together create a new kind of artificial intelligence, one that
increasingly guides our own thinking and behaviour. To draw the threads
together, one key lesson for more effective thinking is to do simple
things, indeed embrace simplicity---but to do so systematically. I hope
the above will indicate to you that this is not only a 'big ideas' book
but one with a difference. Partly because my aim is to challenge some of
the assumptions made by philosophers and psychologists in favour of
'logical thinking' over intuition, and instead make the case for
creative insight, and partly because we live in a time when there is an
exceptional social awareness of the importance of thinking skills across
the whole range of political and social life and most particularly to
innovation and entrepreneurial ideas. The success of many 'new
technology' companies and entrepreneurs has given a renewed emphasis to
the value of original thinking, of finding new ways of looking at old
problems and of generating fresh insights. Because of this, I hope my
book will also mesh with current debates, the nature of education and
the limits of traditional courses and exam-based learning. Here, the
irony is that you can't be told how to think, you have to make the
critical connections, take the final steps, yourself. Alas, too many of
us emerge from formal education with our ability to think not so much
enhanced, but suppressed! At the very least, we will have been
channelled towards one particular and really rather narrow kind of
thinking: routinised and constrained. This is why I invite the reader to
join me on an active journey of discovery and research, rather than to
be merely another member of an audience listening to a lecture. So my
book is intended thus as a serious, practical contribution to a topic
that touches everyone, but at the same time it is definitely a book to
be read for pleasure. It is also a celebration of the diversity of the
human spirit, and the remarkable ability of a few people to
excel---sometimes against all the odds.

In everyday life, of course, most of us have to perform tightly
curtailed routines in which most of the 'thinking' and decisions are
done by others. However, the great thing about 'thinking' is trying on a
different thinking hat (to introduce a metaphor I will use often in this
book), which will refresh the whole way you see the world. And it may
only take one idea to revitalise a whole strategy, a whole life. Maybe
you will find that idea in this book! Martin Cohen Normandy, France,
2021

Chapter 1

Thinking like a Chinese General

Page from work attributed to Sun Tzu, 544--496 BCE

I wouldn't say the Chinese invented the art of thinking, but they
certainly produced one of the first books about it when the great
Chinese strategist Sun Tzu (and likely other sages too) compiled The Art
of War, no less than 2500 years ago. Originally, it was very much a
guide to military strategy, but don't be confused by that. This is a
book offering insights into all kinds of problems---challenges if you
prefer---that we all meet in life. But in terms of 'thinking hats',
meaning different strategies for approaching issues, the 'hat' you want
to wear here is not so

much a tin helmet as the kind of baseball cap favoured by leaders from
Trump to Obama, and film directors from John Huston to Steven Spielberg.
What does that kind of peaked cap signify? That you're in charge of the
situation and managing the details. It's quintessentially the kind of
message leaders of all stripes want to send. And so, today, The Art of
War has evolved from a guide for generals to being considered the
classic work on action directed towards achieving precise ends. Life
strategies, business strategies, military strategies. Hundreds of books
examining its insights have been published in many different languages,
and its ideas have been applied to fiefs as diverse as business
management and training in sports. Surely one of the most unusual 'book
endorsement' quotes ever was offered by Chairman Mao in the middle of
the twentieth century when he claimed the book as an inspiration for his
guerrilla warfare, saying: We must not belittle the saying in the book
of Sun Wu Tzu, the great military expert of ancient China, 'Know your
enemy and know yourself and you can fight a thousand battles without
disaster.'

Indeed, the advice of Sun Tzu was followed during the Vietnam War when
some Vietcong officers are known to have extensively studied The Art of
War and to have liked to recite entire passages from memory. General Võ
Nguyên Giáp, an avid student of Sun Tzu's ideas, successfully
implemented tactics described in the classic text during the Battle of
Dien Bien Phu, ending major French involvement in Indochina and leading
to the accords which partitioned Vietnam into North and South. Indeed,
it was America's defeat there, more than any other event, that brought
Sun Tzu to the attention of leaders of American military theory and thus
to today's business leaders. Likewise, the Korean resistance army
defeated the vastly better-armed United States military by cunning
tricks, shortcuts and rigging the odds in their favour---the strategic
significance

of which has never really been accepted by the US. Looking back on the
lessons of that war in 2017, in an article for Foreign Policy magazine,
Paul Yingling, one of whose roles was as the chief war planner for the
US Army's 2nd Infantry Division in South Korea, recalled that he had
spent 'considerable time' studying not merely the Korean War but the
links to Sun Tzu's teachings, including this passage: The supreme art of
war is to subdue the enemy without fighting.

He realised that the United States should have had a lot to learn from
the ancient classic but had failed to change its thinking. Worse still,
Yingling wrote, Sun Tzu outlined all the options and 'we still picked
the worst of the lot'. The options being, at the top, using the highest
form of generalship to balk the enemy's plans; the next best being to
prevent the junction of the enemy's forces; the next, attacking the
enemy's army in the field; and the worst policy of all being to besiege
walled cities. Despite this advice, Yingling finishes: 'most of my
training as a war planner consisted of exercises devoted to attacking
the enemy's army and besieging walled cities (or at least prepared
defences).' Put another way, one of the factors that make a general
great, and therefore rather a rare creature, is the willingness to
withstand the desire of most people to rush headlong into direct
engagements and instead to work out patiently how to go around rather
than through his opponent... choosing the course of least expectation
and the line of least resistance. 'Doing things the easy way' is perhaps
the core message of The Art of War, and it is advice which, like much
Chinese wisdom, is both wonderfully simple and somehow impenetrable. It
is also known that during the Cold War years the KGB studied---and
used---the strategy of deception at the heart of the book. Sun Tzu's
words, 'I will force the enemy to take our strength for weakness, and
our weakness for strength, and thus will turn his strength into
weakness', rang true for the Bolsheviks.

Maybe that's why Douglas MacArthur, 5 Star General & Supreme Commander
of the Allied Powers, is reputed to have always kept a copy of The Art
of War on his desk, although it may have been more of a fashion
statement than a practical resource as the general is also remembered
for wearing a Japanese ceremonial kimono, cooling himself with an
oriental fan, and smoking cigarettes in a jewelled cigarette holder. On
the other hand, in the early 1990s, American Gulf War generals Norman
Schwarzkopf and Colin Powell are known to have adapted ideas from the
book to ruthless and deadly effect on Saddam Hussein's desert
battalions. For them, the 'easy way' involved ideas like
'outmanoeuvring' and using 'overwhelming force'---which is why Saddam's
conscripts were killed in their tens of thousands by bombs dropped from
planes invisibly high in the sky. The Gulf War showed that the book had
plenty of fans amongst real-life generals. However, as I say, in crucial
ways, the text is only superficially about military tactics. Like all
great books, it is at heart about human needs and values. As, too,
ultimately are thinking skills. It is for this reason that an otherwise
actually rather impenetrable and obscure (impeccably Chinese) ancient
text contains insights, even for some of today's busiest business
leaders. Mind you, I suspect Sun Tzu's wisdom often gets rather lost in
translation. Take one of his favourite metaphors about the leader being
the 'hub at the centre of the wheel'. Today's business gurus habitually
say that this is in contrast to the Western view of leadership, where
leaders are at the top of the organization with the workforce serving
them in ever more remote 'layers'. They insist that, in Sun Tzu's model,
leaders position themselves at the centre and actively control the whole
organization. These active CEOs are like the centre of a wheel in that
they connect to everything, just as all of the spokes meet at the hub.
The only problem is that if the hub is not strong enough the wheel will
inevitably collapse. However, fine point though this may or may not be,
it is not Sun Tzu's one. Indeed, his is rather the opposite! And this is
because The Art of War is also a work of Taoist philosophy, and there is
an

old Taoist saying that we see the spokes in the wheel but it is 'the
empty centre that lets the wheel move'. It is the empty quality of the
centre that matters, not its 'strength', far less any notion of
directing the spokes. Disciples of Taoism will know that this is a
school that just loves 'emptiness', indeed the whole philosophy compares
itself to the space contained by an empty bowl. Like a bowl, it may be
used, but is never emptied, it is bottomless, the ancestor of all
things, it blunts sharpness, it unties knots, it softens the light, it
becomes one with the dusty world. Deep and still, it exists forever...

Work that one out, General! But perhaps one message is clear: rather
than having strong leaders directing everything, in Sun Tzu's model
invisible leaders quietly coordinate. Power comes from this subtle role.
You no longer have to 'own things' to coordinate them. Despite such
misconceptions and misreadings (or maybe a bit because of them), the
book has become a modern business favourite. Nowhere more so than in
Japan. Here, companies make the book required reading for their key
executives. One result could be that the CEOs of their giant
corporations tend to 'encourage and facilitate' rather than conduct
themselves like little kings, as CEOs so often do on the West. Another
surely is that Japanese business follows Sun Tzu's advice about the
importance of research. Rather than invent products from scratch, they
take a long and careful look at whatever is currently successful, and,
well, copy it. Or 'build on it', if you prefer. Given the advice of The
Art of War, it is perhaps not entirely a coincidence that both Japanese
and Chinese corporations should be regularly accused of 'spying' on
Western companies and copying their innovations. Not fair! But learning
from what others have already done is a very effective strategy. 'Not
fair', and yet one of the best kept secrets of US business is that a
vast amount of US tax dollars is directed by the National Security
Agency (NSA) specifically to do much the same thing. Let's pause a
moment and look at one much under-reported way in which a piece of

Sun Tzu's military wisdom has been absorbed into civilian life in
America. Today, the NSA spends billions of dollars every year to
eavesdrop, not on the Ruskies, but on everyday exchanges between
businessmen, including the conversations of allies in countries like
Germany and Japan. The links between talk about military tactics and
company plans seems almost inevitable when you consider that the US
security establishment itself today is a giant business made up of huge
defence contractors: corporations like Boeing and Lockheed-Martin,
Raytheon and Endgame, via less well-known companies like Booz Allen and
Hamilton, who employed not only the now famous whistle-blower Edward
Snowden but also one Mike McDonnell. What's so interesting about Mike
McDonnell? Yet he is himself a former head of the NSA. Scarcely
surprising then that Booz has been entrusted with so many juicy defence
contracts out of an NSA 'secret-defense' budget estimated at around \$10
billion a year. The NSA's centre for stocking computer records at
Bluffdale, Utah, cost on its own \$2 billion at 2019 prices. Pause for a
moment to think about the size of these sums. How can information from
'spying' be worth anything like this? Yet particularly in today's
high-tech world it very easily can be. The official title of the Utah
centre is 'Intelligence Community Comprehensive National Cybersecurity
Initiative Data Center', but the reality is, as the French newspaper Le
Figaro put it on 5 July 2013, quoting an unnamed senior figure in the
French defence department who had been entertained at secret briefings
in Fort Meade (the ones that Snowden leaked the Powerpoints of), ever
since the fall of the Berlin Wall, 'the NSA has been 80% preoccupied
with economic intelligence.' Actually, it goes back well before that
too. On 8 August 1975, NSA Director Lt General Lew Allen admitted to the
Pike Committee of the US House of Representatives that the 'NSA
systematically intercepts international communications, both voice and
cable'. Spying---or shall we say, more neutrally, 'obtaining good
information before acting'---is the strategy at the heart of The Art of
War. Those of us accustomed to thinking of warfare as

being something to do with guns and battles struggle to see how
eavesdropping on businessmen fits in. But the activities are
intertwined, even if, of course, so secretive that what goes on is
rarely made public. However, in the 1990s, concern about eavesdropping
on private American citizens led to the curtain being tweaked to some
extent. It was at this time that a hefty supply contract with Saudi
Arabia negotiated by the European business Airbus instead ended up with
McDonnell-Douglas, the rival of the Airbus consortium, because the
former was privy to the financial terms offered by Airbus thanks to the
electronic interception system. A well-informed press report in the
Baltimore Sun in 1995 noted that: 'from a commercial communications
satellite, NSA lifted all the faxes and phone calls between the European
consortium, Airbus, the Saudi national airline and the Saudi government.
The agency found that Airbus agents were offering bribes to a Saudi
official. It passed the information to U.S. officials pressing the bid
of Boeing Co and McDonnell Douglas Corp., which triumphed last year in
the \$6 billion competition.' Similarly, the French electronics giant
Thomson-CSF lost a \$1.4 billion contract---for a satellite surveillance
system for monitoring drug trafficking and deforestation in the forests
of the Brazilian Amazon---to Raytheon, the Massachusetts-based company
that makes the famous Patriot and Sidewinder missiles. This, after the
Americans intercepted phone calls and details of the negotiations and
passed them on to the corporation---which announced appreciatively that
'the Department of Commerce worked very hard in support of U.S. industry
on this project', which is a nice way to look at it. This spying is just
a snapshot from a period in which the media discovered the story, and
public bodies started investigating. But the year before, that is in
1993, President Clinton extended US intelligence support to commercial
organisations by creating a new National Economic Council, paralleling
the National Security Council. The nature of this intelligence support
was widely reported.

As the Baltimore Sun again put it in 1995: 'Former intelligence
officials and other experts say tips based on spying... regularly flow
from the Commerce Department to U.S. companies to help them win
contracts overseas.' The same newspaper obtained reports from the
Commerce Department demonstrating intelligence support to US companies.
One such document consists of minutes from an August 1994 Commerce
Department meeting intended to identify major contracts open for bid in
Indonesia in order to help US companies win the work. A CIA employee
spoke at the meeting; and five of the 16 people on the routine
distribution list for the minutes were from the CIA! Other accounts
published both by reputable journalists as well as some first-hand
witnesses cite occasions on which the US government has utilised
covertly intercepted communications for national commercial purposes.
These included data about the emission standards of Japanese vehicles;
trade negotiations concerning the import of Japanese luxury cars; French
participation in the GATT trade negotiations in 1993; and the plans of
the AsianPacific Economic Conference (APEC), in 1997. After such
triumphs of US communications intelligence came to light, and most
particularly those detailing the derailing of trade deals for European
companies like Airbus in favour of US contractors, a delegation of MEPs
(Members of the European Parliament) was sent to Washington---but they
returned emptyhanded. The matter was simply 'too secret' to even talk
about, they were told. So it was left to an American Civil Liberties
Union Report, 'The Surveillance-Industrial Complex: How the American
Government is Conscripting Businesses and Individuals in the
Construction of a Surveillance Society', to identify the close, not to
say incestuous, relationship of large corporations with the security
agencies within the US five years later, in August 2004. And if, of
course, I can't prove that Sun Tzu's book helped point the United States
security apparatus in this direction, well, it would be hard to find
another

book that makes the point so firmly. Forget fighting battles:
information is power. I myself first wrote about the extensive network
of US spy stations in 1999 in a book called No Holiday. At the time, I
lived near one, in fact, called Menwith Hill, in Yorkshire. This is a
surreal installation of geodesic domes resembling giant golf balls
dropped into a gently undulating moorland landscape of purple heather.
Yet here's the thing about information: almost invariably it is already
out there, available for you to discover and make use of---if only you
know where to look for it. However, there's a kind of mini-industry in
persuading us that everything important has to be invented or discovered
afresh. That's why, in June 2013, even this thoroughly researched and
much debated issue of NSA spying had to be 'revealed' as if it was a
brand new thing by the editor of the London Guardian, Alan Rusbridger,
and his US deputy, Janine Gibson, as part of a show with Charlie Rose on
PBS in America unveiling the 'revelations', as the paper put it on the
website. Although it is very difficult to quantify the scale of
industrial espionage, the European Parliament report mentioned above
estimated that even in the 1990s, globally, about 15--20 billion
euros---say 25 billion dollars---was being expended annually on the
interception of communications and related activities. That's a lot of
money, but it isn't just about countries not trusting the Soviets, or
even about companies grabbing contracts, it's about nations vacuuming up
ideas too. Because, which of these do you think is the most important
factor for business success? A great idea---or having lots of money?
Humble folk tend to think it is the latter, but the fact is, there's
plenty of businesses that started off with millions in the bank, but
quickly burnt through it and ended up with nothing. In fact, great ideas
are worth more than money or, at the very least, potentially worth 'a
lot' of money. And where do ideas come from? From talking to interesting
people? Sure. From TV and radio? Why not. Of course, lots of ideas can
be found in the form of articles, reports and books. But some big,
'ready-made' ideas can be obtained by spying.

So, although the thirteen chapters of The Art of War include such things
as 'using the lie of the land in your advantage' and tips on deploying
'fiery weapons' that look not only hopelessly dated but rather nerdish,
I would say it is the advice about the importance of information that is
the key part of the guidance on how to achieve your goals. And all the
other advice comes back to this too. Sun Tzu, after all, was a master of
'soft power' and the father of 'agile warfare'. Whenever possible, he
preferred to win without fighting or, at the very least, to win the
easiest battles first. Which is why he also writes, 'In war, the
victorious strategist only seeks battle after the victory has been won.'
He advises his troops to 'make your way by unexpected routes and attack
unguarded spots.' And he further states, 'Military tactics are like
water. For water, in its natural course, runs away from high places and
hastens downwards. So, in war, the way is to avoid what is strong and
strike at what is weak.' This principle, so obvious and yet so often
neglected, is essentially to find the easiest way to achieve a specific
goal. And it puts information first and foremost. Consider scientific
research. A shocking amount of research time, energy and money is quite
simply wasted because researchers produce papers on matters that have
already been studied extensively, published and even patented. Not
because they are rechecking or finessing the work, but because they are
simply ignorant of it. Straightforward duplication of earlier work
simply due to ignorance of it has been estimated at around 20%, and of
course you only need to flick through your local bookstore to see books
broadly repeating themselves because human memories are short and,
anyway, there is an engrained tendency to assume everything in the past
is irrelevant. The reality, though, is that in almost every field of
human endeavour, checking what others have already done and benefiting
from their ground-clearing, and seeing what lessons can be drawn, is
just common sense. Hey, NSA, spying is usually not even necessary!

In fact, a lot of Sun Tzu's book is common sense when you think about
it: advice that seems obvious when put plainly. However, this is
actually the best sort of advice, not least because it fits logically
with other things we believe. The kind of advice that seems really odd
and counter-intuitive needs to be treated very cautiously. It might be
brilliant---but it might be wrong. That tip of Sun Tzu to pick the
'easier route', for example, may seem to be hardly worth saying:
nonetheless, often we don't do it. For example, if we are trying to eat
healthily, we fail to buy lots of healthy foods for the cupboard and
fridge, but instead try to 'go without'---go hungry while living
surrounded by the snacks that we had in the cupboards as a relic of
previous bad habits. That ain't a winning strategy... Or take a humble
author's battle to finish a piece of writing. Surely, it would be easier
without other people in the room, without a telephone ringing, without a
TV. Yet often we don't take even these easy steps and instead battle
against unnecessary odds. As James Clear, one contemporary aficionado of
The Art of War, has put it, in an article on his own website entitled
'Applying Lessons from Sun Tzu': And when we fall off course and fail to
achieve our goals, we blame ourselves for 'not wanting it badly enough'
and for not having enough willpower. In many cases, however, failure is
not a result of poor willpower, but a result of poor strategy.

This is what Sun Tzu is getting at when he says that an army never
fights where the terrain is not to its advantage. Nor does it begin by
attacking the point where the enemy is strongest. Linked to this is
another one of The Art of War's commonsense, yet in this case also a
little bit counter-intuitive, messages, very much in keeping with Taoist
principles of yielding, which is that warfare is essentially undesirable
and to be avoided. Sun Tzu writes: 'Those who are not fully aware of the
harm in waging war are equally unable to understand fully the method of
conducting

war advantageously.' Rather, Sun Tzu says: 'He will win who knows when
to fight and when not to fight.' And that is why: 'He who is skilled in
war... captures the enemy's cities without assaulting them. He
overthrows the enemy's kingdom without prolonged operations in the
field.' Sun Tzu calls this the 'method of attacking by stratagem'. Real
generals don't actually seek battles against their equals---that's for
the sporting enthusiasts. Rather, they take on weaker opponents, and
make sure if possible that they catch them when they are additionally at
a low point, and maybe not even expecting the fight. It's not sporting,
but it's effective. The smart general only attacks much weaker
opponents. The only problem is that some smart opponents sometimes only
pretend to be weak! Of course, Sun Tzu knew about that problem. He
writes: War is a game of deception. Therefore feign incapability when in
fact capable; feign inactivity when ready to strike; appear to be far
away when actually nearby... When the enemy is greedy for gains, put out
a bait to lure him; when he is in disorder, attack and overcome him;
when he boasts substantial strength, be doubly prepared against him; and
when he is formidable, evade him. If he is given to anger, provoke him.
If he is timid and careful, encourage his arrogance. If his forces are
rested, wear them down. If he is united as one, divide him. Attack when
he is least prepared.

Again, don't get distracted or focus too much on all the militaryspeak.
Forget the 'enemy army', and instead think for a moment of 'your own bad
habits' and you can see the applicability of the guidance. The first bad
habit to change is the one that is easiest to. For example, if, like me,
you have limited willpower and your life is a series of projects begun
with enthusiasm but abandoned regretfully a little later, Sun Tzu
reminds us that there is an alternative approach which is to change the
'rules of the game' so that we will win. The evergreen example here is
New Year health resolutions to eat a better diet. In such personal
health matters, if you've set yourself a demanding target, well...
don't. Set yourself an easy target instead. Stack the odds in your
favour!

Don't give up all sweet foods, just give up, say, chocolate waffles and
banoffee pie. For a week! Similarly, if you're surrounded by people who
don't think you can make the change, or maybe don't believe in our
project, leave them out of the conversation and find people instead who
share your enthusiasm and goals. As I say, if it all sounds a bit too
simple, don't discount the advice for that. In reality not only do most
of us spend most of our time fighting losing battles, we are losing easy
battles too. Forget good intentions, ignore gushing accounts of the
lives of 'great people' that may actually be fiction. It is better to
regularly take small steps forward in everyday life than to fail to make
a giant leap in an imagined other existence. In fact, the great thing
about The Art of War is that it is not a brainy book at all. It is
really just unvarnished advice expressed in what was then the plainest
language. Starting with that central idea: knowledge is power. As Eric
Barker put it, in an article for Time magazine in 2014 (called 'Sun
Tzu's Art of War: How Ancient Strategy Can Lead to Modern Success'), the
crucial theme throughout The Art of War is the value information.
Accurate, timely, relevant information. Eric says that in reading and
rereading the book he was struck by how Sun Tzu hit this one idea again
and again and from so many angles. (I'm doing a bit the same thing in
this chapter! And for the same reason: often we think we know something
but, when the same problem or issue arises from an unfamiliar angle, we
find we don't recognise it and don't know what to do.) But perhaps,
these days, the importance of information seems to have become something
of a cliché. After all, ever since the arrival of computers some seventy
or so years ago, we are relentlessly reminded this is the 'information
age'. And it's certainly true that, today, Google brings us a library of
millions of books all full of data and available just with a few
keystrokes. It even often seems that, if anything, we are drowning in
too much information, so maybe getting it is no longer a problem.
However, the real problem is that not just any information will do.

It has to be good information, clearly expressed and available at the
right time. Likewise, as we all know from fruitless internet key term
searches, easy access to vastly more information doesn't guarantee that
it will be relevant or even accurate. So what do the smart keyboard
warriors, just like smart CEOs and generals, spend much of their time
doing? Sorting information. Picking out the key ideas like this is above
all the leader's job and responsibility. In this vital sense, nothing
has changed since Sun Tzu's era. Nor have the real problems changed. For
generals, for CEOs, for researchers in laboratories too pleased with
their new ideas to check whether it has already been done (a hundred
times), the real obstacle to finding the right information is the
conviction that you have it all already. As Harvard Business School's
Gautam Mukunda, author of Indispensable: When Leaders Really Matter
(2012), says, an essential part of hubris is 'thinking you know
everything'. It's such a common mistake, even though so easy to correct.
Or as fellow Harvard professor, Richard Tedlow, puts it: I have been
teaching and writing about business history for four decades, and what
is striking about the dozens of companies and CEOs I have studied is the
large number of them who have made mistakes that could and should have
been avoided, not just with the benefit of hindsight, but on the basis
of information available to decision makers right then and there, in
real time.

This is the point that Sun Tzu made all those centuries ago when he
wrote: 'He who makes full assessment of the station at the prewar
council meeting in the temple is more likely to win. He who makes
insufficient assessment is less likely. This being the case, what chance
has he of winning if he makes no assessment at all?' To draw this
chapter to a close, let's keep these words of the Chinese guru in mind
but consider a recent real-life example of a battle between two
'metaphorical' generals and their Silicon Valley

armies. In this war, one side followed Sun Tzu's playbook and the other
stomped all over it. This is the clash between Facebook, led by Mark
Zuckerberg (likely in a Facebook-branded baseball cap), and Snapchat,
led by Evan Spiegel (also a keen cap wearer), a battle in which Colonel
Zuck had all the divisions but General Spiegel had a carefully
considered strategy, and one that, it seems, came at least in part from
reading the advice of Sun Tzu. I'll tell you who wins in a moment. But
first, we all know about Facebook, the social media site where people
'share' updates with their friends. However, Snapchat may need a brief
introduction. It's more cult: a revenue-less app that makes photos
disappear. Okay, the story starts with Zuckerberg's invitation,
delivered to Spiegel's personal email account: 'Come to Menlo Park and
let's get to know each other.' Spiegel, then just 23 and, as J.J. Colao
has put it, 'the brashest tech wunderkind since, well, Zuckerberg',
responded coolly: 'I'm happy to meet you... if you come to me.' Which,
armed with the excuse of having another (rather grand sounding) meeting
with the architect Frank Gehry about designs for Facebook's
headquarters, Zuckerberg did, flying to Spiegel's hometown, Los Angeles,
and arranging for a private apartment to host the secret showdown. When
Spiegel showed up with his cofounder Bobby Murphy, who serves as
Snapchat's chief technology officer, Zuckerberg had a specific agenda
ready. He tried to draw Spiegel and Murphy's vision for Snapchat, and he
described (boasted might be the better world) Facebook's new product,
christened 'Poke'. This too was a mobile app for sharing photos and
making them disappear. It would debut in a matter of days. And finally,
just in case there was any nuance missed, Zuckerberg would soon change
the large sign outside its Silicon Valley campus from its iconic
thumbs-up 'like' symbol to the Poke icon. Remembers Spiegel: 'It was
basically like, "We're going to crush you".' As Business Insider
reported it, after Zuckerberg and Spiegel first met, the latter left the
meeting feeling deeply unsettled. Because what do you do if your
business is about to be gobbled up by a \$200 billion company? Well,
what did Spiegel and Murphy do? On their

return to the office they immediately ordered a book for each of their
six-person team. And that book was Sun Tzu's The Art of War. Quite what
the team would have made of it is a moot point. Because on the surface
The Art of War is, as the title suggests, all about fighting wars and it
is only by implication and metaphorical allusion about business. That
hasn't stopped it being a popular purchase (I suspect a less popular
read) in business circles. Anyway, I don't know quite what they made of
it all, but in fact when Zuckerberg, the richest twenty-something in
history, reached out to Spiegel, and put all his cards on the table, he
already committed the worst error in the Chinese general's playbook.
Because, remember, information is power. However, Zuckerberg was so
confident that he held all the cards that he didn't care. Indeed, when
Poke debuted, on 21 December 2012, it shot immediately to the top of the
charts. Zuckerberg gleefully emailed Spiegel, telling him that he hoped
he enjoyed it. It didn't seem a friendly wish... Spiegel, who had
deactivated his Facebook account, frantically called Murphy for his
review of the rival app. It was, Murphy responded glumly, a near-exact
copy. But then Zuckerberg saw their relationship as 'zero sum' stuff:
either Facebook or Snapchat. The upstart photo-sharing app represented
to him an existential threat to Facebook. And the reason why was all to
do with the power of information too. Because what had emerged rapidly
online was that sharing personal info is often not quite so cool the day
after: what you post on social media---the good, the bad and the
ugly---is there forever. You can delete it, maybe, but you can't delete
the copy that someone made in the 30 seconds it took you to realise that
sharing the thing wasn't such a good idea. Even if, of course, sensible
grown-ups rarely do anything silly, and happily provide Facebook with
photographs of their allotments and news of bridge tournaments, edgy
teenagers fell on Snapchat, with its 'Mission: Impossible' style
detonation technology, in droves. Within months, Forbes estimated that
Snapchat had 50 million youthful users with a median age of 18.

Facebook, meanwhile, had an average user whose age was closer to 40.
That's ancient, in social media terms. And so something unexpected
happened to the Facebook Panzer division called Poke. Instead of
crushing Spiegel's infantry underfoot, instead of snapping Snapchat in
two, by Christmas day, 25 December, within days of its launch, it was
Snapchat that was number one on the iPhone app store, having been
boosted by the publicity of its battle with Poke, and it was the
Facebook app that was routed, disappearing from the top 30. Which helps
explain why, when Zuckerberg re-engaged Spiegel, he was basically ready
to surrender. The Facebook founder now offered Spiegel terms that, on
the face of it, seemed impossible to refuse: \$3 billion in cash. Forbes
estimated that Spiegel and Murphy each owned about 25% of Snapchat at
the time, which meant they were both looking at a \$750 million
windfall. This for a two-yearold app which (even if it already had a
multibillion valuation) not only made no money but didn't have a
timetable for ever generating revenue. And here is where the advice of
Sun Tzu again comes back in. From a logical, Western point of view,
Zuck's offer was unbeatable. But from the point of view of The Art of
War, Zuck had just made another strategic error. And so, Spiegel turned
Zuck and his measly \$3 billion down. Crazy? But chapter six in The Art
of War specifically addresses the need to attack an enemy where and when
he displays weakness. 'There are very few people in the world who get to
build a business like this,' says Spiegel. 'I think trading that for
some short-term gain isn't very interesting.' Spiegel is described by
Colao, the Forbes contributor, as 'lanky' and habitually dressed in a
button-down shirt, designer jeans and white sneakers. He not only looks
like an overgrown teenager, but behaves like one too. At the time of
their meeting, Spiegel still lived in his dad's house. Colao recalls
that during this interview he would abruptly shift from raucous laughter
to icy glares, while constantly grabbing fistfuls of gummy bears and
Goldfish crackers (fish-shaped cheese crackers that contain a small
imprint of an eye

and a smile!). His conversation was far from rapier like, but instead
opinionated and rambling, punctuated with plenty of examples of 'like'
and 'whatever'. But Spiegel followed these key strategic principles that
can all be found in Sun Tzu: Research the issue. Time spent digging out
background material can pay back handsomely. Find out what people are
saying. Maybe don't eavesdrop on their private calls, but try picking up
the phone to talk! Keep your ego in check. Confidence is much admired
but over-confidence paves the way for error. Keep asking questions. Ask
them of yourself, or better, have someone smart cross-examine you about
the plan.

And don't give up too soon! Who knows what will happen next---or what
errors the other side may be about to make. As Sun Tzu said, great
leaders don't just gather information, they actively exploit and
manipulate the assumptions of the other side. In short, 'thinking like a
general' means using information to gain an advantage. Information is
power---and money. Zuckerberg, whose whole business revolved around
persuading millions of people to give him their personal details and
preferences for free, should have remembered that.

Timeless Advice on the Art of War Sun Tzu lived some 2500 years ago in
the State of Qi in what is today Shandong Province in eastern China (and
was roughly contemporaneous to the European philosopher, Pythagoras, who
inspired so many of Plato's ideas and writings on the other side of the
world, in Ancient Greece. Like the Tao Te Ching, or Book of the Way,
which consists of approximately 5000 characters, The Art of War is very
short and precise. Nonetheless, within it is considered to be the
definitive discussion of the arts of strategy, tactics, planning and
intelligence gathering, as well as insights into politics, economics
and, of course, military tactics. Sun Tzu has an alarmingly severe line
on 'bad advice', complaining that, 'these days', there was a 'plethora
of advisors and experts offering advice on how to get rich, be
successful, impress people generally' as well as expensive courses
promising pretty much the same, and last but not least, there were whole
libraries of supposedly expert books. He says that when their advice is
good, the advisors, the professors, the entrepreneurs and (why not) the
authors too, certainly deserve 'to be celebrated, paid highly, and feted
with garlands'. When it is not so good, however---and here the text
reflects its era!---they deserve to be 'unceremoniously pickled, sawn in
half, boiled, minced, and torn apart by chariots'. Smart authors match
their books to the prevailing times, and this was during what is known
as the Spring and Autumn and the Warring States Period, which was no
passing fashion but stretched from 770-- 221 BC. This was a time of
great philosophical advancement, but also of social upheaval and
military conflicts, as different states within China vied for supremacy.
At the end of the period, it was the State of Qin that would energy
triumphant and unify the whole country under one emperor. And it was
definitely a time when generals ruled the roost.

Chapter 2

Thinking like a Designer

Metalware in the Indianapolis Museum of Art. (Photo: Sailko, Creative
Commons via Wikimedia Commons.)

Everyone wants to think like a 'leader'---which no doubt explains the
book sales of The Art of War, and indeed many more recent
popularisations linked to it. But here's a very different kind of
thinking that is just as powerful---and yet rarely discussed or noted.
'Design thinking' is an approach that, like the slow but relentless
force of the river, reshapes the landscape itself. Nigel Cross, sometime
Professor of Design Studies at Britain's Open University, calls design
thinking 'one of the highest forms of human intelligence'. Such
thinking, he says, is not only about finding solutions to problems, but
finding unexpected solutions.

Like the visual puzzle of the vase that turns into two faces (Figure 1),
or the duck that turns into a rabbit, these are solutions that were
there all the time, yet unperceived until the designer worked their
magic. In this way, design draws upon 'emergent properties', properties
that were indeed there in the original yet not consciously noticed or
appreciated.

Figure 1. The Vase. Do you see two faces --- or a vase? The Danish
psychologist Edgar Rubin created this visual puzzle in 1915 to
illustrate how you cannot see two things at the same time. (Image by
Bryan Derksen, licensed by Creative Commons.)

For reasons like this, he says that design thinkers are comfortable
working with ambiguity and uncertainty, at least, that is, in the
crucial early stages of the process. All of which helps explain why so
many top designers start drawing even before they have consciously
worked out the strategy. They 'draw before they think', we might say.
These quick, early sketches are clearly akin to what we otherwise call
doodles--- and, like doodles, they help to make conscious ideas
otherwise unperceived deep in the workings of our internal mental
processes. In all these ways, designers are people who travel hopefully,
ready to take advantage of unexpected but promising opportunities.

And, to continue the 'thinking hats' metaphor, as a predecessor to
thinking like them, you can put on whatever outrageous headgear you
fancy, because what I'm calling 'design thinking' is all about thinking
the unthinkable, and working 'outside the box'. Consider some examples.
Like Philippe Starck's famous (well, famous in design circles anyway)
lemon squeezer. In the late 1980s, Starck, already a renowned
originator, was invited by the Alessi company to come up with a new
design for that rather humdrum kitchen item, the lemon squeezer. Later,
sitting at a table in a restaurant, Starck began sketching some ideas on
his paper placemat. At first these were indeed just minor variations on
existing products. But then, his first course was ordered, which was to
be squid, and with it his sketches became a little bit, well,
'squid-like'. The images doodled on the restaurant placemat became
bulbous globular spheres with long, spindly legs. However, even as he
sketched, Starck saw that a lemon squeezer cannot actually be shaped
like a squid, and added in his own childhood interests in science
fiction and spaceships. From this emerged, in due course, a 'spaceship'
lemon squeezer, poised on three symmetrical supports, and made not of
plastic or glass---but of aluminium. The story illustrates the idea that
design thinking 'relies on our ability to be intuitive, to recognise
patterns, to construct ideas that have emotional meaning as well as
functionality, to express ourselves in media other than words or
symbols', as Tim Borwon put it in the introduction to Change by Design:
How Design Thinking Transforms Organizations and Inspires. Starck came
up with the idea while eating a plate of calamari garnished with lemon.
He doodled various designs on his napkin and sent the final design off
to Alessi, who immediately thought it was a work of art. However, the
design was actually meant to be practical, allowing a lemon to be
squeezed directly into a glass without having to go via a sieve and
another dish. Nevertheless, it really isn't. Many people have reported
that this squeezer made a mess of the counter top. The design simply is
not conducive to

juicing lemons. Instead, most people buy the Juicy Salif just as an
unusual ornament, with no intention of ever using it to squeeze any
citrus. Perhaps that's okay though. In a publication released by Alessi
celebrating the 25th anniversary of the design, Starck has shifted his
position and now states that the original intent of the product was 'to
spark conversation'. Talking of conversations, though, let me introduce
Tim Brown, quoted above, properly. He is the CEO and president of the
international design consulting firm IDEO, and his credits range from a
new computer mouse for Apple to a better way to meet the needs of
diabetics for insulin. Brown goes on to say, in his book, that the best
designers discover patterns where others see only confusion, and they
convert problems into opportunities. However, another great thing about
it all is that this is a method in which genius 'is not required'. This
is good news, not least because design thinking is not just for these
design problems, but for all kinds of problems, from the delivery of
clean drinking water in the developing world, to improving airport
security---to conceptualising new methods of micro-financing. Starck's
lemon squeezer is indeed a work of art, and I'll talk specifically about
what we can learn from that kind of thinking (which is not what you
might imagine!) later on in the book as part of my look at 'artful
thinking', but here I just want to focus on 'design skills' and 'design
thinking', which are part of a much broader dialogue, as well as a very
old tradition that seeks to put the human factor at the heart of
solutions to problems. The approach draws on a rich seam of ideas and
skillsets hacked from the bedrock of experience not only of design and
engineering, but social science, business studies and computer science
as well. Consider, for example, one tool of 'design thinking', called
'Powers of 10'. Suppose the problem, the 'design brief ', is to come up
with a tree house for children on a budget of \$1,000. What we all might
do, sure, is start there, jotting down some ideas... But you can also
reassess the same challenge by imagining the budget is only \$100--- or
that it is \$10,000! You can additionally stipulate that the tree

house must be ten square metres in size, or just one square metre--- or
100 square metres. Just as a picture can look very different if you put
it in a different frame, reframing issues and problems that you are
tackling can bring about a surprisingly big shift in how you think---and
feel---about them. That's the lesson of the story mentioned in the
introduction about how a lawn mower company came up with idea of the
'strimmer': not by thinking, well, about 'new kinds of lawn mowers', but
instead by thinking about new ways to cut the lawn. It's a small change
of emphasis, but often that is enough. Likewise, I think that 'Powers of
10' is a good place to start examining the insights of design skills
because it straightforwardly obliges you to break out of the
straitjacket of your starting assumptions. It's an antidote to our
school and college training, where children and students are encouraged,
nay obliged, to think in straight lines: to start at the beginning, work
their way through the middle and then stop at the end. Thing is, in
life, things are more complicated than that. Often straight-line
thinking leads to blockages, to tunnel vision and missed opportunities.
Straight-Line Thinking Straight-line thinking is mechanical processing
of data. However, design thinking celebrates the power of the
imagination. Recall the worlds of Carl Hempel, a German philosopher of
science who specialised in thinking about thinking: The transition from
data to theory requires creative imagination. Scientific hypotheses are
not derived from observed facts but invented in order to account for
them. They constitute guesses at the connection that might obtain
between the phenomena under study, at uniformities and patterns that
might underlie the occurrence. 'Happy guesses' of this kind require
great ingenuity... (Carl Hempel, Philosophy of Natural Science, 1966,
p. 15)

By contrast, the promise of design thinking is that it can generate
solutions that are truly innovative, not just incremental. It seeks to
facilitate this is by avoiding 'yes/no' language and questions, and
instead trying to interact with people in a nonlinear, less 'directive'
way. Instead of questions and answers, which are like a series of
straight lines, sometimes it is better to go for narratives---which are
more like shapes. And so design thinking both encourages and values
storytelling. This goes against many prejudices you may have from
school---that stories aren't reliable, may contain unnecessary and
'distracting' details or, worst of all, are not 'true'. But whether or
not stories are true is irrelevant; they reveal how people think about
the world. Indeed, storytelling can be a deeper form of communication
than dealings based on the mere exchange of facts, and when people draw
on their intuition, they are likely to spot inconsistencies. In the same
way as one little girl, listening intently to the tale of Cinderella and
her magical experience at the ball, highlighted just such an
inconsistency when she asked: 'But if Cinderella's shoes fitted her
perfectly, why did it fall off?' In design thinking, such gaps in the
conventional narrative are much-valued signposts to interesting new
paths. Not least because designers know that the difference between what
people say and what they do is worth any number of facts in terms of
understanding processes. Inconsistencies are precious clues giving
access to potentially profound insights. Or, for a more everyday
example, suppose you're talking to someone and wondering how they came
to have such a very fine garden: don't ask, 'Does it take a lot of time
to create a garden like this?', but rather, 'I'm curious to know how you
created such a lovely garden, can you tell me a little about what you
found most difficult---and what you found worked really easily?' The
first kind of question gets a pretty short answer ('a couple of hours a
week'), whereas the second kind may produce all sorts of linked and
unexpected information. Stories have a value in themselves and design
thinkers are quite happy just to listen, waiting patiently for ideas and
insights to

emerge. Stories, brainstorming, doodling and dialogue are the raw
materials for stages one and two of the design thinkers' triad:
Inspiration---Ideation---Implementation Here's another example of the
use of 'design thinking' that illustrates the design skills triad coming
to the aid of a modern business. It's the story of what happened when
IBM commissioned a redesign of their industry booths for trade fairs.
The aim, naturally, was to increase the number of people visiting their
booths, and the degree of interaction once there. What the designers
came up with at stage one---inspiration--- after the usual
brainstorming, was that the IBM booths were too formal, and too much
about what IBM did. If IBM wanted to attract more people in, and get
them to talk, what was needed was 'dialogue' not 'monologue'. This
simple insight guided stage two---ideation---into a remodelling of the
whole IBM experience. Stage three--- implementation---included tables
that were creatively designed as jigsaw pieces (so they could be made
into small intimate spaces or larger seminar ones as people desired---on
the spur of the moment---to new attention to how people were treated in
the spaces). IBM staff would accompany people around different areas,
rather than pass them on as so many parcels. Dialogue would be made more
intimate by not having 'strangers' wandering around, only other guests
who could then be introduced to each other. Most fundamental of all,
information about IBM's products and approaches is shared as and when
people request to know more, rather than pointed at or randomly
distributed. Another way to look at IBM's trade fair strategy, though,
is to see it as the rediscovery of much less esoteric educational
philosophies. In particular, the ideas of progressive educationalists
from Plato to John Dewey who all warned against pontificating at random,
oblivious to the interests of the audience, whether students or clients,
and advised instead on the importance of seeking to

connect to their particular interests and needs---all part of a broad
philosophy known as constructivism. Drawing out obscure or hidden
connections is something that design is all about. And so, another tool
in the design skills locker that is also finding applications across a
much broader range of life than product design, narrowly defined, is
concept mapping. These 'mind maps' draw together disparate elements of a
problem and show pathways to solutions. They are a tool we all have used
even if only when we scribble notes to ourselves. At the core of the
appeal of 'design thinking' is the insight that there are many more ways
to try to understand things than by using what the nineteenth-century
social scientist, Max Weber, called 'instrumental reason' alone. The
human mind is actually very good at grasping complex relationships which
use a very different kind of logic, indeed which often appeal directly
to intuitions. Pictures and diagrams, for example, have their own logic.
During the Cuban Missile Crisis, in 1962, for example---the crisis in
which the appearance of Russian missile silos in Cuba brought the world
to the edge of a nuclear war---President J.F. Kennedy found his solution
to the problem partly through doodling. The centrepiece of the famous
presidential doodle is a yacht, and Kennedy loved recreational yachting.
Other elements are a chessboard and the word 'Castro' (the Cuban leader)
blocked in by lines. Contemplating or drawing all of this led Kennedy to
his vital conclusion: his response to the threat posed by the siting of
Russian missiles in Cuba would be a naval blockade. All the elements of
this are there in the doodle, drawn as if by chance, and later connected
in a flash of insight.

Figure 2. Blockade Cuba! President Kennedy's doodle is said to have
given him the insights he needed to solve the Cuban Missile Crisis in
1963.

However, there's a huge difference between a good chart or diagram (let
alone a doodle), and a useless one that sheds no light at all. Here's
the catch: although everyone can dash off the simplest of diagrams
fairly quickly, to produce a really useful ones requires some skill and
certainly a bit of thinking. Which is why the promise of the special
charts beloved of design thinkers---such as 'mind maps', or more broadly
'concept charts'--- that they can extract ideas from your head, or maybe
a whole group of people's heads, and turn them into something visible
and structured sounds so good. And indeed, such charts can be, but if
you want the diagrams you end up with to be more than just pretty
illustrations, you need to understand the principles and aims of their
construction. That's where more design thinking comes in. I'm going to
call all sorts of diagrams designed to clarify ideas and indicate
relationships 'concept charts' but there are plenty of other names used,
such as concept maps, mind maps, flow diagrams---or even word trees.
Don't get too hung up on the terminology in this new and evolving
area---the key thing is which ideas and techniques work might work for
you. Indeed, you can

and should just 'pick'n'mix' approaches if it seems useful. Make your
own. That's a good design thinking strategy. One distinction that's
often claimed is that a well-made concept chart grows within a context
frame, typically an implied argument or question, whereas a mind map
often has only branches radiating out from a central word, or graphic,
representing an idea or concept. But whichever kind of diagram you are
using, it probably shares a common structural feature which hinges on
using nodes and links. Nodes are representations of ideas and
information, and the connections between these ideas are the links. The
simplest diagram represents a node as a circle or square or other shape
and the link connecting the nodes is---wait for it---just a line. We've
all been doing concept charts already, you understand, when we jotted
notes on a piece of paper, drew a circle around two key terms---and then
a line between them. But that was a primitive use of the approach. More
can be achieved with some of the insights from design. First of all, the
links are defining relationships and, in so doing, are actually making
knowledge explicit. That's why we feel that sense of satisfaction when
we connect two ideas on a piece of paper with line! In such gestures,
knowledge is dragged from the subconscious and put on paper for
perpetual reference. Made 'black and white' (or even glorious
Technicolor). But the best bit is that, when you create concept charts,
you not only become aware of what you already know, you become able, as
a result, to modify and build upon it. Joseph D. Novak, sometime
Professor of Education and Biological Sciences at Cornell University, is
credited with developing the idea of concept charts in the 1970s. At
first it was as a means of representing scientific theories to his
students, but he always claimed that the approach has its roots (as
mentioned above) in a broader philosophy known as constructivism---very
simply, the idea that people actively construct their understandings of
the world on what they already know. As another US Professor,

David Ausubel, has put it, a very important related idea is that, in
constructing their theories, the 'most important single factor
influencing learning is what the learner already knows'. Research into
how people process information, whether in academic, business or in
everyday life, has found that the brain likes to work on the basis of
association, and it connects every idea, memory or piece of information
to tens, hundreds and even thousands of other ideas and concepts. Mind
maps, in particular, are claimed to exploit the way the brain is 'wired'
to automatically associate words and concepts with one another, or a new
experience with a recent experience. The classic mind map, for example,
presents information structured in a radiant rather than linear manner,
as Figure 3 shows. The core idea, 'environmentally friendly transport',
generates multiple sub-divisions, which in turn prompt a whole range of
specific examples.

Figure 3. A Mind Map. This one, on the theme of 'transport', is typical
of the kind of thing a brainstorming session might produce. Charts like
this one depict suggested relationships between concepts.

Basically, all concept charts represent statements, just like a written
description does.

Ox carts → are → an environmentally friendly ('green') form of transport
In my diagram, the words are not in shapes, but they could have
been---with additional information conveyed by the choice of shape. Or
the colour! Mind maps can really go where you like them to. However,
they can also be used very precisely and, shall we say,
'scientifically'. The term 'nodes' hints at this, as it comes from
mathematics, where a node is a point in a network at which lines meet,
separate or terminate. But precision, let alone complexity, is not an
end in itself. Mind maps are a tool. Take them how you want, where you
want. The crucial point to bear in mind is that two nodes plus a
connecting link represent a proposition, a statement that's supposed to
be true. Remember that and you can't go wrong. According to its
inventor, Joe Novak, the technique 'makes concepts, and propositions
composed of concepts, the central elements in the structure of knowledge
and construction of meaning'. As such, it proves useful in the 'soft'
social sciences, for marketing experts' presentations, for hard-nosed
designers, engineers and technical writers, and for countless other
tasks, to present, organise and structure factual information. Another
key characteristic of concept charts is that information 'flows' around
the diagram, with the focus often more on this flow than on the concepts
in the nodes. Some flow charts specify where things start---inputs into
the system---and where they can end--- the outputs. Others may have no
start or end points but describe the flow in terms of self-contained
cycles. Concepts and nodes sound brainy, but the idea is not just for
professors. Joe Novak taught students as young as six years old to make
concept charts! One project he often liked to challenge them with
started with the question: 'What is water?' While another (which I find
more baffling) was 'What causes the seasons?' Actually, even the 'what
is water?' question (and concept chart) might have challenged some of
the six-year-olds though, as the 'answer' in Novak's view was all to do
with molecules and not at

all to do with things like rivers and rain. In his final chart, 'water'
emerges as a halfway stage between 'molecules' and 'living things'. I
still haven't worked that one out yet. There are lots of other graphical
tools for thinking often used alongside mind maps though, including dump
lists, text annotation, group brainstorming and so on. You can think of
any of these tools as organisational strategies for the contents of your
brain! Let's start, though, as many real-life design agencies will, with
a brainstorm. Brainstorming is the name given to the fairly obvious
technique of quickly jotting lots of ideas down in response to a
question---or even simply a concept. You can brainstorm on your own, but
the real advantages of the technique only work when you're in a group,
because that's where other people's ideas can spark new ones among
participants. The approach was popularised by Alex Faickney Osborn in a
1963 book called Applied Imagination: Principles and Procedures of
Creative Problem Solving. But he got the idea from where he worked,
which was... an advertising agency. You can imagine how the executives
sat around a whiteboard while someone wrote up a word such as 'coffee',
and everyone shouted out word associations: 'aroma', 'Brazil', 'going to
work in the early morning'. No one censored the suggestions: instead,
they were taken as was, noted on the board, and then maybe arranged,
highlighted or deleted later. One claim of the method's supporters is
that brainstorming allows a group to think collectively and build on
each other's ideas. And conducting a group brainstorm can also create a
buzz, something that can be absent when you work on your own. But
brainstorming can also be a bit of a 'lowest common denominator'
exercise too---by which I mean that the idea that everyone likes is not
really the best one but only the one that everyone shares. Some of the
group may not recognise a good idea precisely because it is different or
novel. For this reason, groups use particular strategies to capture the
ideas of a brainstorming session. One is to empower a 'scribe', who acts
as a coordinator. Their job is not so much to write down everything the
group comes up with (which can be

a cumbersome and inefficient process) but rather 'capture' on the board
the key ideas from whatever the team members call out. In addition, the
scribe has the responsibility of working out how to sum up the idea, all
of course regardless of his or her own feelings about its merits or
demerits. The opposite approach is sometimes called 'all-in'. During
these sessions, team members can write on the board their ideas as they
come, or just shout them out to each other. One nice touch is to provide
everyone with those ever-useful yellow sticky notes so that everyone can
write their ideas down at their own pace before sharing them with the
group by proudly sticking them on the board. The attitude and abilities
of the coordinator are vital to a successful brainstorm---and although
not everyone automatically has the 'right stuff ' to be a good one, some
principles certainly can be adopted. The first is to be enthusiastic and
encouraging. At the same time, a good coordinator does do a bit more
than that, for example, by choosing different ways to 'frame the
question', or even by adding 'fun' constraints in a bid to spark new
ideas. For example, if the group is wondering about, say, how to tackle
the obesity epidemic in the developed world, the scribe could constrain
the issue by asking instead: 'If you had the job of giving kids school
dinners, and they were only allowed one choice of dish Monday to
Friday---what would it be?' Now 'harvest that brainstorm'! Smart
coordinators both sense and follow up ideas that people seem most
excited, amused or intrigued by, and pick up on hesitantly expressed
half-thoughts that may be worth the group exploring. Certainly, they
won't judge things by thinking that they aren't practical. Because an
impractical idea may still have within it something useable. And the
smart coordinator passes a large part of the job back to the group to
decide, maybe by voting. Actually, the truth is that coming up with
material is much easier than analysing and selecting the key ideas
within it. And so, before you spend ages making huge concept charts,
time spent

pruning the ideas is well invested. This is where something called 'dump
lists' can be another useful tool. The idea here, basically, is to just
empty on to a page, whiteboard, or whatever, everything swirling around
your head or the whole group's collective head (on a particular topic or
issue). With dump lists, a good next step is to add priority numbers---a
simple way to arrange a jumble of thoughts into a hierarchy. Eventually,
though, all the ideas will need to be summed up. This is such a useful
skill. It involves separating the wheat from the chaff, the golden
nuggets from the heaps of spoil, the future executive summary from the
full report. Put more simply, summarising is a key life tool that
enables you to organise and make sense of the world around you.
Fortunately, again, there are some simple techniques that can help you
do it effectively. One easy step is to simply highlight whatever seems
interesting or important---the key points. There's a significance to
this, however erratic people's judgment, as those busybody computer
giants like Amazon and Google know from systematically recording which
bits of electronic books people privately annotate. Nonetheless, if
thousands of little choices eventually point at something interesting,
on your own what you pick out may actually not be that interesting.
Another problem is that some of us start by underlining one key phrase,
then by marking whole paragraphs, and finally end up highlighting whole
pages. When this happens, probably we're not being critical enough---not
summarising but merely, well, highlighting. And don't be fooled by what
other people say is 'the summary'. First, alas, the skills of summary
writing are by no means universal. And second, what that person is
interested in may not be what you're interested in. Their summary
reflects their interests, not yours. Ultimately, the skills of
summarising are those of being organised and systematic. Which leads me
to the final ingredient of design thinking. And maybe it's not its most
obvious characteristic, but design thinking also involves thinking like
a computer

programmer. Surprised? That's because you probably imagine computer
programming is all about math. Well, of course, some of it is---but not
the important bits. The math is the detail, the big work in computer
programming comes with designing the algorithm that both describes and
points to the solution for a problem. To many of us outside computer
science, 'algorithms' may sound like a type of jazz-funk
music---al-go-rhythms!---but the word really means a sequence of steps
taken to solve a problem; a methodical strategy for solving problems in
a systematic manner ---from the way Google searches the web (of which,
more in chapter nine) to how Facebook determines what articles appear in
a news feed, via more modest tasks like a recipe for oat cakes: Place
flour, 1 cup of oatmeal and half a cup of oats, baking powder, and salt
in a bowl. Melt the butter. Make a well in the centre of the mixture and
add the melted butter... Roll out dough and cut into rounds... Bake in a
warm oven for 30 minutes.

That's basically all a robot needs to know. In practice (which is all
that design thinkers are concerned about), though, algorithm's are just
a series of steps, of greater of lesser complexity, ready to be
automated and mechanised. And as the examples of Google and Facebook
underline, algorithms are powerful. And, as both companies know, what is
most important is the process of devising the algorithm in the first
place. Yet here, the bottom line is that no explicit way exists of
devising an algorithm for a problem. Instead, this part remains rooted
in creative insight. So, it is surely significant that the people who
create highlevel code for computers are also called software designers,
partly because, like all designers, the first thing they have to do is
work out exactly what the problem IS---because you can't program a
computer to do something until you know exactly what the task is ---and
partly because (like the most creative inventor) the second thing they
need to do is to conjure out of nowhere this mysterious thing, the
algorithm.

The algorithm is doubly mysterious because it is not merely the answer
to a problem, it is also a very precise description of the question.
This attention to the 'question' before attempting the answer is really
the first and most important, not only of the design skills, but of all
the thinking skills. Socrates understood that! Okay, so designers are
people who specialise in generating new ideas, which is an incredibly
valuable skill---even if one comprehensively neglected in formal
education and indeed most workplaces. But which hat really captures the
essence of design thinking, we might ask? And the answer is, not a
particular hat you wear at all, but rather Philippe Starck's whimsical
design for a table light. This took the idea of a hat stand and
translated it into an object that also functioned as an interior light.
In this way, the designer's daytime hat found a new function at
night---as a lampshade.

Chapter 3

Thinking like a Biologist

Humans create meanings out of both hidden order and random data. Using
systems such as the so-called 'Equi­distant Letter Sequence', apparently
meaningful messages have been found in texts ranging from the Bible to
Moby Dick (as above).

In this chapter, the 'thinking hat' to try on is a floppy but
comfortable Panama hat and the emphasis is on, well, investigating. But
don't be too quick to rush off to the nearest patch of back country to
look for wild animals, or even, at the very least, for some unusual
beetles, because what I mean by 'thinking like a biologist' is much more
about asking how brains work, and exploring issues with a mix of
observation and analysis. It will be seen at once that biologists and
philosophers---even developmental psychologists---have more in common
than might seem at first obvious. But the first question I propose to
look at illustrates why this kind of thinking can be

better appreciated if approached as a biologist in a light-brown hat
than as a metaphysician with a shaggy mane of white hair, as the cliché
image of a philosopher would have us believe. My question is: how do
babies think? And philosophers struggle to answer this because they
treat all issues as linguistic investigations and, well, babies don't
even have words to begin with! Add to which, of course, it seems that
they (babies, I mean, not philosophers) DON'T think much, but instead
'learn' to make sense of the world later, by some complicated process of
interaction with the environment. Nonetheless, implicitly assuming a
great deal of similarity between babies' mental worlds and our own, we
take great interest in a child's first words---was it 'Mummy' or
'Daddy'? Or was it maybe something more food-related like 'Chocomilk!'?
However, according to the French psychologist G. Clotaire Rapaille, the
surprising answer to 'how babies think' is that it is very like the rest
of us. But that's more because we don't 'think' the way philosophers say
we ought to, and certainly not the way you are doing when you read a
book, but because instead most of our information processing takes place
in the twilight zone of the brain. Here, decisions are taken by 'the
reptile mind', operating in the background, often without us even being
aware of it. Dr Rapaille slithered to this understanding while working
as a child psychologist, dedicated to helping children who had trouble
communicating and expressing themselves. He found that most of their
problems could be better understood if it was assumed that our human
minds develop in three stages. The earliest stage, the 'reptile' one, is
simply concerned with survival. This is the stage in which we learn to
breathe, to move around a bit, to eat. After a while, all this kind of
thinking becomes automated and unconscious. The stage after this, which
Rapaille calls the limbic stage, is when children develop emotions and
conscious preferences. It is when bonding takes place, for example
between the child and its mother, and they develop affection for certain
things---for 'home', for 'warmth' and for 'apple pudding'.

The third and final stage, the one so beloved of philosophers, seems to
occur after the age of seven, and sees the development of the outer
brain, or the 'cortex'---the part that gets studied and measured
extensively by neurologists and other importantsounding scientists. This
is the part---the only part---that deals with words, with numbers, with
concepts. You may say that this is when 'real thinking' starts. But we
learn many words before this stage. Dr Rapaille's original interest, as
I say, started with his work concerning children with behavioural or
learning difficulties. He observed that for some of them certain words
seemed to trigger certain problems, and these problems were, he thought,
not attributable to some process going on in the rational mind normally
in charge of handling words, but instead went back much further, to when
the word was first learnt. He realised that these words had not merely a
dictionary meaning, nor even mere metaphorical associations, but a whole
range of psychological associations too. In fact, the children's
difficulties were evidence, he decided, that each and every word we
learn has a special significance. The word 'mummy', for instance, often
claimed as the first one that a baby 'learns', applies to just one
person, who has a certain appearance and does certain motherly things.
It is not just mummy's voice, or mummy's face, or even mummy's smell
that baby remembers. The word is 'imprinted' in its mind along with
numerous associations. In an ideal world, with things like warmth,
safety, love. Sounds plausible? And yet the same, he says, is also true
for other less obvious words, such as 'coffee', 'car', or even
'cigarette'. 'When you learn a word, whatever it is, "coffee," "love,"
"mother," there is always a first time', Rapaille once explained in a
newspaper interview, before adding: 'There's a first time to learn
everything. The first time you understand, you imprint the meaning of
this word; you create a mental connection that you're going to keep
using the rest of your life.' Rapaille calls all these word associations
a code, an unconscious code in the brain. Each word of the code was
introduced to us at

some point, and when it was 'imprinted' on our minds, it was with
various associations. Finding these associations reveals each word's
internalised, secret meaning. Take the word 'coffee', a term he came to
study particularly closely because Nestlé, who amongst other things is a
drinks manufacturer, paid him a lot of money to advise them on how to
market it. • Coffee What does the word mean to you? Take a moment to
mentally note your response and word associations. Rapaille realised
that one of the key associations for many European coffee drinkers was
'aroma'. Why? Because babies don't drink coffee---they don't like the
taste. But they do remember the smell. That's why coffee adverts always
harp on about the aroma, and barely mention the taste, as the good
doctor once explained to PBS television in the US, adding: I don't know
if you remember this commercial, but it was really on code... You have a
young guy coming from the Army in a uniform. Mother is upstairs asleep.
He goes directly to the kitchen, 'Psssst,' open the coffee, and the
smell---you know, because we designed the packaging to make sure that
you smelled it right away. ...He prepares coffee; coffee goes up; the
smell goes upstairs; the mother is asleep; she wakes up; she smiles. And
we know the word she is going to say, because the code for aroma is
'home.' So she is going to say, 'Oh, he is home.' She rushed down the
stairs, hugs the boy. I mean, we tested it. At P&G \[Procter and Gamble,
the food manufacturer\] they test everything 400 times. People were
crying. Why? Because we got the logic of emotion right.

It's getting the emotions right that is the key to solving children's
behavioural problems. But children can have a lot of interlinked
problems and that can be tricky. The result is that, as Rapaille has put
it, therapeutic results are slow and hard to obtain. On the other

hand, marketing results with the general public are quick and easy. Not
to mention incredibly lucrative. All you need to do is work out the
associations between 'things in the world' and emotions in the mind. Or,
as he puts it in shorthand, the codes. And another key part of the code
for 'coffee' is childhood, home. That is, at least in Europe and the
United States. But not in Japan, which was the marketplace that his
employer, Nestlé, was having difficulties selling the drink to. Rapaille
realised that this was because the Japanese actually didn't even have a
collective childhood memory of coffee---they did not have what Rapaille
calls the first imprint. What they had instead was a cultural
fascination with tea. So the first thing he advised Nestlé had to do was
to tear up all their existing marketing strategies and give up trying to
market coffee as an alternative to tea. Instead, he told them, they
needed to go back several stages and try to create a new group of
customers ready 'imprinted' with a liking for coffee. And, indeed,
Nestlé did do this by marketing a sweet-tasting dessert for children
with a taste of coffee. Soon after this, it started selling the Japanese
coffee through other things that were sweet, always playing
surreptitiously on this childhood memory. 'And when the children were
teenagers, the company found that they now had a big market for coffee
in Japan', finishes Rapaille proudly. Not convinced? But take cars. If
you have a car, and like it, pause again a moment and ask yourself
'why'? What prompted you to choose that car? What do you like about it?
Rapaille analysed consumer reactions to car advertisements too. He
realised that cars have two headlights and a grill, that is a face, and
they have characters; in a word, an identity. People like the car's
'identity' to match their image of themselves. The code for car is 'look
at me'. And yet, if you ask people questions about their choice of car,
they will talk about how fast it goes, how safe it is---or how much fuel
it uses. That is because they will always answer with the 'cortex', the
part of the brain that deals with logical questions, and naturally they
come up with logical reasons. But the decision is actually

taken by the reptile mind, which chooses cars because it likes its
'face', its 'expression', its 'voice'. It's no coincidence that so many
cars are given pet names by manufacturers like 'Chloe' or 'Duster'. From
a business marketing perspective, Rapaille's method is an amazing story,
but the insight into the way the mind---and words--- work is far more
important than even that. Step back a moment and look at the broader,
scientific picture. These days, neuroscience increasingly accepts that
the key to understanding how brains work is not to be found in examining
how many neurons you have, but rather in considering the connections
between them. What seems to really distinguish infant thinking from that
of adults is the new pathways and links created in the brain as a result
of experience. But these connections should not be imagined to be things
like tiny wires. Rather, they are things that are ephemeral and
continually evolving. In this way, 'thinking' is truly the ghost in the
human machine. It exists not in things themselves but in the space
between... Or put it another, more biological, way. It is now accepted
that with biological brains, constructed out of synapses and neurons
(information flows from one neuron, or nerve cell, to another neuron
across a gap equipped with maybe thousands of receptors called
synapses), massive overproduction is followed by a gradual reduction.
This process of synapse reduction, or pruning, is highly dependent on
experience and serves as the basis of much of the learning that occurs
during the early years of life. Those pathways that are activated by the
environment are strengthened while the ones that go unused are
eliminated. In this way, the networks of neurons involved in the
development of behaviour are fine-tuned and modified as needed. As such,
the connections in the brain are the key to recording and establishing
meaningful connections 'out there' in the world. But before the links
are created, they exist as playful patterns in the mind. One of the big
breakthroughs in computer learning is the idea that patterns 'emerge'
from otherwise meaningless, random data

---'noise'. Today, artificial intelligence labs are using concepts of
'patternicity' (meaning the ability to find meaningful patterns in
meaningless noise) to teach machines to think. According to some AI
researchers, such as Ray Kurzweil, whose development of an early optical
character recognition system led to his being featured by Time magazine
on its cover, and Fortune describing him as 'a legendary inventor with a
history of mind-blowing ideas' (of the kind all governed by 'logic'),
all learning results from massive, hierarchical and recursive processes
taking place in the brain. Kurzweil's big idea is that the part of the
brain most associated with reasoning and conscious thought, the
neocortex, deploys a hierarchical set of pattern-recognition devices,
and he illustrates this in the context of a computational system for
reading words. At the lowest level, a set of pattern recognisers
searches for properties like horizontal lines, diagonal lines, curves
and so forth; at the next level up, a set of pattern recognisers hunts
for letters (A, B, C and so forth) that are built out of conjunctions of
lines and curves; and at still a higher level, individual pattern
recognisers look for particular words (like APPLE, PEAR and so on that
are built out of conjunctions of letters). Don't get hung up on the
detail: it's patterns all the way down. In Kurzweil's book, How to
Create a Mind: The Secret of Human Thought Revealed, our minds are
described as about 300 million neural pattern recognisers in the
neocortex, which is the distinctive arrangement of tiny fibres that link
one neuron to another. When a stimulus is presented, let's say it's the
letter 'A', these pattern recognisers, acting as tiny little brain
machines, respond by breaking the stimulus down into its geometric
constituents, which are then processed. Thus (Kurzweil thinks, anyway)
the visual stimulus of the letter 'A' is analysed into a horizontal bar
and two angled lines meeting at a point. After recognising each
constituent separately, the 'neural machine' can combine them and
finally recognise that the stimulus is an instance of the letter 'A.' It
can then use this information to combine with other letter recognisers
to identify, say, the word 'APPLE'. This procedure is

said to be 'hierarchical', meaning that it proceeds from examining small
pieces to constructing larger wholes, or in this case, from elementary
shapes, to letters, to words, to sentences. However, to recognise the
whole pattern you first have to recognise the parts. When computer
programmers tackle tasks like this, the process of recognition will
typically include weightings of various features, as well as a lowering
of response thresholds for anticipated constituents of the pattern; for
example, the probability of recognising a presented shape as an 'E' will
be higher if it occurs after 'APPL'. In this way, the computer is not
really 'thinking' but merely following instructions. And yet, biologists
believe that with living creatures, including humans, much of brain
development that occurs postnatally is also experience-dependent and
defined by gene--environment interactions. The bottom line is that, in
the field of artificial intelligence these days, machines are often
programmed to 'know' that some features that they come across are more
important than others. Pattern recognisers in particular are considered
to be 'intelligent' to the extent that they are able to anticipate and
correct for poverty and distortion in the stimulus. Biologists know that
this process mirrors our human ability to recognise a face, say, even
when the owner puts on their sunglasses. It is this uniformity of
anatomy and function that emboldens Ray Kurzweil to claim that he has
actually come up with a brand new, general theory of mind, in which
'pattern recognition' is the key, relying on and made possible by the
most basic architecture of the brain. The elusive secrets of
consciousness are reduced to these practical problems of mechanical
pattern recognition, coupled with a hierarchical structure and suitable
weightings for constituent features. Kurzweil even goes on to claim in
his book that 'the pattern recognition theory of mind (PRTM)...
describes the basic algorithm of the neocortex.' This sounds hopelessly
abstract and you could be excused for glazing over a bit, but how the
neocortex works has huge implications because this is the region of the
brain

responsible for perception, memory and critical thinking. Add to which,
Kurzweil says that his conclusions are 'inescapable' and that the
principles he espouses can be used 'to vastly extend the power of our
own intelligence'. Like a latter-day Doctor Frankenstein, Kurzweil even
suggests that (since we can already duplicate these mechanisms in a
machine) there is nothing to prevent us from creating an artificial
mind---we just need to wire together enough pattern recognisers. But
hold on. Although it is true that computers excel at analysing things in
nanoseconds, and nowadays we cannot help but be impressed by things like
the Google driverless cars or medical computers like IBM's 'Watson' (the
last of which can apparently offer medical diagnosis on a par with the
very best human experts), computers are not quite as smart as they seem
to be. Even if, within the labs of AI researchers, it is now assumed
that computers can not only 'do' just about any field of expert
endeavour---that it's really just a matter of recognising the right
patterns faster than anyone else---Rapaille's theory about words having
associations ought to remind us that patterns can be a lot more subtle
than that. Indeed, sceptics of the approach say that even if a whole
slew of machines have been programmed to be hierarchical pattern
recognisers, none of them works all that well, save for within very
narrow domains like postal computers that recognise digits in
handwritten postal codes. When, in the summer of 2012, Google built the
largest pattern recogniser of them all, a system running on sixteen
thousand processor cores that analysed ten million YouTube videos and
managed to learn, all by itself, to recognise cats and faces---it seemed
initially pretty impressive, but less so when you discover that in a
larger sample (of twenty thousand categories) the system's overall score
fell to a dismal 15.8 percent. What is more intriguing about Google's
study, called 'Building High-Level Features Using Large-Scale
Unsupervised Learning', though, is that it concerned the ability of
computer networks to learn what's meaningful in images---without human
help. It is well-known that computers can quickly learn to recognise
things

if they are shown a certain number of images and told by a human being
which ones contain, say, cats, and which ones don't. (This is why so
many of us are made to spend time answering ridiculous 'captcha' quizzes
online in which we humans are forced to spend time actually training
computers.) Commenting on the research in Slate magazine, Will Oremus
notes that, when an untutored computer looks at an image, all it sees
are thousands of pixels of various colours. With practice and
supervision, it can be trained to home in on certain features---say,
those that tend to indicate the presence of a human face in a
photo---and reliably identify them when they appear. But such training
typically requires images that are labeled, so that the computer can
tell whether it guessed right or wrong and refine its concept of a human
face accordingly. That's called supervised learning. The problem is that
most data in the real world doesn't come in such neat categories. So in
this study, the YouTube stills were unlabeled, and the computers weren't
told what they were supposed to be looking for. They had to teach
themselves what parts of any given photo might be relevant based solely
on patterns in the data. That's called unsupervised learning.

The computers were supposed to mimic some of the functionality of
humans' visual cortex, which neuroscientists now think has evolved to be
expert at recognising the patterns that matter most to us. Which, even
for small babies, is faces and facial expressions. And what Google's
computer did, fed random YouTube images, was to home in on human faces.
However, it also developed the 'concepts' of cat faces and human bodies,
because the algorithm it was running, after examining the arrangement of
pixels in image after image, indicated to it that these features must in
some way be important. One of Google's top engineers, Jeff Dean, who
helped oversee the project, told Oremus that he considered the ability
of the computer to pick out faces a significant step forward.

However, to make a not-too-learned reference to Star Trek: it's
thinking, Jeff, but not as we know it. Instead, the real lesson from
Google's 'cat detector' is that, even with the vast expanses of data and
computing power available to it, hierarchical pattern recognisers still
do not come close to actually understanding natural language, or
anything else for which complex inference is required. Why is extracting
significant information from what seems like random noise so difficult?
One reason that it is a challenge, even to supercomputers, is that
random data also has patterns, 'real' patterns. This is a problem that
is far from straightforward for humans too. An exotic example of
illusory patterns lies behind a book called The Bible Code---a famous
or, perhaps more accurately, an infamous exercise in pattern recognition
at the heart of Michael Drosnin's bestselling book of that name. The
book applies a form of pattern recognition to the text of the Bible, and
in so doing appears to unlock prophecies and other messages buried in
the sacred text. Drosnin's particular method is to take the fifth letter
from each sentence, create words from these letters, and then examine
the results for hidden messages. In doing so, he claims to have
identified passages predicting the assassination of Yitzhak Rabin, the
rise of Hitler and even the moon landing! Spoiler alert: the links were
all coincidental, no human ever coded such messages. Nonetheless, the
system did create a kind of pseudo-order out of the randomness. How can
this happen? And does it happen often? And yes, patterns similar to
those found in The Bible Code have also been found in Hebrew
translations of the collected works of Shakespeare and Tolstoy's War and
Peace. Indeed, it turns out that if you look hard enough for a pattern,
you will find one. We should know this anyway, because of the work of
British mathematician Frank Ramsey in the first half of the of the
twentieth century. Ramsey was the less well-known member of a celebrated
group of Cambridge intellectuals who left a remarkable legacy of work,
with John Maynard Keynes in economics and Bertrand

Russell and Ludwig Wittgenstein in mathematical logic. However, his own
particular accomplishment was to create a new field of mathematics
sometimes called Ramsey Theory. At the heart of this is the finding that
order will eventually emerge given a large number of random trials.
Remember the joke about monkeys typing out Shakespeare's sonnets on an
infinite number of typewriters? Only it turns out not to be a joke at
all. In other words, given a large document like the Bible, there will
inevitably be random samples that spell out specific messages. Unleash
supercomputers on YouTube and you will find all sorts of strange things
that no one actually put there. Not to forget, patterns come in many
more forms than books and words. The appreciation and memory of a tune
also requires recognition of musical patterns. The psychologist Daniel
Levitin has argued that the repetitions, melodic nature and organisation
of this music create meaning for the brain. And, as we all know from
listening to our favourite tunes, the longer we are denied the
anticipated pattern, the greater the 'emotional arousal' when the
pattern returns. Consider the friendly advice to students that listening
to music can help them in their revision. Well, MIT researchers have
found that the whole brain 'lights up' when listening to music. They
believe that this amount of activity boosts memory preservation, and
hence pattern recognition. Consider, too, that although a musician may
play the same notes every time, the details of the frequency will always
be different. It is the pattern that makes the tune, not the notes.
Maybe writers should remember this too, when they throw words on the
page! These days, scientists and psychologists agree with each other
that, fundamentally, humans are pattern seekers. The scientists say that
every aspect of the world arrives via the five senses as an
undifferentiated mass of data, yet is presented to our minds in
organised form following an automated and wholly unconscious process of
pattern solving. The psychologists also are much more interested in what
happens with 'the data'. Both accept that it is in this way that we
learn about and understand our surroundings.

However, in order to pattern match, we become very attentive to some
characteristics, latching onto confirmatory evidence, and
neglect---maybe indeed suppress---information that tends to undermine
our hypothesis. (See box on 'Jigsaws' below.) This is the thinking hit
known as 'confirmation bias' and, looked at from the tidy perspective of
logicians, it is one of several 'cognitive' or thinking errors that we
all make, and they may add disapprovingly that it is so common that it
seems to be actually hardwired into the human brain. (Other names for
similar habits include 'belief perseverance', 'illusory correlation
making' and selective exposure theory.) But looked at more objectively,
confirmation bias enables us to construct meaningful patterns out of a
mass of contradictory and maybe conflicting data. Jigsaw Puzzles Take
the case of the humble jigsaw puzzle. Have you done one recently?
Probably not, such things are the province either of a small group of
eccentric aficionados... or children. Yet jigsaw puzzles are a practical
insight into how our brains work. Take a typical 'adult' puzzle of say
2,000 pieces. Maybe it is a painting, let's say a contemporary graphic
depicting London and its tourist sites.

Now with 2,000 pieces it is actually very inefficient to take one piece
and then examine the other 1,999 to see if any match. At least 1,995
won't. Even if it takes you just one second per piece to check each one,
that's still five hours or so spent looking! So, what helps is to
identify something particular about the piece in our hand (let's say it
is bright red and looks like part of a tiny London bus). Then
immediately the number of potential companion pieces reduce
dramatically: we only need to find others which have at least some of
the same distinctive red on them. In the puzzle I have in mind, there
are only about 100 such pieces.

However, the red alone is not enough to decide which piece is a likely
match, as there is a very similar red used for the distinctive London
postboxes---and the uniforms of the soldiers at Buckingham Palace. So
then a second level of analysis is needed, and that is to eliminate the
red pieces which do not seem to have the right 'shape' to be a bus---or
include some other detail (let's say it is indications that the red is
part of a soldier's tunic, or maybe it is the slot of the postbox).
After this is done, the problem of picking possible matches has reduced
to just a handful of pieces. At which point, the puzzler will typically
do one of two things. The first (known to all children) is to try to fit
the selected pieces together---and see how they match both in terms of
interlocking shapes and in terms of completing the scene. One of the
wonders of jigsaw puzzles is how often you can have 'false'
matches---that is to say, a crease on a soldier's tunic may exactly
match up with the line marking the door of the London bus. Or some such
coincidence. If you haven't done any jigsaws since your childhood, you
may not know what I am talking about, but in real life, too, we have
these 'false positives' when we pattern match. We might be looking at
someone who reminds us of an old neigh¬bour---and wonder whether to
introduce ourselves. Perhaps, we are reassured to realise that they wear
the 'right sort of glasses' and boldly approach them. But then, perhaps,
the moment they speak, their strange voice dispels the illusion. Now the
curious thing is that, once the old theory fails, all the evidence that
this person was not at all like our old acquaintance suddenly presents
itself to our con¬sciousness, as though previously we were only half
awake! Now, in a rush, we see that not only is their voice wrong, but
they are smaller, fatter, have a different colour hair... in fact are
completely different.

But back to the jigsaw. If it were not for the colour matching, it would
seem flat ridiculous, and certainly not very logical, to put pieces of
buses, postboxes and soldier's tunics into one category. Yet as a
practical way of dealing with a problem---'things that are the same
colour may be related'---it can bring dividends. The rationality of the
action depends on the perspective adopted. All of which maybe ought to
remind us to be careful not to attribute more intelligence to our
machines than they really have. Indeed, long ago in the early days of
computing---long before fancy graphics and at a time when computer
programs were simple things people still wrote for themselves!---Joseph
Weizenbaum, an early computer scientist at MIT, conducted research with
a very simple program called ELIZA that was supposed to be a kind of
mock therapist. Users would type in things like 'I'm feeling a bit down
today' and ELIZA would reply meaningfully in pale green sans serif: 'Why
do you think you are feeling a bit down today?' Actually, all ELIZA ever
did was 'listen' to what you said--- i.e. parse your sentence in a very
basic way, and then ask you a question in some way related to the
sentence you had typed. So if you mentioned your mother, say, ELIZA
would reply by saying: 'Tell me more about your mother.' Simple program,
but still a form of pattern matching. And, in fact, in experiments
during the 1960s, people were fooled by ELIZA. They were told that a
real live therapist was talking to them from a second computer, and they
believed it. Weizenbaum was alarmed when his secretary once asked him to
'leave the room' so that she could consult the program with no one
about! Here's a short example of the sort of 'conversation' that took
place: ELIZA: Please, describe your problems. Each time you are finished
talking, type RET twice. PATIENT: I am unhappy. ELIZA: Are you unhappy
often?

PATIENT: Yes. ELIZA: Can you elaborate on that? PATIENT: Too much work.
ELIZA: Why do you say too much work? PATIENT: I am working a lot. ELIZA:
Earlier you said you felt unhappy? PATIENT: Yes. ELIZA: I would
appreciate it if you would continue. PATIENT: Well, I don't know what to
do. ELIZA: Maybe your problems have something to do with this.

Well, today we expect more of our computers. But, in fact, the lack of
sophistication in those early programs only underlines Weizenbaum's
warning. With machines, we are inclined to see an underlying thinking
and agency that is not really there. Take the act of reading, for
example. Recall that Kurzweil says that children first recognise the
patterns of individual letters, then the patterns of individual words,
then groups of words together, then paragraphs, then entire chapters and
books. That's certainly what his computerised character recognition
system does. However, it still doesn't sound quite right to me. Instead,
don't children learn words (like 'mummy') as a whole---and they even
learn phrases ('eat your beans up!') without needing to assemble their
meaning from a complicated application of definitions plus grammar? And
what's the 'pattern' in a book? No wonder that Gary Marcus, professor of
psychology at NYU, and the author of Kluge: The Haphazard Evolution of
the Human Mind, has argued that, while Ray Kurzweil deserves to be taken
seriously, what he actually says is neither particularly novel or
wellgrounded. For example, if the acronym Kurzweil coined, PRTM, for
Pattern Recognition Theory of Mind, was new to scientists in his field,
the basic idea was significantly less so. Indeed, almost anything any
creature does could at some level be seen as hierarchical pattern
recognition; which is why the idea has been around since the late

1950s. Indeed, the basic theory is very much in the spirit of a textbook
model of vision that was proposed in 1979 by Kunihiko Fukushima which
was probably in turn was inspired by a model proposed by David Hubel and
Nils Wiesel in 1959. These two researchers had found two types of cells
in the visual cortex distinguished as being simple and complex, and
proposed a cascading model of the two types of cells for use in...
pattern recognition. All of which has led other theorists of the hidden
workings of the human mind to be more blunt about Kurzweil. The
biologist P.Z. Myers branded him '\[o\]ne of the greatest hucksters of
the age'. Douglas Hofstadter, the Pulitzer Prize winning author of
Gödel, Escher, Bach and coauthor of another book called Surfaces and
Essences: Analogy as the Fuel and Fire of Thinking, said once in an
interview that 'if you read Ray Kurzweil's books... what I find is that
it's a very bizarre mixture of ideas that are solid and good with ideas
that are crazy. It's as if you took a lot of very good food and some dog
excrement and blended it all up so that you can't possibly figure out
what's good or bad.' Now that's a powerful analogy! To many of his
critics, the underlying problem with Kurzweil is that he seems to want
to provide a theory of the mind and not just the brain, but fails to
appreciate that any theory that seeks to engage with what the mind
really is has to reckon with human psychology---with human behaviour and
the mental operations that underlie it. Here, Kurzweil seems completely
out of his depth. How far is indicated by Douglas Hofstadter and
Emmanuel Sander in Surfaces and Essences, which is a 600-page or so
doorstopper of a book exploring the generally unappreciated subtleties
of the relationship of language to the world around us. Hofstadter and
Sander start by claiming that behind every word in our language, from
nouns such as 'teapot' to connectors such as 'and' or 'but', by way of
adjectives and verbs such as 'blue' or to 'paint', 'there lurks a blurry
richness'. Even ordinary words don't just have two or three 'but an
unlimited number of meanings'. Why do we use dictionaries then, one
might ask? But the fault, say Hofstadter and Sander, lies with the
philosophers, or rather all of them up to

one Ludwig Wittgenstein in the 1950s. It was Wittgenstein, they say, who
freed us from the long intellectual legacy of Plato and his notion of
heavenly forms for things like, well, chairs, teapots---and the letter
'A'. And where Kurzweil seems to have not got much past the letter 'A',
Hofstadter and Sander survey the whole alphabet of ideas about the
workings of language from 'Anaphora' to 'Zeugmas' in order to illustrate
the complexity and fecundity of language. I'd never heard of either of
those entities either, but both are venerable (medieval) terms referring
to the way words can 'refer back' to terms used earlier. The fact that
the words go back so far shows how long a history the philosophy of
language has too. The point is that we think using words. But quite how
these work remains a mystery. Indeed, the riddle of language perplexed
the Ancient Greeks too, with the way words work right at the heart of
Plato's famous dialogues. And Plato himself clearly understood the
subtleties of words, using many analogies, even as he warned of them
that 'likeness is a most slippery tribe'. Centuries later, Immanuel
Kant, on the other hand, counted analogy as the wellspring of
creativity, and Friedrich Nietzsche is still often quoted for describing
truth as 'a mobile army of metaphors'. But this was Continental
philosophy. The English empiricists, people like John Locke and Thomas
Hobbes, promoted what they claimed was a more scientific view of the
world in which everything had a precise meaning and reference. Not at
all by coincidence the book generally regarded as the first English
dictionary was written around this time. Who, we might wonder, wrote it,
what sort of person was the author? The answer is one Robert Cawdrey, a
schoolmaster and former Church of England clergyman, who made use of
wordlists published in educational texts, such as Richard Mulcaster's
Elementary (1582) and Edmund Coote's English Schoole-maister (1596), to
compile his own list of words along with their 'meanings' in 1604. The
great thing about dictionaries is that they provide ammunition for those
who wish to pin down the meanings of

words, even when in reality words may not have a fixed meaning. Surely
Hobbes would have loved them, saying as he does at one point:
'metaphors, and senseless and ambiguous words, are like ignes fatui; and
reasoning upon them is wandering amongst innumerable absurdities!' Ignes
fatui, by the way, is Medieval Latin, meaning literally 'foolish fire'.
In fact, for Hobbes, like many a latter-day AI programmer, the mental
discourse is true, and the problem only comes with 'translation' into
words. Indeed, Hobbes, who most people think of as a political scientist
for his bitter political philosophy that describes the life of man as
'nasty, brutish and short', was in his own eyes a much more precise
thinker than that, indeed a keen amateur mathematician. So keen was he
to make the world logical that he even produced a theorem that he
thought demonstrated mathematically how to 'square the circle'. Alas,
his theorem was shot to pieces by other, rather more sophisticated,
mathematicians. And so, too, albeit with a delay of centuries, do
Hofstadter and Sander take apart his and many other people's attempts to
tidy up language. Instead, the two offer considerable evidence that
'concepts designated by a single word are constantly having their
boundaries extended by analogies.' But whoa! That claim itself makes a
few assumptions! What is the relation between 'the concept' and the
'thing out there'---is it one to one? As a word is used more widely,
does the concept too cover more ground? If words 'designate' concepts,
what use is the (ah) 'concept' of concepts? Hofstadter and Sander try to
tackle this ancient debate about the origins of words too, starting with
the word 'mommy'. Plato didn't do that one! (He stuck to things like
'beauty' and 'justice' instead.) This they see being extended over time
from being a label attached to a specific human to an allegorical
relationship, seen in words like motherland. They extend the debate from
nouns to the many assemblages of language---such as 'sour grapes' and
'my Achilles heel'. Warming to their theme, Hofstadter and Sander look
at how 'human cognition relies profoundly on our ability to move up or

down the ladder of abstraction'. (Ladder.) The 'ladder of abstraction'
becomes for them a vital 'analogy'. Not least, because some of the most
interesting analogies are the scientific ones. As Hofstadter and Sander
say, the whole history of mathematics and physics consists of 'a series
of snowballing analogies'. (Snowballing.) For scientists such as Henri
Poincaré, a great thought experimenter as well as a mathematician,
analogy was the route towards mathematical discovery. So too, and
perhaps better known, was it for Albert Einstein. The inventor of
relativity was a great metaphorical thinker, praising his thought
experiments and saying that they helped lead him towards his view of
light as particles (rather than waves). One of Einstein's key analogies,
and one that he himself credits as leading to his later insights, was
that of himself as a boy running down a pier with light as a series of
waves rolling in from the sea. In this simple yet evocative metaphor lay
the seeds for his theory of relativity. As he wrote later (in his
Autobiographical Notes): If I pursue a beam of light with the velocity c
(velocity of light in a vacuum), I should observe such a beam of light
as an electromagnetic field at rest though spatially oscillating. There
seems to be no such thing, however, neither on the basis of experience
nor according to Maxwell's equations. From the very beginning it
appeared to me intuitively clear that, judged from the standpoint of
such an observer, everything would have to happen according to the same
laws as for an observer who, relative to the earth, was at rest. For how
should the first observer know or be able to determine, that he is in a
state of fast uniform motion? One sees in this paradox the germ of the
special relativity theory is already contained.

The idea is that Einstein would be like a surfer riding the light wave.
The physicist John Norton says of it: When Einstein abandoned an
emission theory of light, he had also to abandon the hope that
electrodynamics

could be made to conform to the principle of relativity by the normal
sorts of modifications to electrodynamic theory that occupied the
theorists of the second half of the 19th century. Instead Einstein knew
he must resort to extraordinary measures. He was willing to seek
realisation of his goal in a re-examination of our basic notions of
space and time.

Which is why Einstein concluded his personal account of his youthful
thought experiment thus: One sees that in this paradox the germ of the
special relativity theory is already contained. Today everyone knows, of
course, that all attempts to clarify this paradox satisfactorily were
condemned to failure as long as the axiom of the absolute character of
time, or of simultaneity, was rooted unrecognised in the unconscious. To
recognise clearly this axiom and its arbitrary character already implies
the essentials of the solution of the problem.

Notice that phrase, rooted unrecognised in the unconscious. To escape
the straitjacket of our thoughts, Einstein used analogies too. Thus,
during his lifetime, he 'was driven by an unstoppable desire to seek out
profound conceptual similarities, beautiful, hidden analogies'. The
equation E=mc2, that is, energy = mass times the velocity of light
squared, is actually analogous to the rather mundane (but very
important) relationship in mechanics: kinetic energy = mass times
velocity squared (albeit with the whole lot divided by two).

Equally, as the American sociolinguist (and insurance man) Benjamin Lee
Whorf pointed out (and this rather earlier than Wittgenstein), words can
mislead us. How can light 'weigh' something? Our words (concepts)
confuse us. Light is

quintessentially weightless whatever scientists may say about the mass
of an individual photon. Half a century later, Hofstadter and Sander's
argument is that concepts are mental categories invariably made up of
many allegorical experiences. Thus, a child may call a large, fluffy
white dog a sheep, or describe peeling a banana as 'undressing' it
instead. These are 'semantic approximations', the 'concepts silently
hidden behind these words will continue to develop in the minds of all
these children'. Their insight fits with Rapaille's work on how children
really do use words. So what's the answer to the question we started
with at the beginning of the chapter about how young children think? It
is that words are not labels that they learn to attach to concepts, let
alone to 'things' in the real world. Rather, they are... pieces of a
jigsaw. Now you may say, at the end of this chapter, that it's not
really thinking like a biologist, more like thinking like a philosopher
or cognitive scientist. But that's okay. There are ideas from lots of
disciplines here. I'd say it's about multiple different kinds of
thinking (and observation)---which are gathered and then systematised.
And then, as I say, it's about looking for the patterns ---and then
placing the pieces of the jigsaw.

Chapter 4

Thinking like a Scientist Ought to Think

Albert Einstein, pictured in 1921 by Ferdinand Schmutzer. Einstein was
an enthusiastic thought experimenter, and once said that the method had
played a vital role in his development of the theory of relativity.

This chapter is about a very particular kind of science: that of thought
experiments. Thought experiments are a distinctive kind of imaginary
story, that actually seem to provide a way to test out our ideas,
explore relationships and even discover new information. Their grand
claim is that they are a useful way to gain new knowledge about the
world, by means of 'armchair philosophy' only. And, whether they are
called thought experiments or not,

the approach has had an important role in not only theoretical
philosophy, but in practical science over the centuries. In terms of
'thinking hats', as a strategy for linking different ways of thinking to
a simple, concrete change, for this chapter, you could wear a bowler
hat---in the manner of the besuited figure in René Magritte's paintings
of ordinary landscapes made surreal by the addition of an extra, purely
imaginative element. Let me introduce the secrets of the technique by
describing some famous thought experiments over the years. These are
(naturally) all a bit grand, but the power of the technique is
universal: you can use it just as well to redesign your kitchen, change
your job--- or supercharge your relationships. Thought experimentation,
at root, is about using the power of your imagination. The tests are
carried out in 'the laboratory of the mind'. But there is an art to
using the technique effectively: just because thought experiments can
head off anywhere and be about anything, the people who have used them
most effectively over the centuries turn out to be the scientists:
people like Galileo and Einstein. However, I'd like to start with a
philosopher, perhaps the greatest one: Plato. His dialogues are littered
(for want of a kinder term) with thought experiments. There is Gyges the
shepherd with his magic ring exploring the nature of morality; there is
the problem of your obligations to help a 'mad friend' who turns up at
your door angrily demanding the return of his knife; there is Plato's
less well-known 'breeding experiment' in which eugenics is advanced for
the good of society; and there is the infamous metaphor of the prisoners
in the cave, which seems to be telling us something about the nature of
knowledge. Later on (skipping a few thousand years of scholasticism)
Descartes returned to the technique enthusiastically, offering in his
Meditations (1641) the original disembodied 'brain in a vat' scenario (a
brain that receives a stream of fake sensory data along different
coloured wires and imagines it is really experiencing life), along with
consideration of automata and the general problem of whether we might
all be dreaming. John Locke also raised

questions of personal identity, when he asked what would happen if the
prince awoke to find himself in the body of a pauper, a 'bodytransfer'
theme that has been much explored ever since. I'll come back to this
issue in a moment. In the history of science, the thought experiment
appears to be a prominent scientific method in its own right. To develop
his 'law of falling bodies', Galileo did not actually drop two lead
balls off the Leaning Tower of Pisa---it was a thought experiment. (I'll
also look more closely at this classic thought experiment in a moment.)
Likewise, centuries later, the physicist Erwin Schrödinger did not
actually put his cat into a box with a radioactive rock, it was a
thought experiment. Both are quite practical exercises, but the point of
a thought experiment is that it would not help to carry it out, because
all the information that is necessary is there already, as it were, 'in
the laboratory of the mind'. For the American philosopher Daniel
Dennett, thought experiments are 'intuition pumps'---'little stories
designed to provoke a heartfelt, table-thumping intuition---"Yes, of
course, it has to be so!"---about whatever thesis is being defended.' In
his book on the subject, he discusses seventy-seven intuition pumps and
other tools for thinking, ranging from classics like Ockham's
razor---which is the idea that, everything being equal, simpler
explanations are preferable---to techniques for spotting questions that
come across as deep but in fact turn out to disguise thinking that is
shallow. This chapter will suggest ways to use the technique effectively
in order to shed new light on all kinds of issues and problems. It won't
tell you exactly how to use it, though, or even when not to use it.
Because the correct use of this technique remains shrouded in mystery.
Even the term 'thought experiments' has no precise, agreed meaning, but
covers a range of techniques from mere illustrative examples, to fables
and allegories and carefully worked out hypothetical scenarios or even,
as computer programmers would term it nowadays, 'models'. Thought
experiments are much more flexible than that. After all, surely one of
the most remarkable

powers of the human mind is not to consider what is---but rather what is
not. The ability, that is, to take any set of facts and play with them,
to consider alternatives and hypotheticals. Counterfactuals even. • What
if Hitler had not invaded Russia? • What if everyone in the world
stopped eating meat? • What would the universe look like if a different
set of forces and cosmological parameters shaped its evolution? In fact,
it is such a flexible technique that it can easily get out of hand, and
end up being misleading---or simply irrelevant. Which is why there is a
certain art to designing your thought experiments. Thought experiments
differ from, say, theoretical models or informal conjectures, because
they dredge up deeply held convictions and position them in a new light.
And they're powerful. Thought experiments underpin Einstein's theory of
relativity---and Adam Smith's theories of how economic markets work.
Thought experiments make the running in many areas of psychology and
decision theory---and in many areas of ethics too. But let's stick to
the physical world and say a bit more about Einstein, perhaps the
technique's most famous supporter. In an unpublished 1920 review,
Einstein gives some important insights into how thought experiments led
to an eight-year quest that finished in what is considered to be his
greatest work, the theory of general relativity. He first of all
explains that, in early 1907, he was trying to write a summary of his
work on the theory of special relativity for the, doubtless very worthy
but not particularly distinguished, Yearbook for Radioactivity and
Electronics, but found that, try as he might, he struggled to fit
Newton's laws of gravitation into his theory. And then (emphasis added):
At that moment I got the happiest thought of my life in the following
form: In an example worth considering, the

gravitational field has a relative existence only in a manner similar to
the electric field generated by magneto-electric induction. Because, for
an observer in free-fall from the roof of a house, there is during the
fall---at least in his immediate vicinity---no gravitational field.
Namely, if the observer lets go of any bodies, they remain relative to
him, in a state of rest or uniform motion, independent of their special
chemical or physical nature.

The realisation 'startled' Einstein, and inspired him. Over the years,
the story has become an iconic one, even embellished in the retelling.
In many versions, the falling man is identified as a painter and some
even insist that Einstein witnessed an unfortunate worker falling from
the roof of a building adjacent to the patent office where he worked!
Such versions raise the question why Einstein would describe the
recollection of such an unfortunate accident as the happiest thought in
his life! In any case, Einstein soon refined his thought experiment to
instead consider a man inside a large chest or box falling freely in
space. Einstein notes that the acceleration experienced while falling
would cancel out the feeling of gravity's pull upon oneself and so that,
while in free fall, the man would consider himself weightless and also
that any loose objects that he emptied from his pockets would not fall
but float alongside him. Einstein also imagines a rope attached to the
roof of the chest, which now becomes a kind of 'elevator'. A powerful
being of some sort begins pulling on the rope with constant force,
making the chamber move 'upwards' with a uniformly accelerated motion.
Yet within the chamber, far from realising that he is being pulled
upwards, all of the man's perceptions are consistent with the pull
downwards of a gravitational field. So, Einstein asks: 'Ought we to
smile at the man and say that he errs in his conclusion?' But indeed he
thinks not. Rather, he says that the thought experiment provides 'good
grounds for extending the principle of relativity to

include bodies of reference which are accelerated with respect to each
other'. At the heart of all this is what is known as the 'equivalence
principle'---the principle that asserts that uniform acceleration is
equivalent to the presence of a homogeneous, or uniform, gravitational
field. As with Carroll's 'Falling House' (see box), Einstein's answer to
this old question is that inertial and gravitational mass are equal
because they are, literally, one and the same thing. Through this
thought experiment, Einstein addressed an issue that was so well-known,
scientists rarely worried about it or considered it puzzling: objects
have 'gravitational mass', which determines the force with which they
are attracted to other objects. Objects also have 'inertial mass', which
determines the relationship between the force applied to an object and
how much it accelerates. Newton had pointed out that, even though they
are defined differently, gravitational mass and inertial mass always
seem to be equal. All of this underlines that Einstein did not write
exclusively for specialists; he actively hoped that his ideas would be
read and followed by the general public. One of his papers, for example,
contains an imaginary dialogue between a specialist and a layman on the
concept of absolute space, which runs: Our fearless observer will
interject: You may be incomparably learned, but just as I could never be
persuaded to believe in ghosts, can I believe in that giant object, of
which you speak and that you call space. I can neither see such a thing,
nor can I imagine it. (Emphasis added)

Thought experiments are the crucial part of the communication of
science, because the public, of course, do not want to read mathematical
equations---but they can be invited to join the exotic universe of
thought experimentation. That Einstein welcomed the exoticism of the
technique is made clear by a letter (of 23 March 1922) discussing the
nature of light, in which Einstein offers several

imaginary scenarios and concludes by saying ironically: 'I am now
completely ripe for the insane asylum!' Yet in a more serious account
(in a letter of 3 May 1949), Einstein stresses the creative element of
scientific intuition, saying: A new idea comes suddenly and in a rather
intuitive way. That means it is not reached by conscious logical
conclusions. But, thinking it through afterwards, you can always
discover the reasons which have led you unconsciously to your guess and
you will find a logical way to justify it. Intuition is nothing but the
outcome of earlier intellectual experience.

And, in the same letter, he also argues that 'the irrational, the
inconsistent, the droll, even the insane, which nature, inexhaustibly
operative, implants into the individual, seemingly for her own
amusement', are 'singled out only in the crucible of one's own mind'.
Einstein is indubitably one of our greatest thinkers, but the root of
his greatness lies in his creative imagination---not in his mathematical
skills. Indeed, it is relevant, if of course also rather disgraceful,
that after attending the Polytechnic at Zurich he was the only graduate
in his section not to get an academic post, and instead was obliged to
live a hand-to-mouth existence for almost two years! The world tends
only to remember his next step, taken really via the help from a
friend's father, which was a job at the Swiss patent office. Taking Tea
in the Falling House Eighteen years before the great physicist Albert
Einstein devised his thought experiment describing a man in a falling
'box' or 'elevator', the English mathematician Lewis Carroll, better
known of course as the author of the iconic children's story, Alice's
Adventures in Wonderland, had presented his own version---equally
serious in its mathematical implications but much more colourful.

It is in a novel called Sylvie and Bruno (1889), which is otherwise
definitely not one of his best, that Carroll discusses the difficulty of
taking tea inside a falling house. As part of his tea party, Carroll has
an imaginary host, Lady Muriel, laughingly remark, apropos of someone
having insisted on saving her the trouble of carrying a cup of tea
across the room to the Earl, 'How convenient it would be if cups of tea
had no weight at all! Then perhaps ladies would sometimes be permitted
to carry them for short distances.' In this throw-away remark is clearly
embedded the idea that, if you were in a room falling freely, you could
not detect the force of gravity. But Carroll goes a step further too,
and wonders what would happen if the room were tugged downward by a
rope, making it fall faster than in free fall. Suppose a cord fastened
to the house, from below, and pulled down by someone on the planet. Then
of course the house goes faster than its natural rate of falling: but
the furniture---with our noble selves---would go on falling at their old
pace, and would therefore be left behind.

'Practically, we should rise to the ceiling', said the Earl. 'The
inevitable result of which would be concussion of the brain.'

However, still the question remains, what exactly are 'thought
experiments'? Well, perhaps the best definition is that they are a
particular kind of mental 'what if ' musings that must resemble an
experiment. Unlike everyday experiments, though, they are often launched
with a 'jarring counterfactual' that transports the mind to a distant
alternative world, before then offering a more familiar narrative
constructed around everyday details. One of the most famous, but also
most misunderstood, thought experiments of them all is one that featured
Galileo Galilei (1564-- 1642), the celebrated astronomer, climbing the
Leaning Tower of Pisa, leaning over the parapet and dropping two balls,
a large heavy

one and a smaller lighter one, and watching to see which hits the ground
first. It is misunderstood partly because there is nothing impossible,
let alone 'jarringly' so, about the original hypothesis. Nonetheless,
the account Galileo offers makes clear why this 'experiment' really
takes place, not on the parapet of a tower in Pisa, but in the
'laboratory of the mind'. First of all, though, a word to explain the
background to the experiment. Galileo was at this time thinking of one
of Aristotle's laws, to whit: If a certain weight moves a particular
distance in a particular time, a greater weight will move the same
distance in a shorter time, and whatever is the proportion which the
weights bear one to the other, so too the times will have to each other.
For example if the half as heavy weight covers the distance in a certain
time, a weight that is twice as heavy will cover the distance in one
half the time.

The law appears in a text of Aristotle's called, De Caelo (Book I vi
274a). Now we can all easily imagine someone climbing a tower and
dropping two such balls. But then, what do we imagine will happen? Which
ball will hit the ground first (and at what speed) the large one or the
small one? It helps to be methodical---and this is a key feature of the
thought experiment technique. There are only three possibilities. • The
balls fall at the same speed. • The heavier ball falls faster than the
light one. • The light one falls faster than the heavy one. Thus far, it
remains a matter of conjecture---or reference to experience. But put
both such approaches aside and try a little mental logic. Imagine,
Galileo says, that between the two balls we tie a piece of string. Then,
first, let's say heavy objects DO fall faster than light ones. But in
that case, due to the string between the two balls, it seems that the
heavier weight must fall with the lighter

weight acting, as it were, a bit like a parachute---the little weight
seeking to fall at its more modest rate the whole time. In that case,
the two balls will together fall more slowly than the heavy weight would
on its own. Such an outcome seems ridiculous, but consider the reverse
approach. Consider how, once the two weights are tied together and held
out over the parapet, they have effectively combined their weights,
forming together a single, greater weight. Just holding the big weight,
with the other little one dangling beneath, will reveal this. In which
case, when Galileo releases the two conjoined weights it seems that they
should fall together even faster than the heavy weight would on its own.
Imagine the two are tied tightly together, for example, say by a single
tight loop of string. It seems that if objects of different weights fall
at different rates, then the two weights together must fall both faster
and yet more slowly than before Galileo tied them together. And here is
that thing philosophers love most of all: a contradiction. There is only
one way to avoid it, and that is to assume that the heavy and light
weights fall at the same speed. Galileo describes the experiment as a
conversation between two friends in a retrospective dialogue presenting
the key insights of his work in physics over the preceding thirty years:
Salviati: If we take two bodies whose natural speeds are different, it
is clear that on uniting the two, the more rapid one will be partly
retarded by the slower, and the slower will be somewhat hastened by the
swifter. Do you not agree with me in this opinion? Simplicio: You are
unquestionably right. Salviati: But if this is true, and if a large
stone moves with a speed of, say, eight, while a smaller stone moves
with a speed of four, then when they are united, the system will move
with a speed of less than eight. Yet the two stones tied together make a
stone larger than that which before moved with a speed of eight: hence
the heavier body now moves with less speed than the lighter, an effect
which is

contrary to your supposition. Thus you see how, from the assumption that
the heavier body moves faster than the lighter one, I can infer that the
heavier body moves more slowly... And so, Simplicio, we must conclude
therefore that large and small bodies move with the same speed, provided
only that they are of the same specific gravity. ---Discorsi e
Dimostrazioni Matematiche (1638). Published in English as Discourses and
Mathematical Demonstrations Relating to Two New Sciences.

This is justifiably seen as one of the great thought experiments. As the
great twentieth-century social scientist Thomas Kuhn points out, if
thought experiments seem to congregate at the time of scientific
revolutions, that is precisely because they dredge up deeply held
convictions and position them in a new light. Put another way, thought
experiments are powerful... In an influential paper called 'A Function
for Thought Experiments' (published in 1964), Kuhn considers the
approach using some of the ideas behind his own theory of 'the structure
of scientific revolutions', which seeks to explain how scientific
knowledge develops and progresses over time. He says that, just as in
experimental science a new piece of scientific evidence can force a
whole new approach to an issue, so too can a well-conceived thought
experiment create a crisis, or at least indicate anomalies, in a
reigning theory and force a radical rethink. This is the phenomenon he
calls 'paradigm change', meaning an important shift in the usual way of
thinking about (or doing) something, usually towards a new and different
way. Thus, Kuhn says, thought experiments can teach us something new
about the world, even though we have no new empirical data, by helping
us to reconceptualise the world in a better way. Granting that every
successful thought experiment embodies in its design some prior
information about the world, that information is not itself at issue in
the

experiment. On the contrary, if we have to do with a real thought
experiment, the empirical data upon which it rests must have been both
well-known and generally accepted before the experiment was even
conceived. How, then, relying exclusively upon familiar data, can a
thought experiment lead to new knowledge or to new understanding of
nature? (Kuhn, 1964, pp. 240--65)

Kuhn offers Galileo's example of dropping balls from the Leaning Tower
of Pisa as the perfect illustration of the method, but let's close our
look at the method now by also considering a rather different case too,
from economics, called Hotelling's Law, also known as the law of minimal
differentiation, which claims that there's a natural tendency for
competitors to be pulled toward a common middle ground and predicts that
competitors in a market economy will tend to offer products that are
similar to each other. The theory is even used as an explanation of why
political parties often seem very similar to each other---they have met
in the middle in the process of chasing the most voters. But, in thought
experiments, simple examples are best. Let's stick to a flat, sandy
beach and similar carts selling similar ice creams. The Ice Cream
Seller's Problem Poor Joe Lollies! He used to just roll up and sell his
ice creams from a cool box on his Ice Cream Cart anywhere on Sunset
Beach without any competitors to worry about. (The beach is very
popular, and people tend to spread out all along it.) But now he hears
that Sheila Ices is also targeting the beach, with her cart selling the
same stock and at the same prices. (Probably, they both buy from the
same wholesaler in Surftown.) The question for both Joe and Sheila is,
where should they be setting up stall to get the most customers?

Joe realises that, in the past, it didn't matter where he went as anyone
who wanted an ice cream had to come to him---even if it

meant walking the whole length of the beach. Now though, if he isn't
careful, some may find Sheila's Ices more convenient... So he positions
himself in the middle of the beach. And Sheila, reasoning similarly,
positions her ice cream cart right next to him. In a rational,
cooperative world, the one we don't live in, Joe and Sheila would each
position themselves at the quarter and threequarter mid-points, and
quietly prepare to serve their half of the beach. No shouting, no fuss.
Done this way, the clients are shared out evenly and no one has to walk
more than a quarter of the length of the beach to get their ice cream.
But in the real, dog-eat-dog world, such a sensible arrangement could
never survive. Because, consider what happens if Sheila rolls her ice
cream cart even just a few extra metres towards the centre of the beach?
For every metre nearer the centre, she is also rolling towards Joe's
customers and will pick up any of those for whom her cart now becomes
the nearer! The nice thing for Sheila is that all of her existing
customers at her end of the beach won't change their behaviour as she'll
still be the closest ice cream cart, even though she's now a bit further
away to walk. Similar story for customers at the far end---Joe's end, of
the beach---she'll always be further away than Joe. But for people in
the middle of the beach, Sheila's Ices becomes that crucial bit nearer
than Joe Lollies. They'll switch to go to her cart. In fact, the closer
she moves towards not only the middle, but Joe's position, the more
customers she will get. That's market logic! Of course, Joe thinks the
same thing and starts edging his cart towards Sheila's. So what
eventually happens to the ice cream sellers? The two carts must end up
in the middle of the beach situated sideby-side. Any movement away
potentially loses the ice cream seller clients. The middle is what
economists call an equilibrium point--- neither vendor can benefit by
moving. Harold Hotelling was an early twentieth-century American
economist who led a movement to make sense of economics by using
mathematics. His theory helps explain why supposedly rival products tend
to offer identical features and why car dealerships seem to pop up along
the same stretch of road. But here, with no

mathematics, is his idea expressed as a thought experiment. All that is
appealed to is our own sense of what would we do---common sense. It's a
similar appeal in another imaginary scenario relating to what
philosophers call 'personal identity'. We might call it 'The Case of the
Grisly Brain Transplant' and it runs something like the following.
Already today, skilled surgeons are undertaking sophisticated operations
involving brain tissue. We can easily imagine an operation soon that
involves putting someone's brain into someone else's body, maybe someone
who for some reason is beyond having a need for it themselves any more,
maybe due to an incurable brain disease. And it seems to be only a
matter of technical proficiency that at the moment prevents doctors from
being able to do this in such a way that not only the physical brain but
all of someone's memories and personal psychological characteristics are
transferred intact. Assuming such practical hurdles are resolved by
doctors at some point, then the philosophical question (raised by Derek
Parfit amongst others), and the issue to be settled by the thought
experiment, is simply whether this new creature would qualify in our
normal way of thinking as a continuation of the old 'person' in a new
body.

It seems as if, well, we just might. Particularly if the physical bodies
were similar. Consult your intuitions! But, what (and this is Dr
Parfit's ingenious twist) if it turns out that half a brain is enough to
create a new person? What if doctors find that as long as half a brain
can be successfully transferred, the new 'person' that emerges still
retains those all-important memories and personal habits? Would we
refuse to allow this half-brain person the same recognition, or welcome
them back to the world of the living just the same? (After all, in real
life, it has been found that a single functioning brain
hemisphere---either one---is enough for survival.)

It would seem ridiculous to quibble, once the principle of brain
transfer is established. But now Parfit adds a fiendish touch. What, he
says, if the remaining half a brain is transferred as well---to another
body! In that case, potentially we could end up with two new people made
out of the first one! But then, how would we decide which of the two new
people is the authentic successor to the original one? The thought
experiment raises profound questions about what philosophers call
'personal identity' and what it means to be a particular individual.
Indeed, philosophers have long struggled to define personal identity. In
the seventeenth century, both René Descartes and John Locke offered
their own thought experiments, with Locke concluding, in his 1690 work
An Essay Concerning Human Understanding, that identity consists
essentially in possession of a set of personal memories. He says that
the self is 'a thinking intelligent being, that has reason and
reflection, and can consider itself as itself, the same thinking thing,
in different times and places', and that, in as much as 'consciousness
can be extended backwards to any past action or thought, so far reaches
the identity of that person'. In this way, for John Locke, the
philosophical answer to Derek Parfit's question is that there is still
just one person---only now it is in two bodies! Now I don't claim to
know what the answer is to this question. But that's not the point here
anyway. It's enough for us to recognise that the thought experiment does
make us think harder about what we mean by 'personal identity'. It
forces upon us, in Kuhn's sense, a new approach to an old problem. By
now, I hope, the outlines of a successful thought experiment should be
emerging. First of all, there is a scenario---a plausible story. That
word plausible, of course, is important. It anchors the imaginary in a
vast well of background knowledge that we may not really be fully
conscious of having acquired. 'Plausible' doesn't mean here 'something
that could be done'. It is more subtle than that. It means a story that
seems to make sense on a more generous, almost literary level. Take
Galileo's tale of dropping balls from the

Leaning Tower of Pisa. It is pointless to object (as people have done)
that 'air resistance' and freakish winds might affect the result---as he
talks of dropping a large metal ball and a small metal ball, with
similar air resistance properties. You might as well object that the
custodian of the tourist site would never allow such a risky enterprise!
But offer someone a hypothesis that seems to run against their
intuitions and you can no longer follow the 'logic' of the experiment.
It is as if, with a physical experiment, you have introduced too many
competing variables. For example, as if in testing a diet to lose
weight, you decided in one week to skip breakfast, to eat a lot of fruit
between meals, and to go for a run every evening. At the end of the
week, if there has been any improvement on the bathroom scales, you will
not know which new strategy was responsible, or if it was some
particular alchemy of the combination of strategies--- or indeed none of
these but some other, unknown factor altogether. (That, by the way, was
itself a little thought experiment!) Having produced an imaginary
hypothesis that seems to 'hang together' satisfactorily---the second
step is to kick it apart! This is really the 'fun bit'. Recall our ice
cream sellers on the beach. What if one seller starts playing a loud
musical jingle? What if a long queue develops? And what happens if a
third ice cream cart rolls up at the entrance to the beach? There's a
surprising number of insights that can be obtained from very simple
stories. Or, to return to Magritte and his surrealist paintings: if you
take a man in a bowler hat and dark suit, and just place an apple in
front of his face---the world changes.

How to Experiment For such a powerful technique, it is surprising how
few people are able to use it---or at least use it effectively. So here
are a few ideas, but only as a starting point, it is really for the
reader to explore 'the laboratory of the mind' unfettered. Rule 1. A
good thought experiment should be short and immedi¬ately comprehensible.
Galileo's classic experiment involved no more than walking up a tower,
throwing two balls from the top and imagining how they would fall. Rule
2. A good thought experiment offers its workings open to
scrutiny---without relying on hidden tricks for its effect, or sneaked
in bizarre assumptions in order to make it work. It is for this reason
that conventional experiments take place 'in a laboratory', where other
factors may be more easily eliminated and excluded---factors that may
otherwise confuse relationships or produce misleading results. Rule 3.
Everyone should agree about the ingredients of a good thought
experiment; that is, on the 'facts' at the outset (the ingredients of
the story), the sequence of events and the end results. One key feature
of all experiments (after all) is that they should produce a result. If
you started a debate in physics (as some people have!) by saying
'imagine a rubber band made of copper', then some people will think you
mean it conducts electricity because it is made of copper, and some will
assume you mean that it can't because, well, it was originally a rubber
band. Rule 4. Avoid technicality. Here, the cautionary tale of Professor
Peter Strawson's 'world of sounds' might be mentioned. In this 'bad'
thought experiment, he offers a world in which 'the spatial location of
a particular sound is determined by the gradually changing pitch of a
master-sound'. I still can't work out what this is referring to,
although, yes, I can imagine hunting for an actual physical object aided
by a changing sound. That, of course, is only what treasure hunters do
with metal detectors whose 'beeping' sound speeds up the nearer they are
to metal.

Rule 5. Thought experiments should contain a complete argument. This is
bit harder to explain, but it is surely central to their effect. The
reason I mention it is that it is very easy---perhaps excited by our own
virtuosity---to start producing 'imaginary scenarios', but to have
forgotten what point we were supposed to be making. Rule 6. Thought
experiments must be consistent. No one likes discrepancies. What is
ruled out in one part cannot be snuck back in somewhere else... Rule 7.
The most important one. Thought experiments should be fun and
imaginative...

Chapter 5

Thinking like Mission Control

Martian canals depicted by Percival Lowell (1855--1916) in the book
Distant Worlds (St. Petersburg: Soykin), published in 1914. Of course,
as we all know now, there are no canals on Mars. But for a while, for
some, there appeared to be.

What is President John F. Kennedy's true legacy? In the nation's popular
memory, Kennedy still commands fascination as a compelling, charismatic
leader, whose place in history was secured by the Apollo programme that
against all the odds managed to put a man on the moon. Yet, perhaps, in
venerating Kennedy's achievements, the historians have missed the
essential point--- which is that in many ways Kennedy's true legacy is
that he changed the way that America thinks. There is a direct path from
the space race of the 1960s to American domination today in fields as
diverse as microprocessors and education, films and media. IBM, Apple,

Google---even Facebook---all have their roots in NASA's strategies to
meet the incredible challenge of putting a man on the moon. For this
chapter, the suitable headwear would seem to be an astronaut's helmet.
It's certainly a powerful image. And focusing on images makes sense,
when you remember that Kennedy was not particularly interested in space
exploration. For him, the programme was really about recapturing the
prestige that the nation had lost as a result of Soviet successes and US
failures. It was, as political scientist John M. Logsdon has suggested,
'one of the last major political acts of the Cold War'. When President
Kennedy unveiled that famous commitment to execute Project Apollo on 25
May 1961, it was part of a speech on 'Urgent National Needs', billed as
a second State of the Union message. He told Congress that the US faced
extraordinary challenges and needed to respond extraordinarily. And he
prefaced the lunar landing commitment by saying: If we are to win the
battle that is going on around the world between freedom and tyranny, if
we are to win the battle for men's minds, the dramatic achievements in
space which occurred in recent weeks should have made clear to us all,
as did the Sputnik in 1957, the impact of this adventure on the minds of
men everywhere who are attempting to make a determination of which road
they should take...

It was then that he added some words that have been much better
remembered: I believe this Nation should commit itself to achieving the
goal, before this decade is out, of landing a man on the moon and
returning him safely to earth. No single space project in this period
will be more impressive to mankind, or more important for the long-range
exploration of space; and none will be so difficult or expensive to
accomplish.

Nowadays, we are all much more sceptical of grandiose pronouncements by
politicians. After all, when Kennedy told Americans that they were going
to the moon, and would do so

'before the end of the decade', it was pure political theatre because,
at the time, the nation's total experience of space consisted of just
one 15-minute suborbital flight. By comparison, Yuri Gagarin's actual
orbit of the earth redefined not only space exploration but human
political society too. As the historian Asif Siddiqi has put it, the
flight of 'Vostok 1': ...will undoubtedly remain one of the major
milestones in not only the history of space exploration, but also the
history of the human race itself. The fact that this accomplishment was
successfully carried out by the Soviet Union, a country completely
devastated by war just sixteen years prior, makes the achievement even
more impressive. Unlike the United States, the USSR had to begin from a
position of tremendous disadvantage. Its industrial infrastructure had
been ruined, and its technological capabilities were outdated at best. A
good portion of its land had been devastated by war, and it had lost
about 25 million citizens... but it was the totalitarian state that
overwhelmingly took the lead...

Yet just three years later, by March 1964, for the American side, the
most sophisticated spaceship ever conceived was well along in its
design, and five years after that would land safely on the moon. How can
this extraordinary turnaround be explained? The conventional explanation
is that Apollo is a triumph of military organisation and hierarchical
management. However, I think perhaps a more important (and diametrically
opposed) lesson from it is about America's meritocratic culture and
democratic structures. It could be argued that if both the US and the
Soviets at this time favoured military models of organisation but more
out of ingrained business habits than via any philosophical reasoning,
only the United States allowed an element of competition to reign even
in the matter of theory and ideas. First, something that we can all
agree, though, is that on the way to the moon Apollo taught America to
'think big'. The total cost has been estimated in US \$25.4
billion---about \$150 billion at

today's prices. Yet less often stressed is that this was a whole made up
of smaller, human parts. At its peak, the project involved nearly half a
million private and public stakeholders, most of whom had differing
perspectives on how to go about the task of getting to the moon. The
number of civilians directly employed at NASA, itself part of the
American air force, grew from 10,000 people to 36,000 by 1960.
Additionally, contractor employees working on the programme increased by
a factor of 10, from 36,500 in 1960 to 376,700 in 1965. Private
industry, research institutions and universities provided the majority
of personnel working on Apollo and were the key to its success,
attracting talent and resources from the emerging aerospace industry and
the country's leading research universities. NASA in the 1960s was a
hothouse for innovation and creativity, just as California's Silicon
Valley more famously is now. On its way to the moon, the space programme
spurred advances in medicine, food, geology, manned spaceflight,
avionics, telecommunications, computing, math, astronomy, physics,
bioscience and several other areas of technological and scientific
interest, bringing together people who would otherwise never have shared
their expertise. To solve the issue of overheating upon re-entering the
earth's atmosphere at 26,000 mph, for example, NASA worked with MIT,
Corning Glass, the Jet Propulsion Laboratory in California and others to
create a shield that could withstand 3,000 degrees in order to safeguard
the astronauts. Likewise, for just one example of free thinking, the
enormous 'crawler' that transported Saturn V from the Vehicle Assembly
Plant to the launch pad was the brainchild of a member of the launch
operations team, whose name is now lost to history. He reportedly got
the idea from watching the stripmining process. According to legend,
President John F. Kennedy visited a NASA hangar. There, he asked a
janitor what he was doing. In response, the janitor simply replied that
he was putting a man on the moon. Indeed, it was a fitting tribute to
all these people that the Apollo 11

patch (the one for the first mission to touch down on the moon) did not
include names---in order that it would represent EVERYONE who had
'worked towards a lunar landing'. And so, in addition to acquiring the
skills and machines necessary to get there, NASA became a full-blown
research institute too. They needed to know the composition and
geography of the moon, and the nature of the lunar surface. Was it solid
enough to support a lander, was it composed of dust that would swallow
up the spacecraft? Would communications systems work on the moon? Would
other factors---geology, radiation and so on--- affect the astronauts?
To answer these questions three research programmes---using robots
rather than people---were created to study earth's neighbour which
culminated in 1961 with Project Surveyor which aimed to soft-land a
small craft with tripod landing legs on the moon's surface, from where
it could take post-landing photographs and perform a variety of other
measurements. Indeed, just five years later, on 2 June 1966, Surveyor 1
landed on the moon, from which vantage point it transmitted more than
10,000 highquality photographs of the surface. Mind you, the second and
fourth missions crash landed! Even so, the programme provided enough
high-quality photographs and measurements of the composition and
surface-bearing strength of the lunar crust, and readings on the thermal
and radar reflectivity of the soil, that by1968 NASA felt able to risk a
human landing. Reflecting on the whole process in November 1968, only
months before the glory of the actual moon landing, Science magazine,
the publication of the American Association for the Advancement of
Science, observed: In terms of numbers of dollars or of men, NASA has
not been our largest national undertaking, but in terms of complexity,
rate of growth, and technological sophistication it has been unique. It
may turn out that \[the space programme's\] most valuable spin-off of
all will be human rather than technological: better knowledge of how to
plan,

coordinate, and monitor the multitudinous and varied activities of the
organizations required to accomplish great social undertakings.

And yet, even as NASA morphed into being a kind of 'space university',
it remained firmly rooted in military discipline and habits. Being a
'mission controller' is about combining the best of both these worlds.
Of course, the military is used to managing large numbers of people, and
the method is always strictly hierarchical. However, the more
extraordinary thing about the moon programme was how things were
delegated down. 'NASA responsibilities were delegated to people who
didn't know how to do these things, and were expected to go find out how
to do it', recalled Howard W. Tindall Jr., Mission Technique
Coordinator, years later. Delegating to people who don't have experience
with a certain task may seem counter-intuitive, not to say risky, but it
is an approach familiar to academics charged with the supervision of
post-graduate researchers, and it was something the Apollo project
managers actively encouraged. In fact, the average age of the entire
'Operations' team was just 26, most fresh out of college. A great gift
that this young cohort brought with them, and one very much in the
spirit of this book, was a habit of thinking without preconceptions. But
then, with so much unknown about the task, NASA always understood that
it needed to allow different approaches and ideas to compete. However,
as with any garden, delicate blooms will tend to lose out to ragged
weeds and brambles---unless there is a guiding hand. Apollo's team of
project managers went from managing small projects with a select team of
close colleagues to managing thousands of people they had never met.
Coordinating such a massive effort required constant communication to
avoid costly or dangerous errors. Coordination and communication are
again two key aspects of thinking like Mission Control. In practice, to
manage all the 'stakeholders', NASA created a program office with
centralised authority over design, engineering,

procurement, testing, construction, manufacturing, spare parts,
logistic, training and operations. It is conventionally said about
Apollo that it taught a generation of programme and system managers,
systems engineers and scientists to think, behave and work like this,
with its influence to education extended more widely through NASA's
network of partnerships with universities. Above all, a culture of
valuing 'what works' and of debating issues democratically allowed the
space programme to cope with uncertainty, a skill that in the
twenty-first century seems both more valuable and increasingly difficult
to achieve. For NASA, coping with uncertainty entailed coping with the
inevitable 'wrong turns', of which there were plenty. It also meant that
when the first mission, Apollo 1 or AS-204, failed, the team were ready
to learn from the mistakes and restructure the entire project to launch
the next mission just 20 months later. Throughout, NASA sought to
combine openness to new ideas with rigorous checks and constructive
self-criticism. Self-criticism is a skill not much taught and generally
undervalued, whether in schools, management colleges or military
academics. Yet embracing this kind of thinking was the only strategy
that could possibly cope with the Kennedy challenge. Because, even as
late as 1957, no one at NASA knew anything about space travel, with the
sole exception of a well-established group of military men interested in
building more and more powerful missiles. Crucially, if NASA had been
really a triumph of army-style command control management, as so many
histories of the era insist, then the strategy throughout the early
1960s, by their aerospace engineering teams, dubbed 'Direct Ascent', of
using a single enormous powerful rocket that would launch on earth and
land directly on the moon (where it would become a lunar base before
taking off again back to earth) would never have been challenged---and
man might still not have walked on the lunar surface even today. The
idea of flying straight to the moon was a well-entrenched orthodoxy,
made to seem all the more natural and inevitable in the public mind by
writers from Jules Verne to Hergé. Generations of

Tintinophiles may not have appreciated the many technical details the
Belgian cartoonist included in his two books, Destination Moon (1950)
and Explorers on the Moon (1952), including things science only recently
confirmed---such as the presence of ice in craters. But on the big issue
of how to get there, Hergé ignored the essential challenge of how to
escape the grasp of earth's gravity with an enthusiastic nod instead at
the miraculous power of the atom. However, defying earth's gravitational
pull was absolutely crucial to going there in real life. And that's why
the story of how the NASA ship changed course and abandoned its original
(and rather science fiction favourite) idea of flying a huge rocket to
the moon in favor of the technocratic solution of using three rockets
each with their own specialised function is, I think, the most important
lesson from the Apollo project. And yet, at first, the only real voice
against the strategy of a giant rocket flying all the way to the moon
and then leisurely back was that of the NASA Rendezvous Panel at Langley
Research Center, headed by John C. Houbolt. Looking back sixty years on,
I think that Houbolt was the real visionary behind the Apollo space
programme. A relatively junior figure in the NASA hierarchy in one of
its research centres, even while the rest of the world marvelled at the
size and power of the Saturn V rockets, he focused on the nerdy question
of how to descend to the moon once the rocket got there. This focus led
Houbolt to press hard for an alternative approach called 'Lunar-Orbit
Rendezvous', which restricted the role of using a conventional rocket to
placing a lunar spacecraft up in earth's orbit. This would then circle
the earth briefly before heading to the moon, entering again into orbit,
and dispatching a small lander to the lunar surface. It was the most
practical method, both in terms of development and operational costs,
but it was risky. Critics mocked it as 'a scheme that has a 50 percent
chance of getting a man to the moon and a 1 percent of getting him
back.' In order to get his voice heard, Houbolt had to, almost
single-handedly, change the way the vast NASA machine---at its peak
encompassing almost half a million people---thought. Because if few,
maybe

none, of this overwhelming consensus knew the detail of the issue,
nonetheless their 'common sense' framed the debate. So another aspect, a
surprising one really, of thinking like a Mission Controller is having a
willingness to override conventional opinion and follow up minority
ones. In fact, Houbolt's difficulties in persuading his colleagues (and
superiors) at NASA illustrates the paradox of intuitions that people
have and are confident about even though, put side by side, it is clear
that the views are all different and only one approach can be right. The
social psychologist Daniel Kahneman has explained this as a thinking
error rooted in what he calls 'the mechanism of substitution', saying:
You have been asked a question, and instead you answer another question,
but that answer comes by itself with complete confidence, and you're not
aware that you're doing something that you're not an expert on because
you have one answer. Subjectively, whether it's right or wrong, it feels
exactly the same.

The culprit in such failures of intuition is another cognitive property
that Kahneman calls 'what you see is all there is'---adding that this is
a powerful and persistent flaw of 'System-1 thinking'. This kind of
thinking, Kahneman says, is fast, instinctive and emotional, and quite
inferior to 'System 2', which is slower, more deliberative and more
logical. In Edward de Bono's typology of six coloured 'thinking hats'
(in his book Six Thinking Hats, 2010) these are respectively the 'red
hat' and the 'white hat' approaches. Anyway, whatever they're called,
space is a good place to test out thinking systems. Take a
counter-intuitive factor during the race to the moon which saw both
common sense and intuition failing. The oddity was that when a spaceship
orbiting the moon (or earth) tried to speed up, in the sense of firing
the rocket engine, the process actually led to the craft travelling at a
lower velocity! The reason is that the spacecraft achieves a higher
orbit. Likewise, if a spaceship orbiting the earth decreases speed, it
ends up going faster! Because

it enters a lower orbit... it's all definitely not common sense, but
thinking slowly, deliberatively and logically is a vital strategy for
life as an astronaut. However, somewhere in our evolutionary past, we
learned to use intuition and System 1 thinking as a practical survival
strategy for things like unexpected rustling noises in the forest. The
result is that this kind of 'What you see is all there is...' assumption
became a problem-solving mechanism that takes whatever information is
currently available and makes the best possible story out of it--- but
skips over the gaps in the information it doesn't have. Take the late
nineteenth- and early twentieth-century fascination with the newly
discovered 'canals' on Mars. The information available came via
enthusiastic observations by telescope coupled with a great deal of
subconscious 'gap-filling'. Observers like Percival Lowell were
convinced that Martian civilisations had constructed vast drainage works
on the Red Planet, but improved telescopes soon showed the canals to be
creations of the observers' imaginations. In a nutshell, this always
rather tempting thinking strategy consists of jumping to conclusions.
And brainy bods, experts of all stripes, are not immune. Indeed, they
may be more at risk. Kahneman offers an insight that explains why. 'Most
treacherous of all', he warns, 'is our tendency to use our very
confidence---and overconfidence---as evidence itself.' He goes on: The
confidence people have in their beliefs is not a measure of the quality
of evidence \[but\] of the coherence of the story that the mind has
managed to construct. Quite often you can construct very good stories
out of very little evidence... People tend to have great belief, great
faith in the stories that are based on very little evidence.

Mission Control for the Apollo programme was built around this very
lesson. Every time the controllers ran through their 'go/no go' routines
they were prioritising evidence over confidence. More generally, landing
on the moon always made for a wonderful scientific tale, yet stories do
not respect practical

realities. On the contrary, they encourage imaginative leaps which can
easily generate more thinking errors. Ones such as: • Anchoring, or the
tendency to focus on one particular piece of information when making
decisions or problem solving. • Confirmation bias: which leads us to
focus on information that confirms our existing beliefs. • Hindsight:
which leads, for example, to the conviction that an event just
experienced was predictable. • The representative heuristic: which can
result in the unintentional stereotyping of someone or something. • And
availability bias: where decisions are based on available precedent or
examples that may be faulty. Rooting out these kinds of lazy---or at
least erroneous---thinking and other kinds of 'cognitive bias' is a
central theme of this book, while tackling the specific problem of
'thinking you know' (when you don't) has been the task of philosophy
ever since Socrates walked the streets of Ancient Athens challenging the
arrogant young men to debate. The lessons Plato offers of those imagined
conversations are not so different from those Kahneman does two thousand
years, even though in so many ways their worlds are very different.
Socrates would have had no trouble recognising that the error Mission
Control went to so much trouble to minimise, the one that says 'what you
see is all there is', is in turn constructed out of these kinds of
biases. Alas, thinking errors are sturdy beasts, and many of these
biases were present in those hotly contested debates about the decision
to be taken as to how to land on the moon, once you, as it were, arrive
at it. Take, for example, what happened when, fully aware of the need to
resolve the dispute, a special NASA committee was given the task in 1961
of weighing the merits of various approaches and

choosing one. Its impeccably 'what you see is all there is' conclusion
was that it would be too expensive to develop and implement more than
one lunar landing mission mode, and that NASA should focus on the
science fiction writers' favoured method, of a single rocket making a
direct ascent. Commenting on the decision years later, the Chair of the
committee, George Low, offered that the 'mistaken technical judgment'
was 'rather my fault in trusting a single Committee member instead of
having the entire Committee review Houbolt's studies and
recommendations'. But that is to narrow the perspective. The error was
really a collective one linked to the tendency to only see what you
expect to see: to getting caught up in a particular story. We all think
like this, and maybe sometimes there's no harm there. Think about your
own life-story and how you may have taken key decisions in the
past---maybe about jobs, maybe even about partners. How much of the
'intuition' was based on cherishing an appealing story---and how much on
evidence? Stories are powerful. But flying to the moon required more
than an appealing story. On paper, the task set for the expert
committees at NASA was to take an open-minded look into the alternative
'modes' for getting to the moon, primarily those involving 'mission
staging by rendezvous' and 'alternative Nova vehicles'. From the start,
though, the idea seems to have been seriously compromised. One committee
member, Larry Loft, who attended the first meeting in early June 1961,
remembers that the second most important NASA administrator, Robert
Seamans, sometimes called the 'nuts and bolts boss', came in the first
day and 'sort of gave us our marching orders'. As if this wasn't enough,
Abe Silverstein, director of the Office of Space Flight Programs at NASA
headquarters, also came in to address the members, saying: 'Well, look
fellas, I want you to understand something. I've been right most of my
life about things, and if you guys are going to talk about rendezvous,
any kind of rendezvous, as a way of going to the Moon, forget it. I've
heard all those schemes and I don't want to hear any more of them,
because we're not going to the Moon using any of those schemes.'

As Jim Hansen, perhaps the best-known NASA administrator of them all,
would put it years later, in an account he entitled Enchanted
Rendezvous, with those 'words of warning and damnation, which completely
violated the reason for having the committee in the first place', the
usually masterful but, in this case, angry and self-righteous
Silverstein 'stomped out of the room'. Even if the committee members
formally disregarded the personal admonition and followed the original
strategy which was to consider the broad range of different rendezvous
schemes, in due course the committee 'turned down LOR cold'---'LOR'
being the acronym for 'Lunar-Orbit and Rendezvous', the method
eventually used. Yet, for a while, it seemed that on this issue upon
which the success or failure of the whole space dream rested, NASA's
military discipline would ensure that everyone would obey their
instructions and follow the new plan---whether it had any chance of
working or not. But then, on 15 November 1961, Houbolt fired off a
nine-page letter to Seamans (the second most important NASA
administrator, mentioned a moment ago---the one sometimes called the
'nuts and bolts boss') with two different editions of his Lunar-Orbit
Rendezvous plan 'admiral's sheet' attached to it. The Langley engineer
feared that the letter might cost him his job. He was skipping proper
channels, a bold move for a government employee, in appealing directly
to the Associate Administrator, NASA's number-two official. 'Somewhat as
a voice in the wilderness,' Houbolt's letter opened, 'I would like to
pass on a few thoughts that have been of deep concern to me over recent
months.' He then framed his concerns in terms of questions: 'Do we want
to go to the moon or not?', and, if so, he asked, 'Why do we have to
restrict our thinking to a certain narrow channel?' Specifically, 'Why
is Nova, with its ponderous size, simply just accepted, and why is a
much less grandiose scheme involving rendezvous ostracised or put on the
defensive?' 'I fully realise that contacting you in this manner is
somewhat unorthodox,' Houbolt closed by saying, 'but the issues

at stake are crucial enough to us all that an unusual course is
warranted.' In Houbolt's mind the problem was that bureaucratic
guidelines had made it impossible earlier for the Heaton Committee to
consider the merits of his alternative strategy. 'This is to me
nonsense', he stated frankly. 'I feel very fortunate that I do not have
to confine my thinking to arbitrarily set up ground rules which only
serve to constrain and preclude possible equally good or perhaps better
approaches.' Too often, he declared, NASA narrowly circumscribed its
thinking. What You See Is All There Is. Ground rules were set up,
parameters imposed, so that when finally the question was posed on a
particular issue, the outcome was largely inevitable. When decisions
were supposed to be debated, instead the only real question being asked
was: 'Now, with these ground rules what does it take to do the job?' In
Enchanted Rendezvous, Jim Hansen sums up the episode thus: The story of
the genesis of the LOR concept thus testifies to the essential
importance of the single individual contribution even within the context
of a large organization based on teamwork. It also underscores the
occasionally vital role played by the unpopular and minority opinion.
Sometimes one person alone or a small group of persons may have the best
answer to a problem. And those who believe passionately in their ideas
must not quit, even in the face of the strongest opposition or pressures
for conformity. Thousands of factors contributed to the ultimate success
of the Apollo lunar landing missions, but no single factor was more
essential than the concept of LOR. Without NASA's adoption of this
stubbornly held minority opinion in 1962, the United States may still
have reached the Moon, but almost certainly it would not have been
accomplished by the end of the 1960s, President Kennedy's target date.
One can take this 'what-if ' scenario even further. Without LOR, it is
possible that no one even now---near the beginning of the twenty-first
century---would have landed on the Moon.

Here is one idea that a Mission Controller can apply in many contexts:
the essential importance of the single individual contribution even
within the context of a large organisation based on teamwork. Perhaps,
we can better understand why an 'expert' committee dismissed Houbolt's
ideas when we realise that if the Saturn launch vehicle and the Apollo
spacecraft themselves were huge technological challenges, the third part
of the hardware for the moon landing, the Lunar Module, was well into
the realms of science fiction. It was required to carry two astronauts
down to the moon's surface, be their base of operations on the moon, and
finally to lift them back to orbit for rendezvous with the command
module. To do this, and offending the aesthetics of the missile men, the
lunar module had to perch atop the huge Saturn V rocket ready to come
into use once its shrunken remnant had reached the moon and settled into
lunar orbit. Such were the novelties required that the task of creating
a 'real one' was consistently behind schedule and over budget.
Particular problems stemmed from the demands of devising two separate
spacecraft components---one for descent to the moon and one for ascent
back to the command module---that only manoeuvred outside an atmosphere.
Both engines had to work perfectly or the very real danger existed that
the astronauts would not return home. When a sceptical visitor to the
Langley Research Center offered, with a chuckle, that Lunar-Orbit
Rendezvous was 'like putting a guy in an airplane without a parachute
and having him make a midair transfer', John Bird, one of the engineers
there, set that visitor straight. 'No,' he corrected, 'It's like having
a big ship moored in the harbour while a little rowboat leaves it, goes
ashore, and comes back again.' Guidance, manoeuvrability and spacecraft
control also caused no end of headaches. The landing structure likewise
presented problems; it had to be light and sturdy and shock resistant.
Eventually an ungainly vehicle emerged which two astronauts could fly
while standing. One test flight of a prototype was conducted,
necessarily

of course on earth, with Neil Armstrong at the controls. Armstrong was
unable to control it and it crashed. The technology might work on the
moon with its low gravity, but on earth it certainly didn't. The fact
was, to support the idea of a special lunar module in the 1960s was to
propose the creation of the strangest flying craft ever in history. It
would have had to be a two-stage spacecraft; the full ship would land on
the moon, but only the small upper stage and crew compartment would
blast off again and return the astronauts to the command module, in
orbit. For that reason, it had to have not one but two rocket engines, a
big one to land the ship, and a smaller one to lift the crew compartment
up from the dusty surface of the moon and back into orbit. Eventually,
design-wise, each of those rocket engines weighed less than the engine
in a typical midsize car---and each was a technical marvel. The descent
engine could be throttled: powerful thrust to bring the lunar module
down to the moon from orbit, and lower thrust to allow the lunar module
to hover near the surface while the astronauts picked a final landing
spot. No rocket engine before had ever had variable power! This was
truly 'thinking outside the box'. Or was it maybe thinking in the
process of going awry by being too driven by the narrative? The 9 Dots
Problem

Try this. An exercise in 'thinking outside the box'. Here's the task.
Take a piece of paper and make a simple grid of 9 dots as in the figure
above. Then, try to connect all the dots by four straight lines drawn
without lifting the pen from the paper and without retracing any lines.
(Too easy? Well, also try to do this with just one straight line!) (See
Appendix 1 for the answers.)

What was for sure was that the proposed craft didn't look very
plausible. The first, and still only, manned spacecraft designed solely
for use 'off earth' had four spindly legs and a gold tinfoil skirt that
made it look fragile and unfinished. However, it would never have to fly
through an atmosphere, so it didn't need to be aerodynamic nor have the
structural robustness that would require. It would only fly in space,
with the spindly legs coming along only for the first half of the
journey, because ultimately they would be left half buried in the dust
on the moon's surface. Committees shy away from approaches that are
novel and untested, and 'thinking without preconceptions', so easily
recommended, can actually become dangerous. Recall that NASA had every
reason to avid novelty: at the time of the announcement of Project
Apollo by President Kennedy in May 1961 it was still lurching from
mishap to existential crisis with failures at every turn. • The first
space flight of a NASA astronaut in 1961, made by Alan Shepard, had to
be postponed for weeks while engineers struggled to resolve numerous
details and, even so, during the second flight of the Mercury
spacecraft, the hatch blew off prematurely from the capsule, causing it
to sink into the Atlantic Ocean before it could be recovered, in the
process nearly drowning the astronaut, 'Gus' Grissom. • When John Glenn
became the first American to circle the earth, making three orbits in
the Friendship 7 Mercury spacecraft, the flight came within seconds of
disaster from both an autopilot failure and a loose heat shield. • The
Gemini programme included strange, longitudinal oscillations, nicknamed
the 'pogo' effect because it resembled the bouncing sensation of using a
pogo stick. The fuel cells leaked and had to be redesigned, and

NASA engineers never did get the paraglider to work properly and
eventually dropped it from the programme in favour of a parachute system
like the one used for Mercury. But by far the worst thing of all about
the first Apollo mission was that, during a test, a fire killed the
three-person crew. All of this underlined that the Apollo mission
itself, the journey to the moon and back, was quite possibly the most
difficult and risky voyage undertaken in human history. From technical
failure to human error, any number of things could have gone wrong---
and did. But by developing a 'Mission Control' mindset that involved
both acknowledging and planning for risk, the odds were overcome. Part
of coping with gaps in knowledge and acknowledging uncertainty is
recognising that failure can also be a first and necessary step to
success---as many scientific stories and indeed recent tales of internet
startups illustrate. In particular, the eventual success of Apollo 11
was necessarily built out of that tragic failure of Apollo 1. The
catastrophe forced NASA to confront its culture of complacency for risk
and safety, and to restructure its entire operations. Indeed, great
lessons can be learned from failure as well as success; from accepting
responsibility for non-performance and moving forward from there. It is
surely significant that Gene Kranz, Mission Control's best known and
charismatic controller, talks of NASA 'rolling the dice' in an address
that he delivered following the deadly 1967 Apollo 1 fire on the launch
pad. Kranz tells his audience: 'We did not do our job. We were rolling
the dice, hoping that things would come together by launch day, when in
our hearts we knew it would take a miracle.' This is the subtext to
those later scenes of sweaty young men at rows of computer consoles at
Mission Control, in sequence shouting 'Go' to Kranz, in his role as
ringmaster for the show. Considered a genius of operational procedure,
this launch status check system

('go/no go' ) remains in use, and Kranz himself is praised for doing
what the best leaders do: that is overcoming challenges through careful,
deliberative information processing and focused problem solving. Systems
used in Mission Control like this are one big reason why it is said that
the lasting value of the moon programme is not there in the
technologies, remarkable though the rate of innovation was, but rather
in the rethinking of the way people worked together in teams. Another
innovative aspect of the Apollo strategy was to always do (borrowing the
IBMers' 1969 shorthand) 'the Unk-Unks'---that is, to actively consider
the 'unknown unknowns'. Fast thinking, maybe relying on past experience
or even intuition, has a particular problem with 'Unk-Unks'. As has been
mentioned a few times already, in flying to the moon (and hoping to come
back) NASA knew it lacked experience. So, instead, NASA sought to
minimise the need for instinct and gut feeling and instead handled risk
by actively looking for it and constantly asking themselves 'What if?'.
Having backup systems and procedures in place ensured there was always a
Plan B. The lesson from this is to be proactive in assessing and
managing risk for your own projects. Identify situations that could trip
your team up and plan for them---but don't let an acceptable amount of
risk keep you from pushing ahead. Homer Ahr, a former IBM computer
programmer on Apollo, once said that 85 percent of the code IBM wrote
for Apollo 11 'had nothing to do with everything going right'. As
important as it was to design computers, code and systems that could
successfully land on the moon and return home, it was just as important
to write code to handle the unexpected, to code for the unknown
unknowns. Indeed, one such 'Unk-Unk' came within fifteen seconds of
killing Armstrong and Aldrin as they attempted to close the last few
miles to the lunar surface. On descent to the moon landing site, the
lunar module's computer started flashing up an obscure warning
message---ERROR 1202---which threatened to reboot the spaceship's tiny
computer in the middle of the landing sequence.

Staring at your laptop after the cursor freezes can be rather annoying.
Staring at a blank computer screen while in a steel box hurtling at
speed towards the moon must be rather worse! Actually, at the same time,
as the message flashed in green letters and a buzzer blared in the
cabin, just by peering out through the tiny window of the lunar module
(and yes, those windows were not just a luxury), Armstrong realised they
were going to overshoot their target, and were heading towards a crater
littered with treacherous boulders. But here's where the new kind of
thinking came in: because every possible error had been rehearsed,
Mission Control was able to diagnose the computer problem and realise
that it was because it had become overloaded with tasks and incoming
data, and thus that it was still possible to carry on with the descent,
only now with Armstrong taking semi-automatic control of the lunar
module, while Aldrin fed him altitude and velocity data. They landed on
the moon's surface with only seconds worth of fuel left. There's a
broader lesson in the story. 'Code 1202' events like this were the
Apollo version of what sociologists nowadays call 'black swan
events'---after the unexpected discovery of, yes, black swans in
Australia, which upturned the conviction everyone had up to then that
swans were by definition 'white'. The writer Nicolas Taleb has made a
name for himself applying such thinking to the financial markets---and
the way that people spectacularly failed to anticipate the financial
crash of the early years of the millennium. Indeed no business and no
company is immune to a Code 1202 event. The unforeseeable will occur:
the past does not tell you everything about the future. Leadership, in
the sense not so much of individuals but of systems, can set
expectations concerning risk evaluation that will help the company
respond in crisis situations because the error had been discussed as a
possibility in pre-flight rehearsals. Thus, on that final lunar descent,
when the program alarm (Code 1202) flashed, a young control officer in
Houston had actually already encountered the possible situation as part
of earlier simulations, and so was able to provide the critical 'go on
that alarm' assurance. By contrast, the Soviet model of top-down
direction suppressed negative feedback

and prevented both anticipation of problems and later learning from
mistakes. This dramatic incident also illustrates an important but
mundane truth: great things can be achieved through attention to detail.
Which is partly why conventional histories of the moon programme often
marginalise the remarkable creativity of NASA to focus on a very
different part of the Apollo legacy: the project management part. They
stress how Apollo's managers set a series of milestones, organised all
their work and measured their progress around these. They broke the
stages down into sub-stages, and the sub-stages into yet smaller pieces.
Until, eventually, every part of the task was achievable. In his book,
Systems Architecting of Organizations: Why Eagles Can't Swim, in a
passage that speaks powerfully to the crucial debate at NASA on which
method to use to fly to the moon, Eberhardt Rechtin puts it like this:
Systems engineering is the art and science of developing an operable
system capable of meeting requirements within often opposed constraints.
It is a holistic, integrative discipline, wherein the contributions of
structural engineers, electrical engineers, mechanism designers, power
engineers, human factors engineers, and many more disciplines are
evaluated and balanced, one against another, to produce a coherent whole
that is not dominated by the perspective of a single discipline...
Systems engineering is about looking at the 'big picture' and not only
ensuring that they get the design right (meet requirements) but that
they also get the right design.

Likewise, Jim Webb, the NASA administrator at the height of the
programme between 1961 and 1968, always contended that Apollo was much
more about this kind of project management thinking than anything else.
And many people consider that Webb, whose background was business and
law, not science at all, is the real guiding spirit of the Apollo
project.

Nonetheless, businessman or not, it is revealing that when, in a crucial
early meeting with Kennedy in the White House in 1963, Webb promised the
president that the lunar landing would be 'one of the most important
things that's been done in this nation', the surprising reason he
offered was that 'what will come' from going to the moon will be
measured 'in terms of the development of the human intellect'. That
said, Webb himself favoured corporate procedures over individual flair.
For him, real competence required doing things thoughtfully and
mindfully, rather than by hope, intuition or guesswork. Leaders should
'work the problem' through proper and thorough procedures. It was the
death of the crew of Apollo 1 in the launch pad test fire of 1967 that
chilled Webb's faith in both experts and their procedures. Webb, who up
to then had been the 'high priest' of technocracy ever since arriving at
NASA in 1961, arguing for the authority of experts, well-organised and
led, now felt that the model structure of exemplary management had
failed to anticipate and resolve the shortcomings in the Apollo capsule
design and had not taken what seemed in retrospect to be normal
precautions to ensure the safety of the crew. It was a case of 'thinking
that you know without knowing why you do' that had given confidence in
decision making; but the trouble was that this confidence had not been
based on anything. Webb resolved that the Apollo programme had to remedy
such overconfidence. Webb decided, specifically, that at all levels the
Mission Control managers of the space programme should: Define the
problem Determine goals/objectives Generate an array of alternative
solutions Evaluate the possible consequences of each solution Use this
analysis to choose one or more courses of action Plan the implementation
Implement with full commitment Adapt as needed based on incoming data

Or to put it another way, the rule was that from now on people were not
merely to think that they knew about something---they also would have to
know why they had that feeling. Behind the romance of footsteps in moon
dust and the earth suspended amongst the stars in space, there was a
grid of project targets each carefully subdivided with milestones
identified and all accompanied by a Project Approval Document. And
unlike the Soviet Union, where there was one large central authority
issuing detailed instructions, in the US these interface documents were
living creatures that were continually updated by many people to
correspond with developments at the centres. Recall again those iconic
scenes of rows of earnest space technicians at NASA Mission Control
shouting out 'Go' in response to Gene Kranz's famous 'go' or 'no go'
queries? Each call represented a way of breaking down a complex
assessment into more manageable pieces. But such decisions are taken
throughout the project at every level. Breaking the programme/project
life cycle into phases itself reorganised the entire process. For
instance, at the highest level, the process of breaking the big problem
down into smaller, more manageable parts saw the creation of the three
projects: Mercury, Gemini and Apollo. 'Project Mercury' put a man into
space after less than five years of dynamic activity which saw more than
2,000,000 people from government agencies and much of the aerospace
industry combine their skills, initiative and experience. Project Gemini
was the bridge between Project Mercury and the lunar mission of Project
Apollo: designed to carry two astronauts into earth orbit to test
long-duration flight, rendezvous and docking and other techniques needed
for journeys to the moon. But, of course, it is Project Apollo that
'changed the way we think' most fundamentally. And here's another, more
poetic reason why. On 21 December 1968, Apollo 8 took off atop a Saturn
V booster from the Kennedy Space Center with three astronauts aboard:
Frank Borman, James Lovell and William Anders, for their historic
mission to orbit the moon. After one and a half earth orbits, as its

third stage began a burn to put the spacecraft on a lunar trajectory,
the crew focused a portable television camera on earth and for the first
time humanity saw its home from afar, a tiny, lovely and fragile 'blue
marble' hanging in the blackness of space. To see the Earth as it truly
is, small and blue and beautiful in that eternal silence where it
floats, is to see ourselves as riders on the Earth together, brothers on
that bright loveliness in the eternal cold---brothers who know now that
they are truly brothers.

Those are the words of the writer Archibald MacLeish, but he summed up
the feelings of those watching in Mission Control too. The modern
environmental movement is supposed to have been reinvented and inspired
by this new perception of the planet and the need to protect it and the
life that it supports. But so too was human ambition and problem solving
in the widest sense.

Chapter 6

Thinking like an Artist--- or at Least, a Doodler

Sato's very precise drawing called '50 Manga Chairs' is an image that
just cries out to be a doodle... Here's an imaginative look at
'possibilities in chair design' by the French polymath, Milo Micelo,
especially for the book.

What is meant by 'thinking like an artist'? A clue is given by a
quotation attributed to the famous surrealist, Pablo Picasso: Every
child is an artist. The problem is how to remain an artist once we grow
up.

The clue is there in the idea that we are all born with a wider range of
thinking skills, but typically are led, through education

and life more generally, to embrace a much narrower 'palette'---to
borrow a word and a metaphor from art. This idea that we 'learn' to
think in more or less artistic and creative ways is also there in the
theory of right- and left-brain thinking, which links left- and
right-handedness to suggest that people use one hemisphere of their
brain more than the other, and that has many subtle effects. The idea
has been popularised in books like Drawing on the Right Side of the
Brain by the (rather conventional) Betty Edwards, an artist who says
that the right side of the brain is linked to creativity, while the left
side is more analytical. (Remember that, in terms of handedness, the
left side of the brain controls the right hand, and vice versa.) The
claim is that, as we grow up, one hemisphere becomes increasingly
dominant and this affects the ways in which we think and process
information. Left brainers specialise in language and logic, structure
information and make rational deductions based on evidence. Right
brainers are more spontaneous and intuitive, making quick connections
intuitively and often emotionally. At times, artistic thinkers may seem
to leap to conclusions over their plodding, logical companions, busy
jotting down notes and weighing up competing options. This is because
they rely on intuition and their subconscious to do the reasoning work
for them instead. Or else, faced with a complex problem to solve, they
are much more likely to put it to one side, to 'sleep on it'---and
indeed, we all know that this approach can work very well. Another key
element is how the artist brings emotion into their thinking through a
problem. Logical, rational people will be alarmed by this, of course,
saying it opens the way for rash decisions and subjectivity. But history
surely shows us that those who imagine they are thinking rationally are
not immune from such errors either. Indeed, not so long ago,
left-handedness was discouraged and seen as antisocial, with children
forced to write with their right hands. Nowadays, by happy contrast,
left-handed children are

often admired as potentially part of a rather select group of the
artistically gifted! Actually, in recent years, research has poured cold
water on the simplistic idea and rather seems to indicate that humans
use both sides of their brain in similar ways. But whether the theory
has a physiological basis or not, it is certainly a valid way to
reconsider how we tend to think and approach life. Above all, 'thinking
like an artist' involves imagining possibilities. For that reason,
although in this chapter I mostly talk about painters and sculptors and
designers employing this different kind of thinking, 'right-brain
thinking' can distinguish great poets, writers, inventors, lawyers,
philosophers---people in all walks of life---from their routinised
companions. Above all, 'thinking like an artist', like great art, is
rooted in the imagination. At least you don't have to think too hard to
know what kind of hat an artist wears: quintessentially it is a beret.
The great Picasso wore one, but so too do a whole range of people
seeking that certain je ne sais quoi... In recent years, everyone from
Rihanna to Bella Hadid and Kendall Jenner has been spotted wearing the
pancake-shaped hats. So the good news is that it is easy to look like an
artist---but is it as easy to think like one too? Now, as to that, I
have some good news. Thinking like an artist is quite different from
being one, and the kinds of thinkers I have in mind as exemplars range
from people like Einstein, Thomas Edison and Marie Curie to product
designers and inventors---not iconic artists like Picasso, Van Gogh or
Michelangelo... But why do I say that those first three thinkers were
'artists' anyway? They're not usually thought of like that. However, one
surprising claim made is that they were all inveterate doodlers! I say
'surprising' because even a cursory check of the usual internet sources
indicates that all three famous scientists didn't so much 'doodle' as
rough out really rather technical diagrams designed to prove something
brainy. Nonetheless, it's still a nice idea and the notebooks of the
three scientists do reveal them thinking through visual representations
as well as through words and mathematics---

and thinking like that is what this chapter is about. After all, the
primary function of our brains is processing visual and spatial data
about two-dimensional and three-dimensional objects! The link is
certainly enough for Sunni Brown, a 'visual-thinking skills' expert who
looks like she ought to be an artist and who has popularised the doodle
in several bestselling books. She also runs workshops for businesses
such as banks and retailers telling the executives that doodling: •
boosts comprehension and recall; • allows you to organise information in
novel ways with increased clarity; • and best of all, doesn't require
any skill at drawing. Nor does Cubism, some might quip, but quips aside,
here I have to part company with Ms. Brown as, to be honest, doodling
does seem to require some drawing skills and that's why most people give
it up---along with being told off repeatedly by teachers. At school,
doodling is associated with not concentrating, and that's the mark of a
duffer, not an innovator. Teachers don't seem to be keeping up with
recent research that suggests turning issues into images can help you
generate new inferences and refine your reasoning---without even
noticing it. But this is a promise worth exploring. The key thing about
doodling is that it uses parts of the brain that you're not actually
aware of. The word means 'scribble absentmindedly', which sounds bad
until you remember how little of our brain power is consciously employed
and how incredibly subtle the mind is. Language taps only a tiny
proportion of the power of the human brain, and visual thinking can draw
on some of the rest. That's why, as the saying goes, a picture really is
worth a thousand words. And that's why some researchers have advocated
that doodling be recognised as a key element in education, up there in
value with the 'Three Rs': reading, writing and arithmetic.

If doodling began with 'R', maybe it would be more Respected by
teachers! Alas, it doesn't, not even nearly, and few teachers seem aware
of, far less interested in, the psychological research that suggests
that doodling really can help you stay focused, grasp new concepts and
retain information. But, as I say, a bigger problem for the approach,
even if teachers were more tolerant of their charges scribbling in the
margins, is that most of us can't draw. Fortunately, there are ways
around even that roadblock. For a start, don't be put off by thinking
that doodling has to be 'representational', as artists say, because it
doesn't. Yes, if you have a good visual memory, it might be images of
objects, or landscapes, or people. But equally it could be squiggles,
abstract patterns or decorated letters and word clouds. If you are
technically minded, it might be a flow diagram. If you like cartoons it
might be a few of those. The point is to let your hand do the thinking
and recognise that the elegance of what it comes up with doesn't really
matter. Above all, remember that doodles are just starting points. Or
maybe we should call them 'intuition pumps'. What you start with can
lead to something much bigger and better. Let's investigate the links
between art and inspiration a bit more closely with someone who has more
than their fair share of skills in both. Oki Sato is a Canadian-born
Japanese designer whose ideas often start out as quickly jotted
sketches. The chief designer and founder of the design firm Nendo says
new ideas often start small and then spread. He compares them to the
idea of the 'butterfly effect' in weather, where it is sometimes said
that even the tiny movement of a butterfly's wing in China can, by a
process of feedback and amplification, eventually create a tornado
half-way across the world in North America. To Matthew Ponsford, of CNN
Style, Sato once explained: 'I come up with my ideas from everyday life.
Small differences hidden within the normal routine. They all become
small starting points for all my projects. The smaller the better, to
let it sneak into people's minds and emotions.'

Another time, when design writer Katie Treggiden asked him to give an
example of an 'ah-ha' moment, Sato responded: 'It's all about emotions;
small emotions.' This alone is an intriguing insight. And Sato goes on:
When I was a student studying architecture, I was taught to see things
from a very high point of view. So when I designed a cup, for instance,
I had to see the city first, and then think about how the building
should be designed, and then think about how the room should be
designed, and thinking about the furniture and then... I design the
cup--- that was how I had to think about it. Now I'm thinking totally
the opposite way---I start from very small emotions, small ideas and let
it grow into furniture and interiors and buildings and hopefully into
cities.

Sato touches here upon something important about the way we are taught
to think. Specifically, that very often we are supposed to 'deduce'
logical connections from facts and data. But art frees us from the
straitjacket of logic, allowing us to roam wherever we want. For Sato,
the role of design all about discovering unexpected connections. 'It is
about what kind of story you can find behind the object, \[and\] whether
it's products or architecture or graphic... is all the same to me', he
says. These are amazing statements, really. Very much 'thinking outside
the box'. And also quite different from the way that Art and Design
Colleges would like to have their students brought up. But Sato is in
his own way an artistic radical. He explains: In Japan, people who have
studied architecture usually only design architecture; interior
designers design interiors; and product designers design products. But I
wanted to design more flexibly and to transcend genres. That is why I
started Nendo.

That was in 2002. And the name of the company comes from the Japanese
term for modelling clay. It expresses his desire for a

certain amount of flexibility and the ability to reinvent oneself. Sato
says of his philosophy: Whenever I was a small child, I liked reading
manga books, and my mother was angry at me, \[yet\] while going to the
museums my mother was happy with it. So I started thinking, what is the
difference between going to museums and reading manga? Then, I really
didn't understand the difference and that fact was in my head for quite
a long time.

And then he did a little bit of research about manga and learned that it
was deeply rooted with Japanese culture, and started thinking whether he
could use the techniques used in manga for furniture and objects. After
all, he says, both two-dimensional and threedimensional objects are ways
of 'expressing feelings and emotions and movements...' Consider one
early commission that Nendo had: to design a new kind of umbrella. Now
there are plenty of people who have tried to rethink this humble object,
resulting in umbrellas with anti-slip coatings, umbrellas that look like
a head of lettuce---even inverted 'UnBRELLAs'. But Sato focused not on
the canopy, let alone the ribs, but rather on the handle. The result was
'an umbrella whose handle makes it not only stable when in use, but able
to stand on its own when turned on its handle', as its designer puts it.
Sato's 'Staybrella' came in a rainbow of hues and could even 'hang
securely from tables and stay propped up on a wall when not in use'.
Marshall Smith, writing for the website Industry Tap, said
enthusiastically of it: 'Umbrellas are awkward, malfunction constantly,
and are simply frustrating in general, but thanks to Nendo and Japanese
designer Oki Sato, we have a new-age umbrella that makes staying dry
less of a hassle.' Sato works on hundreds of projects at the same time,
which is a thinking strategy we can all try. Doing this can be extremely
confusing, not to say stressful, of course, but Sato actually likes the
feel, not least because he thinks 'multitasking' gives space to the

subconscious to come up with unexpected connections and new ideas. Visit
an artist in their studio (famously cluttered spaces) and you'll often
find that they seem to be switching erratically between projects.
Illogical? But no, it's a different way of thinking. One result is that
the Nendo range is endless: from designing high-concept furniture,
retail stores and exhibition spaces all over the world to adding unique
and more unexpected items such as watches, chopsticks and shoes, and
even 'magne-hinge' (sic.) glasses. These last are spectacle frames whose
hinges employ magnets rather than the usual screws, making the temples
removable, allowing both for creative colour mixing as well as meaning
that the frames are more flexible and less likely to break. Over the
years, Sato has worked on everything from punched metal chairs and a
stool for Cappellini, with its construction looping like ribbons of
ballet slippers, to a portable lamp for Louis Vuitton's first-ever
furniture collection, to a human-sized birdhouse containing
seventy-eight nest spaces located in a forest, and tableware made from
recycled Coca-Cola glass bottles. It's all about cross-fertilisation, he
says, which is another key thinking skills concept we can all grab hold
of. Or, as he explained to Daniel Scheffler in an interview for the
South China Morning Post: When we work in many different fields, \[the
projects\] influence each other. I can find new ideas within different
projects. And the more projects I work on, the more ideas I develop.

As I say, writing and math all follow a linear logic, and Sato's
antipathy to conventional ideas about this lead him to hate being asked
to define his work. However, if forced to give an opinion, he ventures
that it is not about making things but about observations: observations
that lead to small ideas. This is where the idea of the butterfly effect
comes in again. Small ideas, tiny ones that people would hardly think
worth mentioning, are allowed to take off and develop into something
influential. 'I try to look for the small

things that are hidden, things that people don't really care about or
things that they have already forgotten', he says. The same thinking
underlies Sato's daily routine---which is very regular---even, he
allows, rather boring! But following the same routine every day helps
him notice small changes in the world around him. And he says that, for
him, the small changes prompt insights and inspirations. Another
thinking strategy that Sato uses is to look for 'borders', but here what
he is really saying is that he wants to work at the boundaries. Between
light and dark, hard and soft, private and public. That last boundary
being particularly important in designing workspaces, for example.
Boundaries (and gaps too) are fundamental to Sato. As he told (design
writer) Katie Treggiden: Everybody looks at the beauty of the stars but
I'm trying to design the sky itself, the darkness, which enhances the
stars' appearance. By assessing what's in between, I'm able to find
something slightly different: new, small ideas, but very interesting
ones.

In short, Sato says, 'I'm trying to think about things that are in
between things.' An illustration of the approach that he offers of this
is a table named 'Clear Shadow' that he produced for the highend (and
rather expensive) company Glas Italia and the Milan Expo, explaining: 'I
came up with this idea when I was washing my face one morning in a
hotel. I noticed that there was the shadow of the bubbles on the water.
It was really interesting even though I couldn't see the bubbles
themselves. Then I thought up the design of the table.' On the other
hand, a very different tactic that Sato also uses (which we all might
borrow) is designed not to solve problems but to CREATE them instead.
For example, he told glass blowers making lamps to 'suck' the air
instead, which created interesting effects in the glass, particularly
when lit from within. Anyway, for Sato, as for all true doodlers, it is
all about the idea, not the implementation. 'I make awful sketches, but,
when you're

very good at sketching, you feel that the design and idea are good as
well. If it's a very bad sketch, but you're excited, that means your
idea is good, so you feel very confident.' That's a nice idea, and
definitely reassuring, although, looking at Sato's elegant roughs, there
are obviously degrees of awful drawing. One time, he was commissioned to
make a chair out of bamboo. He made it out of steel pipes instead. Why?
To be perverse? Not at all. Instead, what was important to him was not
the material but the response to it. The pipes had the FEEL of bamboo.
As he put it in an Interior Design Show talk (19 November 2013): 'What
is interesting is not the bamboo, not the material itself, but how you
treat the material.' That's why Sato even used the techniques of the
bamboo manufacturers on the steel pipes---including diagonal slicing and
shaving cross-sections. It sounds like the hard way to get from A to B,
but as Sato explained to Y-Jean Mun-Delsalle at Forbes.com (17 October
2019): 'When I feel that design is just a job, I don't think I should
continue anymore.' Specifying 'design', but it might just as well be
'art', he goes on: Design is my hobby, my life; it's almost like
breathing. In the future, I think design is going to be everywhere, in
the air. It's not going to be something special anymore. This creative
way of thinking is going to solve all of life's problems. In that sense,
design might disappear within culture and lifestyles, which is very
beautiful.

Sato's central theme is that good design taps into emotions, and that's
an insight that the American sculptor Paula Crown shares. As she says in
an 'artist statement' on her website: It all begins with a mark. A mark
that is informed by where we are, what material we hold, and how we
exist in the world. Mark making creates an index of experience, where
sensory input is translated through the hand, and art becomes thought
manifested. (Emphasis added)

Crown's work has a social dimension. She established a successful
atelier (studio) in Chicago, was appointed to President Obama's
Committee on the Arts and the Humanities, and currently serves as a
member of the board of trustees at the Museum of Modern Art in New York,
chairing the Education Committee. The most iconic and recognisable body
of work is her large-scale sculpture, JOKESTER: a giant, deformed, red
plastic cup (made of painted epoxy resin, fibreglass and urethane foam
on a welded stainless steel armature), which took one of Crown's earlier
series of lifesize 'crushed cup' sculptures and brought it up to a
monumental scale, thereby taking on a new role of serving as a reminder
of consumption, waste, pollution and re-use. But back to the importance
of 'emotions', and Crown says of her crushed cups that she began to see
each individual form no longer solely as leftover garbage, but 'as a
representation of its previous holder'. Expanding on the psychological,
she continues: 'We constantly make unconscious gestures and vibrations
in the world, and I noticed how holders often expended energy to alter
their cup in some way. Cups were twisted, crushed, or split apart and
assumed new forms. These marks become reflections and documentation of
the person who has left it behind.' So she went on to crush 300 separate
cups, make plaster casts of each, and paint them in detail. Unlike the
originals, the plaster cups were 'uncannily weighty', which she sees as
appropriate as she perceives them becoming 'recipients of the holder's
energy'. Finally, a name for each cup was imagined and then applied,
bestowing personalities to each individual cup, reflecting cultural
concerns of being 'Insta-worthy', 'Never good as Dad', 'Failing out' and
'Jokester'. Finally, when exhibited on the floor-space, the cups
collectively became something else... 'a zombie space of prior gestures
and memory.' The installation is political. 'The cup reflects a consumer
culture defined by optimism and abundance. It is convenient, low cost,
and tossed after one use. But, after the party, who will clean up?'

Like Sato, Crown also embraces the 'disruptive', saying: 'Distorting,
inverting, or dimensionalizing work challenges my perception. Something
that I thought I understood, becomes unfamiliar. I am standing in the
same place yet everything is different.' Crown thinks that the artist
'enlarges', that they add a new dimension to a 'conversation'. They
'think differently' but they also should be listening. So they should
not be telling people what is right and wrong but 'honour the gestures'.
A crushed cup is a very common gesture that many of us will have made
when, after finishing a drink in a plastic or paper cup, we then, well,
crushed it. And yet each 'crushed cup' in our hands is 'also a mark we
try to make on the world'. Describing her installation of crushed cups
cast in plaster and painted meticulously by hand, Crown says: Each cup
has a named identity that imagines an unconscious transference of
emotions onto the form of each cup. Failing Out is flattened,
Insta-worthy is sinuously turned, and Eileen tilts to one side.

The broader idea is that art is part of the political realm. As the cave
paintings of Europe illustrate, 'these marks show that our need to
express ourselves, to be creative... go back to the very start of human
civilizations... there is something in humanity that needs to make these
gestures.' The error is to sideline art as something 'subordinate'. She
admires the idea of 'art for all', as illustrated by Barack and Michelle
Obama inviting children to workshops with the nation's top artists. When
she got involved in a strategy of embedding art in schools and colleges
she found attendance went up and disciplinary problems went down. In an
essay celebrating the 80th anniversary of New York's Museum of Modern
Art's commitment to museum education, entitled 'Thinking Like an Artist:
Translating Ideas into Form', Paula Crown says that as a child she used
to scratch images onto her father's record collection and paint patterns
on the walls of the

family home, both without permission. Years later, she realises how
vexing it must have been for her parents but rationalises thus: 'Sight
is our most important sense; it allows us to communicate in images and
symbols. With this expanded language we are able to think in
abstractions and feel things that we cannot clearly say. Here, surely,
is the essence of "artful thinking".' Her father flew Boeing B29
'Superfortress' planes and she explored the language of flying, precise
and depersonalised, as well as the austere geometry of a plane cockpit.
She tried to convey what it would be like to be in such a cockpit. But
then, on the other hand, she went to Japan to explore the experience of
the survivors. She came across a Buddha that had been melted in one of
the explosions following the dropping of atomic bombs on Japan. But in
this she found not merely horror but also a symbol of transcendence. The
story illustrates a vital difference between artistic thinkers and
verbal thinkers: how they will look at something, be it a scene, a
person or an object. As has been well remarked, although many people
feel that learning to draw is hard, it is actually learning to see that
is difficult. Conceptual, logical thinkers will too quickly assume they
know what is there, what something looks like, because they have slotted
it into a previously encountered category. The artistic thinker may look
longer and is more open to discovery. The former looks for the expected,
the latter for the unexpected. This why, in terms of problem solving, in
fact, the very process of visualising something can sometimes force
another perspective. As even the simple images of doodling occasionally
illustrate, transforming a thought into a picture can bring new depth to
a thought process and a clear conclusion. Which brings me back to
doodling and to perhaps the best known 'doodles' of today: the Google
Doodles. This is one of Google's most popular innovations---but the idea
of turning the logo on the search engine's homepage into something fun
and dynamic came about really by accident. It all started when the
Google founders, Larry Page and Segey Brin, decided to place a stick
figure behind the second 'O' of Google, as 'a comical message

to Google users that the founders were 'out of office visiting the
"Burning Man" festival', as the Google history on the website itself
likes to record it. In a way, then, this is not a true doodle, as it
wasn't exploring anything and certainly wasn't absentminded. But in the
best spirit of doodling, as the history goes on, while this first doodle
was 'relatively simple', it contained the seeds of something unintended
and much bigger. In this case, 'the idea of decorating the company logo
to celebrate notable events'. But first, in true doodle style, there was
a purely amusing segue to Martians and flying saucers. For some googlean
reason, it's not stressed on the company website now, but in May 2000,
the site showcased its first Doodle series, called 'Google Aliens',
courtesy of freelance illustrator, Ian Marsden. Five doodles in sequence
depicted little green men encountering the Google logo, before hauling
it off to Mars, and in the process turning the Google logo upside down.
It was only after this that Page and Brin asked a Google intern, called
Dennis Hwang, to produce a doodle for Bastille Day. This was pretty
conventional stuff though, consisting of some fireworks over the letters
and the words Liberté, Égalité, Fraternité. If you don't know what they
mean then you must have been doodling that day in school. But boring bit
of history or not, Page and Brin liked it so much that they ended up
giving Hwang the oh-so-Googley title of 'Chief Doodler'. And if, at
first, rather practically, the doodles focused on familiar national
holidays, eventually they expanded their range to include everything
from niche histories to random celebrations of mundane objects: ones
like the arcade game 'Pac Man' and the ice cream sundae. 'In the early
years, it was a controversial thing to do', Hwang told TIME in an
interview, meaning a company tampering with its logo, rather than
counter-cultural references to arcade games. 'If you read any kind of
corporate marketing or branding textbook, the one thing they tell you is
to make your corporate branding consistent no matter what. But Larry and
Sergey said, "Why not?"' And, in many ways, the Google doodle fits the
company. This is

because it is truly part of a family of artistic activities built on the
valuing of pure experimentation---coupled with a willingness to later
follow up what works. Indeed, experimentation followed by reinforcement
of what works is something that Google's engineers (in the Google AI
Strategy & Research team) have written many brainy papers on, while
explaining that nature favours contradictions and mixing over logic and
rigid categories. Yes, in other words, Google's engineers emphasise
Darwinian evolution over computers. They theorise that what they call a
'biodiverse ecology', with many different species, is more resilient to
change and disturbance than any single system can be. Coral reefs
provide for them the perfect example. Such reefs inhabit a zone between
land and sea, neither one thing nor the other. They also occupy less
than 1% of the ocean floor but support 25% of marine life, giving them
what is thought to be the highest biodiversity of any ecosystem on the
planet. And recent studies suggest their adaptive resilience to warming
sea water temperatures is due to the diversity and interdependence of
life forms that comprise it. Which thought brings us back to the notion
of boundaries. And boundaries, as we saw Oki Sato saying earlier, are
where design thinkers work. That's not the only link between doodling
and Google, though. Another scientific concept that fits with design
thinking is 'gap dynamics'---where gaps produced by natural disturbances
in forests, such as fire, liberate resources and provide colonisation
opportunities for species that otherwise would not be able to establish
themselves. This pattern of new plant growth after a forest gap
increases species diversity and in turn leads to greater resiliency of
the ecology. We can apply this insight in our own lives and work by
allowing---even embracing---a little disruption. Google has made
practical use of the concept when creating a 'regenerative neural net'
capable of regenerating destroyed or obstructed pixels, something with
applications in both art and photography. But note too that ordinary,
everyday language encourages us, nay, requires

us, to put nature into discrete categories even though (like the soap
suds in Sato's sink that morning), in reality, boundaries are blurred
and everything is complex and interdependent. Unfortunately, far from
embracing diversity and disruption, most art education, just like all
the other kinds, provides plenty of formal training and little
opportunity to think creatively. Art courses are long on how to
accurately represent the world, and even art appreciation is tied to
visual literacy---the ability to interpret and find meaning in images.
Take, for example, the approach of Julie Carmean and Shari Tishman,
principal investigators on artful thinking at Project Zero in the
Harvard Graduate School of Education, who have written about the artful
thinking approach--- only it turns out that it maybe isn't as artful as
it claims. Supporters of the method explain that seeing, thinking and
wondering are 'thinking moves' that students already know how to do---at
least at the entry level---and are tremendously important, not just for
looking at art, but for developing understanding in any discipline.‬
'Observing naturally leads to reasoning, which connects to questioning,
which in turn links to connectionmaking, and so on', says Tishman. 'What
do you see? What do you think about that? What does it make you wonder?'
These 'comprise a thinking routine', she adds. However, I think that it
all sounds very linear, not to say teacherly, even if the method's
supporters insist that the 'thinking routines' are designed to be used
'flexibly and frequently'. Indeed, if I was listening to a course on
this, I think I might well start to doodle...

Chapter 7

Thinking like a Cyberneticist Or is it a Weather-Forecaster?

A murmuration of starlings (photographer William Baxter for Geograph
Project).

If only life were more simple! Or so people are supposed to exclaim in
books... But certainly, you are more likely to hear that than the
converse: 'If only life were more complex!' Yet complexity is the thing
that gets cyberneticists and weather-forecasters alike excited, and it
is the claim of this chapter that complexity is actually as much our
friend as it is our deadly foe.

Embracing complexity is very much the flavour of modern business, from
Google to Amazon and beyond... From their perspective, complexity is
competitive advantage and opportunity--- and simplicity, let alone E.F.
Schumaker's 'small is beautiful' approach, is for suckers. But that's
looking at the issues in, well, the simple way. The whole point of
complexity is that there may be lots of different answers and lots of
different outcomes, and you really don't know in advance what they will
be. Embracing complexity, far less its shadow, 'chaos', requires quite a
radical stepping back from everything we've been brought up to take as
wisdom. For that reason, the 'hat' that might be adopted to symbolise
this approach is a casquette, a peaked cotton cap, as worn by one of the
approach's pioneers, Charles West Churchman, an American philosopher and
systems scientist, who was additionally Professor of Business
Administration and Professor of Peace and Conflict Studies. This bloke
lived and breathed complexity! Stepping back from conventional thinking
and looking at issues from all sorts of angles really does take a
conscious effort, because if there's one thing that we are taught in
school, and later in college or in the workplace, it is to value order
and organisation. We learn 'RULES IS RULES' or, more grandly, to think
logically. 'Snow', we are assured, 'is white', what goes up, must come
down, and, most certainly, God does not play dice. Only, as real life
(let alone quantum physics) regularly likes to remind us, it seems that,
quite often, He does. The truth is much more as James Gleick explained
it in his classic book on the subject, entitled Chaos: Making a New
Science, that chaos is all around us. It's there in the sudden falls in
the stock markets, like that which sparked the 2008 sub-prime crisis.
It's there in sudden spikes in illnesses that strike unsuspecting people
down. And it is there, lurking in the myriads of
tiny---trivial!---factors that can make or break so many of our daily
plans. On a much grander scale, too, chaos plunges the planet into ice
ages, and rips continents apart. Yet we've become accustomed by endless
repetition to the idea that climate is normally stable and

even that we humans can influence it (for better or worse). Maybe we can
on a short (decadal) time frame. But chaos theory puts such claims in a
wider perspective. Gleick's Goldfish Pond To better understand the
nature of chaos, and how we can work with it, let's start with a
manageably chaotic, indeed rather homely, example Gleick uses to
illustrate the theory: that of a simple garden pond of goldfish. Imagine
just such a pond with goldfish swimming around in it. Each summer, you
go out and count the number of fish. Some summers the number is similar
to the number you got last year; sometimes it's a bit lower, sometimes a
bit higher---and sometimes it is way different. Ever so often, the fish
population defies predictions. But why? You might think that the number
of fish in the pond would steadily increase---visits from greedy storks
notwithstanding. But no, the pond only contains a certain amount of food
and also limited amounts of oxygen. So as fish numbers go up, sooner or
later they must also start to go down. Otherwise the earth would belong
to goldfish in no time. However, if fish numbers drop, the pressures
exerted by food and oxygen limits drop away. At certain points and
times, the factors are all in a subtle balance. The end result is that
the number of fish in the future cannot be reliably predicted. You can
count the number of fish, regulate their food, check the oxygen---and
still get it wrong. That's because the variables---fish, oxygen, food,
etc.---all affect each other. Too many fish, oxygen levels drop, and
fish die---but the amount of food around increases, for example.

Where our ancestors once recognised their powerlessness in the face of
nature, today our response to problems is to seek more detail and better
information. Then, surely, we can anticipate and maybe control events.
But that's not how chaos and complexity work. Because, here, there is no
hope of ever being sure of what comes next. Inherent unpredictability is
the same problem that Isaac Newton found centuries ago with heavenly
bodies---when he realised that tiny gravitational influences, exerted by
heavenly bodies on each other, ruled out absolute precision in
calculating their orbits. Tiny differences yet, given enough time, they
could make asteroids suddenly leap out of nowhere to hit the earth! One
day, they could even cause Mercury to collide with Venus---or the moon
to break free from her orbit. These days, astronomers routinely find the
fingerprints of chaos in violence on the sun's surface and in the
distribution of galaxies. The heavenly firmament that seems eternal is
actually telling us a very different story. Meanwhile, back on earth,
the role of chaos in regulating our own bodies is a much-neglected
medical tale, and not just because we so much value clarity and
certainty here. Yet certain relationships demand that it be tackled,
which is why scientists track chaotic behaviour in the workings of the
human immune system, with its billions of components and its capacity
for learning, memory and pattern recognition. Or take the role of chaos
in measles and chicken pox outbreaks, issues so practical and
commonplace. William Schaffer and Mark Kot, two academics at the
University of Arizona, have found that patterns of stability within
randomness play an important role. The two researchers found that their
data on common childhood illnesses fitted the mathematical model of a
damped, driven pendulum. They realised that this is because diseases are
driven each year by infections spread among children returning to
school, and damped by natural resistance. But here's the chaotic thing:
the model predicts strikingly different behaviour for each

disease. Chicken pox should vary periodically. Measles should vary
chaotically. Which, as it happens, is exactly what the data do show.
Where, previously, yearly variations in measles seemed
inexplicable---random and noisy---the researchers were able to show that
the illnesses actually fitted with one of chaos theory's core ideas,
that of 'strange attractors'. These are physical states that systems
seem to try to return to, generally without quite managing it. But
forget the technical jargon. A practical way to look at it is that a
year of high measles infection will be followed by a crash. After a year
of medium infection, the level will change only slightly. Perhaps,
counter-intuitively, it is the years of low infection that produce the
greatest unpredictability. In this way, chaos theory explains otherwise
inexplicable effects of mass inoculation programmes---something that
could not be done within standard epidemiology. The point is that chaos
theory actually is about finding order--- within the disorder. Those
fractal images created by computer programs reveal that in a very
immediate, intuitive way. But so too do actual photographs of nature. J.
Doyne Farmer, one of the principals in the Prediction Company in Santa
Fe, New Mexico, has a photograph that he likes to show classes along
with his charts and diagrams when he gives a lecture. It looks like the
cross-section of a long seashell, but it's actually the meeting of a
stream of liquid nitrogen with another made of helium and argon
---resulting in 'a very turbulent fluid flow', as Farmer says to the
students, adding: 'Until this picture was taken at Cal Tech in 1972,
people believed turbulent fluid flows were totally random. But look at
it.' The challenge is (and the rewards come from) working out the
'rules' that give rise to such complexity. One important regularity is
that fractals grow, and complex systems thrive, on the boundary between
order and chaos. This is something as everyday a phenomenon as plants
growing in abundance along forest paths illustrate. Because it is only
at the margins that there is enough structure to preserve the stability
of the system, and enough

instability to generate novelty. Workplaces and classrooms should be
more like forest paths and less like plantations by embracing a little
chaos and uncertainty in order to encourage innovation and creativity.
Another useful theoretical takeaway is that in complex adaptive systems
there is no central command structure; rather, control is distributed
throughout the system, allowing it to react and adapt better. Both of
these insights have considerable resonance for organisations of all
kinds, from schools to companies---and even governments. But let's start
with the structure of just one human being. And take human cells, for
instance. While they can act as independent agents, they can also
congregate and self-organise to form more complex, multicellular life.
In a similar way, human beings too can act as individuals or as part of
groups, communities and organisations. Which raises the question: what
is it to be a human being? The question already implies the answer. Each
of us 'feels' like one unique individual, united by one controlling
consciousness. Yet, in reality, we are made up of millions of parts,
cells and viruses and bacteria that cooperate, to a greater or lesser
extent to enable a more complex organism to take shape and exit. Our
bodies are coalitions of competing interests, governed by the rules that
decide what works, what survives and flourishes in nature. At the larger
scale, each human being is part of human society---fundamentally
dependent on another intricate web of relationships in which success for
a few can as easily be at the expense of the many, as it may be part of
an innovative reorganisation creating some general benefit. Human
society continually reshapes itself, and it does so more successfully
when not following an imposed pattern. The self-organisation effect can
be seen most dramatically in human political life when a political
leader may emerge (along with a new political programme) and change the
direction of travel of the entire community. The emergence of Barack
Obama in the

United States is illustrative of this---and it is also perhaps
significant that Obama started out as a community organiser. So, too, is
the rise---and fall---of Donald Trump and his 'Make America Great Again'
red-capped movement revealing. MAGA came to a disreputable end, with the
invasion of the US Capitol in 2021 illustrating another aspect of
complex systems, namely that they rely on a previous balance between
forces being maintained. For this reason, relationships between
individual parts of a system are more important than the parts
themselves. This is why there is one thing complexity theorists and
business executives alike agree on: that complex systems composed of
many actors should be allowed to regulate themselves. After all, ant
colonies stay organised. Trump and Obama represented two very different
political outcomes in the US. We might indeed see the political
situation there as 'chaotic'. Yet, if we think about human
organisations, as well as nature more generally, as complex adaptive
systems, rather than as 'machines', focusing our attention on such
things as the 'initial conditions' and the history of an organisation,
it can lead to quite different and often deeper insights into how to
live and manage our lives. The term 'chaos theory' is attractive, or, at
least, eye-catching. Yet it is also misleading. Better would be to talk
of 'the study of complex, adaptive systems'. Yes, that's a mouthful, but
it is more descriptive. The word 'complex' implies diversity, through a
great number, and wide variety of interdependent, yet autonomous parts.
'Adaptive' refers to the system's ability to alter, change and learn
from past experiences. And the 'system' portion refers to a set of
connected, interdependent parts---a network. Once you start looking for
them, it turns out that there are a great number of complex adaptive
systems existing at different scales, and chaos theory additionally
reveals that there are common, interrelated principles that they share.
The contemporary science writer Stuart Kauffman puts it rather nicely by
saying: 'Life exists at the edge of chaos.' Adding: 'I suspect

that the fate of all complex adapting systems in the biosphere--- from
single cells to economies---is to evolve to a natural state between
order and chaos, a grand compromise between structure and surprise.'
Kauffman thinks that the rewards of embracing chaotic systems go well
beyond abstract theory. In his book, At Home in the Universe: The Search
for the Laws of Self-Organization and Complexity, a tome that is
sometimes called the complexity field's 'new bible', he promises that
complexity and chaos theory will allow both the practitioners and the
theoreticians of the business world to rewrite their playbooks. To see
how complexity theory offers new insights into real-world issues, he
offers the example of the circulation of brightly coloured yellow cars
in New York. At one level, say that of an alien looking down from outer
space, it looks like, well, chaos. But underneath the disorder is a
pattern. Taxis roam around until hailed. They then go precisely where
the customer directs. They have certain points ('strange attractors')
like railway stations, where they tend to return to when there are no
customers hailing them. 'Suppose you come down from Mars and you see
taxicabs working the streets of New York', he says. 'You want to
simulate that. You might think, well, there's probably a big dispatcher
in the sky sending all these cabs where they need to be.' However,
underneath the randomness is order. Or, as Chris Meyer, a partner at
Ernst & Young who heads the firm's Center for Business Innovation, says:
'Each yellow car has a simple brain following a few simple rules: Stop
for anything that waves. Go where it says.' Such a simulation, he says,
gives you reality---all these cabs moving about, coalescing and
separating in complicated patterns. 'That simulation is going to be
robust and simple and it's going to account for the diversity you
actually see.' The point is, the fabulously complex movements of
thousands of New York taxis can be expressed by just a few simple rules.
And the moral is: stop trying to control a complex system from above.
Meaning, stop trying to understand it all and work

out how it operates like an engineer might, but instead become an
observer, step back and watch for the 'emergent properties' that arise
as a system organises itself and devote yourself to preserving the
conditions in which the best solutions evolve. The movements of taxis in
New York look quite random, yet in reality are quite logical. A very
different kind of natural occurrence, in which order seems to emerge out
of otherwise uncoordinated behaviour, is the astonishing natural
phenomenon of thousands, indeed hundreds of thousands, of birds swirling
in fantastic shaped formations in the skies in the autumn. When
starlings do this, it is called a murmuration. Sometimes the birds form
spheres and other more complex shapes, shapes that move in smooth
sequence across the sky, before perhaps settling instantaneously into
trees---and lifting off again. Yet there is no 'master bird' directing
the murmuration. Nonetheless, it seems that each bird is again following
certain rules. Starlings gather in the autumn evenings, and when they
form in a huge flock that shape-shifts in the sky it is as if it were an
intelligent cloud, or even, more metaphorically, as if ink were being
dramatically painted by an unseen hand on a giant canvas. More
prosaically, part of the behaviour can be explained as a collective
defensive response to the presence of a predator like a merlin, hawk or
peregrine falcon. Since there is safety in numbers, individual starlings
do not scatter but rather remain in formation, while collectively
feinting away from a diving raptor, thousands of birds changing
direction almost simultaneously. 'Evolution is chaos with feedback', the
American physicist Joseph Ford once said. The universe is random and
dissipated, yes. But randomness with an element of direction can produce
surprising complexity. However, the mystery remains of how each bird,
perhaps hundreds of birds away from the danger, is able to react so
instantly and smoothly. Murmurations like this are something that will
continue to fascinate scientists (as well as bird lovers) for a long
time. But it seems that one secret lies in something called 'scale-

free correlation', and every shift of the murmuration is called a
critical transition. Giorgio Parisi, a theoretical physicist with the
University of Rome, puts it this way: The change in the behavioural
state of one animal affects and is affected by that of all other animals
in the group, no matter how large the group is.

In short, if one bird changes speed or direction, so do the others. And
yet, clearly, there isn't a jagged mass of competing movements. Another
part of the mystery remains in the synchronisation. One study that
attempted to investigate this looked at how a change in direction by one
bird affected those around it, and discovered that one bird's movement
only seemed to affect its seven closest neighbours. The number seven is
both real and significant. It's one of those numbers that plays a
special role in nature, and in the case of bird murmurations provides 'a
balance between group cohesiveness and individual effort', as the
researchers put it. It is this balance that means you end up with a
flock of birds that behaves like a twisting, morphing cloud, some parts
moving in one direction at one speed and other parts moving in another
direction and at another speed. Maybe it's only because this is a
science led by mathematicians, but it seems that complexity conceals
certain secret numbers. One is 4.669201, 'discovered', if you like, in
1975 by Mitchell Feigenbaum at the Los Alamos National Laboratory in New
Mexico, the same year that the study of chaos was officially
inaugurated. It's a number that keeps cropping up, just as pi does in
geometry. Pi is an unending decimal that starts 3.14, but can also be
expressed as 22/7. Yes, seven again! Anyway, the Feigenbaum number helps
to define the change in circumstances ---the speed of a stream for
example---needed to provoke transitions from one rhythm to the next.
Reoccurring numbers are evidence of latent orderliness that
distinguishes certain kinds of erratic behaviour from mere chance.
Intriguingly, it seems that 'Chaos is not random: it is apparently

random behaviour resulting from precise rules', explained Ian Stewart of
Warwick. 'Chaos is a cryptic form of order.' Or, as Benoit Mandelbrot,
the Polish-born mathematician, turned celebrity by his appearance in
James Gleick's book, put it more elegantly (and he himself wouldn't have
been seen dead in a casquette, but rather sported a brown Trilby):
within 'the most disorderly realms of data' lives an unexpected kind of
order. This insight may be the key thread running through the
twenty-first century. We must hope so, anyway, because, as Klaus Mainzer
concludes in his book Thinking in Complexity: Computational Dynamics of
Matter, Mind, and Mankind, many, if not most, of our social, ecological,
economical and political problems today are essentially of a complex and
nonlinear nature. With that in mind, let's switch the focus back to the
political aspect for a moment. For thousands of years, philosophers and
scientists had stressed the value of order and prediction. Plato's
description of the ideal society---the Republic---has rightly been
described as a blueprint for totalitarianism. Philosophers always urge
us to analyse the world, to break it down into parts, with the
assumption that once you do that you can then reassemble all your tiny
insights into a theory that will explain and predict everything. Yet,
socioeconomic systems are like ecosystems. In fact, they are ecosystems.
Multiple factors are at work, often with 'feedback' effects. And this
makes their behaviour unpredictable, even chaotic. But in social life,
perhaps more obviously than almost any other area, the trouble with
chaos, as Thomas Hobbes, the seventeenthcentury English philosopher,
warned centuries ago, is it is so often scary and dangerous. For that
reason, our political systems are all geared to three things: control,
organisation and stability. How could it be otherwise? As Hobbes
famously said, without stability, human life is nasty, brutish and
short. Chaos, for Hobbes, was a time of civil conflict coupled with the
twin disasters of the Great Plague and the Great Fire of London, so it
is no wonder that he argued in favour of order in the form of one,
strict rule (even that of a dictator) and saw political debate as a form

of anarchy. However, he saw his rigid system as a triumph of logic, not
merely pragmatic politics. Hobbes was a keen mathematician who even
considered himself to have solved one of the subject's great puzzles:
that of how to 'square a circle'. This is not, as you might imagine, a
magical feat, but rather a geometrical challenge. Whatever, Hobbes'
method was faulty, and although his political theory is also, grandly,
presented in the manner of a mathematical proof, it too has crucial
omissions. But perhaps he could be forgiven for not realising that, in
society as elsewhere, chaos can actually be both productive and
beneficial. One of Thomas Hobbes' contemporaries, the French philosopher
and mathematician René Descartes, also shared this new way of thinking.
He thought both animals and human beings could be dissected and 'the way
they worked' divined from examining the bloody morsels, just as he
thought the universe too could be explained by a handful of fundamental
laws. Descartes' method of breaking down problems into smaller
components, and organising them in a logical framework, became the
essential characteristic of modern thought. We've all been brought up to
celebrate all this. This was a time when new ideas presented and
explored by the likes of Francis Bacon, Galileo Galilei, Johannes Kepler
and Isaac Newton amounted to a scientific revolution---even though,
prior to it, even the term, let alone the activity of, 'science' hadn't
existed. It was a time too when, not only the certainties but also the
mysteries of the Church and Christian ethics were being jettisoned too,
replaced by a new logic, that of an inanimate, mechanistic universe
governed by natural forces and mathematical rules. Yet such thinking has
its limits---because even Descartes could not escape complexity.
Although he reduced most of nature to the interactions of inanimate
atoms, he searched in vain for a physical base for the home of the soul
in the human body, before famously, and rather cavalierly, deciding that
it must be the (then poorly understood) pineal gland in the centre of
the brain. (Partly, this was because, in his day, no one had a clue what
purpose the gland served.) Consciousness remains, to today, a mystery.

Likewise, although the genius of Isaac Newton enabled him to develop a
comprehensive system of mathematics to model the universe, he, too, came
to accept that the movement of the heavenly bodies was not quite as
orderly as his own physics pretends, because (as mentioned earlier)
every rock in space has an effect on every other. Even today, when
astronomers predict the movements of, for example, comets, the
predictions come with error bars and significant unknowns, as even a
small tug on the orbit of a comet by a passing asteroid can lead to a
whole series of cascading gravitational effects. Textbooks speak of
Newton 'taming' the movements of the heavens, but he himself realised he
had done no such thing. Indeed, in his mature research, he became much
more interested in the mysteries and missing pieces of nature, devoting
the great bulk of his research to really rather esoteric matters, such
as how to turn base metal into gold and the architectural significance
of various ancient temples. It is not for nothing that Newton is also
called 'the last of the magicians'! However, history, being prone to
tidy conclusions, prefers to record about Newton that his new
mathematics was applied with tremendous success to a wide variety of
scientific and technological endeavours throughout the eighteenth and
nineteenth centuries. It is this Newton who in many ways laid the
foundations for the Industrial Revolution. And in everyday life, as
Fritjof Capra and Pier Luigi Luisi put it in an essay entitled 'The
Newtonian WorldMachine', this simplified version of reality still
dominates. Even if physicists today see the world through the frame of
Einstein's relativity and quantum physics, the rest of us see it very
much as Newton wanted us too---as a machine. (Actually, in the heady
early days, researchers described chaos as the century's third
revolution in the physical sciences, after relativity and quantum
mechanics. But these are palace revolutions.) But it is well past time
to break free of that legacy. The social significance of how we imagine
the universe works can be seen by recalling that, at the same time as he
ordered the movements of the planets, Newton was a fitting but unsavoury

exponent of a new kind of management and organisational thinking, in
which work everywhere was broken down into particular roles and
functions in a drive to create precise, regular and predictable
activity. In this new world, organisations are created and owned by
powerful individuals or bodies, their structures and goals are set out
by management, and their policies imposed in a top-down fashion through
a bureaucracy. In his later life, Newton actually abandoned science to
become a zealous government servant, appointed 'Master of the Royal
Mint' and an indefatigable enemy of all who would tamper with the
nation's currency. He metamorphosed into a detective-cum-executioner,
who tracked down counterfeiters and punished them with the otherwise
abandoned legal penalty of death by hanging, drawing and
quartering---which involves cutting offenders into four pieces! This
darker influence of Newton's ideas was pinned down by the
nineteenth-century sociologist Max Weber---when he wrote of the links
between mechanisation and bureaucratisation of production, and the
resulting routinisation of human life along with the erosion of meaning
and purpose. Weber spoke of an 'iron cage' of purely technical
efficiency in which human values are lost. It has even been argued that
the logic of this thinking led inexorably to the Nazi death camps. If
so, maybe that is why complexity theory, offering a way out of the cage,
emerged as part of the aftermath of the Second World War. It was during
the 1940s that the Josiah Macy, Jr. Foundation sponsored a series of
scientific meetings organised by a small group of scientists and
mathematicians who were exploring common principles of widely varying
complex systems. A prime mover of this group was the mathematician
Norbert Wiener (another Trilby wearer), whose work on the control of
anti-aircraft guns during the war had convinced him that the science
underlying both biology and engineering should focus not on Newtonian
concepts of mass, energy and force, but rather on feedback, control,
information, communication and purpose (or 'teleology').

The meetings led to a new discipline of 'cybernetics'. The term comes
from the Greek word for 'helmsman'---that is, one who controls a ship.
Wiener summed up cybernetics as 'the entire field of control and
communication theory, whether in the machine or in the animal.' The
questions the new discipline investigated were things like: 'How is
information manifested in living organisms?' 'What analogies can be made
between living systems and machines?' And crucially, 'What is the role
of feedback in complex behaviour?' Supporters heralded it as the
beginning of a new era of science. The anthropologist Gregory Bateson
wrote that, in his assessment, the two most important historical events
'were the Treaty of Versailles and the discovery of Cybernetics'.
Critics, however, argued that the approach was too broad, too vague and
too lacking in rigorous theoretical foundations to be useful. The
biophysicist and Nobel Prize winner, Max Delbrück, characterised the
cybernetics meeting he attended as 'vacuous in the extreme and
positively inane'. Today, however, in the third millennium, without
doubt, the culture of science is changing. The ultimate tool in most
fields, the all-powerful equation, is now giving way to the all-powerful
computer, and this change describes a shift from abstract formulas to a
more open-ended willingness to plug in 'the data' and see what happens.
It is a shift from explanations to descriptions. The End of Complexity?
In 1995, the science journalist John Horgan published an article in
Scientific American, attacking the field of complex systems in general
and the Santa Fe Institute in particular. His article was advertised on
the magazine's cover with the headline 'Is Complexity a Sham?'

Horgan had two main criticisms. First, he argued that it was unlikely
that the field of complex systems would uncover any useful general
principles, and second, he believed that the reliance on computer
modelling made complexity a 'fact-free science'. He called the field of
complexity studies 'pop science' and its researchers 'complexologists'.
Noting that some scientists have given up on the chaos and complexity
hype, Horgan argued that complexity should join catastrophe theory,
cybernetics and information theory on the ash-heap of ideas that,
whatever their utility in their own spheres, were once touted as
all-powerful explanations. Horgan closed by saying that the term
'complexity' had little meaning but that we keep it for its
'public-relations value'. Horgan's article grew into a book called The
End of Science, in which he proposed that, anyway, all of the really
important discoveries of science have already been made, and that
humanity would make only minor adjustments to the grand edifice from now
on.

In his 1995 book A Tour of the Calculus, the mathematician David
Berlinski describes this sea change as a shift in importance from math
to biology: Living systems may best be understood in terms of their
constituents. Going down, one encounters organ systems, organs, tissues,
cells, cell parts and then, on a much smaller scale of organization,
molecular constituents of which the most important are the proteins and
the master molecule, DNA. But there, in contrast to physics, things come
to an end. In place of depth, the biologist requires intellectual
extent. He or she wishes to trace connections among biological
constituents, following pathways across a living system and coming to
understand how influences are transmitted.

Nonetheless, the criticism had an impact. In fact, although it was a
wonderful term, cybernetics yielded way eventually to a much more
businesslike sounding discipline concerned with finding common

'principles', called General System Theory. This approach had been
launched in the 1950s by a biologist called Ludwig von Bertalanffy, who
characterised the task as 'the formulation and deduction of those
principles which are valid for "systems" in general.' 'System' was
defined in a very general sense as 'a collection of interacting elements
that together produce, by virtue of their interactions, some form of
system-wide behaviour.' Think 'taxis in New York'. It all sounds
promising, indeed rather clever, but step back a moment and it looks
like a problem with the definition that can describe just about
anything. Think 'taxis in New York' again. Bertalanffy's focus was
primarily on living and social systems, which it seemed to make sense to
talk of in terms of 'selfconstruction', a term that he elaborated on as
indicating a selfmaintaining process by which a complex entity
continually produces the components which they themselves are made up
of. This process was seen as a key, and likely the key, feature of life.
However, all attempts to pin down the precise rules for such processes
proved elusive, despite in the process spawning new areas of science and
engineering. Indeed, artificial intelligence, artificial life, systems
ecology, systems biology, neural networks, systems analysis, control
theory and the sciences of complexity itself all emerged from the
debates of these pioneer cyberneticists, before, in time, entirely
overshadowing it. These were new terms, for new ideas and new ways of
seeing the world. Eventually, and in appropriately unlikely fashion, the
emergent leader of the new movement was not a grand political theorist
or highly respected physicist---but rather a very humble meteorologist
clutching a paper on weather forecasting! Edward Lorenz had discovered
that, at least in weather matters, even tiny changes in initial starting
conditions not only produced large variations, but produced them in ways
that defied logical or mathematical prediction. As we're talking
'weather', take snowflakes for example. Sure, snow, as the philosophers
like tell us so firmly (ignoring the views of the Inuit) 'is white', and
you're not allowed to disagree

with them because that's logic. Yet complexity theory opens up a much
wider and richer perspective. Because here, every snowflake is unique.
The variation and unpredictability arises because, as a growing
snowflake falls to earth, typically floating in the wind for an hour or
more, the choices made by the branching tips at any instant depend
sensitively on such things as the temperature, the humidity, and the
presence of impurities in the atmosphere. The six tips of a single
snowflake, spreading within a millimetre space, feel the same
temperatures, and because the laws of growth are purely deterministic,
they maintain a near-perfect symmetry. Finally, the nature of turbulent
air is such that any pair of snowflakes will experience very different
paths. Every snowflake records the history of all the changing weather
conditions it has experienced, and the combinations either are, or may
as well be, infinite. The end result is both that its exact shape is
beyond prediction, and that certain crucial regularities are not. All
this makes snowflakes, despite their exquisite symmetry, another of the
symbols of chaos. Every snowflake model is a delicate balance between
forces of stability and forces of instability; a powerful interplay of
forces on atomic scales and forces on everyday scales. Poincaré's
unpredictable effect Nothing under the sun is entirely new and nor was
Lorenz's idea. The French mathematician Jules Henri Poincaré had
remarked in Science and Method, a good century before, how 'A very small
cause which escapes our notice determines a considerable effect that we
cannot fail to see, and then we say that the effect is due to chance.'
Poincaré even goes on:

If we knew exactly the laws of nature and the situation of the universe
at the initial moment, we could predict exactly the situation of that
same universe at a succeeding moment. But even if it were the case that
the natural laws had no longer any secret for us, we could still know
the situation approximately. If that enabled us to predict the
succeeding situation with the same approximation, that is all we
require, and we should say that the phenomenon had been predicted, that
it is governed by the laws. But it is not always so; it may happen that
small differences in the initial conditions produce very great ones in
the final phenomena. A small error in the former will produce an
enormous error in the latter. Prediction becomes impossible...

However, this was all completely 'off message' at the turn of the
century and so was duly ignored; with the only mathematician to
seriously follow Poincaré's lead in the twenties and thirties being
George D. Birkhoff in the United States. And yet, as we have been
seeing, in life, small events can lead to larger effects: in this case
because, as it happened, Birkhoff taught a young, casquette-wearing,
Edward Lorenz at MIT.

As an atmospheric scientist of mathematical bent at the Massachusetts
Institute of Technology, Lorenz used a very simple simulation of the
atmosphere by numbers, and computed changes at a network of points. And
he had been startled to find that successive runs from the same starting
point gave quite different weather predictions. Lorenz traced the
reason. The starting points were not exactly the same. To launch a new
calculation, he was using rounded numbers from a previous calculation.
For example, 654321 became 654000. He had assumed, wrongly, that such
slight differences were inconsequential. After all, they corresponded to
mere millimetres per second in the speed of the wind. This was the
butterfly effect. Lorenz's computer was telling him that the flap of a
butterfly's wings in Brazil might stir up a tornado in Texas. Actually,
Lorenz originally used the example of a seagull, but a later paper
introduced the even more diminutive insect and it is this analogy that
caught the popular imagination.

Butterfly or seagull? Trilby or casquette? How could such details
matter? But details do, and maybe that's both the simple and most
important lesson to take away from chaos theory.

Chapter 8

Thinking like a Social Scientist (and not a Gambler)

One of the least remarked effects of the corona virus is how it has made
previously rather esoteric issues in social science into matters of
angry, public debate. (Representation by HFCM Communicatie via Creative
Commons.)

According, at least, to social scientists, humans indulge in two kinds
of thinking: 'fast' and 'slow'. Contrary to what you might assume from
school, 'fast' is actually worse than 'slow' because it involves
shortcuts and comes with lots of thinking errors. By contrast, 'slow
thinking' is cool and dispassionate (the kind philosophers value),
employing logic and scientific method to arrive at certainty and eternal
truths. Anyway, that's the theory.

However, in this chapter I'm going to take a closer look at the
difference between good and bad thinking skills and explore some of the
ways in which even slow thinking is a lot less reliable than we imagine.
Nowhere is this better illustrated than with the recent debates
surrounding the corona virus. Following its arrival on the scene in
2020, every one of us has been forced to evaluate risks, or at least to
come to terms with evaluations made by others, be it in the form of
restricted social contact, curtailed daily routines or precautionary
medical treatments. Responses to a virus are a particularly good example
of cognitive bias, as so many deep emotions are tied up with them,
whether it is the imperative to 'save lives' on the one side, or the
equally powerful one to defend liberties and make life worth living on
the other. It's also a very political matter, which only means that it
is a philosophical matter with real-world implications rather than mere
ivory-tower speculation. The virus story illustrates all too well the
problems identified by the so-called cognitive bias codex, a list of
cognitive biases, distilled by Wikipedians from numerous academic
sources, arranged and designed by John Manoogian and Buster Benson. The
codex neatly highlights these thinking errors in particular: • We reduce
events to their key elements, in the process discarding specifics. • We
favour simple options over complex, ambiguous ones. • We simplify
probabilities and numbers to make them easier to think about. • We fill
in characteristics from stereotypes and generalities. • We find stories
and patterns even when looking at sparse data. (The issue alluded to at
the start of chapter nine, 'Thinking like a Search Engine'.)

• We are drawn to details that confirm our existing beliefs
(prejudices). • We exaggerate the importance of atypical things. I'll
take a closer look at all these issues in a minute. But first, abstract
ideas throw light on non-political issues too, and these have the added
advantage of not raising ideological hackles, so let's start with a
humble but revealing phenomenon, regularly pored over in philosophical
circles, which is that after viewing a long run of red on the roulette
wheel, gamblers will often place large bets on the next turn giving
black. Eight reds in a row? The next ball must be black! This is a
misconception of chance as, in simple terms, the roulette wheel has no
memory or interest in the past. And yet, from a certain perspective, an
unusual and distinctive run of events, like a perfect hand in bridge,
has a much lower probability even though, of course, the significance is
something that only exists to humans. (A real-life example of all this
occurred in a game of roulette at the Monte Carlo Casino on 18 August
1913, with the ball falling on black... twenty-six times in a row. The
probability of such a sequence is one in sixty-six million. Needless, to
say, during the run, the players lost a lot of money betting on the next
colour being red.) A recent (2010) study of decision making provides an
intriguing corollary to this much discussed fallacy. In an experiment
built around predicting the outcome of a simple coin toss, researchers
gave participants a choice of methods, including the use of an 'expert'
opinion and a risk-free alternative method offering a much smaller
financial reward. Told that, for whatever reason, the 'experts' had got
the last dozen calls right, the number of participants who turned to the
'expert' opinion to make their decision increased from 24% of the time
to 78%---based on their apparent record of successful predictions. This
is the assumption known in gambling as the 'hot-hand', and it flies
against the apparent logic of the situation which indicates that the
experts could not really have any insight into the outcomes of a coin
toss.

For the researchers, the experiment indicated that people actually are
prepared to suspend rational judgment in favour of blind trust in human
capabilities. Faith in experts, whose methods we know we don't
understand, recalls how, in explaining their corona virus policy,
politicians invariably present their conclusions as simply following
from 'expert advice', along with an emphasis on the experts' 'track
record' on predicting things like virus spread and hospitalisations. But
although a policy of 'following experts' makes good political and
mediatic sense, it can never hide the essential reality that in science
there are always different views (that is why it is science and not
religion) and with viruses, in particular, there is a lot of randomness
and unpredictability due to the complex and interacting nature of the
factors driving their spread. For all the emphasis on following and not
judging, scientists actually had different views and it was the
selection of the experts that pretty much determined the subsequent
recommendations. Whether buying a newspaper or writing a PhD, you can
easily end up choosing sources that reinforce your existing
view---unless you make a conscious effort to also look at contrary
positions, your assessment is incomplete. 'Framing' errors---slanting
the question---is a thinking error that we all constantly need to guard
against. Do cows cause global warming? Is full-fat milk giving you heart
disease? Was the last election rigged to ensure a certain party won?
Even if someone weights up the evidence on a matter carefully, deciding
just what the issue is and what will count as evidence, whether for or
against, is often uncritically assumed. However, belief in the
omniscience and impeccable neutrality of experts is favoured by the
experts themselves. Let me tell a little personal story to illustrate
this. A few years ago, I was invited, in my capacity as writer on
thinking errors, to write an op-ed for the influential journal Nature
about 'how science works'. After some head-scratching, I wrote a piece
suggesting that good policy requires the active searching out of
contrary, minority opinions, so that any prevailing orthodoxy can be
properly examined and tested.

What is sauce for the goose is sauce for the gander, as they say, and
the editors promptly sought out alternative views to my oped. However,
not 'minority' ones, rather those from exactly the people being
criticised in my piece. These 'experts' reported that in most cases, and
certainly in all important matters, the scientific consensus was, by
definition, right and alternative views were mere confusion. And so, my
ever-so-slightly sceptical view, like so many 'off-message' scientific
things, was never published, and indeed, the scholarly journal continued
to promote a simplistic view of science as 'black and white'. Indeed,
life would be much simpler if it were! The moral is, when journals
publish, you have to wonder what alternative perspectives and research
have been suppressed. This reverence for the supposed scientific
consensus is particularly dangerous, as tolerating ambivalence and 'not
knowing' plays a key role in maintaining openness to new information. In
contrast, anxiety during a perceived crisis leads to over commitment to
preferred narratives, and a failure to recognise their provisional
nature. It blocks out certain facts. Thinking strategies that rely
heavily on notions of a consensus like this are particularly
'unscientific' because the history of science shows that mainstream
opinion is an amorphous blob constantly shape-shifting. Not least
because, as Rupert Sheldrake puts it in his book called The Science
Delusion (2012), a refreshingly contrarian book on the subject,
scientific texts are radically 'out-of-date' in just a few short years
to the extent that the belief that science already understands the
nature of reality (with the fundamental questions all answered, leaving
only the details to be filled in) is a restrictive straitjacket. Other
science writers have noted that we habitually dismiss any textbook more
than thirty years old as being mostly wrong, and indeed, on many points,
they certainly are. Yet as we accept that old books are full of opinion
and error, we treat new ones very reverentially. It is all part of the
very human conceit of imagining that 'the theory' is almost perfect, and
most of what we think today is what we will still believe tomorrow.

Science as Business 'It is simply no longer possible to believe much of
the clinical research that is published, or to rely on the judgment of
trusted physicians or authoritative medical guidelines. I take no
pleasure in this conclusion, which I reached slowly and reluctantly over
my two decades as an editor of The New England Journal of Medicine',
said Marcia Angell in conversation with Christine Mitchell, of the
Center for Bioethics at Harvard Medical school. Angell says drugs
companies are not in the 'science' business. 'Their mission is to sell
profitable drugs, not necessarily good drugs, valuable drugs, profitable
drugs... and they do that increasingly by co-opting the faculty at
academic medical centres by saying we're in the same business. But we're
not in the same business... I can't think of a single large drug company
that has not been found guilty of fraud. Most of them have actually
pleaded guilty and paid hundreds of millions, billions of dollars which
to them is just the cost of doing business and then they do the same
thing over again... So they know how to behave, they just don't find it
profitable.' And if medical science, upon which lives depend, is so
easily distorted by the profit motive---how confident can we be that any
scientific research is really a dispassionate search for the truth?

Of course, when you ask them, scientists will happily agree about the
need to keep testing ideas and revising theories, but that's in the
abstract. As Thomas Kuhn, the iconic philosopher of science, says,
scientists form groups, and behave more like a priesthood than priests
themselves, stifling competing views. In practice, 'science' today is
not really science but a religion, with its teachings accepted as holy
writ, even ones that go against our lived experience. Consider, for
example, that they say that the world is a machine, ultimately made up
of inanimate parts; consciousness is nothing but the physical activity
of the brain and free will is an illusion. These are things that 'we
think we know'---but do we?

The thinking error of thinking that we know when in fact we have
incomplete information is one that we all make when evaluating practical
actions in terms of their expected outcomes and contribution to future
happiness, whether it is buying a new car, dreaming of purchasing 'a
place in the sun'---or deciding to wear masks to ward off a scary virus.
And 'thinking we know', when in fact we don't, is, of course, closely
linked to that mysterious human faculty known as 'intuition'. This is
one of the most fascinating elements of 'heuristics and biases'
discussed by the social scientists Amos Tversky and Daniel Kahneman in
their celebrated books and articles about how we think. 'Heuristics' is
an imposing sounding word but it essentially designates practical
approaches to problem solving. Kahneman identifies such 'heuristics' as
really being not one thing but a complex cluster of thinking processes.
He starts his discussion by noting that thoughts arrive in our minds via
two main routes: 'orderly computation,' which involves a series of
stages including remembering rules and then applying them, and secondly,
via perception. Kahneman then goes on to explain that there is no sharp
line between the two kinds of thinking although, intriguingly, it is
only the first kind that uses language. 'Intuitive thinking has a lot of
word knowledge organized in different ways more than mere perception',
he adds, slightly ungrammatically. This is a point we shall often come
back to. You see, words--- language---are how we organise those raw
perceptions, and how we make sense of reality. The corona virus story
starts with the change in the meaning of a word. For many years, a World
Health Organization website offered a de facto definition of what
counted as a pandemic by including a reference to 'enormous numbers
deaths and illnesses' but, in 2009, apparently to facilitate emergency
measures against the swine flu, that phrase was deleted and a 'pandemic'
became instead simply a new virus against which the human population had
no immunity. Talk about setting a low bar! But the change certainly
skewed popular discourse, with the corona virus ten

years later freely compared to earlier 'pandemics' such as that of
Spanish flu in 1919---thought to have directly caused fifty million
deaths, across all age groups---and the Black Death in the Middle
Ages---which historians believe may have killed up to two hundred
million people. This links to something Kahneman calls 'associative
coherence'--- the notion that 'everything reinforces everything else'.
Surely this is the thinking error of thinking errors! It leads us to see
what we expect to see and discount evidence that contradicts
expectations. Ambiguity is suppressed with the result that we see the
world as much more coherent than it is. In this way, our brain, as
Kahneman puts it, is a 'machine for jumping to conclusions'. At which
point, let's step back to reconsider the revealing case of the corona
virus, an illness that has caused globally so much suffering and death.
The episode created great divisions, and likely the reader will have
firm views about it, either on the 'too little was done' side, or the
converse 'too much was done' side. But just for this reason, the virus
is a good one to use as a thread to link the ideas about how we really
think---about how we arrive at our opinions, how we handle complex
issues and conflicting desires and, above all, how often we act not on
'data' but on irrational impulses. Thinking Errors According to the
psychologist Daniel Kahneman, when we get something wrong, it is usually
due to one of five 'heuristic biases' or 'thinking errors'. These are:
Anchoring bias---the tendency to focus on one particular piece of
information when making decisions or problem solving.

Confirmation bias---the thinking error that involves focusing on
information that confirms existing beliefs. People seek out information
and data that confirms their pre-existing ideas while ignoring contrary
information, however potentially significant for the decision. The
almost non-existent political and media examination of the range of
views and strategies for the corona virus shows that this is one of the
most dangerous biases of them all. Hindsight bias---the error of
convincing yourself that the event just experienced was predictable.
Representative bias---which is the stereotyping of someone or something,
all the worse for being unconscious. And lastly, Availability
bias---which is where decisions are based either on available precedent
or an example that may be faulty.

The corona virus is also a good place to start asking how much our
responses are governed by reason informed by evidence---and how much
they are, well, irrational. Consider all those newspaper reports of
rising (or falling) corona virus case numbers and deaths. Because,
frankly, the statistics were really pretty meaningless. Take the
long-running tale, soon a media favourite, of the latest 'case numbers'.
With the corona virus, newspapers and experts alike would regularly
update their readers on sudden surges in the number of cases, without
linking that to the numbers of people tested. But if you test ten times
as many people one day, then the number of cases is bound to go up. And
then the tests themselves were rather suspect. There was no clear
science to what counted as a positive result, with the sensitivity of
testing as much a political judgment as a scientific one.

On 29 August 2020, the New York Times published a lengthy article
headlined, 'Your Corona Virus Test is Positive. Maybe it Shouldn't Be'.
Its main message was that: The standard \[Covid PCR\] tests are
diagnosing huge numbers of people who may be carrying relatively
insignificant amounts of the virus... Most of these people are not
likely to be contagious...

Indeed, when it reviewed the evidence, The Times found that: In three
sets of testing data... compiled by officials in Massachusetts, New York
and Nevada, up to 90 percent of people testing positive carried barely
any virus...

All of which highlights the very modern thinking error sometimes dubbed
GIGO, for 'Garbage In, Garbage Out'. This involves treating the
pronouncements of computers (and computer models) with far more weight
than anything produced by a human being---even though, in reality, what
the computer says is determined by the information fed into it by
humans. I say it is a very modern error but, in fact, as early as 1964,
in an era when computers didn't have any of the aura that they do today,
the researcher Joseph Weizenbaum warned that even 'extremely short
exposure to a relatively simple computer program could produce powerful
delusional thinking in normal people'. Over-respect for computer
generated data is a huge issue. Objectively, all the shocking figures
printed in newspapers everywhere for the 'number of cases' were
meaningless. Not just because the number of cases depended on the number
of tests, but because people who tested positive often had no symptoms,
and were not infectious either. The reality was that the way cases were
measured varied depending on political decisions and, of course, on how
much testing was being done at any one time. Add to which, the 'how' of
the testing was vague, with official bodies regularly changing crucial
parameters and radically affecting published results. Even

official guidance on recording deaths was also highly politicised, with
hospitals instructed to 'assume' patients had died of the virus without
needing to test for it (which would be using tests that experts admitted
didn't actually reliably test for the virus), far less narrow down the
factors in the death to just the key one. Thus, a much argued about
statistic from Italy, the country which to a large extent set the
pattern for the virus response in Europe, showed that 96% of people
dying from the virus had serious co-morbidities. Meaning that the extent
to which they really died from the virus was unknowable and subjective.
Of course some people, perhaps in mid-life and in good health, were
among the victims. These were the kinds of cases people instinctively
imagined as being typical. But the average age of people dying was
around 72 years. Twisting evidence to fit preconceptions is, however, a
very common and hard to avoid thinking error. And the preconception that
public policy for the corona virus laboured under was that, as H.G.
Wells memorably put it back in 1946: The end of everything we call life
is at hand and cannot be evaded.

Put baldly here, it seems ridiculous, yet I'm not exaggerating because
some of the reports said pretty much exactly that. For many people, at
its peak, in 2020 and 2021, the corona virus seemed to threaten the
whole of society. In no small part because the measures used to control
the virus destroyed people's livelihoods, trampled basic freedoms and
threatened to bring about wholesale societal collapse. Yet few
governments seemed to be able to draw back from the abyss. After all,
'the science' had to be 'followed'. And so, countries entered
'lockdowns' in which whole populations were instructed to remain in
their homes, only allowed to leave for 'essential' trips, such as a
once-a-week visit to a supermarket to buy food. 'Inessential' activity
was forbidden. Hotels, cinemas, bars and restaurants were shuttered for
months on end ---by law. Schools and colleges, too, were deemed
inessential and closed. Tests and even treatment for society's perennial
killers,

things like cancer and heart disease, were cancelled too! Hospitals
stopped all but emergency operations, and cleared wards ready for the
anticipated wave of victims of the new virus. The authorities even
ordered that hospitals be emptied of elderly patients, including some
who actually had the corona virus, to free up space for expected waves
of corona virus victims. Placed amongst the vulnerable population of the
care homes, these individuals then indeed triggered localised epidemics
of the disease, as elderly people in care homes cannot even cope with
that other corona virus, the common cold, let alone new variations.
'Common sense' gave way to radical health strategies guided by
statistical predictions---by computer models. Speaking of computer
models, it was these things, based on assumptions and not conventional
science, based on measurement, that also dictated that the corona virus
spread on surfaces. It was computer models that mandated that people
should regularly 'disinfect' their shopping, and supermarkets their
shopping trolleys; that lorries would spray streets with disinfectant,
and workers in rubber gloves and face masks would stand by escalators
wiping down the handrails. Yet there never was any evidence that
Covid-19 spread by 'formites' (surfaces), as the technical jargon put
it, and, by April 2021, the World Health Organization formally revised
its advice to say that surfaces did not need to be disinfected after
all. Yet many people who had followed the advice earlier on, stressing
their actions were based on respect for 'the science', continued in
their efforts to avoid touching or to disinfect surfaces. Because what
was really guiding their behaviour was not rationally evaluated factual
data at all, but rather deeply held and largely irrational intuitions.
Mattias Demet, a professor of social psychology at Ghent in Belgium, has
spoken of the Covid epidemic as an episode of 'mass psychosis' in which
the ability to distinguish between what is 'real' and what is not is
lost. In such episodes, he says, a population in which there is a
pre-existing sense of meaninglessness latch on to the crisis, the new
enemy, and find within it a source of meaning.

Mask-wearing, repeated doses of vaccines become rituals, the more
meaningless, even the more dehumanising, the better! And at the same
time, the psychosis requires that everyone perform the same actions.
Even infants must be masked and vaccinated. As in hypnosis, pain is no
longer even felt---not for others, not for themselves. Let me stress
again that the focus here is not on this or that issue or debate, but on
how we humans evaluate evidence and arrive at our views (and decisions).
Take the question of vaccines, for example. Here, the scientific
consensus was quite clear that any one of the range of new vaccines
offered (let us say) significant additional protection to the corona
virus. On the other hand, neutral bodies like the European Medical
Agency documented in detail the many tens of thousands of severe adverse
reactions, ranging from headaches and blood clots to death. Reports of
healthy people dying shortly after being vaccinated were appalling and
tragic. A minority of people genuinely thought they were at far greater
risk from the vaccine than they were from the illness. Indeed, under a
certain age, it seemed that this might even be true. Yet, just as with
the general issue of Covid policy, the rational thing was to look at the
relative risk of dying from the vaccine and compare it to the risk from
the virus. The figures seemed unambiguously to say that the risk of the
former was but a tiny fraction of the latter. Over a certain age,
anyway, even the risk of, say, a blood clot was far greater if you were
unvaccinated when you contracted the virus than if you were vaccinated.
The rational policy seemed to be to offer vaccination to those at
increased risk from the virus (that is the elderly and those with
weakened natural defences) but not to roll out universal vaccination
which increased the 'costs' of the policy in health as well as financial
terms without significantly increasing the benefits. That indeed was the
strategy many public health specialists advocated, and yet all over the
world governments adopted one of 'universal vaccination', which ignored
two key facts about Covid. First, that vaccinated people were still
infectious to others and, second, that animals too could carry the

virus. Thus policy was invariably a mixture of the rational and
irrational. And what about the science for mandatory face masks? The
ones that for decades experts had said could not protect you from
inhaling tiny viruses. Yet now, faced with the new corona virus, the
'consensus' on that shifted very fast. You might imagine that it would
take some very solid new evidence to overturn decades of scientific
consensus, yet in the case of masks and viruses, the evidence required
was very thin indeed. One much-quoted study was actually of two
hairdressers who tested positive for the virus but had worn masks and
not infected their clients. If that doesn't sound very scientific well,
it wasn't. Another study, known as the 'Danish study', into the
effectiveness of surgical or cloth masks in limiting the spread of the
virus was also much referenced by governments to justify mask mandates.
Curiously though, the study actually stated that there was no evidence
that masks for the general public protected the wearer from tiny
respiratory viruses, although it was not proven that they did not work.
As to whether they helped prevent wearers spreading the virus, the
authors stated plainly that they simply had not investigated this. And
yet, commenting on the study in an opinion piece headed 'The Curious
Case of the Danish Mask Study' (26 November 2020), another influential
source of expert knowledge, the British Medical Journal, counted the
study as supporting maskwearing, on the grounds that the trial was
'inconclusive rather than negative'. Is that following, or twisting, the
science? In the US, Joe Biden became president, in no small part due to
public approval of his policy of mask-wearing. That is, masks that for a
good fifty years, right up to a few months before, have been accepted to
be ineffective and possibly even harmful in the context of a respiratory
virus epidemic. 'Fast thinking' adopted maskwearing, slow thinking
warned it did not work, but fast thinking is what we use when we are
scared. Masks reassured some people, but, alas, they also come with
wider costs---social, financial and in terms of health. Corona virus

policy generally meant that millions of people lost their incomes, and
where governments offered support, often it went not to the most
vulnerable groups of casual and part-time workers but to the most
comfortable groups---people with permanent jobs and salaries. Citizens
entered a lottery for health treatment with operations cancelled and
tests for cancer and other illnesses postponed or left undone. For some,
they would not receive treatment until 'too late'. Yet without doubt,
though, the corona virus response was broadly accepted. No one really
wanted to listen to those like Sunetra Gupta, a professor of theoretical
epidemiology at Oxford university in the UK, who pointed out that
lockdowns protect the comfortable and privileged while leaving the
vulnerable to carry the burden. Nor that, as for the effects on
children, 'we should hang our heads in shame', as she wrote.
'Slow-thinking' Professor Gupta warned that Covid policy was directed at
preventing deaths--- which meant it was massively skewed towards those
who were already very old. The average age of someone dying of Covid was
82 for men and 84 for women. Which was close to the average age men and
women died at anyway. Thus the strategy seemed to be directed at saving
maybe a year, maybe a few months, for people at the very end of their
lives. However, rather than listen to sceptical experts like these, the
fast-thinking British Prime Minster gave public addresses with not one
but three messages strapped to his podium: 'Stay home'; 'Protect the
NHS' (the UK's much loved national health care system); and 'Save
lives'. The catchy slogan sounds good---but public policy is not well
served by thinking shortcuts. And fast thinking meant that few compared
the epidemic 'curves' of places that had radically opposed
strategies---like North Carolina and South Carolina in the US, or Sweden
and the UK in Europe, nor reflected on the evidence they provided that
radical measures appeared to make no significant difference to the
disease spread and impact. 'Fast thinking' meant that the public were
not attracted to strategies like the one flagged up by Yale University's
Dr David

Katz, of 'Total Harm Reduction'. Here, instead of policies which focused
on 'virus deaths', was a plan designed to protect people both from
Covid-19 and a whole range of ills, including those which result from
social isolation and economic shutdown. All of which, as I say,
underlines the importance of how questions are framed. The mathematical
modellers of epidemics concentrated on just two computable figures: the
number of 'cases' (people who returned a positive test for the virus
rather than the more difficult to obtain figure for those actually ill
from the virus) and the number of hospitalisations. But it is standard
practice not only in medical ethics courses (and more importantly in
hospital management) to consider resource decisions in more holistic
terms than those of merely counting 'lives saved'---because in the real
world resources are limited and lives extended by one policy may be
lives shortened elsewhere. Here, people talk about maximising 'quality
life years'. However, corona virus policy took 'quality life years' of
millions of people, and redistributed them to (mainly) the very old.
Take children, for example. Their schooling, and ability to play with
friends---indeed, just to develop normally---was disrupted. The virus
years certainly had reduced 'quality' for them---but worse, their
futures were blighted too. And a similar burden was placed on young
people generally---almost none of whom were at risk from the virus. Seen
this way, was public policy irrational, excessively focused on
prolonging literal lifespans of the elderly and medically vulnerable, at
the cost of lowering of the quality of life of millions of people.
Globally, virus strategies cost trillions---thousands of billions---of
dollars. That certainly looks irrational in a world where resources are
limited. In the UK, where everyone's health care is funded collectively
via taxes, in brute money terms, given the astronomical costs of
lockdowns, it was calculated that each life year 'saved' by the current
strategies cost £400 million! By contrast, the National Institute for
Health and Care Excellence, an official body with the rather Orwellian
acronym 'NICE', only allows a year of British life

to be worth £20,000 to £30,000---say \$30,000 to \$40,000. That's an
astronomical difference---how could it be justified? However, whether as
a rational response or as a panicked reaction, faced with the corona
virus, governments everywhere came up with fantastical new schemes, with
eye-watering price tags (and remember, money spent on these schemes was
money not spent elsewhere), from tracking and testing whole populations
not just once, but more or less continuously, to vaccinating everyone.
Let's pause here a moment to look at the history of vaccines and
immunisation. Amazingly, it seems that the principle of
vaccination---that is, introducing weak doses of a disease into the body
to encourage an immune response---was established as far back as 200 BC
in China---with smallpox one of the diseases tackled. There is even some
evidence that traditional medicine, tribesmen and ancient civilisations
also used, or at least inadvertently 'knew' something of the benefits
of, limited exposure to a disease, in order to establish some degree of
immunity. However, the modern era of the 'traditional' vaccine only
begins with Edward Jenner (1749--1823) observing that milkmaids appeared
to be relatively immune to smallpox, the viral illness that was, in
Jenner's day, responsible for much suffering and death. Milkmaids,
Jenner hypothesised, were protected due to their earlier exposure to
'cowpox'---a milder illness carried by the cows. Jenner impregnated
water with cowpox and tested his theory out---wait for it!---on the
eight-year-old son of his gardener. Fortunately, we must say, his
reasoning was sound, and the child not only did not suffer any ill
effects but turned out to be protected from the deadly smallpox which
Jenner next deliberately exposed him to. It was only half a century
later that Louis Pasteur coined the phrase 'vaccination'---adapting the
word vacca, the Latin for cow. For years, the procedure was
controversial, with opponents claiming it was 'unnatural' and even
'contaminating'. Nonetheless, today, vaccination is generally counted as
one of the most widespread and successful of all health
interventions---after the provision of safe drinking water. Part of this
turnaround in public

opinion is that the first immunisation campaigns were directed at
diseases that had a very high mortality rate, and the impact of
immunisation on diseases each year was so great, and so obvious, that
public support soon followed. Smallpox is the great triumph. The last
naturally occurring case of it was diagnosed in October 1977, and,
thanks to vaccination, the World Health Organization certified the
global eradication of the disease in 1980. Smallpox is a big part of the
reason that the term 'anti-vaxxer' is one of the most caustic insults on
social media. And yet, the paradox of vaccination is that as
formerly-feared diseases disappear, the benefits of immunisation become
less, while the risks of side effects remain the same. In cash terms,
too, it has been calculated that the smallpox programme cost a mere
\$100 million and generated annual savings of well over thirteen times
that! Yet, with the corona virus, the risk of a healthy person under
sixty not making a full recovery, far less dying, were tiny, while the
vaccine costs were eye-watering and some degree of risk
unavoidable---particularly because, in the rush to find vaccines,
testing for long-term side effects had to be delayed or even side-lined.
Nor, even on the terms of its advocates, were the vaccinated safe from
infecting other people, or spreading the virus further. Nonetheless,
public policy soon became that everyone, including children, needed to
be vaccinated. What was the reasoning there? Vaccination is essentially
a utilitarian calculation, utilitarianism being the approach in ethics
forever opposed to that of guidance by principles. Yet even on
utilitarian terms, vaccines are sometimes found wanting: concerns about
the balance of risks and benefits led to a change in the polio vaccine
programme. Although cheap and effective, this oral vaccine had the rare
side effect of actually causing, in a tiny proportion of recipients
(perhaps one in a million), an awful disease called paralytic
poliomyelitis. While the risk may well be negligible when there is a
substantial threat of infection, it may become significant once the
disease has largely been eradicated. So, once again, a policy of
vaccination is not as simple as 'vaccination

works, let's do it'. A calculation of benefits and costs is still
needed. This is the domain of facts and slow thinking. However, as we've
been seeing, and as Thomas Kuhn, the sociologist, warned in the last
century, in science, facts are always political. On 'facts', there often
is an astonishing lack of agreement. As I say, with the corona virus,
even several years after the onset of the virus, policy makers still had
no real idea of how dangerous it was, nor how easily it spread, nor even
how many people at any given moment had it. Yet the absence of
information didn't cause people or even governments to move more
cautiously. On the contrary, it encouraged ever more radical steps.
Those nationwide 'states of emergency' were declared; 'stay at home'
orders announced; face coverings (masks) made mandatory; gatherings
forbidden; cinemas, restaurants and bars shuttered; schools and colleges
closed. If epidemiologists wrote in specialist magazines that if you
were under age 75 you really were at negligible risk---no one read them.
Instead, all the time, as a backdrop, the TV and newspapers churned out
terrifying images of people in intensive care, of coffins piling up in
hospitals and graves being dug in fields. Foreign correspondents
reported on the virus's deadly progress from around the world: from
Chile, where the president was declaring a 90-day 'state of
catastrophe', to Malaysia, where the government warned of 'a tsunami' of
cases if people did not follow the new restrictions. An internet-fuelled
appetite for 'click bait' headlines and tantalising images left the
press and politicians ---and health experts too---combining forces to
convince their audiences that the world faced an existential threat.
Today, looking back, we can see that, in most countries, the toll of the
virus, although real enough and tragic enough, was not unprecedented at
all. The 'all cause mortality' figures rose, but only incrementally, and
that often in the context of aging populations. To which governments
could argue, and indeed did argue, that it was their bold actions that
had 'flattened the curve' and saved the

day. Here, recent history offers a cautionary tale that points to a very
different conclusion. For the American public, the corona virus
nightmare started on 7 March 2020, when Dr James Lawler, a University of
Nebraska Medical Center professor, warned that about 96 million
Americans could become infected with corona virus, five million would be
hospitalised and nearly half a million would die. But it was the British
professor Neil Ferguson's supplementary evidence, days later, that
(without drastic action), not a mere half a million but two and half
million Americans would die at the hands of the virus that caused normal
politics to be upended. The computer model of Ferguson and his team in
the biomathematics department at Imperial College in London also
predicted half a million deaths in the UK alone. If the public were not
familiar with the term 'corona viruses', they certainly didn't remember
the name of Neil Ferguson. The British professor was, however, an
influential figure in international public health matters, a
mathematical modeller (not a medical expert) hailing from Imperial
College London---itself a grand university with a consolidated income of
£1,033 million in 2017/18, and including important links to the
pharmaceutical industry. And just days after the paper was published,
his department of biomathematics announced it was sharing in £20 million
of emergency corona virus research investment courtesy of the UK
government. Not bad for one paper! But the most revealing thing about
Neil Ferguson and his department is that they had exactly the same
concerns and exactly the same policy advice during the so-called 'swine
flu' crisis back in 2009. Flashback to 11 June that year, and the World
Health Organization was declaring a 'six-level alert'---its grimmest
ever ---for a new pandemic sweeping the world. Despite having in
previous years been found to have been wrong about the dangers of
several other viruses, the WHO once again sounded the deathly warning
that 'this early pandemic and flu is somewhat similar to the 1918--1919
pandemic swine flu that killed millions.' Exactly

what this strain would do in the autumn and winter of 2009 and into 2010
was unclear, the WHO said, but everyone needed to be prepared!
Governments everywhere respected the advice of the World Health
Organization, a UN agency which after all does so much good work
combating disease and guiding research related to public health. And so,
after the swine flu warning, where necessary, they promised, schools and
offices would be closed. Face masks were bought in the millions, and
vaccines were stockpiled. Yet, at the end of the day, swine flu proved
to be a paper tiger. A year on, annoyingly for the governments and their
advisors, almost no people could be found to be said to have died from
'H5N1'. Quite possibly more people, suffering from other complaints,
died from the 'emergency precautions' surrounding the virus, such as
being refused entry to doctors surgeries or from being injected with the
vaccine. Not long after, two independent reports, one by the Council of
Europe and another that appeared as a paper for the British Medical
Journal (3 June 2010), put a belated spotlight on the fact that three of
the crucial experts arguing for expensive programmes of vaccine
preparation by companies like Roche (the makers of Tamiflu) and
GlaxoSmithKline (the makers of Relenza) were also paid consultants for
the companies. Swine Flu In 1976, President Gerald Ford's administration
reacted to the swine flu outbreak, at the time considered the new world
'pandemic', ignoring the World Health Organization's then words of
caution and vowing to vaccinate 'every man, woman and child in the
United States'. After 45 million people were vaccinated, the flu turned
out to be mild. Worse, researchers discovered that some of the
vaccinated--- roughly 450 in all---had developed Guillain-Barré
syndrome, a rare disorder in which the body's immune system attacks the
nerves, leading to paralysis. At least 30 people died.

Or from 2017, when a rushed campaign---this one endorsed by the WHO---to
vaccinate nearly 1 million children for mosquitoborne dengue in the
Philippines was halted for safety reasons. The Philippine government
indicted 14 state officials in connection with the deaths of 10
vaccinated children, saying the programme was launched 'in haste'.

Now I must stress that don't myself believe that individual researchers
are knowingly skewing research reports in order to make money, either
for themselves or their institutions---or their industry sponsors. But
the structural pressures are there and they can create the same effect.
Research is a business and so its conclusions are skewed towards the
'needs' of the paymasters. And how big a business is an epidemic? The
answer is that they can be a very big deal indeed. And so, in the grim
spring of 2020, the news and media coverage, academic research and
computer models and, above all, actual policy announcements all became a
self-reinforcing swirl of facts, opinions and plain misinformation.
After all, the reality is that it is often easier for people, if they do
not have either the ability or the interest to find out for themselves,
to adopt the views of others. What is more, this is without doubt a
useful social instinct. As the economist Pierre Lemieux has put it,
cascade theory reconciles 'herd behaviour' with rational-choice because
it is often perfectly rational for an individual to rely on information
passed on to them by others. Often... but not always! The point is that
with the corona virus, as with earlier 'pandemics', a radical shift in
ideas and beliefs was driven not by carefully assessed and evaluated
data but rather by uncritically embraced observation and reinforcement
of the views of others. This created an information cascade, in which
the actions and decisions of everyone else became more important than
evidence people were directly acquainted with, let alone their own
judgment. In this way, a particular view 'cascades' down the side of an
'informational

pyramid'---like a waterfall. Since deaths drive clicks, much of the
media instead plays the role of 'availability entrepreneurs', as Edward
Chancellor put in an article for 'Reuters Breaking Views', placing
excessive weight on images that are particularly vivid--- such as halls
full of grim-looking beds inside emergency hospitals or workers clad in
full biohazard gear lowering coffins into graves. Amazing, high
magnification images of the virus exploding out of a human cell added a
final ghastly, science fiction aspect to the tale. In the USA, even as
President Donald Trump 'hesitated', state, territorial, tribal and local
governments responded to the threat with various declarations of
emergency, closure of schools and public meeting places, lockdowns and
other restrictions intended to slow the progression of the virus. Again,
what I'm suggesting isn't that the corona virus was not, is not, a nasty
virus that has had tragic consequences for many people. Instead, I'm
asking why, given that every year the broad range of respiratory viruses
kill millions of people and yet we generally accept the toll and carry
on with our lives, was this virus treated very differently? Social
science, not epidemiology, offers the bigger insights. One of which is
'rear-view mirror syndrome'. This occurs when we evaluate a crisis by
trying to find parallels with the past. The parallels chosen in this
case were not, for example, the swine flu fiasco, where terrible
prophecies came to naught (see box above) nor to earlier SARS epidemics
(despite Covid-19 actually being a variant of SARS, but then SARS
fizzled out too), but rather the great flu epidemic of 1918 and even the
Black Death of the Middle Ages. Neither journalists nor politicians, far
less the general public, were minded to recognise that neither epidemic
was at all like the current one. Often, as I say, people assumed the new
'corona virus' was entirely novel. For most of us, of course, the term
was. However, it only required a quick trip to the fount of human
knowledge, Wikipedia, to learn that corona viruses are neither new nor
especially dangerous.

Corona viruses are a group of related viruses that cause diseases in
mammals and birds. In humans, corona viruses cause respiratory tract
infections that can be mild, such as some cases of the common cold
(among other possible causes, predominantly rhinoviruses), and others
that can be lethal, such as SARS, MERS, and COVID-19.

This illustrates something important about the role of words in how we
think. The term 'corona virus' linked in the public mind to 'new' and
'deadly' and not to 'known' and 'manageable'. Advertisers know the
importance of such associations... Wikipedia is notoriously unreliable
(but the same can be said of books and journal articles!) and on its own
terms only really reflects current opinion, but policy makers seemed to
have failed to read even short pieces by specialists. Ones like those by
John Ioannidis, Stanford's Professor of Public Health, in newspapers and
online which tried to explain that 'mild' corona viruses may be
implicated in several thousands of deaths every year worldwide, although
the vast majority of them are not documented. Instead, they are lost as
noise among 60 million deaths from various causes every year. On the
face of it, it's quite an information failure when policy makers, let
alone the general public, don't appreciate the difference between
terrifying science fiction scourges that can wipe out entire species and
everyday viruses that are especially common in the elderly and in
hospitalised patients with respiratory illness in the winter.
Nonetheless, the reader can be forgiven for feeling that this corona
virus was nearer the former than the latter. After all, over the two
winters of 2020 and 2021, it is thought that between one and maybe three
million people have died 'of ', or more precisely 'with', the virus and
of associated conditions. Either number is a sad toll. But equally,
either figure, which represents the virus's peak activity, still leaves
the virus only one player in the grisly 'league table' of annual world
deaths, alongside:

• coronary heart disease (8.8 million deaths); • strokes (6.2 million );
• lower respiratory infections (3.2 million) including influenza and
pneumonia; • lung disease (3.1 million); • respiratory cancers (1.7
million); • diabetes (1.6 million); • Alzheimer's (1.5 million); •
diarrhoea (1.4 million). Actually, the response to the corona virus
might well be contrasted with the approach to this last disease,
diarrhoea. The corona virus, as we have seen, affects mainly elderly
people in relatively wealthy countries, but the latter's victims include
many of the very young in developing economies (its 1.5 million victims
correspond to 60 million years of life lost). And this disease (caused
by an intestinal virus or bacteria transmitted through contaminated
water or food) is both avoidable and treatable. Note, too, that the
effect of the corona virus on the developing world, which by and large
eschewed lockdowns, barely made the news pages in the rich countries
which largely did. Or consider, for example, the well-known, indeed
'everyday', fact that the leading cause of death in most developed
economies, including the United States, is heart disease. Now, sceptics
of Covid policy frequently objected that we don't shut down the country
for this, even though it kills millions of people a year, but the
objection was swiftly opposed with the argument that heart disease is
not infectious, so shutdowns would not make a lot of sense, since
there's no reason to think that they would slow the spread of heart
disease. Yet the response is a little too glib. People increase their
risk of heart disease by things like unhealthy eating, by smoking, by
drinking wine and beer, by not getting enough fresh air and exercise.
All of these factors 'could' be used to justify interventions by
governments (some of course are). Perhaps, another year, we

may see compulsory 'get out of home orders' and certificates needed to
justify staying in and watching TV! The toll from heart disease never
makes daily news, nor do countries actually ban alcohol, popular foods,
eating out, leisure activities, marriages within vulnerable groups. But
with this Covid, unthinkable interventions became entirely
thinkable---indeed unarguable. Why did this happen? What changed our
thinking patterns? John Ioannidis, who I mentioned above as the sort of
expert not getting on to official panels, or indeed the mainstream
media, early on queried the rush to act despite a solid factual evidence
base, and warned that, in broad social terms, 'the cure' might be more
dangerous than the illness. He once offered a colourful analogy for
government policies: 'It's like an elephant being attacked by a house
cat. Frustrated and trying to avoid the cat, the elephant accidentally
jumps off a cliff and dies.' Overconfidence One of the most dangerous
cognitive biases is that of overconfidence. Overconfidence results from
a false sense of your skills and capabilities. And governments are
particularly prone to it. One common manifestation is an illusion of
control in matters over which you actually have no control---things like
the spread of an essentially airborne virus, for example. Illusions of
control prompt people to talk with over-optimism about events and
timings, such as that the 'curve of the epidemic will be flattened in
two weeks' or that a vaccine will be ready by September, or that virus
spores will only travel a fixed distance of two metres.

Delusions of control lead to governments spending billions of dollars on
things like 'contact tracing'---an activity that might (let us be
generous) be effective in populations largely free of the virus when a
handful of cases appear, but is surely quite irrational when the virus
is already freely circulating---as was clearly the case, for example, in
the United Kingdom where the cost of contact tracing over the two years
2020 and 2021 was put at £37 billion---about \$50 billion. Brits love
their faithful 'free at point of need' health service, but it is always
chronically short of money even for essential operations. But NHS Test
and Trace was---despite the name---a largely privatised operation
involving companies like the outsourcing giant Serco, Sitel and Capita.
Meanwhile, as they say in all good stories, when the nurses on the
'Covid frontlines' asked for their annual pay review in 2021, they were
offered a rise actually under the rate of inflation: a pay cut.

In other words, we panic, throw out 'slow thinking' and grab the results
of 'intuition' along with a whole bundle of prejudices instead. But we
call it 'science'! However, corona virus policy cannot be explained just
in terms of panicky over-reaction. Yet, if the evidence did not support
the radical actions, what---or who---did? On the internet, fingers
pointed at a 'hypothetical corona virus-type pandemic' that advocated
essentially the responses that governments now implemented--- first and
foremost mass vaccination programmes. 'Event 201' was an invitation-only
tabletop exercise held 17 October 2019 that simulated a global pandemic
from a new corona virus. The event was hosted by the Johns Hopkins
Center for Health Security in partnership with the Bill and Melinda
Gates Foundation and the World Economic Forum. Now it is true that the
Gates Foundation is a major investor in vaccines, and it is also rather
unusual to find a philanthropic foundation that annually turns a
substantial profit. And it is undeniable that, all over the world, many
of the experts or indeed government ministers advocating pharmaceutical
interventions

had links to, or investments in, the companies they were promoting.
However, I still work on the general principle that people are not
entirely cynical and corrupt. Rather, I think it makes sense to assume
most people are like you and me---balancing self-interest with a sense
of what they honestly believe is the 'right thing'. But that doesn't
mean most people take good decisions. No, the problem is if anything
worse! Politicians and experts too fall victim to thinking errors. Ones
such as delusions of control. Indeed, the kind of people invited to
meetings like 'Event 201', politicians, advisors and billionaires
themselves, fall easy prey to such delusions---ones rooted deep in our
reptile brain to the idea that something will happen because we want it
to. These are the delusions that prompt tiny infants to stamp their feet
and throw toys around! Politicians, conscious of the need for
re-election, also fall easy prey to the cognitive bias known as 'loss
aversion'. Another of Amos Tversky's and Daniel Kahneman's nice little
experiments into 'how we think' concerns this bias. The two carried out
research in which people were asked if they would accept a bet based on
the flip of a coin. If the coin came up tails the person would lose
\$100, but if it came up heads they would win amounts varying from \$100
to \$300. The results of the experiment showed that on average people
needed to gain twice as much before they were willing to contemplate
risking the hundred dollars. Tversky and Kahneman thought that this
showed that possible losses weigh twice as much in people's thoughts as
possible gains. People presumably are wondering 'why' should they risk
their money? Their \$100 is 100% safe as it is, they need a good reason
to enter into the gamble. Surely, it was something like loss aversion
that led governments to prioritise the threat of illness over concerns
about writing off trillions of dollars of business and undermining the
structures of society, just as fears led individuals at the 'personal
scale' to lurk indoors for months on end rather than risk going out and
enjoying themselves. President Trump didn't lose the 2020 US
presidential election because wearing masks actually protects you

from respiratory viruses---he lost because the act of mask-wearing
corresponds to a deeply buried cognitive reflex. As the political
philosophers Thomas Hobbes and Georg Hegel have both noted, fear plays a
fundamental role in human society. Indeed, they both agree it is fear,
not a desire to cooperate, that brings us together. Fear is also at the
heart of the phenomenon called 'herd mentality'. The classic instance of
this is finance, but herds rush about in many areas, from management
innovations to everyday consumer fashions for clothes or foods. When
people opt to follow others on the sole basis that, if so many people
are doing something, well, 'there must be a good reason for it', you
have the potential for disastrous collective decisions. And just as herd
thinking propels everyone to join the rush, the related phenomenon known
as groupthink means everyone in the herd is required to defer to
authority and for individualism to give way to imitation. For
sophisticated, human animals, shared 'social facts' reduce anxiety by
offering a sense of order and control. Indeed, in a crisis, contrarians
are swiftly attacked by 'mind guards', as a quick trip to Twitter will
illustrate. Which brings me finally to one last cognitive bias: the
so-called 'narrative fallacy'. With things like the corona virus, we
have been offered many stories, but one of the most compelling is that
about people going out, passing the virus on and killing other people.
Notice, too, that we are subliminally wired to expect this kind of
three-part story---the beginning, the middle and then the end.
Advertisers know the power of triples, but so do political advisors.
'Stay Alert, Control the Virus, Save Lives.' The bias recognises that
humans are story-telling animals who naturally try to arrange facts and
events in a sequence. Alas, sometimes the story pushes aside logic and
takes on greater authority than it really deserves.

Chapter 9

Thinking like a Search Engine

Images like this one, called 'Idesawa's spiky sphere', demonstrate how,
instead of our minds simply observing the world, we actively 'fill in
the gaps' that perception leaves---or, indeed, sometimes creates. Search
engines, too, are much more than just passive observers.

Thinking like a search engine? What's all that about then? The
explanation brings us back to Google. Google is the world's most-used
search engine. It has achieved that status by creating a vast network of
computers that not only can keep up with all the webpages being created
for the internet, but also respond in fractions of a second to human
enquiries. Yet that's only the easy bit---the technological bit. Google
is also about something much

more subtle, which is weighing and evaluating information. And in some
ways here, its methods are revealing. Google uses an algorithm (yes, one
of those) that privileges websites that lots of other, especially
reputable, websites have links to---because those links represent votes
in terms of quality. It also assumes that if the words in your query are
in the page, in headings in the page and even in the filename for the
page, then that page is likely relevant. Google's method is not actually
particularly clever when taken apart---but it sure is complex when all
put together. Likewise, in living organisms, complexity emerges as the
result of simple chemical reactions following certain rules. It is these
more complex molecules that later become cells, and these cells which in
turn interact to become specialised organs. Organs interact to form
organisms, which interact, communicate and reproduce on ever higher
scales to form eventually the universe. Within Google's search engine
there are virtual molecules, again guided by rules, that together create
a new kind of artificial intelligence, one that increasingly guides our
own thinking and behaviour. To draw the threads together, one key lesson
for more effective thinking is to do simple things, indeed embrace
simplicity---but do so systematically. Steven Levy, the author of In the
Plex: How Google Thinks, Works, and Shapes Our Lives, has tried to tweak
the curtain on Google, to see the cogs whirring behind that deceptively
simple and friendly webpage interface. Levy is a technology journalist,
for many years at Newsweek, whose job involves explaining the
complexities of computers and the internet for a general readership.
Other interpreters of Google have focused on the tech giant's creative
processes or the cunning of its business plan. Ken Auletta, author of
Googled, and Jeff Jarvis, author of What Would Google Do, both argue
that the kind of thinking Google uses is not so much mathematical, or
scientific or anything wonderfully abstract and new, but rather the very
methodical procedural approach of engineering. Auletta writes:

Google's leaders are not cold businessmen; they are cold engineers. They
are scientists, always seeking new answers. They seek a construct, a
formula, an algorithm that both graphs and predicts behaviour. They
naively believe that most mysteries, including the mysteries of human
behaviour, are unlocked with data. Of course, Wall Street's faith in
such mathematical models for derivatives helped cripple the American
economy. Naïveté and passion make a potent mix; combine the two with
power and you have an extraordinary force, one that can effect great
change for good or for ill. Google fervently believes it has a mission.
'Our goal is to change the world...' (Auletta, 2009)

Of course, Google is pretty inventive and original too: it's had to be
or else it would never have been able to come up with its innovations in
the science of searching vast databases of information, nor the
development of its Android mobile operating system that nowadays runs
most smart phones around the world. Every second of the day, Google
performs extraordinary feats, such as searching billions of pages in the
blink of an eye, but (and this is actually much more difficult and
impressive) it makes sense of them too. No wonder that Google likes to
play up the idea that its engineers are all super-intelligent demigods,
guided by Sergey Brin and Larry Page themselves, nowadays with a bit of
help from the company's CEO, Sundar Pichai, a specialist in
metallurgical engineering who doesn't attract the same attention and
maybe doesn't deserve to, given that speciality and the fact that his
interests centre on cricket. A glimpse of what it is like to talk to
'the brains behind Google' is given in an admiring character profile
offered by the Press Trust of India in 2019, when they report the tale
of Mr Sundar keenly awaiting the Cricket World Cup final match,
predicting that the teams in the final would be England and
India---although, Googlestyle, he immediately noted all the other
arithmetic possibilities, saying that: 'But, you know, Australia and New
Zealand, these are all very, very good teams.' In the event, the final
was not between

England and India, but rather saw England narrowly beating New Zealand.
The point I'm making here is that thinking like a search engine is very
different from some of the other approaches I've been looking at---such
as 'design thinking' or even 'cybernetic thinking'. Put short, this
approach is at root unimaginative. In place of inspiration comes method,
in place of analysis---which involves taking part and, well,
analysing---comes the rather dull alternative approach of piecing
together and seeing what emerges. In fact, thinking like a search engine
is really better termed 'emergent thinking'. I say it is better, but
there is a problem with the term which is that none of us know what it
means. So that's why I stick with the inferior term. But by the end of
this chapter you will indeed know all about 'emergent thinking' and
maybe will be converted to the approach. Mel Schwarz, writing for the
magazine Psychology Today, says that emergent thinking is about
'\[m\]astering our thinking', as opposed to being 'enslaved by old
thought', and links it to something in Fritjof Capra's book, The Turning
Point. He doesn't go into any details, but likely it is Capra's warning
that conventional thinking, which he dubs the Cartesian-Newtonian
paradigm, results in a too tightly focused, even 'reductionist',
approach that, Capra argues, results in a world that is impoverished,
polluted and disillusioned. Capra's solution instead is to adopt a
'systems' view of the world. The idea here, which has been developed
really over the past thirty years or so, is that learning requires a new
emphasis on complexity, networks and patterns of organisation. This is
what is meant by 'systemic' thinking: and it is also the kind of
thinking that search engines excel in. Emergent thinking is the opposite
of 'divergent thinking', which, as the name suggests, involves
generating new ideas, maybe even 'an explosion of ideas'. Divergent
thinking, in order to be successful, is free, open and playful. Quantity
is more important than quality because you can always discard ideas
later, but, if you don't let them sprout, you can't see what they grow
into, and you certainly can't

'harvest' them. Creative thinking like this often begins with an
open-ended question like 'Why' or (as with thought experiments) maybe
'What if?' The questions generate new scenarios, new possibilities The
writer, Charles Leon, a Brit who describes himself on his LinkedIn page
as an 'Open Genius Instructor', working with organisations and
individuals to help them achieve 'optimum creative potential', has
characterised divergent thinking as: Metaphor, Daydreaming,
Visualisation, Playfulness, Humor, Non-linear, Imagination,
Generalisation, Hunch, Intuition, Unconscious (gut) reaction.

But what is more interesting for us is how he contrasts the 'divergent',
creative approach with the 'emergent', and yet also equally 'handsoff ',
one. Leon describes the 'emergent thinking' process like this: Somewhere
in the middle of ideation completely new ideas and concepts will begin
to form that is greater than the sum of their parts. By exploring the
possibilities and generating a number of ideas, several new concepts and
solutions begin to form out of the combination of those ideas which
could not be found in individual ideas. Emergent thinking happens when
you move towards completely unforeseen possibilities, that only become
apparent when comparing and combining generated ideas.

It's actually the fun bit, before you need to sort through your
'ideation' and try to make sense of it all. At this point, emergent
thinking has to give way to 'convergent thinking', which is, Leon says:
'Linear, Analytical, Reason, Logic, Precision, Consistency, Critical
thinking, Facts, Rationality, Deliberate, Work (effort), Reality,
Direct, Focussed, Sequential, Number, Conscious.' You see, thinking like
a search engine involves a little bit of fun (think the 'I feel lucky'
button on the Google search engine) but far more utterly boring database
sorting and interrogation, all of which is part of computer science, and
although books like Levy's

attempt to make this seem exciting, I've done courses on it and it
really it isn't. Or, to be precise, it may be interesting when you're
directly involved but is not interesting to be told about. Which is
maybe why Google wisely doesn't tell us very much about how it works:
how it ranks webpages to decide which ones to show us first, or how it
finds out, if not our innermost secrets, certainly details of our
private lives, so that it can sell web advertising space all the more
profitably. Which highlights another thing that comes with 'thinking
like a search engine' which is that it also involves putting aside any
morals. Two of Google's most glaring privacy debacles---the collection
of wi-fi data from open hubs by Google cars taking photos for its Street
View maps and the violation of Google's own privacy policies through its
short-lived 'Buzz' social networking service---were not mere glitches
but rather resulted from something hardwired into the company's DNA.
Behind that friendly search page, with its jolly 'Google Doodle', Google
builds multifaceted profiles of users based on their search history, as
well as their browsing history---particularly on the sites that the
Google behemoth actually owns, like Blogger and YouTube. If you list the
top 100 sites in the world (according to Alexa), you'll find that 23 are
owned by Google. Many thousands of smaller sites run on Blogger and you
might never know that as you visit them you are providing data to
Google. Lots of little bits, which might seem useless and, well,
'harmless' to provide, but Google pieces the information together and
makes use of it. It then uses that data to build an advertising profile,
before serving users with ads that it thinks will 'be relevant' to them.
Actually, in a nod at 'openness', Google offers a link that is for all
those who have agreed to create a 'Google account', on its account
settings page, which allows users to see what Google knows about them,
or, to be more precise, what it 'thinks it knows', such as your age,
gender, marital status, income, and even your personal interests. I
tried this link and the spying didn't seem to have been very effective
in my case, but then, if I were as crafty as I think

Google is, I would not give away how much I really know so easily.
Instead, I might offer a free link on my site that appears to show I
know only a few harmless bits of publicly available information... Even
so, on the other side of the privacy debate, some people report their
experience of Google having had some rather piercing insights: such as
knowing whether they own or rent their apartments, and whether they were
looking for a new partner. So, thinking like a search engine involves
digging out trivial bits of information on everyone and everything.
Indeed, we all know people at dinner parties who seem to think already
like this, and often they are quite successful people in real life too.
Many of the people I know who have quizzed me like this then go on to
make hierarchical judgments about me (or other people) based on these
tiny bits of data, and sure enough, that's what Google does too!
However, unlike random people at dinner parties, Google's views matter.
Everyone is online now, well almost everyone, and it's no longer
acceptable for a business not to have a website. If it doesn't have one,
it won't get a fair shake in the market. That's because so much business
is generated online nowadays. Having a website that ranks well in search
engines like Google has become a deal-breaker. It's no good being on the
fifth page of search results: you need to be in the first handful of
links returned by Google's search engine if you want to be visible and
successful. No matter how gorgeous your website may be, no matter how
carefully you protected it from rogue viruses or unauthorised visitors,
all that html magic won't do you any good if your website doesn't rank
well on the all-powerful search engines like Google. In years gone by,
companies paid for ads in newspapers or maybe used publicists to get
mentions on television and radio stations in order to get their name in
front of the noses of the public. Money essentially bought prominence.
But today, money is not enough. Instead, unlike the newspaper executives
of yesteryear, Google combines 'who pays' with more objective
calculations about relative corporate merit, based on its algorithms. To
the

extent that the algorithms are public, this is in theory quite a good
thing because if you have a clear understanding of the algorithm and how
it works you can use it to your advantage and leverage Google's
algorithms to play to your strengths. It's a dark art as, for your
website to rank well on Google, you need all of your content to fit with
the behemoth's criteria as well. Every little bit of content, no matter
how seemingly insignificant, has to fit within Google's worldview. And
the key thing about that worldview is that it's rooted in machine
learning. When someone enters an enquiry into Google's search engine it
triggers a massive sequence of events. It all happens in a fraction of a
second and many users have no clue as to exactly how the results they
see on their screen are produced and presented to them. As soon as you
enter an enquiry into Google's search engine, it examines and interprets
the intent of that enquiry. The algorithm takes the strings of words
that are entered and runs them against the index to produce the search
results that users see on their screens whenever they search for
something through Google. The first search engines more or less
conducted what are called 'concordance searches' with lists of terms
previously created by those famous 'spiders' that supposedly toured the
worldwide web (sic) during the night. Nowadays, things are much smarter.
We'll all have noticed how our lousy typing is patiently and politely
corrected by the search engine, even with oddities like a misremembered
person or place name. The software engineers refer to this kind of
activity as checking the 'intent' of the user query. These days, the
algorithms are now so smart that even very poorly worded queries bring
back relevant results. Algorithms will also take synonyms into account
when generating search results, which is handy as often we can't find
quite the right word for what we really mean. Another thing that can be
less helpful is that search engines assume whatever you are asking about
is topical, and prefers new information over older material. That kind
of assumption often throws search engines off

the scent when you try to look up something a politician or other
contemporary figure did even a few years ago. So far, all of this is, or
could be, about how a smart programmer might design an algorithm. But
today, much of what goes on is a matter of the computer designing
itself---or 'machine learning'. Machine learning is the key to how
Google is able to provide users with accurate and pertinent results.
Without machine learning, Google wouldn't be able to accommodate either
the complexity or the volume of the queries it faces every second of the
day from innumerable users around the world. Google PageRank is one of
the most important Google algorithms to pay attention to. This algorithm
takes multiple factors into consideration, including the quality of
backlinks on the page from other sites, and the number of links on the
linking page. This is equivalent to judging a book by it having a
reputable publisher, being well reviewed and so on. And Google (like a
stern teacher) also sifts for poor quality information by considering
things like plagiarism and grammatical or spelling mistakes. Lastly,
search engines check how well they are doing by... asking humans!
Algorithms and complex machine learning routines only get you so far. In
fact, Google also asks teams of external employees to rate the quality
of Google searches. The gold standard for judging the quality of search
results ultimately is what we, the humans, think. For now, anyway, thank
goodness for that! And so the point. Search engines don't wear hats, but
we can still learn from them. The algorithms that govern their activity
suggest rules that we too can follow. Whether you're a student
researching a project, an expert developing a new system or a politician
trying to work out the best policy, rule number 1 is: 1. Read as widely
as possible. Search engines read, if not everything, incomprehensibly
vast amounts of things. But they don't get bogged down in the details.
Rule number two is:

2. Skim for what you need, and ignore the rest. The crucial element is
to have a system for identifying what is valuable. Search engines follow
surprisingly simple rules of thumb: if a page is on a much linked-to
website, it is considered to be more 'important'. If the page has the
topic in its heading, then it is considered to be more focused on the
topic than one which hasn't. Here, the smart reader needs to do
something more sophisticated, and what I suggest is consider judging the
source in two ways: is it presenting a new perspective, or rehashing a
rather conventional view? If it's the latter, surely you only need one
version of this, and it should be the most clear and concise one. If
it's the former, then a bit more work has to be done, in both
summarising and evaluating the material. Here, a search algorithm falls
down, as really they do not make sophisticated judgments, only 'follow
the pack'. We can do better than that! To return to the all-important
hat question: to think like a search engine, I think you should wear a
hard hat. It will, if nothing else, remind you that you are taking a
risk when you consider all sources and entertain all possible
theories---the risk of ending up talking nonsense and being completely
wrong. We've all been led by internet searches to pages doing this. But
likely, after you searched and found nothing useful came up, you refined
your search and tried again. And maybe this time, like a surveyor, you
will start taking notes and recording details. Remember, too, that
inspiration which was at the heart of several of the earlier chapters,
after all, also requires a bit of 'perspiration', as surely jobs on
construction sites do. However, don't let's get hung up on the
particular kind of hat. Because, after all, here in this book, the hat
is only a thinking 'tool', a reference with which to associate a more
subtle and varied theory. Nonetheless, I hope each of the hats I have
worn here will provide a genuine and real pointer towards a particular
kind of thinking and a route towards new horizons.

Appendix 1

The 9 Dots Problem The puzzle is attributed to the US psychologist
Norman Maier (1900--1977) who introduced it in an article in the Journal
of Comparative Psychology in 1930. There are multiple solutions, but
they all involve extending the lines 'outside the box', as in the figure
below.

Okay, cunning, but so far so relatively undramatic. But to connect the
dots with one line? How is that done? The answer here requires you to
think in three dimensions. The figure on the next page shows how it
might be done.

Here, coning the paper three-dimensionally and aligning the dots along a
spiral allows a single line to be drawn connecting all nine dots.

Notes and Sources Key Sources and Suggestions for Further Reading
Introduction Yes, yes, yes, I know that there is already a book about
different kinds of thinking hats! It is by Edward de Bono, the very
respected populariser of 'lateral thinking'. His book Six Thinking Hats
takes, however, a very different approach to thinking as he rather
arbitrarily creates six different ways of thinking, things like thinking
emotionally and thinking logically, and then cutely gives each approach
a distinguishing colour. My approach here, however, is to consider the
very different ways people think in reality: my divisions are based on
something real and solid. That said, if you enjoy my book, I'm sure
you'll enjoy---and get many more insights---from de Bono's one too.
Chapter 1: Thinking like a Chinese General The key text for this chapter
is Sun Tzu's The Art of War. Being a very ancient text, and written in
Chinese, there is a lot of subjectivity about how the book should be
translated---and often literal translations are misleading. The most
often used 'canon' translation is the one by Lionel Giles entitled The
Art of War by Sun Tzu: The Oldest Military Treatise in the World,
published in 1910. Some translations, such as that of Thomas Cleary
(Shambhala, 2003), attempt to explain the text in a wider scope of
'Eastern Philosophy'. Others, like that of Victor Mair's The Art of War:
Sun Zi's Military Methods (Colombia, 2009), put the emphasis firmly on

the military aspects which is presented as being truer to the history of
the text and thus a better way to understand the context the book was
compiled in. Ralph D. Sawyer is also a military historian, and his
translation (LITTL, 11 February 1994) is very detailed. There is also
Roger Ames (Ballantine Books, 1993), who translated the slips found at
Yiqueshan in 1972---the oldest version of the text found. All of the
versions have strengths and weaknesses, but I'd go for Lionel Giles
which additionally is online at: http://pinkmonkey.
com/dl/library1/digi085.pdf. The reference to Mao---in the middle of the
twentieth century, claiming the book as an inspiration for his guerrilla
warfare, saying: 'We must not belittle the saying in the book of Sun Wu
Tzu, the great military expert of ancient China, "Know your enemy and
know yourself and you can fight a thousand battles without
disaster"'---is from Samuel B. Griffith (2005) The Illustrated Art of
War (Oxford University Press, pp. 17, 141--43). Thomas E. Ricks
prominently noted Mao's debt to Sun Tzu at the start of his article 'The
Supreme Art of War on the Korean Peninsula: Regime Change Through
Targeting the Mind of Kim Jong Un', for ForeignPolicy.com,
https://foreignpolicy.com/
2017/08/15/the-supreme-art-of-war-on-the-korean-peninsularegime-change-through-targeting-the-mind-of-kim-jong-un/.
The views quoted in the chapter of Yingling are also drawn from this
article. The quotations attributed to Sun Tzu throughout, including 'I
will force the enemy to take our strength for weakness, and our weakness
for strength, and thus will turn his strength into weakness', are from
The Art of War. As to the reference to the Taoist saying that we see the
spokes in the wheel but it is 'the empty centre that lets the wheel
move', this is a nod at the historic text called the Tao Te Ching where
chapter eleven offers: 'Thirty spokes share the hub of a wheel; yet it
is its center that makes it useful.' The reference to the emptiness of a
bowl relates to the Tao Te Ching, chapter four.

As to the NSA and spying on businesses, I wrote about this in my book No
Holiday: 80 Places You Don't Want to Visit (Disinformation Press, 2005)
and later also in an article for a website called OpEd News, 11 March
2013, called 'US Spying on Europe ---The Gloves Come Off '. This is
online at: https://www. opednews.com/articles/
US-spying-on-Europe--the-by-Martin-Cohen-Allies_Boeing\_
Commerce_Contracts-131103-512.html. More details on the
allegations---and the NSA director's admission to the Pike Committee of
the US House of Representatives that: 'NSA systematically intercepts
international communications, both voice and cable'---can be found at
the website of the Federation of American Scientists,
https://fas.org/irp/eprint/ic2000/ic2000. htm. The 'well-informed press
report in the Baltimore Sun', in 1995, is the article 'America's
Fortress of Spies', 3 December 1995 (with a follow-up a week later
called 'Rigging the Game', by Scott Shane and Tom Bowman. The story is
at https://www.google.com/ url?
sa=t&rct=j&q=&esrc=s&source=web&cd=&ved=2ahUKEwj1iI
aQz-zwAhUOmhQKHdZEDQgQFnoECAQQAA&url=https%3
A%2F%2Fwww.baltimoresun.com%2Fnews%2Fbs-xpm-1995-1210-1995344001-story.html.
The ACLU report, 'The Surveillance-Industrial Complex: How the American
Government is Conscripting Businesses and Individuals in the
Construction of a Surveillance Society', is available online as a PDF at
https://www.aclu.org/sites/default/ files/
FilesPDFs/surveillance_report.pdf. Alan Rusbridger, the Guardian editor
not shy to praise his own work, or indeed his own piano playing (!),
talking about how the paper was handed the story, and why he told the UK
government that it should be pleased it was in his hands rather than
left to the uncontrollable forces of the internet, can be found on
YouTube at: https://www.youtube.com/watch?v=MYjdGUBn938. The quote from
James Clear, entitled 'Applying Lessons from Sun Tzu and The Art of War
to Everyday Life' is in the article published

10 October 2016, now online at https://lifehacker.com/
applyinglessons-from-sun-tzu-and-the-art-of-war-to-eve-1787621192. Eric
Barker's article for Time magazine, 2 June 2014, is entitled 'Sun Tzu's
Art of War: How Ancient Strategy Can Lead to Modern Success' and is
online at
https://time.com/2801517/sun-tzus-artof-war-how-ancient-strategy-can-lead-to-modern-success/.
Richard S. Tedlow offers his thoughts in the book Denial: Why Business
Leaders Fail to Look Facts in the Face---and What to Do (Portfolio,
2011). The Zuckerberg-Spiegel battle is recounted in 'The Inside Story
of Snapchat: The World's Hottest App or a \$3 Billion Disappearing Act?'
by J.J. Colao in a Forbes piece published on 6 January 2014, and
available online at https://www.forbes.com/
sites/jjcolao/2014/01/06/the-inside-story-of-snapchat-the-worldshottest-app-or-a-3-billion-disappearing-act/.
Chapter 2: Thinking like a Designer 'Nigel Cross, sometime Professor of
Design Studies at Britain's Open University, calls design thinking "one
of the highest forms of human intelligence".' Cross did this originally
in a speech delivered to the Conference on Artificial Intelligence in
Design (AID98), Lisbon, Portugal, July 1998. Extracts are at:
https://blog. hslu.ch/ product/files/2013/02/Cross_DesignThinking.pdf.
The views of Tim Brown are from his book called Change by Design: How
Design Thinking Transforms Organizations and Inspires, published by
Harper Business in 2009. The Washington Post described IBM's design
experiment with show booths in an article by Andrew King and Jeanne
Liedtka on 4 October 2014 called 'How IBM Overhauled Its Trade Show
Booths, Step By Step'. This is online at https://www.
washingtonpost.com/
business/how-ibm-overhauled-its-trade-show-booths-step-bystep/2014/10/03/f807323e-47f8-11e4-b72e-d60a9229cc10_story.
html.

Carl Hempel's view on creativity is from his book Philosophy of Natural
Science (Prentice-Hall, 1966, p. 15). President Kennedy's doodle is
preserved at the National Archives and Records Administration
contributed by the John F. Kennedy Library. The doodles include the
words: euphoria, nonoffensive, practical, moving ships, 14 ships,
evasive, norm, 8PM, missile, base. See:
https://dp.la/item/314b8df9b92533dc3c9bc75b 7c203f72. I described the
uses of mind maps in my book Philosophy for Dummies (Wiley, 2010). David
Ausebel's quote is originally from his 1968 book Educational Psychology:
A Cognitive View (Holt, Rinehart & Winston). Joe Novak's view that the
technique 'makes concepts, and propositions composed of concepts, the
central elements in the structure of knowledge and construction of
meaning' is originally expressed in Novak, J.D. & Gowin, D.B. Learning
How to Learn (Cambridge University Press, 1996, p. 7). Alex Faickney
Osborn's book called Applied Imagination: Principles and Procedures of
Creative Problem Solving was published by Scribner in 1963. Chapter 3:
Thinking like a Biologist I described the work of Dr Rapaille in my book
Mind Games: 31 Days to Rediscover Your Brain (Wiley-Blackwell, 2010).
Rapaille's interview can be found at Frontline on the PBS website:
https:// www.pbs.org/wgbh/pages/frontline/shows/persuaders/interviews/
rapaille.html. Fortune Magazine's article on Ray Kurzweil was written by
Brian O'Keefe, and was in the 14 May 2007 edition. Kurzweil's book, How
to Create a Mind: The Secret of Human Thought Revealed, was published by
Viking in 2012 and indeed was a bestseller. For more about Google's
study, called 'Building High-level Features Using Large Scale
Unsupervised Learning', see https://
static.googleusercontent.com/media/research.google.com/en//
pubs/archive/38115.pdf.

Will Oremus, on the discovery of cats, can be found in Slate Magazine as
the article 'In Artificial Intelligence Breakthrough, Google Computers
Teach Themselves to Spot Cats on YouTube', online at
https://slate.com/technology/2012/06/google-compu
ters-learn-to-identify-cats-on-youtube-in-artificial-intelligencestudy.html.
The Bible Code, by Michael Drosnin, was published by Penguin Random
House in 1997. I originally described the computer program ELIZA in my
book 101 Philosophy Problems in 1999. The program itself, and more
details about it, is online at https://web.
njit.edu/\~ronkowit/eliza.html. Gary Marcus's book Kluge: The Haphazard
Evolution of the Human Mind was published by Faber in 2008. Doug
Hofstadter's critique was for the American Scientist and is online at
The 3 Quarks Daily, https://3quarksdaily.com/3quarksdaily/2007/03/greg\_
ross\_ inter.html. Douglas R. Hofstadter and Emmanuel Sander's book,
Surfaces and Essences, was published by Basic Books in 2013. The book
Autobiographical Notes by Albert Einstein, the closest Einstein ever
came to writing an autobiography, was published in 1949 and is available
via Google books: https://books.google.fr/
books/about/Autobiographical_Notes.html. Chapter 4: 'Thinking like a
Scientist Ought to Think' Exploring the laboratory of the mind, thought
experiments and intuition pumps draws on my earlier book on thought
experiments called Wittgenstein's Beetle and Other Classic Thought
Experiments (Blackwell, 2005), and this is indeed a good, general
introduction into both the range of thought experiments in science and
philosophy, and to the mechanics of the technique itself. Plato's story
of Gyges the shepherd and his magic ring appears in his classic work The
Republic (2:359a--2:360d), while the 'brain in a vat argument' is a
modern version of René Descartes' argument in another philosophical
classic, the Meditations on First Philosophy (1641), that centres on the
possibility of an evil demon who

systematically deceives us. Daniel Dennett's theory about 'intuition
pumps' is set out in his book Intuition Pumps and Other Tools for
Thinking (Penguin, 2014). Einstein on 'the happiest thought of his life'
can be discovered in The Collected Papers of Albert Einstein, Volume 7:
The Berlin Years: Writings 1918--1921 by Michael Janssen (ed.)
(Princeton University Press, 2002) on p. 136. The comparison Einstein
makes with ghosts is in Volume 4: The Swiss Years: Writings 1912--1914,
on p. xviii. The description of intuition as 'nothing but the outcome of
earlier intellectual experience' can be found in context in Einstein
from B to Z by John Stachell (Center for Einstein Studies, 2002, p. 89).
The full story of Lewis Carroll's influential experiment with cords
attached to falling houses can be found in Lewis Carroll in Numberland:
His Fantastical Mathematical Logical Life by Robin Wilson (Penguin,
2008). Aristotle's faulty law for falling objects appears in a text of
Aristotle's called De Caelo, Book I vi 274a. Galileo's replacement
version is set out in my book Wittgenstein's Beetle, mentioned already;
the original text being The Discourses and Mathematical Demonstrations
Relating to Two New Sciences, published in 1638. Thomas Kuhn's paper, 'A
Function for Thought Experiments', can be found in The Essential
Tension: Selected Studies in Scientific Tradition and Change (University
of Chicago Press, 1964, pp. 240--65). Derek Parfit's imaginings on brain
transplants appear in several places, but the first was his book Reasons
and Persons (Oxford University Press, 1984). John Locke explains that 'a
thinking intelligent being, that has reason and reflection, and can
consider itself as itself, the same thinking thing, in different times
and places' in An Essay Concerning Human Understanding. The debate
appears in chapter XXVII. Chapter 5: Thinking like Mission Control

J.F. Kennedy's speech heralding the Apollo programme is archived online
at https://www.nasa.gov/vision/space/ features/jfk_speech\_ text.html.
Asif Siddiqi's description of the flight of 'Vostok 1' as 'one of the
major milestones in not only the history of space exploration, but also
the history of the human race' is part of a NASA History Division ebook
entitled Beyond Earth: A Chronicle of Deep Space Exploration, NASA
SP-2018-4041. See https://history.nasa.gov/ printFriendly/series95.html.
Facts and figures on NASA are in 'Project Apollo: A Retrospective
Analysis', a booklet produced by NASA History, online at
https://history.nasa.gov/Apollomon/ Apollo.html. The quotation from
Science magazine is included in the ebook Apollo Program Management:
Staff Study for the Subcommittee on NASA, Serial C, July 1969, produced
by the United States Congress Science and Astronautics (January 1969). A
free PDF is online at Google Play:
https://play.google.com/store/books/details?id=
UC8VAAAAIAAJ&rdid=book-UC8VAAAAIAAJ&rdot=1. Warnings to Houbolt about
his plan as 'a scheme that has a 50 percent chance of getting a man to
the moon and a 1 percent of getting him back' are recalled in James R.
Hansen (December 1995): Enchanted Rendezvous: John Houbolt and the
Genesis of the Lunar-Orbit Rendezvous Concept in the Monographs in
Aerospace History Series, #4. Archived from the original on 29 September
2006. Online at https://history.nasa.gov/monograph4.pdf. George Low's
admission of his 'mistaken technical judgement' and the other internal
wranglings at NASA are remembered in chapter eight of Enchanted
Rendezvous: The Lunar-Orbit Rendezvous Concept, which is online at NASA
History, as SP-4308 SPACEFLIGHT REVOLUTION:
https://history.nasa.gov/SP-4308/ ch8.htm. Kranz's 'tough and competent'
address was made to his flight control team on the Monday morning
following the Apollo 1 disaster (30 January 1967) and was published in
Failure Is Not An

Option: Mission Control from Mercury to Apollo 13 and Beyond (2000) by
Gene Kranz, p. 204. Kranz's speech goes on: From this day forward,
Flight Control will be known by two words: 'Tough and Competent'. Tough
means we are forever accountable for what we do or what we fail to do.
We will never again compromise our responsibilities. Every time we walk
into Mission Control we will know what we stand for. Competent means we
will never take anything for granted. We will never be found short in
our knowledge and in our skills. Mission Control will be perfect. When
you leave this meeting today you will go to your office and the first
thing you will do there is to write 'Tough and Competent' on your
blackboards. It will never be erased. Each day when you enter the room
these words will remind you of the price paid by Grissom, White, and
Chaffee. These words are the price of admission to the ranks of Mission
Control.

The phrase 'tough and competent' was echoed by NASA Director Sean
O'Keefe following the Space Shuttle Columbia disaster, adding that
'these words are the price of admission to the ranks of NASA, and we
should adopt it that way.' Homer Ahr, a former IBM computer programmer
on Apollo, says that 85% of the code IBM wrote for Apollo 11 'had
nothing to do with everything going right' in an IBM 'Think Blog'
entitled 'The Apollo 11 Lessons We Live by Today', published 19 July
2019 and online at https://www.ibm.com/blogs/think/2019/
07/ibm-apollo11-the-lessons-learned-50-years-ago-that-we-live-by-today/.
Maxime Faget's, Chief Engineer & Designer of the Apollo command and
lunar modules, view that 'We knew what had to be done. How to do it in
10 years was never addressed before the announcement was made. But quite
simply, we considered the program a number of phases' is recalled in a
NASA History publication, 'Managing the Moon Program: Lessons Learned
from Project Apollo', Proceedings of an Oral History Workshop, 21 July

1989. The minutes are collected online at: https://history.nasa. gov/
monograph14.pdf. Onto the books quoted next, and Daniel Kahneman's
thoughts on intuition are in Thinking Fast and Slow (Farar, Strauss &
Giroux, 2011). Nicolas Taleb's book on 'black swan events' is The Black
Swan: The Impact of the Highly Improbable (Penguin, 2008). Eberhardt
Rechtin's book Systems Architecting of Organizations: Why Eagles Can't
Swim was published by CRC Press in 1999. And finally, Archibald
MacLeish's poetic vision was originally offered via the New York Times
on 25 December 1968, on p. 1. The article was collected into a chapter
called 'Bubble of Blue Air' in the book Riders on the Earth; Essays and
Recollections, which was published in 1978 by Houghton Mifflin. Chapter
6: Thinking like an Artist---or at Least a Doodler Oki Sato talked to
Matthew Ponsford of CNN Style about his approach in a piece entitled
'Nendo: The Japanese Studio Making Design Fun Again', published 5 August
2015. It is online at https://
edition.cnn.com/style/article/oki-sato-nendo-design/ index.html. Katie
Treggiden talked to Sato for 'Design Milk', as part of the Milan 2013
installation on 29 April 2013. A transcript is available online at
https://design-milk.com/milan-2013-interview-withnendos-oki-sato/.
Daniel Scheffler and The South China Morning Post is the source for the
longer discussion about why Sato started Nendo and design as telling
stories. The article 'Japanese Designer Oki Sato Finds Inspiration
Across Different Industries and Projects', categorised under 'Icons and
Influencers', has been on the SCP website since 12 April 2017. The link
is: https://www.scmp.com/ magazines/style/
tech-design/article/1983064/japanese-designer-oki-sato-findsinspiration-across.
Sato's interview with architect Elisabetta Rizzato of the magazine
ItalianBark is entitled, rather literally, 'My Interview with Oki Sato

of Nendo at Milan Design Week 2016', and is online at https://www.
italianbark.com/nendo-milan-interview-oki-sato/. The source for the 50
Manga Chairs image is http://www. nendo.
jp/wp/wp-content/uploads/2016/04/50_manga_chairs\_ sketch. jpg, and
Sato's Interior Design Show talk of 2013 is on YouTube at
https://www.youtube.com/watch?v=\_Q1wJ7RZ6ro. Sato explained to Y-Jean
Mun-Delsalle at Forbes.com (17 October 2019) in an article entitled 'The
Creations of Japanese Designer Oki Sato are Filled with Simplicity,
Lightness and Humor'. The article is online at
https://www.forbes.com/sites/yjeanmundel salle/2019/10/17/
the-creations-of-japanese-designer-oki-sato-are-filled-withsimplicity-lightness-and-humor/.
Paula Crown's 'artist statement' is on her website since 25 September
2020: https://www.atelierpaulacrown.com/about/ artist-statement/. The
discussion 'We constantly make unconscious gestures' is at Art She Says,
6 August 2020, by Paula Crown: 'Wielding Art in the Fight for Justice'.
The link is: https://artshesays.
com/paula-crown-wielding-art-in-the-fight-for-justice/. The crushed cups
are described in a feature 'Solo Together', 1 November 2020--1 February
2021, online at https://paulacrown.
viewingrooms.com/viewing-room/8-solo-together. 'Thinking Like an Artist:
Translating Ideas into Form', by Paula Crown, is at
https://stories.moma.org/thinking-like-an-artist-translatingideas-into-form-1ebed5bbd45,
published 10 October 2017. Nigel Cross's book Design Thinking:
Understanding How Designers Think and Work was published by Berg
Publishers (1 April 2011). The book Change by Design: How Design
Thinking Transforms Organizations and Inspires was published by Harper
Business (2009). More on Google doodling issues on 25 June 2013 at:
'Google Doodles: Social Form to Celebrate Architecture', https://www.
archilovers.com/stories/3261/google-doodles-social-form-tocelebrate-architecture.html.

Chapter 7: Thinking like a Cyberneticist---or is it a WeatherForecaster?
James Gleick's book on why everything is not only very complicated but
inherently disorganized---hence 'chaotic'---came out in 1987, and was
published by Penguin. Described as a 'debut' book, which I think is
pushing it for such a well-known writer, Chaos: Making a New Science
introduced the principles of the then definitely very new 'chaos theory'
to the public. The book was a finalist for the National Book Award and
the Pulitzer Prize in 1987, and was shortlisted for the Science Book
Prize in 1989. Gleick gives a good overview of how the MIT meteorologist
Edward Lorenz tried to model the weather in the 1960s and stumbled on
'chaos theory' instead. You can read more of William Schaffer and Mark
Kot's investigations in the scholarly tome Self-Organization in Complex
Ecosystems, by Ricard Solé and Jordi Bascompte, published by Princeton
University Press 2006. The book is partly readable via Google Books too.
For more on the views of Stuart Kauffman and also J. Doyne Farmer, see
Kauffman's book, mentioned in the chapter, At Home in the Universe: The
Search for Laws of SelfOrganisation and Complexity (Oxford University
Press, 1995). David Berreby mused 'Suppose you come down from Mars and
you see taxicabs working the streets of New York' in a piece for
Strategy + Business, 1 April 1996, entitled 'Between Chaos and Order:
What Complexity Theory Can Teach Business'. The article can be read
online at https://www.strategy-business.com/ article/15099?gko=d48d4.
Chris Meyer's suggestion that 'Each yellow car has a simple brain
following a few simple rules: Stop for anything that waves. Go where it
says' is part of this discussion. The American physicist Joseph Ford's
view that 'Evolution is chaos with feedback' is recalled by Mikhail V.
Volkenstein in a scholarly article 'Chaos and Order in Evolution' for
the journal Physical Approaches to Biological Evolution, pp. 295--334.
Giorgio Parisi's very detailed thoughts on that puzzling phenomenon

of murmurations are part of a multi-authored article 'ScaleFree
Correlations in Starling Focks' for the June 2010 edition of Proceedings
of the National Academy of Sciences (107 (26): 11865-- 70). More on
those strange numbers that crop up repeatedly in nature in this paper:
'A Brief History of the Most Remarkable Numbers', by Lokenath Debnath
and published 1 April 2015, for the International Journal of Applied and
Computational Mathematics, (volume 1, pp. 607--638). Be warned, though,
for non-mathematicians (like me) the article will make your head spin!
Ian Stewart, however, really does manage to combine maths with writing,
and more of his explorations are in books like Does God Play Dice? The
Mathematics of Chaos (Blackwell, 1990). Mandelbrot on order within
'chaos' or, more precisely, 'the most disorderly realms of data' is
detailed in James Gleick's book cited above already, as well as in an
intriguing collection called The Islands of Benoît Mandelbrot: Fractals,
Chaos, and the Materiality of Thinking, published by the Bard Graduate
Center (2012). Klaus Mainzer's book, Thinking in Complexity:
Computational Dynamics of Matter, Mind, and Mankind, was originally
published in 1994 by Springer, and became an unexpected bestseller.
Fritjof Capra and Pier Luigi Luisi's essay entitled 'The Newtonian
World-Machine' is chapter one (pp. 19--34) of the collection The Systems
View of Life, published by Cambridge University Press in 2014. More
terrible tales of Sir Isaac Newton in Newton the Alchemist: Science,
Enigma, and the Quest for Nature's 'Secret Fire' by William R. Newman
(Princeton University Press, 2018), as well as in the scholarly article
'Newton and the Money Men' by Robert Iliffe, published in Nature (volume
462, pp. 39--40, 2009). Gregory Bateson expands on his views on
cybernetics in a webpage headed 'From Versailles to Cybernetics' which
is online at http://www.krabarchive.com/ralphmag/batesonP.html. The
dismissive verdict on cybernetics to the effect that 'It was vacuous in
the extreme and positively inane. Genetics did not, and at that

time could not enter it at all' of Max Delbrück is recalled in Steve J.
Heims book The Cybernetics Group (MIT Press, 1991, p. 95). John Horgan's
book is called in full: The End of Science: Facing the Limits of
Knowledge in the Twilight of the Scientific Age (Basic Books, 1996). And
A Tour of the Calculus, by the mathematician David Berlinski, was
published by Random House in 1995. The 'Free Library' continues the
story of the development of cybernetics, featuring Ludwig von
Bertalanffy, in Systems Research and Behavioral Science (2010) by
Manfred Drack and Gregor Schwarz, online at
https://www.thefreelibrary.com/Recent+developments
+in+general+system+theory-a0243451997. And finally, Jules Henri
Poincaré's writings, Science and Method, originally published in 1908,
is now in an accessible Dover Edition. Chapter 8: Thinking like a Social
Scientist (and not a Gambler) The key text for this chapter is Thinking
Fast and Slow (Macmillan, 2011) by Daniel Kahneman, which incorporates
the ideas from a celebrated article 'Judgment Under Uncertainty:
Heuristics and Biases', written with his long-time collaborator, Amos
Tversky, and published in the journal Science, 27 September 1974. The
book was reviewed by Jim Holt in the New York Times under the title 'Two
Brains Running' on 25 November 2011. The review is online at:
https://www.nytimes.com/2011/11/27/books/review/
thinkingfast-and-slow-by-daniel-kahneman-book-review.html. The 'recent
(2010) study of decision making' was the study by Huber, Kirchler, and
Stockl in 2010 that examined how the hot hand and the gambler's fallacy
are exhibited in the financial market: 'The Hot Hand Belief and the
Gambler's Fallacy in Investment Decisions Under Risk', Theory and
Decision (68 (4): 445--462). Rupert Sheldrake's book The Science
Delusion was published by Coronet in 2012. In my review of the book for
the The Times Higher Educational Supplement (8 March 2012), I wrote that
there is 'a lot to be said for debunking orthodox science's pretensions
to be on the verge of fitting the last grain of information into its
towering

edifice of universal knowledge', but also suggested that Sheldrake 'goes
a bit too far here and there, as in promoting his morphic resonance
theory'! Marcia Angell was in conversation with Christine Mitchell, of
the Center for Bioethics at Harvard Medical School, and the whole
exchange is online at https://www.youtube.com/watch?v= q8FeLQVI5Lw.
Mattias Demet, can be found explaining his idea fully at https://
www.youtube.com/watch?v=CRo-ieBEw-8. Thomas Kuhn's comments about
science as a 'priesthood' are in his 1962 book The Structure of
Scientific Revolutions, published by the University of Chicago Press.
The back story of the WHO website offering a de facto definition of what
counted as a pandemic by including a reference to 'enormous numbers
deaths and illnesses' is discussed in a PDF by the Word Health
Organization online at:
https://www.who.int/bulletin/volumes/89/7/11-086173.pdf. The article on
29 August 2020 by the New York Times headlined 'Your Corona Virus Test
is Positive. Maybe it Shouldn't Be' is online at
https://www.nytimes.com/2020/08/29/health/ coronavirustesting.html. That
much argued-about statistic showing that in Italy 96% of people dying
from the corona virus had serious comorbidities came originally in a
story headlined just that 'Italy Says 96% of Virus Fatalities Suffered
From Other Illnesses', written by Tommaso Ebhardt and Marco Bertacche on
26 May 2020 for Bloomberg. It is online at https://www.
bloomberg.com/news/
articles/2020-05-26/italy-says-96-of-virus-fatalities-sufferedfrom-other-illnesses.
That extraordinary fact that 'The authorities even ordered that
hospitals be emptied of elderly patients, including some who actually
had the corona virus' is noted by Amnesty International in an online
article headed 'UK: Older People in Care Homes Abandoned to Die Amid
Government Failures During COVID-19 Pandemic', published 4 October 2020
and online at: https:// www.
amnesty.org/en/latest/news/2020/10/uk-older-people-in-care-

homes-abandoned-to-die-amid-government-failures-duringcovid-19-pandemic/.
The 'two hairdressers' study was by M.J. Hendrix, C. Walde, K. Findley
and R. Trotman and entitled 'Absence of Apparent Transmission of
SARS-CoV-2 from Two Stylists After Exposure at a Hair Salon with a
Universal Face Covering Policy'--- Springfield, Missouri, May 2020: MMWR
Morb Mortal Wkly Rep (69: 930-- 932). DOI:
http://dx.doi.org/10.15585/mmwr.mm6928e2. The 'Danish study' that
recommended masks against respiratory viruses after all is discussed in
this article: 'Effectiveness of Adding a Mask Recommendation to Other
Public Health Measures to Prevent SARS-CoV-2 Infection in Danish Mask
Wearers: A Randomized Controlled Trial', by Henning Bundgaard and Johan
Skov Bundgaard. The paper is at https:// doi.org/10.7326/ M20-6817. The
full interview with Sunetra Gupta, Professor of Theoretical Epidemiology
at Oxford University, in which she suggested 'we should hang our heads
in shame' was put online by the American Institute for Economic Research
on 5 April 2021 and is at https://www.facebook.com/
watch/?v=204931357684090. The National Institute for Health and Care
Excellence estimate of the value of a year of British life is in a Local
Government briefing paper: 'How NICE Measures Value for Money in
Relation to Public Health Interventions', published 1 September 2013 at
nice.org.uk/ guidance/lgb10 and online at: https://www.
nice.org.uk/Media/ Default/guidance/LGB10-Briefing-20150126.pdf. For the
back story on scary predictions about all kinds of viruses, and not just
the corona one, see my book Paradigm Shift which I published with
Imprint Academic in 2015. The 'elephant' quote of Professor John
Ioannidis came in an opinion piece for STAT Magazine entitled 'A Fiasco
in the Making? As the Coronavirus Pandemic Takes Hold, We Are Making
Decisions Without Reliable Data', published March 17 2020 and online at
https://www.statnews.com/2020/03/17/a-fiasco-inthe-making-as-the-coronavirus-pandemic-takes-hold-we-aremaking-decisions-without-reliable-data/.

That extraordinary figure for contact tracing of £37 billion can be
found in an official UK Parliamentary Committee report entitled
'"Unimaginable" Cost of Test & Trace Failed to Deliver Central Promise
of Averting Another Lockdown', published 10 March 2021 and online at:
https://committees.parliament.uk/ committee/127/
public-accounts-committee/news/150988/
unimaginable-cost-oftest-trace-failed-to-deliver-central-promise-of-averting-anotherlockdown/.
Lots more about 'Event 201', if you are curious, in a web article
entitled 'The Event 201 Scenario: A Pandemic Tabletop Exercise', online
at https://www.centerforhealthsecurity.org/event201/ scenario.html. The
account starts: Event 201 simulates an outbreak of a novel zoonotic
coronavirus transmitted from bats to pigs to people that eventually
becomes efficiently transmissible from person to person, leading to a
severe pandemic. The pathogen and the disease it causes are modelled
largely on SARS, but it is more transmissible in the community setting
by people with mild symptoms. The disease starts in pig farms in Brazil,
quietly and slowly at first, but then it starts to spread more rapidly
in health care settings. When it starts to spread efficiently from
person to person in the lowincome, densely packed neighbourhoods of some
of the megacities in South America, the epidemic explodes. It is first
exported by air travel to Portugal, the United States, and China and
then to many other countries. Although at first some countries are able
to control it, it continues to spread and be reintroduced, and
eventually no country can maintain control.

Adding 'significantly', as we might say, that: 'There is no possibility
of a vaccine being available in the first year.' Chapter 9: Thinking
like a Search Engine

There's a lot to read about Google, but much less to read about
'emergent thinking', which is maybe more useful. But, on the first, and
less useful, the sources quoted are Steven Levy, the author of In the
Plex: How Google Thinks, Works, and Shapes Our Lives (Simon & Schuster,
2011). Googled by Ken Auletta, or rather Googled: The End of the World
as We Know It, to give it its full title, was published by
Penguin-Random House in 2009. Charles Leon's LinkedIn page is at
https://www.linkedin.com/in/chleon/. The discussion of emergent thinking
also draws on a fascinating article by (presumably) David Butler
online---at an online search company of all things---but that sort of
fits with the subject matter. Anyway, the article is at:
https://alchemyleads.com/
machinelearning-and-how-googles-algorithm-works/. Mel Schwarz, was
writing for Psychology Today on 7 April 2011, and the article is
entitled 'What is Emergent Thinking?' The article is online at:
https://www.psychologytoday.com/intl/blog/shiftmind/201104/what-is-emergent-thinking.

Also Available


